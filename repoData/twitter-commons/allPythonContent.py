__FILENAME__ = pantsbuild_migration
# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import (nested_scopes, generators, division, absolute_import, with_statement,
                        print_function, unicode_literals)

import os
import re
import sys


PANTS_ROOT = os.path.dirname(os.path.realpath(__file__))
SRC_ROOT = os.path.join(PANTS_ROOT, 'src', 'python')
TESTS_ROOT = os.path.join(PANTS_ROOT, 'tests', 'python')


KNOWN_STD_LIBS = set(["abc", "anydbm", "argparse", "array", "asynchat", "asyncore", "atexit", "base64",
                      "BaseHTTPServer", "bisect", "bz2", "calendar", "cgitb", "cmd", "codecs",
                      "collections", "commands", "compileall", "ConfigParser", "contextlib", "Cookie",
                      "copy", "cPickle", "cProfile", "cStringIO", "csv", "datetime", "dbhash", "dbm",
                      "decimal", "difflib", "dircache", "dis", "doctest", "dumbdbm", "EasyDialogs",
                      "errno", "exceptions", "filecmp", "fileinput", "fnmatch", "fractions",
                      "functools", "gc", "gdbm", "getopt", "getpass", "gettext", "glob", "grp", "gzip",
                      "hashlib", "heapq", "hmac", "imaplib", "imp", "inspect", "itertools", "json",
                      "linecache", "locale", "logging", "mailbox", "math", "mhlib", "mmap",
                      "multiprocessing", "operator", "optparse", "os", "pdb", "pickle", "pipes",
                      "pkgutil", "platform", "plistlib", "pprint", "profile", "pstats", "pwd", "pyclbr",
                      "pydoc", "Queue", "random", "re", "readline", "resource", "rlcompleter",
                      "robotparser", "sched", "select", "shelve", "shlex", "shutil", "signal",
                      "SimpleXMLRPCServer", "site", "sitecustomize", "smtpd", "smtplib", "socket",
                      "SocketServer", "sqlite3", "string", "StringIO", "struct", "subprocess", "sys",
                      "sysconfig", "tabnanny", "tarfile", "tempfile", "textwrap", "threading", "time",
                      "timeit", "trace", "traceback", "unittest", "urllib", "urllib2", "urlparse",
                      "usercustomize", "uuid", "warnings", "weakref", "webbrowser", "whichdb", "xml",
                      "xmlrpclib", "zipfile", "zipimport", "zlib", 'builtins', '__builtin__'])

OLD_PANTS_PACKAGE = 'twitter.pants'
NEW_PANTS_PACKAGE = 'pants'

IMPORT_RE = re.compile(r'import\s+(.*)')
FROM_IMPORT_RE = re.compile(r'from\s+(.*)\s+import\s+(.*)')

AUTHOR_RE = re.compile(r'__author__\s*=\s*.+')

def has_continuation(line):
  return line.endswith('\\')

HEADER_COMMENT = [
  '# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).',
  '# Licensed under the Apache License, Version 2.0 (see LICENSE).'
]

FUTURE_IMPORTS = [
  'from __future__ import (nested_scopes, generators, division, absolute_import, with_statement,',
  '                        print_function, unicode_literals)'
]

class Import(object):
  def __init__(self, symbol):
    self._symbol = symbol.strip()
    if self._symbol.startswith(OLD_PANTS_PACKAGE):
      self._symbol = self._symbol[8:]

  def package(self):
    return self._symbol

  def sort_key(self):
    return 'AAA' + self._symbol

  def __str__(self):
    return 'import %s' % self._symbol


class FromImport(object):
  def __init__(self, frm, symbols):
    self._from = frm.strip()
    if self._from.startswith(OLD_PANTS_PACKAGE):
      self._from = NEW_PANTS_PACKAGE + self._from[len(OLD_PANTS_PACKAGE):]
    self._symbols = filter(None, [filter(lambda c: c not in '()', s.strip()).strip() for s in symbols])

  def package(self):
    return self._from

  def sort_key(self):
    return 'ZZZ' + self._from

  def __str__(self):
    return 'from %s import %s' % (self._from, ', '.join(sorted(self._symbols)))


class BuildFile(object):
  def __init__(self, path):
    self._path = path
    self._body = []

  def process(self):
    self.load()
    self.parse_header()
    self.save()

  def load(self):
    with open(self._path, 'r') as infile:
      self._old_lines = [line.rstrip() for line in infile.read().splitlines()]

  def parse_header(self):
    # Find first non-header-comment line.
    try:
      p = next(i for i, line in enumerate(self._old_lines) if line and not line.startswith('#'))
    except StopIteration:
      return  # File is empty (possibly except for a comment).
    def _translate(line):
      return line.replace('twitter/pants', 'pants').replace('twitter.pants', 'pants').replace(
        'src/python/twitter/common/', 'src/python/pants/BUILD.commons:twitter.common.'
      )
    self._body = map(_translate, self._old_lines[p:])
    # Remove any trailing empty lines.
    while not self._body[-1]:
      self._body = self._body[0:-1]

  def save(self):
    with open(self._path, 'w') as outfile:
      if self._body:
        for line in HEADER_COMMENT:
          outfile.write(line)
          outfile.write('\n')
        outfile.write('\n')
        for line in self._body:
          outfile.write(line)
          outfile.write('\n')


class PantsSourceFile(object):
  def __init__(self, path):
    self._path = path
    absdir = os.path.dirname(os.path.abspath(path))
    if absdir.startswith(SRC_ROOT):
      root = SRC_ROOT
    elif absdir.startswith(TESTS_ROOT):
      root = TESTS_ROOT
    else:
      raise Exception('File not in src or tests roots: %s' % path)
    self._package = os.path.relpath(absdir, root).replace(os.path.sep, '.')
    self._old_lines = []
    self._stdlib_imports = []
    self._thirdparty_imports = []
    self._pants_imports = []
    self._body = []

  def process(self):
    self.load()
    self.parse_header()
    self.save()

  def is_empty(self):
    return not (self._stdlib_imports or self._thirdparty_imports or self._pants_imports or self._body)

  def load(self):
    with open(self._path, 'r') as infile:
      self._old_lines = [line.rstrip() for line in infile.read().splitlines()]

  def parse_header(self):
    # Strip __author__.
    lines = filter(lambda x: not AUTHOR_RE.match(x), self._old_lines)

    # Find first non-header-comment line.
    try:
      p = next(i for i, line in enumerate(lines) if line and not line.startswith('#'))
    except StopIteration:
      return  # File is empty (possibly except for a comment).

    content_lines = lines[p:]

    def add_import(imp):
      s = imp.package()
      if s.split('.', 1)[0] in KNOWN_STD_LIBS:
        self._stdlib_imports.append(imp)
      elif s.startswith(NEW_PANTS_PACKAGE):
        self._pants_imports.append(imp)
      else:
        self._thirdparty_imports.append(imp)

    def is_import(line):
      m = IMPORT_RE.match(line)
      if m:
        add_import(Import(m.group(1)))
        return True
      else:
        return False

    def is_from_import(line):
      def absify(imp):
        if imp == '.':
          return self._package
        elif imp.startswith('.'):
          return '%s.' % self._package + imp[1:]
        else:
          return imp
      m = FROM_IMPORT_RE.match(line)
      if m:
        if not m.group(1) == '__future__':
          add_import(FromImport(absify(m.group(1)), m.group(2).split(',')))
        return True
      else:
        return False

    # Parse imports.
    lines_iter = iter(content_lines)
    line = ''
    line_parts = []
    try:
      while not line or is_import(line) or is_from_import(line):
        line_parts = [lines_iter.next()]
        while has_continuation(line_parts[-1]):
          line_parts.append(lines_iter.next())
        line = ' '.join([x[:-1].strip() for x in line_parts[:-1]] + [line_parts[-1].strip()])
        if line.startswith('from ') and '(' in line:
          line_parts = [line]
          next_line = ''
          while not ')' in next_line:
            next_line = lines_iter.next().strip()
            line_parts.append(next_line)
          line = ' '.join(line_parts)
    except StopIteration:
      line_parts = []

    def _translate(line):
      return line.replace('twitter/pants', 'pants').replace('twitter.pants', 'pants')
    self._body = map(_translate, [''] + line_parts + list(lines_iter))

    # Remove any trailing empty lines.
    while self._body and not self._body[-1]:
      self._body = self._body[0:-1]

  def save(self):
    sorted_stdlib_imports = map(str, sorted(self._stdlib_imports, key=lambda x: x.sort_key()))
    sorted_thirdparty_imports = map(str, sorted(self._thirdparty_imports, key=lambda x: x.sort_key()))
    sorted_pants_imports = map(str, sorted(self._pants_imports, key=lambda x: x.sort_key()))
    with open(self._path, 'w') as outfile:
      if not self.is_empty():
        for lines in [HEADER_COMMENT, FUTURE_IMPORTS, sorted_stdlib_imports,
                      sorted_thirdparty_imports, sorted_pants_imports]:
          for line in lines:
            outfile.write(line)
            outfile.write('\n')
          if lines:
            outfile.write('\n')
        for line in self._body:
          outfile.write(line)
          outfile.write('\n')


def handle_path(path):
  if os.path.isfile(path):
    if path.endswith('.py') and not path.endswith('pantsbuild_migration.py'):
      print('PROCESSING: %s' % path)
      srcfile = PantsSourceFile(path)
      srcfile.process()
    elif os.path.basename(path).startswith('BUILD'):
      print('PROCESSING: %s' % path)
      srcfile = BuildFile(path)
      srcfile.process()
    elif path.endswith('.rst') or path.endswith('.sh') or path.endswith('pants.bootstrap'):
      print('PROCESSING: %s' % path)
      with open(path, 'r') as infile:
        content = infile.read()
      new_content = content.replace('twitter.pants', 'pants').replace('twitter/pants', 'pants')
      with open(path, 'w') as outfile:
        outfile.write(new_content)
  elif os.path.isdir(path):
    for p in os.listdir(path):
      handle_path(os.path.join(path, p))

if __name__ == '__main__':
  path = sys.argv[1]
  handle_path(path)

########NEW FILE########
__FILENAME__ = checker
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import re

from twitter.common import app

from .common import Nit, PythonFile
from .iterators import git_iterator, path_iterator
from .plugins import list_plugins


DEFAULT_BRANCH = 'master'


app.add_option(
  '-p',
  action='append',
  type='str',
  default=[],
  dest='plugins',
  help='Explicitly list plugins to enable.')


app.add_option(
  '-n',
  action='append',
  type='str',
  default=[],
  dest='skip_plugins',
  help='Explicitly list plugins to disable.')


app.add_option(
  '-l', '--list',
  action='store_true',
  default=False,
  dest='list_plugins',
  help='List available plugins and exit.')


app.add_option(
  '--diff',
  type='str',
  default=None,
  dest='diff',
  help='If specified, only checkstyle against the diff of the supplied branch, e.g. --diff=master.'
    ' Defaults to master if no paths are specified.')


app.add_option(
  '-s', '--severity',
  default='COMMENT',
  type='choice',
  choices=('COMMENT', 'WARNING', 'ERROR'),
  dest='severity',
  help='Only messages at this severity or higher are logged.  Options: COMMENT, WARNING, ERROR.')


app.add_option(
  '--strict',
  default=False,
  action='store_true',
  dest='strict',
  help='If enabled, have non-zero exit status for any nit at WARNING or higher.')


_NOQA_LINE_SEARCH = re.compile(r'# noqa\b').search
_NOQA_FILE_SEARCH = re.compile(r'# (flake8|checkstyle): noqa$').search


def noqa_line_filter(python_file, line_number):
  return _NOQA_LINE_SEARCH(python_file.lines[line_number]) is not None


def noqa_file_filter(python_file):
  return any(_NOQA_FILE_SEARCH(line) is not None for line in python_file.lines)


def apply_filter(python_file, checker, line_filter):
  if noqa_file_filter(python_file):
    return

  plugin = checker(python_file)

  for nit in plugin:
    if nit._line_number is None:
      yield nit
      continue

    nit_slice = python_file.line_range(nit._line_number)

    for line_number in range(nit_slice.start, nit_slice.stop):
      if noqa_line_filter(python_file, line_number):
        break
      if line_filter and line_filter(python_file, line_number):
        break
    else:
      yield nit


def proxy_main():
  def main(args, options):
    plugins = list_plugins()

    if options.list_plugins:
      for plugin in plugins:
        print('\n%s' % plugin.__name__)
        if plugin.__doc__:
          for line in plugin.__doc__.splitlines():
            print('    %s' % line)
        else:
          print('    No information')
      return

    if options.plugins:
      plugins_map = dict((plugin.__name__, plugin) for plugin in plugins)
      plugins = list(filter(None, map(plugins_map.get, options.plugins)))

    if options.skip_plugins:
      plugins_map = dict((plugin.__name__, plugin) for plugin in plugins)
      for plugin in options.skip_plugins:
        plugins_map.pop(plugin, None)
      plugins = list(plugins_map.values())

    if not args and options.diff is None:
      options.diff = DEFAULT_BRANCH

    if options.diff:
      iterator = git_iterator(args, options)
    else:
      iterator = path_iterator(args, options)

    severity = Nit.COMMENT
    for number, name in Nit.SEVERITY.items():
      if name == options.severity:
        severity = number

    should_fail = False
    for filename, line_filter in iterator:
      try:
        python_file = PythonFile.parse(filename)
      except SyntaxError as e:
        print('%s:SyntaxError: %s' % (filename, e))
        continue
      for checker in plugins:
        for nit in apply_filter(python_file, checker, line_filter):
          if nit.severity >= severity:
            print(nit)
            print()
          should_fail |= nit.severity >= Nit.ERROR or (
              nit.severity >= Nit.WARNING and options.strict)

    return int(should_fail)

  app.main()

########NEW FILE########
__FILENAME__ = common
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from abc import abstractmethod
import ast
from collections import Sequence
import itertools
import textwrap
import tokenize

from twitter.common.lang import Compatibility, Interface


__all__ = (
  'CheckstylePlugin',
  'PythonFile',
)


class OffByOneList(Sequence):
  def __init__(self, iterator):
    self._list = list(iterator)

  def __getslice(self, sl):
    if sl.start == 0 or sl.stop == 0:
      raise IndexError
    new_slice = slice(sl.start - 1 if sl.start > 0 else sl.start,
                      sl.stop - 1 if sl.stop > 0 else sl.stop)
    return self._list[new_slice]

  def __getitem(self, item):
    if item == 0:
      raise IndexError
    if item < 0:
      return self._list[item]
    return self._list[item - 1]

  def __getitem__(self, element_id):
    if isinstance(element_id, Compatibility.integer):
      return self.__getitem(element_id)
    elif isinstance(element_id, slice):
      return self.__getslice(element_id)
    raise TypeError('__getitem__ only supports integers and slices')

  def index(self, value):
    return self._list.index(value) + 1

  def __iter__(self):
    return iter(self._list)

  def __reversed__(self):
    return reversed(self._list)

  def __len__(self):
    return len(self._list)


class PythonFile(object):
  """Checkstyle wrapper for Python source files."""

  SKIP_TOKENS = frozenset((tokenize.COMMENT, tokenize.NL, tokenize.DEDENT))

  @classmethod
  def iter_tokens(cls, blob):
    return tokenize.generate_tokens(Compatibility.StringIO(blob).readline)

  @classmethod
  def iter_logical_lines(cls, blob):
    """Returns an iterator of (start_line, stop_line, indent) for logical lines given the source
       blob.
    """
    indent_stack = []
    contents = []
    line_number_start = None

    def translate_logical_line(start, end, contents, endmarker=False):
      while contents[0] == '\n':
        start += 1
        contents.pop(0)
      while contents[-1] == '\n':
        end -= 1
        contents.pop()
      indent = len(indent_stack[-1]) if indent_stack else 0
      if endmarker:
        indent = len(contents[0])
      return (start, end + 1, indent)

    for token in cls.iter_tokens(blob):
      token_type, token_text, token_start = token[0:3]
      if token_type == tokenize.INDENT:
        indent_stack.append(token_text)
      if token_type == tokenize.DEDENT:
        indent_stack.pop()
      if token_type in cls.SKIP_TOKENS:
        continue
      contents.append(token_text)
      if line_number_start is None:
        line_number_start = token_start[0]
      elif token_type in (tokenize.NEWLINE, tokenize.ENDMARKER):
        yield translate_logical_line(
            line_number_start,
            token_start[0] + (1 if token_type is tokenize.NEWLINE else -1),
            list(filter(None, contents)),
            endmarker=token_type == tokenize.ENDMARKER)
        contents = []
        line_number_start = None

  @classmethod
  def parse(cls, filename):
    with open(filename) as fp:
      blob = fp.read()
    return cls(blob, filename)

  @classmethod
  def from_statement(cls, statement):
    """A helper to construct a PythonFile from a triple-quoted string, for testing."""
    return cls('\n'.join(textwrap.dedent(statement).splitlines()[1:]))

  def __init__(self, blob, filename='<expr>'):
    self._blob = blob
    self._tree = ast.parse(blob, filename)
    self._lines = OffByOneList(blob.splitlines())
    self._filename = filename
    self._logical_lines = dict((start, (start, stop, indent))
        for start, stop, indent in self.iter_logical_lines(blob))

  @property
  def filename(self):
    """The filename of this Python file."""
    return self._filename

  @property
  def tokens(self):
    """An iterator over tokens for this Python file from the tokenize module."""
    return self.iter_tokens(self._blob)

  @property
  def logical_lines(self):
    return self._logical_lines

  @property
  def lines(self):
    return self._lines

  def __iter__(self):
    return iter(self._lines)

  def line_range(self, line_number):
    if line_number <= 0 or line_number > len(self._lines):
      raise IndexError('NOTE: Python file line numbers are offset by 1.')
    if line_number not in self.logical_lines:
      return slice(line_number, line_number + 1)
    start, stop, _ = self.logical_lines[line_number]
    return slice(start, stop)

  def __getitem__(self, line_number):
    return self._lines[self.line_range(line_number)]

  def enumerate(self):
    """Return an enumeration of line_number, line pairs."""
    return enumerate(self, 1)

  @property
  def tree(self):
    """The parsed AST of this file."""
    return self._tree

  def __str__(self):
    return 'PythonFile(%s)' % self._filename


class Nit(object):
  """Encapsulate a Style faux pas.

  The general taxonomy of nits:

  Prefix
    F => Flake8 errors
    E => PEP8 error
    W => PEP8 warning
    T => Twitter error

  Prefix:
    0 Naming
    1 Indentation
    2 Whitespace
    3 Blank line
    4 Import
    5 Line length
    6 Deprecation
    7 Statement
    8 Flake / Logic
    9 Runtime
  """

  COMMENT = 0
  WARNING = 1
  ERROR = 2

  SEVERITY = {
    COMMENT: 'COMMENT',
    WARNING: 'WARNING',
    ERROR: 'ERROR'
  }

  @classmethod
  def flatten_lines(self, *line_or_line_list):
    return itertools.chain(*line_or_line_list)

  def __init__(self, code, severity, python_file, message, line_number=None):
    if not severity in self.SEVERITY:
      raise ValueError('Severity should be one of %s' % ' '.join(self.SEVERITY.values()))
    self.python_file = python_file
    # TODO(wickman) Enforce that the code matches [Letter][3 letter number]
    self._code = code
    self._severity = severity
    self._message = message
    self._line_number = line_number

  @property
  def line_number(self):
    if self._line_number:
      line_range = self.python_file.line_range(self._line_number)
      if line_range.stop - line_range.start > 1:
        return '%03d-%03d' % (line_range.start, line_range.stop - 1)
      else:
        return '%03d' % line_range.start

  @property
  def severity(self):
    return self._severity

  @property
  def message(self):
    return '%s:%-7s %s:%s %s' % (
        self._code,
        self.SEVERITY[self.severity],
        self.python_file.filename,
        self.line_number or '*',
        self._message)

  @property
  def code(self):
    return self._code

  @property
  def lines(self):
    return self.python_file[self._line_number] if self._line_number else []

  def __str__(self):
    return '\n     |'.join(self.flatten_lines([self.message], self.lines))


class CheckstylePlugin(Interface):
  """Interface for checkstyle plugins."""

  def __init__(self, python_file):
    if not isinstance(python_file, PythonFile):
      raise TypeError('CheckstylePlugin takes PythonFile objects.')
    self.python_file = python_file

  def iter_ast_types(self, ast_type):
    for node in ast.walk(self.python_file.tree):
      if isinstance(node, ast_type):
        yield node

  @abstractmethod
  def nits(self):
    """Returns an iterable of Nit pertinent to the enclosed python file."""

  def __iter__(self):
    for nit in self.nits():
      yield nit

  def errors(self):
    for nit in self:
      if nit.severity is Nit.ERROR:
        yield nit

  def nit(self, code, severity, message, line_number_or_ast=None):
    line_number = None
    if isinstance(line_number_or_ast, Compatibility.integer):
      line_number = line_number_or_ast
    elif isinstance(line_number_or_ast, ast.AST):
      line_number = getattr(line_number_or_ast, 'lineno', None)
    return Nit(code, severity, self.python_file, message, line_number)

  def comment(self, code, message, line_number_or_ast=None):
    return self.nit(code, Nit.COMMENT, message, line_number_or_ast)

  def warning(self, code, message, line_number_or_ast=None):
    return self.nit(code, Nit.WARNING, message, line_number_or_ast)

  def error(self, code, message, line_number_or_ast=None):
    return self.nit(code, Nit.ERROR, message, line_number_or_ast)

########NEW FILE########
__FILENAME__ = iterators
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""
File iterators for determining over which files checkstyle should be run.
"""

from difflib import SequenceMatcher
from functools import partial
import os

from twitter.common.dirutil.fileset import Fileset

try:
  from git import Diff, Repo
  HAS_GIT = True
except ImportError:
  HAS_GIT = False


def path_iterator(args, options):
  for path in args:
    if os.path.isdir(path):
      for filename in Fileset.rglobs('*.py', root=path)():
        yield os.path.join(path, filename), None
    elif os.path.isfile(path):
      yield path, None


def read_blob(blob):
  """Helper to read a blob which may possibly be unstaged and in the working tree."""
  if blob.hexsha != Diff.NULL_HEX_SHA:
    return blob.data_stream.read()
  else:
    with open(blob.path) as fp:
      return fp.read()


def diff_lines(old, new):
  matcher = SequenceMatcher(None, read_blob(old).splitlines(1), read_blob(new).splitlines(1))
  # From get_opcodes documentation:
  # |      'replace':  a[i1:i2] should be replaced by b[j1:j2]
  # |      'delete':   a[i1:i2] should be deleted.
  # |                  Note that j1==j2 in this case.
  # |      'insert':   b[j1:j2] should be inserted at a[i1:i1].
  # |                  Note that i1==i2 in this case.
  # |      'equal':    a[i1:i2] == b[j1:j2]
  for opcode in matcher.get_opcodes():
    match_type, start_line_old, stop_line_old, start_line_new, stop_line_new = opcode
    if match_type in ('insert', 'replace'):
      for lineno in range(start_line_new, stop_line_new):
        # Line numbers are off-by-one in PythonFile
        yield lineno + 1


def line_filter_from_blobs(a_blob, b_blob):
  lines = frozenset(diff_lines(a_blob, b_blob))

  def line_filter(python_file, line_number):
    return line_number not in lines

  return line_filter


def permissive_line_filter(python_file, line_number):
  return False


def tuple_from_diff(diff):
  """
    From GitPython:

    It contains two sides a and b of the diff, members are prefixed with
    "a" and "b" respectively to inidcate that.

    Diffs keep information about the changed blob objects, the file mode, renames,
    deletions and new files.

    There are a few cases where None has to be expected as member variable value:

        ``New File``::

            a_mode is None
            a_blob is None

        ``Deleted File``::

            b_mode is None
            b_blob is None
  """
  # New file => check all
  if diff.b_blob and not diff.a_blob and diff.b_blob.path.endswith('.py'):
    return diff.b_blob.path, permissive_line_filter

  # Check diff lines between two
  if diff.a_blob and diff.b_blob and diff.b_blob.path.endswith('.py'):
    paths = diff.b_blob.path.split('\t')  # Handle rename, which are "old.py\tnew.py"
    paths = paths[1] if len(paths) != 1 else paths[0]
    return paths, line_filter_from_blobs(diff.a_blob, diff.b_blob)


def git_iterator(args, options):
  if not HAS_GIT:
    raise ValueError('Git has not been enabled for this checkstyle library!')

  repo = Repo()
  diff_commit = repo.rev_parse(options.diff or 'master')
  for filename, line_filter in filter(None, map(tuple_from_diff, diff_commit.diff(None))):
    yield filename, line_filter

########NEW FILE########
__FILENAME__ = class_factoring
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import ast

from ..common import CheckstylePlugin


class ClassFactoring(CheckstylePlugin):
  """Enforces recommendations for accessing class attributes.

  Within classes, if you see:
    class Distiller(object):
      CONSTANT = "Foo"
      def foo(self, value):
         return os.path.join(Distiller.CONSTANT, value)

  recommend using self.CONSTANT instead of Distiller.CONSTANT as otherwise
  it makes subclassing impossible."""

  def iter_class_accessors(self, class_node):
    for node in ast.walk(class_node):
      if isinstance(node, ast.Attribute) and isinstance(node.value, ast.Name) and (
          node.value.id == class_node.name):
        yield node

  def nits(self):
    for class_def in self.iter_ast_types(ast.ClassDef):
      for node in self.iter_class_accessors(class_def):
        yield self.warning('T800',
            'Instead of %s.%s use self.%s or cls.%s with instancemethods and classmethods '
            'respectively.' % (class_def.name, node.attr, node.attr, node.attr),
            node)

########NEW FILE########
__FILENAME__ = except_statements
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import ast

from ..common import CheckstylePlugin


class ExceptStatements(CheckstylePlugin):
  """Do not allow non-3.x-compatible and/or dangerous except statements."""

  @classmethod
  def blanket_excepts(cls, node):
    for handler in node.handlers:
      if handler.type is None and handler.name is None:
        return handler

  @classmethod
  def iter_excepts(cls, tree):
    for ast_node in ast.walk(tree):
      if isinstance(ast_node, ast.TryExcept):
        yield ast_node

  def nits(self):
    for try_except in self.iter_excepts(self.python_file.tree):
      # Check case 1, blanket except
      handler = self.blanket_excepts(try_except)
      if handler:
        yield self.error('T803', 'Blanket except: not allowed.', handler)

      # Check case 2, except Foo, bar:
      for handler in try_except.handlers:
        statement = ''.join(self.python_file[handler.lineno])
        except_index = statement.index('except')
        except_suffix = statement[except_index + len('except'):]

        if handler.name and ' as ' not in except_suffix:
          yield self.error('T601', 'Old-style except statements forbidden.', handler)

########NEW FILE########
__FILENAME__ = future_compatibility
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# Warn on non 2.x/3.x compatible symbols:
#   - basestring
#   - xrange
#
# Methods:
#   - .iteritems
#   - .iterkeys
#
# Comprehension builtins
#   - filter
#   - map
#   - range
#
#   => Make sure that these are not assigned.
#   Warn if they are assigned or returned directly from functions
#
# Class internals:
#   __metaclass__


import ast

from ..common import CheckstylePlugin


class FutureCompatibility(CheckstylePlugin):
  """Warns about behavior that will likely break when moving to Python 3.x"""
  BAD_ITERS = frozenset(('iteritems', 'iterkeys', 'itervalues'))
  BAD_FUNCTIONS = frozenset(('xrange',))
  BAD_NAMES = frozenset(('basestring', 'unicode'))

  def nits(self):
    for call in self.iter_ast_types(ast.Call):
      if isinstance(call.func, ast.Attribute):
        if call.func.attr in self.BAD_ITERS:
          yield self.error(
              'T602', '%s disappears in Python 3.x.  Use non-iter instead.' % call.func.attr, call)
      elif isinstance(call.func, ast.Name):
        if call.func.id in self.BAD_FUNCTIONS:
          yield self.error(
              'T603', 'Please avoid %s as it disappears in Python 3.x.' % call.func.id, call)
    for name in self.iter_ast_types(ast.Name):
      if name.id in self.BAD_NAMES:
        yield self.error(
            'T604', 'Please avoid %s as it disappears in Python 3.x.' % name.id, name)
    for class_def in self.iter_ast_types(ast.ClassDef):
      for node in class_def.body:
        if not isinstance(node, ast.Assign):
          continue
        for name in node.targets:
          if not isinstance(name, ast.Name):
            continue
          if name.id == '__metaclass__':
            yield self.warning('T605',
                'This metaclass style is deprecated and gone entirely in Python 3.x.', name)

########NEW FILE########
__FILENAME__ = import_order
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import ast
from distutils import sysconfig

from ..common import CheckstylePlugin


class ImportType(object):
  """Enforce a consistent import order.

  Imports are currently grouped into five separate groups:
    stdlib
    twitter
    gen
    package-local
    third-party

  Imports should be in this order and separated by a single space.
  """

  STDLIB = 1
  TWITTER = 2
  GEN = 3
  PACKAGE = 4
  THIRD_PARTY = 5
  UNKNOWN = 0

  NAMES = {
    UNKNOWN: 'unknown',
    STDLIB: 'stdlib',
    TWITTER: 'twitter',
    GEN: 'gen',
    PACKAGE: 'package',
    THIRD_PARTY: '3rdparty'
  }

  @classmethod
  def order_names(cls, import_order):
    return ' '.join(cls.NAMES.get(import_id, 'unknown') for import_id in import_order)


class ImportOrder(CheckstylePlugin):
  # TODO(wickman)
  #   - Warn if a package is marked as a 3rdparty but it's actually a package
  #     in the current working directory that should be a package-absolute
  #     import (i.e. from __future__ import absolute_imports)

  PLAT_SPECIFIC_PATH = sysconfig.get_python_lib(plat_specific=1)
  STANDARD_LIB_PATH = sysconfig.get_python_lib(standard_lib=1)

  @classmethod
  def extract_import_modules(cls, node):
    if isinstance(node, ast.Import):
      return [alias.name for alias in node.names]
    elif isinstance(node, ast.ImportFrom):
      return [node.module]
    return []

  @classmethod
  def classify_import(cls, node, name):
    if name == '' or (isinstance(node, ast.ImportFrom) and node.level > 0):
      return ImportType.PACKAGE
    if name.startswith('twitter.'):
      return ImportType.TWITTER
    if name.startswith('gen.'):
      return ImportType.GEN
    try:
      module = __import__(name)
    except ImportError:
      return ImportType.THIRD_PARTY
    if not hasattr(module, '__file__') or module.__file__.startswith(cls.STANDARD_LIB_PATH):
      return ImportType.STDLIB
    # Assume anything we can't classify is third-party
    return ImportType.THIRD_PARTY

  @classmethod
  def classify_import_node(cls, node):
    return set(cls.classify_import(node, module_name)
               for module_name in cls.extract_import_modules(node))

  def import_errors(self, node):
    errors = []
    if isinstance(node, ast.ImportFrom):
      if len(node.names) == 1 and node.names[0].name == '*':
        errors.append(self.error('T400', 'Wildcard imports are not allowed.', node))
      names = [alias.name.lower() for alias in node.names]
      if names != sorted(names):
        errors.append(self.error('T401', 'From import must import names in lexical order.', node))
    if isinstance(node, ast.Import):
      if len(node.names) > 1:
        errors.append(self.error('T402',
            'Absolute import statements should only import one module at a time.', node))
    return errors

  def classify_imports(self, chunk):
    """
      Possible import statements:

      import name
      from name import subname
      from name import subname1 as subname2
      from name import *
      from name import tuple

      AST representations:

      ImportFrom:
         module=name
         names=[alias(name, asname), ...]
                    name can be '*'

      Import:
        names=[alias(name, asname), ...]

      Imports are classified into 5 classes:
        stdlib      => Python standard library
        twitter.*   => Twitter internal / standard library
        gen.*       => Thrift gen namespaces
        .*          => Package-local imports
        3rdparty    => site-packages or third party

      classify_imports classifies the import into one of these forms.
    """
    errors = []
    all_module_types = set()
    for node in chunk:
      errors.extend(self.import_errors(node))
      module_types = self.classify_import_node(node)
      if len(module_types) > 1:
        errors.append(self.error('T403',
            'Import statement imports from multiple module types: %s.'
            % ImportType.order_names(module_types), node))
      if ImportType.UNKNOWN in module_types:
        errors.append(self.warning('T404', 'Unclassifiable import.', node))
      all_module_types.update(module_types)
    if len(chunk) > 0 and len(all_module_types) > 1:
      errors.append(
          self.error('T405',
              'Import block starting here contains imports from multiple module types: %s.'
              % ImportType.order_names(all_module_types), chunk[0].lineno))
    return all_module_types, errors

  # TODO(wickman) Classify imports within top-level try/except ImportError blocks.
  def iter_import_chunks(self):
    """Iterate over space-separated import chunks in a file."""
    chunk = []
    last_line = None
    for leaf in self.python_file.tree.body:
      if isinstance(leaf, (ast.Import, ast.ImportFrom)):
        # we've seen previous imports but this import is not in the same chunk
        if last_line and leaf.lineno != last_line[1]:
          yield chunk
          chunk = [leaf]
        # we've either not seen previous imports or this is part of the same chunk
        elif not last_line or last_line and leaf.lineno == last_line[1]:
          chunk.append(leaf)
        last_line = self.python_file.logical_lines[leaf.lineno]
    if chunk:
      yield chunk

  def nits(self):
    errors = []
    module_order = []

    for chunk in self.iter_import_chunks():
      module_types, chunk_errors = self.classify_imports(chunk)
      errors.extend(chunk_errors)
      module_order.append(list(module_types))

    numbered_module_order = []
    for modules in module_order:
      if len(modules) > 0:
        if modules[0] is not ImportType.UNKNOWN:
          numbered_module_order.append(modules[0])

    if numbered_module_order != sorted(numbered_module_order):
      errors.append(self.error('T406',
          'Out of order import chunks: Got %s and expect %s.' % (
          ImportType.order_names(numbered_module_order),
          ImportType.order_names(sorted(numbered_module_order))),
          self.python_file.tree))

    return errors

########NEW FILE########
__FILENAME__ = indentation
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import tokenize

from ..common import CheckstylePlugin


# TODO(wickman) Update this to sanitize line continuation styling as we have
# disabled it from pep8.py due to mismatched indentation styles.
class Indentation(CheckstylePlugin):
  """Enforce proper indentation."""
  INDENT_LEVEL = 2  # the one true way

  def nits(self):
    indents = []

    for token in self.python_file.tokens:
      token_type, token_text, token_start = token[0:3]
      if token_type is tokenize.INDENT:
        last_indent = len(indents[-1]) if indents else 0
        current_indent = len(token_text)
        if current_indent - last_indent != self.INDENT_LEVEL:
          yield self.error('T100',
              'Indentation of %d instead of %d' % (current_indent - last_indent, self.INDENT_LEVEL),
              token_start[0])
        indents.append(token_text)
      elif token_type is tokenize.DEDENT:
        indents.pop()

########NEW FILE########
__FILENAME__ = missing_contextmanager
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# TODO(wickman)
#
# 1. open(foo) should always be done in a with context.
#
# 2. if you see acquire/release on the same variable in a particular ast
#    body, warn about context manager use.

import ast

from ..common import CheckstylePlugin


class MissingContextManager(CheckstylePlugin):
  """Recommend the use of contextmanagers when it seems appropriate."""

  def nits(self):
    with_contexts = set(self.iter_ast_types(ast.With))
    with_context_calls = set(node.context_expr for node in with_contexts
        if isinstance(node.context_expr, ast.Call))

    for call in self.iter_ast_types(ast.Call):
      if isinstance(call.func, ast.Name) and call.func.id == 'open' and (
          call not in with_context_calls):
        yield self.warning('T802', 'open() calls should be made within a contextmanager.', call)

########NEW FILE########
__FILENAME__ = newlines
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import ast

from ..common import CheckstylePlugin


class Newlines(CheckstylePlugin):
  def iter_toplevel_defs(self):
    for node in self.python_file.tree.body:
      if isinstance(node, ast.FunctionDef) or isinstance(node, ast.ClassDef):
        yield node

  def previous_blank_lines(self, line_number):
    blanks = 0
    while line_number > 1:
      line_number -= 1
      line_value = self.python_file.lines[line_number].strip()
      if line_value.startswith('#'):
        continue
      if line_value:
        break
      blanks += 1
    return blanks

  def nits(self):
    for node in self.iter_toplevel_defs():
      previous_blank_lines = self.previous_blank_lines(node.lineno)
      if node.lineno > 2 and previous_blank_lines != 2:
        yield self.error('T302', 'Expected 2 blank lines, found %d' % previous_blank_lines,
            node)
    for node in self.iter_ast_types(ast.ClassDef):
      for subnode in node.body:
        if not isinstance(subnode, ast.FunctionDef):
          continue
        previous_blank_lines = self.previous_blank_lines(subnode.lineno)
        if subnode.lineno - node.lineno > 1 and previous_blank_lines != 1:
          yield self.error('T301', 'Expected 1 blank lines, found %d' % previous_blank_lines,
              subnode)

########NEW FILE########
__FILENAME__ = new_style_classes
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import ast

from ..common import CheckstylePlugin


class NewStyleClasses(CheckstylePlugin):
  """Enforce the use of new-style classes."""

  def nits(self):
    for class_def in self.iter_ast_types(ast.ClassDef):
      if not class_def.bases:
        yield self.error('T606', 'Classes must be new-style classes.', class_def)

########NEW FILE########
__FILENAME__ = pep8
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import absolute_import

from ..common import (
    CheckstylePlugin,
    Nit,
    PythonFile)

import pep8


class PEP8Error(Nit):
  def __init__(self, python_file, code, line_number, offset, text, doc):
    super(PEP8Error, self).__init__(code, Nit.ERROR, python_file, text, line_number)


class TwitterReporter(pep8.BaseReport):
  def init_file(self, filename, lines, expected, line_offset):
    super(TwitterReporter, self).init_file(filename, lines, expected, line_offset)
    self._python_file = PythonFile.parse(filename)
    self._twitter_errors = []

  def error(self, line_number, offset, text, check):
    code = super(TwitterReporter, self).error(line_number, offset, text, check)
    if code:
      self._twitter_errors.append(
          PEP8Error(self._python_file, code, line_number, offset, text[5:], check.__doc__))
    return code

  @property
  def twitter_errors(self):
    return self._twitter_errors


IGNORE_CODES = (
  # continuation_line_indentation
  'E121',
  'E124',
  'E125',
  'E127',
  'E128',

  # imports_on_separate_lines
  'E401',

  # indentation
  'E111',

  # trailing_whitespace
  'W291',
  'W293',

  # multiple statements
  # A common (acceptable) exception pattern at Twitter is:
  #   class MyClass(object):
  #     class Error(Exception): pass
  #     class DerpError(Error): pass
  #     class HerpError(Error): pass
  # We disable the pep8.py checking for these and instead have a more lenient filter
  # in the whitespace checker.
  'E701',
  'E301',
  'E302',
)


class PEP8Checker(CheckstylePlugin):
  """Enforce PEP8 checks from the pep8 tool."""

  STYLE_GUIDE = pep8.StyleGuide(
      max_line_length=100,
      verbose=False,
      reporter=TwitterReporter,
      ignore=IGNORE_CODES)

  def nits(self):
    report = self.STYLE_GUIDE.check_files([self.python_file.filename])
    return report.twitter_errors

########NEW FILE########
__FILENAME__ = print_statements
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import ast
import re

from ..common import CheckstylePlugin


class PrintStatements(CheckstylePlugin):
  """Enforce the use of print as a function and not a statement."""

  FUNCTIONY_EXPRESSION = re.compile(r'^\s*\(.*\)\s*$')

  def nits(self):
    for print_stmt in self.iter_ast_types(ast.Print):
      # In Python 3.x and in 2.x with __future__ print_function, prints show up as plain old
      # function expressions.  ast.Print does not exist in Python 3.x.  However, allow use
      # syntactically as a function, i.e. ast.Print but with ws "(" .* ")" ws
      logical_line = ''.join(self.python_file[print_stmt.lineno])
      print_offset = logical_line.index('print')
      stripped_line = logical_line[print_offset + len('print'):]
      if not self.FUNCTIONY_EXPRESSION.match(stripped_line):
        yield self.error('T607', 'Print used as a statement.', print_stmt)

########NEW FILE########
__FILENAME__ = pyflakes
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import absolute_import

from ..common import CheckstylePlugin, Nit

from pyflakes.checker import Checker as FlakesChecker


class FlakeError(Nit):
  # TODO(wickman) There is overlap between this and Flake8 -- consider integrating
  # checkstyle plug-ins into the PEP8 tool directly so that this can be inherited
  # by flake8.
  CLASS_ERRORS = {
    'DuplicateArgument': 'F831',
    'ImportShadowedByLoopVar': 'F402',
    'ImportStarUsed': 'F403',
    'LateFutureImport': 'F404',
    'Redefined': 'F810',
    'RedefinedInListComp': 'F812',
    'RedefinedWhileUnused': 'F811',
    'UndefinedExport': 'F822',
    'UndefinedLocal': 'F823',
    'UndefinedName': 'F821',
    'UnusedImport': 'F401',
    'UnusedVariable': 'F841',
  }

  def __init__(self, python_file, flake_message):
    super(FlakeError, self).__init__(
        self.CLASS_ERRORS.get(flake_message.__class__.__name__, 'F999'),
        Nit.ERROR,
        python_file,
        flake_message.message % flake_message.message_args,
        flake_message.lineno)


class PyflakesChecker(CheckstylePlugin):
  """Detect common coding errors via the pyflakes package."""

  def nits(self):
    checker = FlakesChecker(self.python_file.tree, self.python_file.filename)
    for message in sorted(checker.messages, key=lambda msg: msg.lineno):
      yield FlakeError(self.python_file, message)

########NEW FILE########
__FILENAME__ = trailing_whitespace
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict
import tokenize
import sys

from ..common import CheckstylePlugin


class TrailingWhitespace(CheckstylePlugin):
  """Warn on invalid trailing whitespace."""

  @classmethod
  def build_exception_map(cls, tokens):
    """Generates a set of ranges where we accept trailing slashes, specifically within comments
       and strings.
    """
    exception_ranges = defaultdict(list)
    for token in tokens:
      token_type, _, token_start, token_end = token[0:4]
      if token_type in (tokenize.COMMENT, tokenize.STRING):
        if token_start[0] == token_end[0]:
          exception_ranges[token_start[0]].append((token_start[1], token_end[1]))
        else:
          exception_ranges[token_start[0]].append((token_start[1], sys.maxint))
          for line in range(token_start[0] + 1, token_end[0]):
            exception_ranges[line].append((0, sys.maxint))
          exception_ranges[token_end[0]].append((0, token_end[1]))
    return exception_ranges

  def __init__(self, *args, **kw):
    super(TrailingWhitespace, self).__init__(*args, **kw)
    self._exception_map = self.build_exception_map(self.python_file.tokens)

  def has_exception(self, line_number, exception_start, exception_end=None):
    exception_end = exception_end or exception_start
    for start, end in self._exception_map.get(line_number, ()):
      if start <= exception_start and exception_end <= end:
        return True
    return False

  def nits(self):
    for line_number, line in self.python_file.enumerate():
      stripped_line = line.rstrip()
      if stripped_line != line and not self.has_exception(line_number,
          len(stripped_line), len(line)):
        yield self.error('T200', 'Line has trailing whitespace.', line_number)
      if line.rstrip().endswith('\\'):
        if not self.has_exception(line_number, len(line.rstrip()) - 1):
          yield self.error('T201', 'Line has trailing slashes.', line_number)

########NEW FILE########
__FILENAME__ = variable_names
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import ast
from functools import wraps
import keyword
import re

from twitter.common.lang import Compatibility

from ..common import CheckstylePlugin


ALL_LOWER_CASE_RE = re.compile(r'^[a-z][a-z\d]*$')
ALL_UPPER_CASE_RE = re.compile(r'^[A-Z][A-Z\d]+$')
LOWER_SNAKE_RE = re.compile(r'^([a-z][a-z\d]*)(_[a-z\d]+)*$')
UPPER_SNAKE_RE = re.compile(r'^([A-Z][A-Z\d]*)(_[A-Z\d]+)*$')
UPPER_CAMEL_RE = re.compile(r'^([A-Z][a-z\d]*)+$')
RESERVED_NAMES = frozenset(keyword.kwlist)


if Compatibility.PY2:
  import __builtin__
  BUILTIN_NAMES = dir(__builtin__)
else:
  import builtin
  BUILTIN_NAMES = dir(builtin)


def allow_underscores(num):
  def wrap(function):
    @wraps(function)
    def wrapped_function(name):
      if name.startswith('_' * (num + 1)):
        return False
      return function(name.lstrip('_'))
    return wrapped_function
  return wrap


@allow_underscores(1)
def is_upper_camel(name):
  """UpperCamel, AllowingHTTPAbbrevations, _WithUpToOneUnderscoreAllowable."""
  return bool(UPPER_CAMEL_RE.match(name) and not ALL_UPPER_CASE_RE.match(name))


@allow_underscores(2)
def is_lower_snake(name):
  """lower_snake_case, _with, __two_underscores_allowable."""
  return LOWER_SNAKE_RE.match(name) is not None


def is_reserved_name(name):
  return name in BUILTIN_NAMES or name in RESERVED_NAMES


def is_reserved_with_trailing_underscore(name):
  """For example, super_, id_, type_"""
  if name.endswith('_') and not name.endswith('__'):
    return is_reserved_name(name[:-1])
  return False


def is_builtin_name(name):
  """For example, __foo__ or __bar__."""
  if name.startswith('__') and name.endswith('__'):
    return ALL_LOWER_CASE_RE.match(name[2:-2]) is not None
  return False


@allow_underscores(2)
def is_constant(name):
  return UPPER_SNAKE_RE.match(name) is not None


class PEP8VariableNames(CheckstylePlugin):
  """Enforces PEP8 recommendations for variable names.

  Specifically:
     UpperCamel class names
     lower_snake / _lower_snake / __lower_snake function names
     lower_snake expression variable names
     CLASS_LEVEL_CONSTANTS = {}
     GLOBAL_LEVEL_CONSTANTS = {}
  """

  CLASS_GLOBAL_BUILTINS = frozenset((
    '__slots__',
    '__metaclass__',
  ))

  def iter_class_methods(self, class_node):
    for node in class_node.body:
      if isinstance(node, ast.FunctionDef):
        yield node

  def iter_class_globals(self, class_node):
    for node in class_node.body:
      # TODO(wickman) Occasionally you have the pattern where you set methods equal to each other
      # which should be allowable, for example:
      #   class Foo(object):
      #     def bar(self):
      #       pass
      #     alt_bar = bar
      if isinstance(node, ast.Assign):
        for name in node.targets:
          if isinstance(name, ast.Name):
            yield name

  def nits(self):
    class_methods = set()
    all_methods = set(function_def for function_def in ast.walk(self.python_file.tree)
        if isinstance(function_def, ast.FunctionDef))

    for class_def in self.iter_ast_types(ast.ClassDef):
      if not is_upper_camel(class_def.name):
        yield self.error('T000', 'Classes must be UpperCamelCased', class_def)
      for class_global in self.iter_class_globals(class_def):
        if not is_constant(class_global.id) and class_global.id not in self.CLASS_GLOBAL_BUILTINS:
          yield self.error('T001', 'Class globals must be UPPER_SNAKE_CASED', class_global)
      if not class_def.bases or all(isinstance(base, ast.Name) and base.id == 'object'
          for base in class_def.bases):
        class_methods.update(self.iter_class_methods(class_def))
      else:
        # If the class is inheriting from anything that is potentially a bad actor, rely
        # upon checking that bad actor out of band.  Fixes PANTS-172.
        for method in self.iter_class_methods(class_def):
          all_methods.discard(method)

    for function_def in all_methods - class_methods:
      if is_reserved_name(function_def.name):
        yield self.error('T801', 'Method name overrides a builtin.', function_def)

    # TODO(wickman) Only enforce this for classes that derive from object.  If they
    # don't derive object, it's possible that the superclass naming is out of its
    # control.
    for function_def in all_methods:
      if not any((is_lower_snake(function_def.name),
                  is_builtin_name(function_def.name),
                  is_reserved_with_trailing_underscore(function_def.name))):
        yield self.error('T002', 'Method names must be lower_snake_cased', function_def)

########NEW FILE########
__FILENAME__ = application
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

try:
  import ConfigParser
except ImportError:
  import configparser as ConfigParser

from collections import defaultdict, deque
import copy
from functools import partial, wraps
import inspect
import optparse
import os
import shlex
import signal
import sys
import threading
import time
import traceback

from twitter.common import options
from twitter.common.lang import Compatibility
from twitter.common.process import daemonize
from twitter.common.util import topological_sort

from .inspection import Inspection
from .module import AppModule


class Application(object):
  class Error(Exception): pass

  # enforce a quasi-singleton interface (for resettable applications in test)
  _GLOBAL = None

  HELP_OPTIONS = [
    options.Option("-h", "--help", "--short-help",
      action="callback",
      callback=lambda *args, **kwargs: Application.active()._short_help(*args, **kwargs),
      help="show this help message and exit."),
    options.Option("--long-help",
      action="callback",
      callback=lambda *args, **kwargs: Application.active()._long_help(*args, **kwargs),
      help="show options from all registered modules, not just the __main__ module.")
  ]

  IGNORE_RC_FLAG = '--app_ignore_rc_file'

  APP_OPTIONS = {
    'daemonize':
       options.Option('--app_daemonize',
           action='store_true',
           default=False,
           dest='twitter_common_app_daemonize',
           help="Daemonize this application."),

    'daemon_stdout':
       options.Option('--app_daemon_stdout',
           default='/dev/null',
           dest='twitter_common_app_daemon_stdout',
           help="Direct this app's stdout to this file if daemonized."),

    'daemon_stderr':
       options.Option('--app_daemon_stderr',
           default='/dev/null',
           dest='twitter_common_app_daemon_stderr',
           help="Direct this app's stderr to this file if daemonized."),

    'pidfile':
       options.Option('--app_pidfile',
           default=None,
           dest='twitter_common_app_pidfile',
           help="The pidfile to use if --app_daemonize is specified."),

    'debug':
       options.Option('--app_debug',
           action='store_true',
           default=False,
           dest='twitter_common_app_debug',
           help="Print extra debugging information during application initialization."),

    'profiling':
       options.Option('--app_profiling',
           action='store_true',
           default=False,
           dest='twitter_common_app_profiling',
           help="Run profiler on the code while it runs.  Note this can cause slowdowns."),

    'profile_output':
       options.Option('--app_profile_output',
           default=None,
           metavar='FILENAME',
           dest='twitter_common_app_profile_output',
           help="Dump the profiling output to a binary profiling format."),

    'rc_filename':
       options.Option('--app_rc_filename',
           action='store_true',
           default=False,
           dest='twitter_common_app_rc_filename',
           help="Print the filename for the rc file and quit."),

    'ignore_rc_file':
       options.Option(IGNORE_RC_FLAG,
           action='store_true',
           default=False,
           dest='twitter_common_app_ignore_rc_file',
           help="Ignore default arguments from the rc file."),
  }

  OPTIONS = 'options'
  OPTIONS_ATTR = '__options__'
  NO_COMMAND = 'DEFAULT'
  SIGINT_RETURN_CODE = 130  # see http://tldp.org/LDP/abs/html/exitcodes.html

  INITIALIZING = 1
  INITIALIZED = 2
  RUNNING = 3
  ABORTING = 4
  SHUTDOWN = 5

  @classmethod
  def reset(cls):
    """Reset the global application.  Only useful for testing."""
    cls._GLOBAL = cls()

  @classmethod
  def active(cls):
    """Return the current resident application object."""
    return cls._GLOBAL

  def __init__(self, exit_function=sys.exit, force_args=None):
    self._name = None
    self._exit_function = exit_function
    self._force_args = force_args
    self._registered_modules = []
    self._init_modules = []
    self._option_targets = defaultdict(dict)
    self._global_options = {}
    self._interspersed_args = False
    self._main_options = self.HELP_OPTIONS[:]
    self._main_thread = None
    self._shutdown_commands = []
    self._usage = ""
    self._profiler = None
    self._commands = {}
    self._state = self.INITIALIZING

    self._reset()
    for opt in self.APP_OPTIONS.values():
      self.add_option(opt)
    self._configure_options(None, self.APP_OPTIONS)

  def pre_initialization(method):
    @wraps(method)
    def wrapped_method(self, *args, **kw):
      if self._state > self.INITIALIZING:
        raise self.Error("Cannot perform operation after initialization!")
      return method(self, *args, **kw)
    return wrapped_method

  def post_initialization(method):
    @wraps(method)
    def wrapped_method(self, *args, **kw):
      if self._state < self.INITIALIZED:
        raise self.Error("Cannot perform operation before initialization!")
      return method(self, *args, **kw)
    return wrapped_method

  def _reset(self):
    """
      Resets the state set up by init() so that init() may be called again.
    """
    self._state = self.INITIALIZING
    self._option_values = options.Values()
    self._argv = []

  def interspersed_args(self, value):
    self._interspersed_args = bool(value)

  def _configure_options(self, module, option_dict):
    for opt_name, opt in option_dict.items():
      self._option_targets[module][opt_name] = opt.dest

  @pre_initialization
  def configure(self, module=None, **kw):
    """
      Configure the application object or its activated modules.

      Typically application modules export flags that can be defined on the
      command-line.  In order to allow the application to override defaults,
      these modules may export named parameters to be overridden.  For example,
      the Application object itself exports named variables such as "debug" or
      "profiling", which can be enabled via:
         app.configure(debug=True)
      and
         app.configure(profiling=True)
      respectively.  They can also be enabled with their command-line argument
      counterpart, e.g.
        ./my_application --app_debug --app_profiling

      Some modules export named options, e.g. twitter.common.app.modules.http exports
      'enable', 'host', 'port'.  The command-line arguments still take precedence and
      will override any defaults set by the application in app.configure.  To activate
      these options, just pass along the module name:
        app.configure(module='twitter.common.app.modules.http', enable=True)
    """
    if module not in self._option_targets:
      if not self._import_module(module):
        raise self.Error('Unknown module to configure: %s' % module)
    def configure_option(name, value):
      if name not in self._option_targets[module]:
        raise self.Error('Module %s has no option %s' % (module, name))
      self.set_option(self._option_targets[module][name], value)
    for option_name, option_value in kw.items():
      configure_option(option_name, option_value)

  def _main_parser(self):
    return (options.parser().interspersed_arguments(self._interspersed_args)
                            .options(self._main_options)
                            .usage(self._usage))

  def command_parser(self, command):
    assert command in self._commands
    values_copy = copy.deepcopy(self._option_values)
    parser = self._main_parser()
    command_group = options.new_group(('For %s only' % command) if command else 'Default')
    for option in getattr(self._commands[command], Application.OPTIONS_ATTR, []):
      op = copy.deepcopy(option)
      if not hasattr(values_copy, op.dest):
        setattr(values_copy, op.dest, op.default if op.default != optparse.NO_DEFAULT else None)
      self.rewrite_help(op)
      op.default = optparse.NO_DEFAULT
      command_group.add_option(op)
    parser = parser.groups([command_group]).values(values_copy)
    usage = self._commands[command].__doc__
    if usage:
      parser = parser.usage(usage)
    return parser

  def _construct_partial_parser(self):
    """
      Construct an options parser containing only options added by __main__
      or global help options registered by the application.
    """
    if hasattr(self._commands.get(self._command), self.OPTIONS_ATTR):
      return self.command_parser(self._command)
    else:
      return self._main_parser().values(copy.deepcopy(self._option_values))

  def _construct_full_parser(self):
    """
      Construct an options parser containing both local and global (module-level) options.
    """
    return self._construct_partial_parser().groups(self._global_options.values())

  def _rc_filename(self):
    rc_short_filename = '~/.%src' % self.name()
    return os.path.expanduser(rc_short_filename)

  def _add_default_options(self, argv):
    """
      Return an argument list with options from the rc file prepended.
    """
    rc_filename = self._rc_filename()

    options = argv

    if self.IGNORE_RC_FLAG not in argv and os.path.exists(rc_filename):
      command = self._command or self.NO_COMMAND
      rc_config = ConfigParser.SafeConfigParser()
      rc_config.read(rc_filename)

      if rc_config.has_option(command, self.OPTIONS):
        default_options_str = rc_config.get(command, self.OPTIONS)
        default_options = shlex.split(default_options_str, True)
        options = default_options + options

    return options

  def _parse_options(self, force_args=None):
    """
      Parse options and set self.option_values and self.argv to the values to be passed into
      the application's main() method.
    """
    argv = sys.argv[1:] if force_args is None else force_args
    if argv and argv[0] in self._commands:
      self._command = argv.pop(0)
    else:
      self._command = None
    parser = self._construct_full_parser()
    self._option_values, self._argv = parser.parse(self._add_default_options(argv))

  def _short_help(self, option, opt, value, parser):
    self._construct_partial_parser().print_help()
    self._exit_function(1)
    return

  def _long_help(self, option, opt, value, parser):
    self._construct_full_parser().print_help()
    self._exit_function(1)
    return

  @pre_initialization
  def _setup_modules(self):
    """
      Setup all initialized modules.
    """
    module_registry = AppModule.module_registry()
    for bundle in topological_sort(AppModule.module_dependencies()):
      for module_label in bundle:
        assert module_label in module_registry
        module = module_registry[module_label]
        self._debug_log('Initializing: %s (%s)' % (module.label(), module.description()))
        try:
          module.setup_function()
        except AppModule.Unimplemented:
          pass
        self._init_modules.append(module.label())

  def _teardown_modules(self):
    """
      Teardown initialized module in reverse initialization order.
    """
    if self._state != self.SHUTDOWN:
      raise self.Error('Expected application to be in SHUTDOWN state!')
    module_registry = AppModule.module_registry()
    for module_label in reversed(self._init_modules):
      assert module_label in module_registry
      module = module_registry[module_label]
      self._debug_log('Running exit function for %s (%s)' % (module_label, module.description()))
      try:
        module.teardown_function()
      except AppModule.Unimplemented:
        pass

  def _maybe_daemonize(self):
    if self._option_values.twitter_common_app_daemonize:
      daemonize(pidfile=self._option_values.twitter_common_app_pidfile,
                stdout=self._option_values.twitter_common_app_daemon_stdout,
                stderr=self._option_values.twitter_common_app_daemon_stderr)

  # ------- public exported methods -------
  @pre_initialization
  def init(self):
    """
      Initialize the state necessary to run the application's main() function but
      without actually invoking main.
    """
    self._parse_options(self._force_args)
    self._maybe_daemonize()
    self._setup_modules()
    self._state = self.INITIALIZED

  def reinit(self, force_args=None):
    """
      Reinitialize the application.  This clears the stateful parts of the application
      framework and reruns init().  Mostly useful for testing.
    """
    self._reset()
    self.init(force_args)

  @post_initialization
  def argv(self):
    return self._argv

  @pre_initialization
  def add_module_path(self, name, path):
    """
      Add all app.Modules defined by name at path.

      Typical usage (e.g. from the __init__.py of something containing many
      app modules):

        app.add_module_path(__name__, __path__)
    """
    import pkgutil
    for _, mod, ispkg in pkgutil.iter_modules(path):
      if ispkg:
        continue
      fq_module = '.'.join([name, mod])
      __import__(fq_module)
      for (kls_name, kls) in inspect.getmembers(sys.modules[fq_module], inspect.isclass):
        if issubclass(kls, AppModule):
          self.register_module(kls())

  @pre_initialization
  def register_module(self, module):
    """
      Register an app.Module and all its options.
    """
    if not isinstance(module, AppModule):
      raise TypeError('register_module should be called with a subclass of AppModule')
    if module.label() in self._registered_modules:
      # Do not reregister.
      return
    if hasattr(module, 'OPTIONS'):
      if not isinstance(module.OPTIONS, dict):
        raise self.Error('Registered app.Module %s has invalid OPTIONS.' % module.__module__)
      for opt in module.OPTIONS.values():
        self._add_option(module.__module__, opt)
      self._configure_options(module.label(), module.OPTIONS)
    self._registered_modules.append(module.label())

  @classmethod
  def _get_module_key(cls, module):
    return 'From module %s' % module

  @pre_initialization
  def _add_main_option(self, option):
    self._main_options.append(option)

  @pre_initialization
  def _add_module_option(self, module, option):
    calling_module = self._get_module_key(module)
    if calling_module not in self._global_options:
      self._global_options[calling_module] = options.new_group(calling_module)
    self._global_options[calling_module].add_option(option)

  @staticmethod
  def rewrite_help(op):
    if hasattr(op, 'help') and isinstance(op.help, Compatibility.string):
      if op.help.find('%default') != -1 and op.default != optparse.NO_DEFAULT:
        op.help = op.help.replace('%default', str(op.default))
      else:
        op.help = op.help + ((' [default: %s]' % str(op.default))
          if op.default != optparse.NO_DEFAULT else '')

  def _add_option(self, calling_module, option):
    op = copy.deepcopy(option)
    if op.dest and hasattr(op, 'default'):
      self.set_option(op.dest, op.default if op.default != optparse.NO_DEFAULT else None,
        force=False)
      self.rewrite_help(op)
      op.default = optparse.NO_DEFAULT
    if calling_module == '__main__':
      self._add_main_option(op)
    else:
      self._add_module_option(calling_module, op)

  def _get_option_from_args(self, args, kwargs):
    if len(args) == 1 and kwargs == {} and isinstance(args[0], options.Option):
      return args[0]
    else:
      return options.TwitterOption(*args, **kwargs)

  @pre_initialization
  def add_option(self, *args, **kwargs):
    """
      Add an option to the application.

      You may pass either an Option object from the optparse/options module, or
      pass the *args/**kwargs necessary to construct an Option.
    """
    calling_module = Inspection.find_calling_module()
    added_option = self._get_option_from_args(args, kwargs)
    self._add_option(calling_module, added_option)

  def _set_command_origin(self, function, command_name):
    function.__app_command_origin__ = (self, command_name)

  def _get_command_name(self, function):
    assert self._is_app_command(function)
    return function.__app_command_origin__[1]

  def _is_app_command(self, function):
    return callable(function) and (
        getattr(function, '__app_command_origin__', (None, None))[0] == self)

  def command(self, function=None, name=None):
    """
      Decorator to turn a function into an application command.

      To add a command foo, the following patterns will both work:

      @app.command
      def foo(args, options):
        ...

      @app.command(name='foo')
      def bar(args, options):
        ...
    """
    if name is None:
      return self._command(function)
    else:
      return partial(self._command, name=name)

  def _command(self, function, name=None):
    command_name = name or function.__name__
    self._set_command_origin(function, command_name)
    if Inspection.find_calling_module() == '__main__':
      self._register_command(function, command_name)
    return function

  def register_commands_from(self, *modules):
    """
      Given an imported module, walk the module for commands that have been
      annotated with @app.command and register them against this
      application.
    """
    for module in modules:
      for _, function in inspect.getmembers(module, predicate=lambda fn: callable(fn)):
        if self._is_app_command(function):
          self._register_command(function, self._get_command_name(function))

  @pre_initialization
  def _register_command(self, function, command_name):
    """
      Registers function as the handler for command_name. Uses function.__name__ if command_name
      is None.
    """
    if command_name in self._commands:
      raise self.Error('Found two definitions for command %s' % command_name)
    self._commands[command_name] = function
    return function

  def default_command(self, function):
    """
      Decorator to make a command default.
    """
    if Inspection.find_calling_module() == '__main__':
      if None in self._commands:
        defaults = (self._commands[None].__name__, function.__name__)
        raise self.Error('Found two default commands: %s and %s' % defaults)
      self._commands[None] = function
    return function

  @pre_initialization
  def command_option(self, *args, **kwargs):
    """
      Decorator to add an option only for a specific command.
    """
    def register_option(function):
      added_option = self._get_option_from_args(args, kwargs)
      if not hasattr(function, self.OPTIONS_ATTR):
        setattr(function, self.OPTIONS_ATTR, deque())
      getattr(function, self.OPTIONS_ATTR).appendleft(added_option)
      return function
    return register_option

  @pre_initialization
  def copy_command_options(self, command_function):
    """
      Decorator to copy command options from another command.
    """
    def register_options(function):
      if hasattr(command_function, self.OPTIONS_ATTR):
        if not hasattr(function, self.OPTIONS_ATTR):
          setattr(function, self.OPTIONS_ATTR, deque())
        command_options = getattr(command_function, self.OPTIONS_ATTR)
        getattr(function, self.OPTIONS_ATTR).extendleft(command_options)
      return function
    return register_options

  def add_command_options(self, command_function):
    """
      Function to add all options from a command
    """
    module = inspect.getmodule(command_function).__name__
    for option in getattr(command_function, self.OPTIONS_ATTR, ()):
      self._add_option(module, option)

  def _debug_log(self, msg):
    if hasattr(self._option_values, 'twitter_common_app_debug') and (
        self._option_values.twitter_common_app_debug):
      print('twitter.common.app debug: %s' % msg, file=sys.stderr)

  def set_option(self, dest, value, force=True):
    """
      Set a global option value either pre- or post-initialization.

      If force=False, do not set the default if already overridden by a manual call to
      set_option.
    """
    if hasattr(self._option_values, dest) and not force:
      return
    setattr(self._option_values, dest, value)

  def get_options(self):
    """
      Return all application options, both registered by __main__ and all imported modules.
    """
    return self._option_values

  def get_commands(self):
    """
      Return all valid commands registered by __main__
    """
    return list(filter(None, self._commands.keys()))

  def get_commands_and_docstrings(self):
    """
      Generate all valid commands together with their docstrings
    """
    for command, function in self._commands.items():
      if command is not None:
        yield command, function.__doc__

  def get_local_options(self):
    """
      Return the options only defined by __main__.
    """
    new_values = options.Values()
    for opt in self._main_options:
      if opt.dest:
        setattr(new_values, opt.dest, getattr(self._option_values, opt.dest))
    return new_values

  @pre_initialization
  def set_usage(self, usage):
    """
      Set the usage message should the user call --help or invalidly specify options.
    """
    self._usage = usage

  def error(self, message):
    """
      Print the application help message, an error message, then exit.
    """
    self._construct_partial_parser().error(message)

  def help(self):
    """
      Print the application help message and exit.
    """
    self._short_help(None, None, None, None)

  @pre_initialization
  def set_name(self, application_name):
    """
      Set the application name.  (Autodetect otherwise.)
    """
    self._name = application_name

  def name(self):
    """
      Return the name of the application.  If set_name was never explicitly called,
      the application framework will attempt to autodetect the name of the application
      based upon the location of __main__.
    """
    if self._name is not None:
      return self._name
    else:
      try:
        return Inspection.find_application_name()
      # TODO(wickman) Be more specific
      except Exception:
        return 'unknown'

  def quit(self, return_code):
    nondaemons = 0
    for thr in threading.enumerate():
      self._debug_log('  Active thread%s: %s' % (' (daemon)' if thr.isDaemon() else '', thr))
      if thr is not threading.current_thread() and not thr.isDaemon():
        nondaemons += 1
    if nondaemons:
      self._debug_log('More than one active non-daemon thread, your application may hang!')
    else:
      self._debug_log('Exiting cleanly.')
    self._exit_function(return_code)

  def profiler(self):
    if self._option_values.twitter_common_app_profiling:
      if self._profiler is None:
        try:
          import cProfile as profile
        except ImportError:
          import profile
        self._profiler = profile.Profile()
      return self._profiler
    else:
      return None

  def dump_profile(self):
    if self._option_values.twitter_common_app_profiling:
      if self._option_values.twitter_common_app_profile_output:
        self.profiler().dump_stats(self._option_values.twitter_common_app_profile_output)
      else:
        self.profiler().print_stats(sort='time')

  # The thread module provides the interrupt_main() function which does
  # precisely what it says, sending a KeyboardInterrupt to MainThread.  The
  # only problem is that it only delivers the exception while the MainThread
  # is running.  If one does time.sleep(10000000) it will simply block
  # forever.  Sending an actual SIGINT seems to be the only way around this.
  # Of course, applications can trap SIGINT and prevent the quitquitquit
  # handlers from working.
  #
  # Furthermore, the following cannot work:
  #
  # def main():
  #   shutdown_event = threading.Event()
  #   app.register_shutdown_command(lambda rc: shutdown_event.set())
  #   shutdown_event.wait()
  #
  # because threading.Event.wait() is uninterruptible.  This is why
  # abortabortabort is so severe.  An application that traps SIGTERM will
  # render the framework unable to abort it, so SIGKILL is really the only
  # way to be sure to force termination because it cannot be trapped.
  #
  # For the particular case where the bulk of the work is taking place in
  # background threads, use app.wait_forever().
  def quitquitquit(self):
    self._state = self.ABORTING
    os.kill(os.getpid(), signal.SIGINT)

  def abortabortabort(self):
    self._state = self.SHUTDOWN
    os.kill(os.getpid(), signal.SIGKILL)

  def register_shutdown_command(self, command):
    if not callable(command):
      raise TypeError('Shutdown command must be a callable.')
    if self._state >= self.ABORTING:
      raise self.Error('Cannot register a shutdown command while shutting down.')
    self._shutdown_commands.append(command)

  def _wrap_method(self, method, method_name=None):
    method_name = method_name or method.__name__
    try:
      return_code = method()
    except SystemExit as e:
      self._debug_log('%s sys.exited' % method_name)
      return_code = e.code
    except KeyboardInterrupt as e:
      if self._state >= self.ABORTING:
        self._debug_log('%s being shutdown' % method_name)
        return_code = 0
      else:
        self._debug_log('%s exited with ^C' % method_name)
        return_code = self.SIGINT_RETURN_CODE
    except Exception as e:
      return_code = 1
      self._debug_log('%s excepted with %s' % (method_name, type(e)))
      print(traceback.format_exc(), file=sys.stderr)
    return return_code

  @post_initialization
  def _run_main(self, main_method, *args, **kwargs):
    if self.profiler():
      main = lambda: self.profiler().runcall(main_method, *args, **kwargs)
    else:
      main = lambda: main_method(*args, **kwargs)

    self._state = self.RUNNING
    return self._wrap_method(main, method_name='main')

  def _run_shutdown_commands(self, return_code):
    while self._state != self.SHUTDOWN and self._shutdown_commands:
      command = self._shutdown_commands.pop(0)
      command(return_code)

  def _run_module_teardown(self):
    if self._state != self.SHUTDOWN:
      raise self.Error('Expected application to be in SHUTDOWN state!')
    self._debug_log('Shutting application down.')
    self._teardown_modules()
    self._debug_log('Finishing up module teardown.')
    self.dump_profile()

  def _import_module(self, name):
    """
      Import the module, return True on success, False if the import failed.
    """
    try:
      __import__(name)
      return True
    except ImportError:
      return False

  def _validate_main_module(self):
    main_module = Inspection.find_calling_module()
    return main_module == '__main__'

  def _default_command_is_defined(self):
    return None in self._commands

  # Allow for overrides in test
  def _find_main_method(self):
    try:
      return Inspection.find_main_from_caller()
    except Inspection.InternalError:
      pass

  def _get_main_method(self):
    caller_main = self._find_main_method()
    if self._default_command_is_defined() and caller_main is not None:
      print('Error: Cannot define both main and a default command.', file=sys.stderr)
      self._exit_function(1)
      return
    main_method = self._commands.get(self._command) or caller_main
    if main_method is None:
      commands = sorted(self.get_commands())
      if commands:
        print('Must supply one of the following commands:', ', '.join(commands), file=sys.stderr)
      else:
        print('No main() or command defined! Application must define one of these.',
            file=sys.stderr)
    return main_method

  def wait_forever(self):
    """Convenience function to block the application until it is terminated
       by ^C or lifecycle functions."""
    while True:
      time.sleep(0.5)

  def shutdown(self, return_code):
    self._wrap_method(lambda: self._run_shutdown_commands(return_code),
         method_name='shutdown commands')
    self._state = self.SHUTDOWN
    self._run_module_teardown()
    self.quit(return_code)

  def main(self):
    """
      If called from __main__ module, run script's main() method with arguments passed
      and global options parsed.

      The following patterns are acceptable for the main method:
         main()
         main(args)
         main(args, options)
    """
    if not self._validate_main_module():
      # only support if __name__ == '__main__'
      return

    # Pull in modules in twitter.common.app.modules
    if not self._import_module('twitter.common.app.modules'):
      print('Unable to import twitter app modules!', file=sys.stderr)
      self._exit_function(1)
      return

    # defer init as long as possible.
    self.init()

    if self._option_values.twitter_common_app_rc_filename:
      print('RC filename: %s' % self._rc_filename())
      return

    main_method = self._get_main_method()
    if main_method is None:
      self._exit_function(1)
      return

    try:
      argspec = inspect.getargspec(main_method)
    except TypeError as e:
      print('Malformed main(): %s' % e, file=sys.stderr)
      self._exit_function(1)
      return

    if len(argspec.args) == 1:
      args = [self._argv]
    elif len(argspec.args) == 2:
      args = [self._argv, self._option_values]
    else:
      if len(self._argv) != 0:
        print('main() takes no arguments but got leftover arguments: %s!' %
          ' '.join(self._argv), file=sys.stderr)
        self._exit_function(1)
        return
      args = []

    self.shutdown(self._run_main(main_method, *args))

  del post_initialization
  del pre_initialization

########NEW FILE########
__FILENAME__ = inspection
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import inspect
import os
import sys

class Inspection(object):
  class InternalError(Exception): pass

  # TODO(wickman)
  #   Remove all calls to inspect.stack().  This is just bad.  Port everything over
  #   to iterating from currentframe => outer frames.
  @staticmethod
  def find_main_from_caller():
    last_frame = inspect.currentframe()
    while True:
      inspect_frame = last_frame.f_back
      if not inspect_frame:
        break
      if 'main' in inspect_frame.f_locals:
        return inspect_frame.f_locals['main']
      last_frame = inspect_frame
    raise Inspection.InternalError("Unable to detect main from the stack!")

  @staticmethod
  def print_stack_locals(out=sys.stderr):
    stack = inspect.stack()[1:]
    for fr_n in range(len(stack)):
      print('--- frame %s ---\n' % fr_n, file=out)
      for key in stack[fr_n][0].f_locals:
        print('  %s => %s' % (key, stack[fr_n][0].f_locals[key]), file=out)

  @staticmethod
  def find_main_module():
    stack = inspect.stack()[1:]
    for fr_n in range(len(stack)):
      if 'main' in stack[fr_n][0].f_locals:
        return stack[fr_n][0].f_locals['__name__']
    return None

  @staticmethod
  def get_main_locals():
    stack = inspect.stack()[1:]
    for fr_n in range(len(stack)):
      if '__name__' in stack[fr_n][0].f_locals and (
          stack[fr_n][0].f_locals['__name__'] == '__main__'):
        return stack[fr_n][0].f_locals
    return {}

  @staticmethod
  def find_calling_module():
    last_frame = inspect.currentframe()
    while True:
      inspect_frame = last_frame.f_back
      if not inspect_frame:
        break
      if '__name__' in inspect_frame.f_locals:
        return inspect_frame.f_locals['__name__']
      last_frame = inspect_frame
    raise Inspection.InternalError("Unable to interpret stack frame!")

  @staticmethod
  def find_application_name():
    __entry_point__ = None
    locals = Inspection.get_main_locals()
    if '__file__' in locals and locals['__file__'] is not None:
      __entry_point__ = locals['__file__']
    elif '__loader__' in locals:
      from zipimport import zipimporter
      from pkgutil import ImpLoader
      # TODO(wickman) The monkeypatched zipimporter should probably not be a function
      # but instead a properly delegating proxy.
      if hasattr(locals['__loader__'], 'archive'):
        # assuming it ends in .zip or .egg, it may be of package format, so
        # foo-version-py2.6-arch.egg, so split off anything after '-'.
        __entry_point__ = os.path.basename(locals['__loader__'].archive)
        __entry_point__ = __entry_point__.split('-')[0].split('.')[0]
      elif isinstance(locals['__loader__'], ImpLoader):
        __entry_point__ = locals['__loader__'].get_filename()
    else:
      __entry_point__ = '__interpreter__'
    app_name = os.path.basename(__entry_point__)
    return app_name.split('.')[0]

########NEW FILE########
__FILENAME__ = module
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict

from twitter.common.collections import maybe_list
from twitter.common.lang import Compatibility, Singleton
from twitter.common.util import topological_sort, DependencyCycle


class AppModule(Singleton):
  """
    Base class for application module registration:
      - module setup
      - module teardown
      - module dependencies

    If your application needs setup/teardown functionality, override the
    setup_function and teardown_function respectively.
  """

  class Error(Exception): pass
  class Unimplemented(Error): pass
  class DependencyCycle(Error): pass

  _MODULE_REGISTRY = {}
  _MODULE_DEPENDENCIES = defaultdict(set)

  @classmethod
  def module_registry(cls):
    return cls._MODULE_REGISTRY

  @classmethod
  def module_dependencies(cls):
    return cls._MODULE_DEPENDENCIES

  # for testing
  @classmethod
  def clear_registry(cls):
    cls._MODULE_REGISTRY = {}
    cls._MODULE_DEPENDENCIES = defaultdict(set)

  def __init__(self, label, dependencies=None, dependents=None, description=None):
    """
      @label = the label that identifies this module for dependency management
      @dependencies = a string or list of strings of modules this module depends upon (optional)
      @dependents = a string or list of strings of modules that depend upon this module (optional)
      @description = a one-liner describing this application module, e.g. "Logging module"
    """
    self._label = label
    self._description = description
    self._dependencies = maybe_list(dependencies or [])
    self._dependents = maybe_list(dependents or [])
    self._MODULE_REGISTRY[label] = self
    self._MODULE_DEPENDENCIES[label].update(self._dependencies)
    for dependent in self._dependents:
      self._MODULE_DEPENDENCIES[dependent].add(label)
    try:
      list(topological_sort(self._MODULE_DEPENDENCIES))
    except DependencyCycle:
      raise AppModule.DependencyCycle("Found a cycle in app module dependencies!")

  def description(self):
    return self._description

  def label(self):
    return self._label

  def dependencies(self):
    return self._dependencies if self._dependencies else []

  def setup_function(self):
    """
      The setup function for this application module.  If you wish to have a
      setup function, override this method.  This is called by app.main()
      just before running the main method of the application.

      To control the order in which this setup function is run with respect
      to other application modules, you may list a set of module
      dependencies in the AppModule constructor.
    """
    raise AppModule.Unimplemented()

  def teardown_function(self):
    """
      The teardown function for this application module.  This is called if
      the application exits "cleanly":
         if your main() method returns
         if your application calls sys.exit()
         your application gets a SIGINT

      The teardown functions are run in an order compatible with the reverse
      topological order of the setup functions.
    """
    raise AppModule.Unimplemented()

########NEW FILE########
__FILENAME__ = exception_handler
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import sys
from twitter.common import app
from twitter.common.exceptions import BasicExceptionHandler

class AppExceptionHandler(app.Module):
  """
    An application module that logs or scribes uncaught exceptions.
  """

  def __init__(self):
    app.Module.__init__(self, __name__, description="twitter.common.log handler.")

  def setup_function(self):
    self._builtin_hook = sys.excepthook
    def forwarding_handler(*args, **kw):
      BasicExceptionHandler.handle_error(*args, **kw)
      self._builtin_hook(*args, **kw)
    sys.excepthook = forwarding_handler

  def teardown_function(self):
    sys.excepthook = getattr(self, '_builtin_hook', sys.__excepthook__)

########NEW FILE########
__FILENAME__ = http
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import threading

from twitter.common import app, options
from twitter.common.exceptions import ExceptionalThread
from twitter.common.http.diagnostics import DiagnosticsEndpoints
from twitter.common.http.server import HttpServer


class LifecycleEndpoints(object):
  @HttpServer.route('/quitquitquit', method='POST')
  def quitquitquit(self):
    app.quitquitquit()

  @HttpServer.route('/abortabortabort', method='POST')
  def abortabortabort(self):
    app.abortabortabort()


class RootServer(HttpServer, app.Module):
  """
    A root singleton server for all your http endpoints to bind to.
  """

  OPTIONS = {
    'enable':
      options.Option('--enable_http',
          default=False,
          action='store_true',
          dest='twitter_common_http_root_server_enabled',
          help='Enable root http server for various subsystems, e.g. metrics exporting.'),

    'disable_lifecycle':
      options.Option('--http_disable_lifecycle',
          default=False,
          action='store_true',
          dest='twitter_common_http_root_server_disable_lifecycle',
          help='Disable the lifecycle commands, i.e. /quitquitquit and /abortabortabort.'),

    'port':
      options.Option('--http_port',
          default=8888,
          type='int',
          metavar='PORT',
          dest='twitter_common_http_root_server_port',
          help='The port the root http server will be listening on.'),

    'host':
      options.Option('--http_host',
          default='localhost',
          type='string',
          metavar='HOSTNAME',
          dest='twitter_common_http_root_server_host',
          help='The host the root http server will be listening on.'),

    'framework':
      options.Option('--http_framework',
          default='wsgiref',
          type='string',
          metavar='FRAMEWORK',
          dest='twitter_common_http_root_server_framework',
          help='The framework that will be running the integrated http server.')
  }

  def __init__(self):
    self._thread = None
    HttpServer.__init__(self)
    app.Module.__init__(self, __name__, description="Http subsystem.")

  def setup_function(self):
    assert self._thread is None, "Attempting to call start() after server has been started!"
    options = app.get_options()
    parent = self

    self.mount_routes(DiagnosticsEndpoints())
    if not options.twitter_common_http_root_server_disable_lifecycle:
      self.mount_routes(LifecycleEndpoints())

    class RootServerThread(ExceptionalThread):
      def __init__(self):
        super(RootServerThread, self).__init__()
        self.daemon = True

      def run(self):
        rs = parent
        rs.run(options.twitter_common_http_root_server_host,
               options.twitter_common_http_root_server_port,
               server=options.twitter_common_http_root_server_framework)

    if options.twitter_common_http_root_server_enabled:
      self._thread = RootServerThread()
      self._thread.start()

########NEW FILE########
__FILENAME__ = scribe_exception_handler
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import sys
from twitter.common import app, options
from twitter.common.exceptions import BasicExceptionHandler

from scribe import scribe
from thrift.transport import TTransport, TSocket
from thrift.protocol import TBinaryProtocol

class AppScribeExceptionHandler(app.Module):
  """
    An application module that logs or scribes uncaught exceptions.
  """

  OPTIONS = {
    'port':
      options.Option('--scribe_exception_port',
          default=1463,
          type='int',
          metavar='PORT',
          dest='twitter_common_scribe_port',
          help='The port on which scribe aggregator listens.'),
    'host':
      options.Option('--scribe_exception_host',
          default='localhost',
          type='string',
          metavar='HOSTNAME',
          dest='twitter_common_scribe_host',
          help='The host to which scribe exceptions should be written.'),
    'category':
      options.Option('--scribe_exception_category',
          default='python_default',
          type='string',
          metavar='CATEGORY',
          dest='twitter_common_scribe_category',
          help='The scribe category into which we write exceptions.')
  }


  def __init__(self):
    app.Module.__init__(self, __name__, description="twitter.common.log handler.")

  def setup_function(self):
    self._builtin_hook = sys.excepthook
    def forwarding_handler(*args, **kw):
      AppScribeExceptionHandler.scribe_error(*args, **kw)
      self._builtin_hook(*args, **kw)
    sys.excepthook = forwarding_handler

  def teardown_function(self):
    sys.excepthook = getattr(self, '_builtin_hook', sys.__excepthook__)

  @staticmethod
  def log_error(msg):
    try:
      from twitter.common import log
      log.error(msg)
    except ImportError:
      sys.stderr.write(msg + '\n')

  @staticmethod
  def scribe_error(*args, **kw):
    options = app.get_options()
    socket = TSocket.TSocket(host=options.twitter_common_scribe_host,
                             port=options.twitter_common_scribe_port)
    transport = TTransport.TFramedTransport(socket)
    protocol = TBinaryProtocol.TBinaryProtocol(trans=transport, strictRead=False, strictWrite=False)
    client = scribe.Client(iprot=protocol, oprot=protocol)
    value = BasicExceptionHandler.format(*args, **kw)
    log_entry = scribe.LogEntry(category=options.twitter_common_scribe_category,
      message=value)

    try:
      transport.open()
      result = client.Log(messages=[log_entry])
      transport.close()
      if result != scribe.ResultCode.OK:
        AppScribeExceptionHandler.log_error('Failed to scribe exception!')
    except TTransport.TTransportException:
      AppScribeExceptionHandler.log_error('Could not connect to scribe!')

########NEW FILE########
__FILENAME__ = serverset
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import sys
import threading

from twitter.common import app, options

try:
  from twitter.common import log
except ImportError:
  import logging as log

class ParseError(Exception): pass

def add_port_to(option_name):
  def add_port_callback(option, opt, value, parser):
    if not getattr(parser.values, option_name, None):
      setattr(parser.values, option_name, {})
    try:
      name, port = value.split(':')
    except (ValueError, TypeError):
      raise ParseError('Invalid value for %s: %s should be of form NAME:PORT' % (
        opt, value))
    try:
      port = int(port)
    except ValueError:
      raise ParseError('Port does not appear to be an integer: %s' % port)
    getattr(parser.values, option_name)[name] = port
  return add_port_callback


def set_bool(option, opt_str, value, parser):
  setattr(parser.values, option.dest, not opt_str.startswith('--no'))


class ServerSetModule(app.Module):
  """
    Binds this application to a Zookeeper ServerSet.
  """
  OPTIONS = {
    'serverset-enable': options.Option('--serverset-enable',
        default=False, action='store_true', dest='serverset_module_enable',
        help='Enable the ServerSet module.  Requires --serverset-path and --serverset-primary.'),
    'serverset-ensemble': options.Option('--serverset-ensemble',
        default='zookeeper.local.twitter.com:2181', dest='serverset_module_ensemble',
        metavar='HOST[:PORT]',
        help='The serverset ensemble to talk to.  HOST or HOST:PORT pair.  If the HOST is a RR DNS '
             'record, we fan out to the entire ensemble.  If no port is specified, 2181 assumed.'),
    'serverset-path': options.Option('--serverset-path',
        default=None, dest='serverset_module_path', metavar='PATH', type='str',
        help='The serverset path to join, preferably /twitter/service/(role)/(service)/(env) '
             'where env is prod, staging, devel.'),
    'serverset-primary': options.Option('--serverset-primary',
        type='int', metavar='PORT', dest='serverset_module_primary_port', default=None,
        help='Port on which to bind the primary endpoint.'),
    'serverset-shard-id': options.Option('--serverset-shard-id',
        type='int', metavar='INT', dest='serverset_module_shard_id', default=None,
        help='Shard id to assign this serverset entry.'),
    'serverset-extra': options.Option('--serverset-extra',
        default={}, type='string', nargs=1, action='callback', metavar='NAME:PORT',
        callback=add_port_to('serverset_module_extra'), dest='serverset_module_extra',
        help='Additional endpoints to bind.  Format NAME:PORT.  May be specified multiple times.'),
    'serverset-persistence': options.Option('--serverset-persistence', '--no-serverset-persistence',
        action='callback', callback=set_bool, dest='serverset_module_persistence', default=True,
        help='If serverset persistence is enabled, if the serverset connection is dropped for any '
             'reason, we will retry to connect forever.  If serverset persistence is turned off, '
             'the application will commit seppuku -- sys.exit(1) -- upon session disconnection.'),
  }

  def __init__(self):
    app.Module.__init__(self, __name__, description="ServerSet module")
    self._zookeeper = None
    self._serverset = None
    self._membership = None
    self._join_args = None
    self._torndown = False
    self._rejoin_event = threading.Event()
    self._joiner = None

  @property
  def serverset(self):
    return self._serverset

  @property
  def zh(self):
    if self._zookeeper:
      return self._zookeeper._zh

  def _assert_valid_inputs(self, options):
    if not options.serverset_module_enable:
      return

    assert options.serverset_module_path is not None, (
        'If serverset module enabled, serverset path must be specified.')
    assert options.serverset_module_primary_port is not None, (
        'If serverset module enabled, serverset primary port must be specified.')
    assert isinstance(options.serverset_module_extra, dict), (
        'Serverset additional endpoints must be a dictionary!')
    for name, value in options.serverset_module_extra.items():
      assert isinstance(name, str), 'Additional endpoints must be named by strings!'
      assert isinstance(value, int), 'Additional endpoint ports must be integers!'

    try:
      primary_port = int(options.serverset_module_primary_port)
    except ValueError as e:
      raise ValueError('Could not parse serverset primary port: %s' % e)

  def _construct_serverset(self, options):
    import socket
    import threading
    import zookeeper
    from twitter.common.zookeeper.client import ZooKeeper
    from twitter.common.zookeeper.serverset import Endpoint, ServerSet
    log.debug('ServerSet module constructing serverset.')

    hostname = socket.gethostname()
    primary_port = int(options.serverset_module_primary_port)
    primary = Endpoint(hostname, primary_port)
    additional = dict((port_name, Endpoint(hostname, port_number))
        for port_name, port_number in options.serverset_module_extra.items())

    # TODO(wickman) Add timeout parameterization here.
    self._zookeeper = ZooKeeper(options.serverset_module_ensemble)
    self._serverset = ServerSet(self._zookeeper, options.serverset_module_path)
    self._join_args = (primary, additional)
    self._join_kwargs = ({'shard': options.serverset_module_shard_id}
                         if options.serverset_module_shard_id else {})

  def _join(self):
    log.debug('ServerSet module joining serverset.')
    primary, additional = self._join_args
    self._membership = self._serverset.join(primary, additional, expire_callback=self.on_expiration,
        **self._join_kwargs)

  def on_expiration(self):
    if self._torndown:
      return

    log.debug('Serverset session expired.')
    if not app.get_options().serverset_module_persistence:
      log.debug('Committing seppuku...')
      sys.exit(1)
    else:
      log.debug('Rejoining...')

    self._rejoin_event.set()

  def setup_function(self):
    options = app.get_options()
    if options.serverset_module_enable:
      self._assert_valid_inputs(options)
      self._construct_serverset(options)
      self._thread = ServerSetJoinThread(self._rejoin_event, self._join)
      self._thread.start()
      self._rejoin_event.set()

  def teardown_function(self):
    self._torndown = True
    if self._membership:
      self._serverset.cancel(self._membership)
      self._zookeeper.stop()


class ServerSetJoinThread(threading.Thread):
  """
    A thread to maintain serverset session.
  """
  def __init__(self, event, joiner):
    self._event = event
    self._joiner = joiner
    threading.Thread.__init__(self)
    self.daemon = True

  def run(self):
    while True:
      self._event.wait()
      log.debug('Join event triggered, joining serverset.')
      self._event.clear()
      self._joiner()

########NEW FILE########
__FILENAME__ = varz
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# NOTE, this is called "varz" instead of "vars" because "vars" is a reserved
# word in Python.  All characters appearing in this work are fictitious.
# Any resemblance to real persons, living or dead, is purely coincidental.

from functools import wraps
import os
import sys
import time

from twitter.common import app, options
from twitter.common.http import HttpServer, Plugin
from twitter.common.metrics import (
  AtomicGauge,
  Label,
  LambdaGauge,
  MetricSampler,
  MutatorGauge,
  Observable,
  RootMetrics,
)
from twitter.common.quantity import Amount, Time

from .http import RootServer


try:
  from twitter.common.python.pex import PEX
  HAS_PEX = True
except ImportError:
  HAS_PEX = False


def set_bool(option, opt_str, value, parser):
  setattr(parser.values, option.dest, not opt_str.startswith('--no'))



class VarsSubsystem(app.Module):
  """
    Exports a /vars endpoint on the root http server bound to twitter.common.metrics.RootMetrics.
  """
  OPTIONS = {
    'sampling_delay':
      options.Option('--vars-sampling-delay-ms',
          default=1000,
          type='int',
          metavar='MILLISECONDS',
          dest='twitter_common_metrics_vars_sampling_delay_ms',
          help='How long between taking samples of the vars subsystem.'),

    'trace_endpoints':
      options.Option('--vars-trace-endpoints', '--no-vars-trace-endpoints',
          default=True,
          action='callback',
          callback=set_bool,
          dest='twitter_common_app_modules_varz_trace_endpoints',
          help='Trace all registered http endpoints in this application.'),

    'trace_namespace':
      options.Option('--trace-namespace',
          default='http',
          dest='twitter_common_app_modules_varz_trace_namespace',
          help='The prefix for http request metrics.')
  }

  def __init__(self):
    app.Module.__init__(self, __name__, description='Vars subsystem',
                        dependencies='twitter.common.app.modules.http')

  def setup_function(self):
    options = app.get_options()
    rs = RootServer()
    if rs:
      varz = VarsEndpoint(period = Amount(
        options.twitter_common_metrics_vars_sampling_delay_ms, Time.MILLISECONDS))
      rs.mount_routes(varz)
      register_diagnostics()
      register_build_properties()
      if options.twitter_common_app_modules_varz_trace_endpoints:
        plugin = EndpointTracePlugin()
        rs.install(plugin)
        RootMetrics().register_observable(
            options.twitter_common_app_modules_varz_trace_namespace,
            plugin)


class VarsEndpoint(object):
  """
    Wrap a MetricSampler to export the /vars endpoint for applications that register
    exported variables.
  """

  def __init__(self, period=None):
    self._metrics = RootMetrics()
    if period is not None:
      self._monitor = MetricSampler(self._metrics, period)
    else:
      self._monitor = MetricSampler(self._metrics)
    self._monitor.start()

  @HttpServer.route("/vars")
  @HttpServer.route("/vars/:var")
  def handle_vars(self, var=None):
    HttpServer.set_content_type('text/plain; charset=iso-8859-1')
    samples = self._monitor.sample()

    if var is None:
      return '\n'.join(
        '%s %s' % (key, val) for key, val in sorted(samples.items()))
    else:
      if var in samples:
        return samples[var]
      else:
        HttpServer.abort(404, 'Unknown exported variable')

  @HttpServer.route("/vars.json")
  def handle_vars_json(self, var=None, value=None):
    return self._monitor.sample()

  def shutdown(self):
    self._monitor.shutdown()
    self._monitor.join()


class StatusStats(Observable):
  def __init__(self):
    self._count = AtomicGauge('count')
    self._ns = AtomicGauge('total_ns')
    self.metrics.register(self._count)
    self.metrics.register(self._ns)

  def increment(self, ns):
    self._count.increment()
    self._ns.add(ns)


class EndpointTracePlugin(Observable, Plugin):
  def setup(self, app):
    self._stats = dict((k, StatusStats()) for k in (1, 2, 3, 4, 5))
    for code_prefix, observable in self._stats.items():
      self.metrics.register_observable('%dxx' % code_prefix, observable)

  def apply(self, callback, route):
    @wraps(callback)
    def wrapped_callback(*args, **kw):
      start = time.time()
      body = callback(*args, **kw)
      ns = int((time.time() - start) * 1e9)
      observable = self._stats.get(HttpServer.response.status_code / 100)
      if observable:
        observable.increment(ns)
      return body
    return wrapped_callback


def register_diagnostics():
  rm = RootMetrics().scope('sys')
  now = time.time()
  rm.register(LambdaGauge('uptime', lambda: time.time() - now))
  rm.register(Label('argv', repr(sys.argv)))
  rm.register(Label('path', repr(sys.path)))
  rm.register(Label('version', sys.version))
  rm.register(Label('platform', sys.platform))
  rm.register(Label('executable', sys.executable))
  rm.register(Label('prefix', sys.prefix))
  rm.register(Label('exec_prefix', sys.exec_prefix))
  rm.register(Label('uname', ' '.join(os.uname())))


def register_build_properties():
  if not HAS_PEX:
    return
  rm = RootMetrics().scope('build')
  try:
    build_properties = PEX().info.build_properties
  except PEX.NotFound:
    return
  for key, value in build_properties.items():
    rm.register(Label(str(key), str(value)))

########NEW FILE########
__FILENAME__ = ordereddict
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# Julpan comments -- START
# NOTE(ugo): Copied this from http://code.activestate.com/recipes/576693/
# NOTE(ugo): I ported the last few methods from the Python 3.2 library.
# Julpan comments -- END

# Backport of OrderedDict() class that runs on Python 2.4, 2.5, 2.6, 2.7 and pypy.
# Passes Python2.7's test suite and incorporates all the latest updates.

try:
    from thread import get_ident as _get_ident
except ImportError:
    try:
         from dummy_thread import get_ident as _get_ident
    except ImportError:
         from _dummy_thread import get_ident as _get_ident

try:
    from _abcoll import KeysView, ValuesView, ItemsView
except ImportError:
    pass


class OrderedDict(dict):
    'Dictionary that remembers insertion order'
    # An inherited dict maps keys to values.
    # The inherited dict provides __getitem__, __len__, __contains__, and get.
    # The remaining methods are order-aware.
    # Big-O running times for all methods are the same as for regular dictionaries.

    # The internal self.__map dictionary maps keys to links in a doubly linked list.
    # The circular doubly linked list starts and ends with a sentinel element.
    # The sentinel element never gets deleted (this simplifies the algorithm).
    # Each link is stored as a list of length three:  [PREV, NEXT, KEY].

    def __init__(self, *args, **kwds):
        '''Initialize an ordered dictionary.  Signature is the same as for
        regular dictionaries, but keyword arguments are not recommended
        because their insertion order is arbitrary.

        '''
        if len(args) > 1:
            raise TypeError('expected at most 1 arguments, got %d' % len(args))
        try:
            self.__root
        except AttributeError:
            self.__root = root = []                     # sentinel node
            root[:] = [root, root, None]
            self.__map = {}
        self.__update(*args, **kwds)

    def __setitem__(self, key, value, dict_setitem=dict.__setitem__):
        'od.__setitem__(i, y) <==> od[i]=y'
        # Setting a new item creates a new link which goes at the end of the linked
        # list, and the inherited dictionary is updated with the new key/value pair.
        if key not in self:
            root = self.__root
            last = root[0]
            last[1] = root[0] = self.__map[key] = [last, root, key]
        dict_setitem(self, key, value)

    def __delitem__(self, key, dict_delitem=dict.__delitem__):
        'od.__delitem__(y) <==> del od[y]'
        # Deleting an existing item uses self.__map to find the link which is
        # then removed by updating the links in the predecessor and successor nodes.
        dict_delitem(self, key)
        link_prev, link_next, key = self.__map.pop(key)
        link_prev[1] = link_next
        link_next[0] = link_prev

    def __iter__(self):
        'od.__iter__() <==> iter(od)'
        root = self.__root
        curr = root[1]
        while curr is not root:
            yield curr[2]
            curr = curr[1]

    def __reversed__(self):
        'od.__reversed__() <==> reversed(od)'
        root = self.__root
        curr = root[0]
        while curr is not root:
            yield curr[2]
            curr = curr[0]

    def clear(self):
        'od.clear() -> None.  Remove all items from od.'
        try:
            for node in self.__map.itervalues():
                del node[:]
            root = self.__root
            root[:] = [root, root, None]
            self.__map.clear()
        except AttributeError:
            pass
        dict.clear(self)

    def popitem(self, last=True):
        '''od.popitem() -> (k, v), return and remove a (key, value) pair.
        Pairs are returned in LIFO order if last is true or FIFO order if false.

        '''
        if not self:
            raise KeyError('dictionary is empty')
        root = self.__root
        if last:
            link = root[0]
            link_prev = link[0]
            link_prev[1] = root
            root[0] = link_prev
        else:
            link = root[1]
            link_next = link[1]
            root[1] = link_next
            link_next[0] = root
        key = link[2]
        del self.__map[key]
        value = dict.pop(self, key)
        return key, value

    # -- the following methods do not depend on the internal structure --

    def keys(self):
        'od.keys() -> list of keys in od'
        return list(self)

    def values(self):
        'od.values() -> list of values in od'
        return [self[key] for key in self]

    def items(self):
        'od.items() -> list of (key, value) pairs in od'
        return [(key, self[key]) for key in self]

    def iterkeys(self):
        'od.iterkeys() -> an iterator over the keys in od'
        return iter(self)

    def itervalues(self):
        'od.itervalues -> an iterator over the values in od'
        for k in self:
            yield self[k]

    def iteritems(self):
        'od.iteritems -> an iterator over the (key, value) items in od'
        for k in self:
            yield (k, self[k])

    def update(*args, **kwds):
        '''od.update(E, **F) -> None.  Update od from dict/iterable E and F.

        If E is a dict instance, does:           for k in E: od[k] = E[k]
        If E has a .keys() method, does:         for k in E.keys(): od[k] = E[k]
        Or if E is an iterable of items, does:   for k, v in E: od[k] = v
        In either case, this is followed by:     for k, v in F.items(): od[k] = v

        '''
        if len(args) > 2:
            raise TypeError('update() takes at most 2 positional '
                            'arguments (%d given)' % (len(args),))
        elif not args:
            raise TypeError('update() takes at least 1 argument (0 given)')
        self = args[0]
        # Make progressively weaker assumptions about "other"
        other = ()
        if len(args) == 2:
            other = args[1]
        if isinstance(other, dict):
            for key in other:
                self[key] = other[key]
        elif hasattr(other, 'keys'):
            for key in other.keys():
                self[key] = other[key]
        else:
            for key, value in other:
                self[key] = value
        for key, value in kwds.items():
            self[key] = value

    __update = update  # let subclasses override update without breaking __init__

    __marker = object()

    def pop(self, key, default=__marker):
        '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.
        If key is not found, d is returned if given, otherwise KeyError is raised.

        '''
        if key in self:
            result = self[key]
            del self[key]
            return result
        if default is self.__marker:
            raise KeyError(key)
        return default

    def setdefault(self, key, default=None):
        'od.setdefault(k[,d]) -> od.get(k,d), also set od[k]=d if k not in od'
        if key in self:
            return self[key]
        self[key] = default
        return default

    def __repr__(self, _repr_running={}):
        'od.__repr__() <==> repr(od)'
        call_key = id(self), _get_ident()
        if call_key in _repr_running:
            return '...'
        _repr_running[call_key] = 1
        try:
            if not self:
                return '%s()' % (self.__class__.__name__,)
            return '%s(%r)' % (self.__class__.__name__, self.items())
        finally:
            del _repr_running[call_key]

    def __reduce__(self):
        'Return state information for pickling'
        items = [[k, self[k]] for k in self]
        inst_dict = vars(self).copy()
        for k in vars(OrderedDict()):
            inst_dict.pop(k, None)
        if inst_dict:
            return (self.__class__, (items,), inst_dict)
        return self.__class__, (items,)

    def copy(self):
        'od.copy() -> a shallow copy of od'
        return self.__class__(self)

    @classmethod
    def fromkeys(cls, iterable, value=None):
        '''OD.fromkeys(S[, v]) -> New ordered dictionary with keys from S
        and values equal to v (which defaults to None).

        '''
        d = cls()
        for key in iterable:
            d[key] = value
        return d

    def __eq__(self, other):
        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive
        while comparison to a regular mapping is order-insensitive.

        '''
        if isinstance(other, OrderedDict):
            return len(self)==len(other) and self.items() == other.items()
        return dict.__eq__(self, other)

    def __ne__(self, other):
        return not self == other

    # -- the following methods are only used in Python 2.7 --

    def viewkeys(self):
        "od.viewkeys() -> a set-like object providing a view on od's keys"
        return KeysView(self)

    def viewvalues(self):
        "od.viewvalues() -> an object providing a view on od's values"
        return ValuesView(self)

    def viewitems(self):
        "od.viewitems() -> a set-like object providing a view on od's items"
        return ItemsView(self)

    # NOTE(ugo): I ported this method from the Python3.2 source.
    def move_to_end(self, key, last=True):
        '''Move an existing element to the end (or beginning if last==False).

        Raises KeyError if the element does not exist.
        When last=True, acts like a fast version of self[key]=self.pop(key).

        '''
        link_prev, link_next, key = link = self.__map[key]
        link_prev[1] = link_next
        link_next[0] = link_prev
        root = self.__root
        if last:
            last = root[0]
            link[0] = last
            link[1] = root
            last[1] = root[0] = link
        else:
            first = root[1]
            link[0] = root
            link[1] = first
            root[1] = first[0] = link

    # NOTE(ugo): I ported this method from the Python3.2 source.
    def popitem(self, last=True):
        '''od.popitem() -> (k, v), return and remove a (key, value) pair.
        Pairs are returned in LIFO order if last is true or FIFO order if false.

        '''
        if not self:
            raise KeyError('dictionary is empty')
        root = self.__root
        if last:
            link = root[0]
            link_prev = link[0]
            link_prev[1] = root
            root[0] = link_prev
        else:
            link = root[1]
            link_next = link[1]
            root[1] = link_next
            link_next[0] = root
        key = link[2]
        del self.__map[key]
        value = dict.pop(self, key)
        return key, value

########NEW FILE########
__FILENAME__ = orderedset
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================
# OrderedSet recipe referenced in the Python standard library docs (bottom):
#     http://docs.python.org/library/collections.html
#
# Copied from recipe code found here: http://code.activestate.com/recipes/576694/ with small
# modifications
#

import collections


class OrderedSet(collections.MutableSet):
  KEY, PREV, NEXT = range(3)

  def __init__(self, iterable=None):
    self.end = end = []
    end += [None, end, end]         # sentinel node for doubly linked list
    self.map = {}                   # key --> [key, prev, next]
    if iterable is not None:
      self |= iterable

  def __len__(self):
    return len(self.map)

  def __contains__(self, key):
    return key in self.map

  def add(self, key):
    if key not in self.map:
      end = self.end
      curr = end[self.PREV]
      curr[self.NEXT] = end[self.PREV] = self.map[key] = [key, curr, end]

  def update(self, iterable):
    for key in iterable:
      self.add(key)

  def discard(self, key):
    if key in self.map:
      key, prev, next = self.map.pop(key)
      prev[self.NEXT] = next
      next[self.PREV] = prev

  def __iter__(self):
    end = self.end
    curr = end[self.NEXT]
    while curr is not end:
      yield curr[self.KEY]
      curr = curr[self.NEXT]

  def __reversed__(self):
    end = self.end
    curr = end[self.PREV]
    while curr is not end:
      yield curr[self.KEY]
      curr = curr[self.PREV]

  def pop(self, last=True):
    if not self:
      raise KeyError('set is empty')
    key = next(reversed(self)) if last else next(iter(self))
    self.discard(key)
    return key

  def __repr__(self):
    if not self:
      return '%s()' % (self.__class__.__name__,)
    return '%s(%r)' % (self.__class__.__name__, list(self))

  def __eq__(self, other):
    if isinstance(other, OrderedSet):
      return len(self) == len(other) and list(self) == list(other)
    return set(self) == set(other)

  def __del__(self):
    self.clear()                    # remove circular references

########NEW FILE########
__FILENAME__ = ringbuffer
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

class RingBuffer(list):
  """List-based, capped-length circular buffer container.

  While this behaves similarly to a collections.deque(maxlen), the chief
  advantage is O(1) characteristics for random access.

    >>> from twitter.common.collections import RingBuffer
    >>> rr = RingBuffer(5)
    >>> rr
    RingBuffer([], size=5)
    >>> for i in xrange(0, 5):
    ...   rr.append(i)
    ...
    >>> rr
    RingBuffer([0, 1, 2, 3, 4], size=5)
    >>> for i in xrange(5, 8):
    ...   rr.append(i)
    ...
    >>> rr
    RingBuffer([3, 4, 5, 6, 7], size=5)
    >>> rr[1]
    4
    >>> rr[-2]
    6

  """

  __slots__ = ('_zero', '_size', '_count')

  class InvalidOperation(Exception): pass

  def __init__(self, size=1, iv=None):
    if not isinstance(size, int) or not size >= 1:
      raise ValueError('Size must be an integer >= 1')
    if iv is not None:
      super(RingBuffer, self).__init__([iv] * size)
      self._count = size
    else:
      self._count = 0
    self._zero = 0
    self._size = size

  def __index(self, key):
    if not self._count:
      raise IndexError('list index out of range')
    return (key + self._zero) % self._count

  def append(self, value):
    if self._count < self._size:
      super(RingBuffer, self).append(value)
      self._count += 1
    else:
      super(RingBuffer, self).__setitem__(self._zero % self._size, value)
      self._zero += 1

  def __getitem__(self, key):
    return super(RingBuffer, self).__getitem__(self.__index(key))

  def __setitem__(self, key, value):
    return super(RingBuffer, self).__setitem__(self.__index(key), value)

  def __delitem__(self, key):
    raise self.InvalidOperation('Cannot delete in a RingBuffer.')

  def __str__(self):
    return str(list(iter(self)))

  def __repr__(self):
    return "RingBuffer(size=%s, %s)" % (self._size, str(self))

  def __iter__(self):
    for x in xrange(0, self._count):
      yield self[x]

########NEW FILE########
__FILENAME__ = deadline
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from Queue import Queue, Empty
from threading import Thread

from twitter.common.exceptions import ExceptionalThread
from twitter.common.lang import Compatibility
from twitter.common.quantity import Amount, Time


class Timeout(Exception):
  pass


def deadline(closure, timeout=Amount(150, Time.MILLISECONDS), daemon=False, propagate=False):
  """Run a closure with a timeout, raising an exception if the timeout is exceeded.

    args:
      closure   - function to be run (e.g. functools.partial, or lambda)
    kwargs:
      timeout   - in seconds, or Amount of Time, [default: Amount(150, Time.MILLISECONDS]
      daemon    - booleanish indicating whether to daemonize the thread used to run the closure
                  (otherwise, a timed-out closure can potentially exist beyond the life of the
                  calling thread) [default: False]
      propagate - booleanish indicating whether to re-raise exceptions thrown by the closure
                  [default: False]
  """
  if isinstance(timeout, Compatibility.numeric):
    pass
  elif isinstance(timeout, Amount) and isinstance(timeout.unit(), Time):
    timeout = timeout.as_(Time.SECONDS)
  else:
    raise ValueError('timeout must be either numeric or Amount of Time.')
  q = Queue(maxsize=1)
  class AnonymousThread(Thread):
    def __init__(self):
      super(AnonymousThread, self).__init__()
      self.daemon = bool(daemon)
    def run(self):
      try:
        result = closure()
      except Exception as result:
        if not propagate:
          # conform to standard behaviour of an exception being raised inside a Thread
          raise result
      q.put(result)
  AnonymousThread().start()
  try:
    result = q.get(timeout=timeout)
  except Empty:
    raise Timeout("Timeout exceeded!")
  else:
    if propagate and isinstance(result, Exception):
      raise result
    return result

########NEW FILE########
__FILENAME__ = deferred
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import time

from twitter.common.exceptions import ExceptionalThread
from twitter.common.lang import Compatibility
from twitter.common.quantity import Amount, Time


class Deferred(ExceptionalThread):
  """
    Wrapper for a delayed closure.
  """
  def __init__(self, closure, delay=Amount(0, Time.SECONDS), clock=time):
    super(Deferred, self).__init__()
    self._closure = closure
    if isinstance(delay, Compatibility.numeric):
      self._delay = delay
    elif isinstance(delay, Amount) and isinstance(delay.unit(), Time):
      self._delay = delay.as_(Time.SECONDS)
    else:
      raise ValueError('Deferred must take a numeric or Amount of Time.')
    self._clock = clock
    self._initialized = clock.time()
    self.daemon = True

  def run(self):
    self._clock.sleep(self._delay)
    self._closure()


def defer(closure, **kw):
  """Run a closure with a specified delay on its own thread.

    Args:
      closure (function)
    Keyword args:
      delay (in seconds, or Amount of Time, default 0)
      clock (the clock to use for time() and sleep(), default time)
  """

  Deferred(closure, **kw).start()

########NEW FILE########
__FILENAME__ = event_muxer
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from Queue import Empty, Queue
import threading

class EventMuxer(object):
  """Mux multiple threading.Events and trigger if any of them are set.

  This class is primarily of interest in the situation where multiple Events could trigger an
  action, but the specific one is not of interest.

  wait() _does not_ support re-entry; new objects should be instantiated as needed.

  Usage:
    >>> from twitter.common.concurrent import EventMuxer
    >>> e1, e2 = threading.Event(), threading.Event()
    # will block until e1 or e2 is set or timeout expires:
    >>> EventMuxer(e1, e2).wait(timeout=5)
    # will block indefinitely until e1 or e2 is set:
    >>> EventMuxer(e1, e2).wait()

  """
  class WaitThread(threading.Thread):
    """Wait on an event, and update a parent when the event occurs """
    def __init__(self, event, parentq):
      self.event = event
      self.parentq = parentq
      super(EventMuxer.WaitThread, self).__init__()
      self.daemon = True
    def run(self):
      self.parentq.put(self.event.wait(timeout=self.timeout))

  def __init__(self, *events):
    if not all(isinstance(arg, threading._Event) for arg in events):
      raise ValueError("arguments must be threading.Events()!")
    self._lock = threading.Lock()
    self._queue = Queue()
    self._wait_events = [self.WaitThread(event, self._queue) for event in events]
    self._started = False
    self._finished = False

  def wait(self, timeout=None):
    """ Wait until any of the dependent events are set, or the timeout expires

    While it should be usable across threads, there are two caveats:
      - Only the initial wait() call will pass on the timeout to the dependent events. (Subsequent
        EventMuxer.wait() calls will still time out as expected, but the original threads may
        potentially be longer-lived.)
      - This function does not support re-entry after any previous wait() call has returned (due to
        timeout or dependent event being set). Instantiate new EventMuxers as needed.

    Note: in Python <2.7, threading.Event.wait(timeout) does not indicate on return whether or not
    the timeout was reached. In this scenario, EventMuxer.wait() will always return False.

    In Python >=2.7, EventMuxer.wait() should return as per Event.wait():
      - True indicates one or more of the dependent events were set
      - False indicates that the timeout occurred before any of the events were set
    """
    with self._lock:
      if self._finished:
        raise RuntimeError("wait() does not support re-entry!")
      if not self._started:
        for thread in self._wait_events:
          thread.timeout = timeout
          thread.start()
        self._started = True
    try:
      if self._queue.get(timeout=timeout):
        return True
      return False
    except Empty:
      return False
    finally:
      with self._lock:
        self._finished = True

########NEW FILE########
__FILENAME__ = properties
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import re
from twitter.common.lang import Compatibility

from twitter.common.collections import OrderedDict

class Properties(object):
  """
    A Python reader for java.util.Properties formatted data as oulined here:
    http://download.oracle.com/javase/6/docs/api/java/util/Properties.html#load(java.io.Reader)
  """

  @staticmethod
  def load(data):
    """
      Loads properties from an open stream or the contents of a string and returns a dict of the
      parsed property data.
    """

    if hasattr(data, 'read') and callable(data.read):
      contents = data.read()
    elif isinstance(data, Compatibility.string):
      contents = data
    else:
      raise TypeError('Can only process data from a string or a readable object, given: %s' % data)

    return Properties._parse(contents.splitlines())


  # An unescaped '=' or ':' forms an explicit separator
  _EXPLICIT_KV_SEP = re.compile(r'(?<!\\)[=:]')


  @staticmethod
  def _parse(lines):
    def coalesce_lines():
      line_iter = iter(lines)
      try:
        buffer = ''
        while True:
          line = next(line_iter)
          if line.strip().endswith('\\'):
            # Continuation.
            buffer += line.strip()[:-1]
          else:
            if buffer:
              # Continuation join, preserve left hand ws (could be a kv separator)
              buffer += line.rstrip()
            else:
              # Plain old line
              buffer = line.strip()

            try:
              yield buffer
            finally:
              buffer = ''
      except StopIteration:
        pass

    def normalize(atom):
      return re.sub(r'\\([:=\s])', r'\1', atom.strip())

    def parse_line(line):
      if line and not (line.startswith('#') or line.startswith('!')):
        match = Properties._EXPLICIT_KV_SEP.search(line)
        if match:
          return normalize(line[:match.start()]), normalize(line[match.end():])
        else:
          space_sep = line.find(' ')
          if space_sep == -1:
            return normalize(line), ''
          else:
            return normalize(line[:space_sep]), normalize(line[space_sep:])

    props = OrderedDict()
    for line in coalesce_lines():
      kv_pair = parse_line(line)
      if kv_pair:
        key, value = kv_pair
        props[key] = value
    return props

  @staticmethod
  def dump(props, output):
    """Dumps a dict of properties to the specified open stream or file path."""
    def escape(token):
      return re.sub(r'([=:\s])', r'\\\1', token)

    def write(out):
      for k, v in props.items():
        out.write('%s=%s\n' % (escape(str(k)), escape(str(v))))

    if hasattr(output, 'write') and callable(output.write):
      write(output)
    elif isinstance(output, Compatibility.string):
      with open(output, 'w+a') as out:
        write(out)
    else:
      raise TypeError('Can only dump data to a path or a writable object, given: %s' % output)

########NEW FILE########
__FILENAME__ = confluence
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""Code to ease publishing text to Confluence wikis."""

import getpass
import mimetypes
import urllib

from twitter.common import log

from os.path import basename


try:
  from xmlrpclib import ServerProxy, Error as XMLRPCError, Binary
except ImportError:
  from xmlrpc.client import ServerProxy, Error as XMLRPCError, Binary

mimetypes.init()

class ConfluenceError(Exception):
  """Indicates a problem performing an action with confluence."""


class Confluence(object):
  """Interface for fetching and storing data in confluence."""

  def __init__(self, api_entrypoint, server_url, session_token, content_format='markdown'):
    """Initialize with an established confluence connection."""
    self._api_entrypoint = api_entrypoint
    self._server_url = server_url
    self._session_token = session_token
    self._content_format = content_format

  @staticmethod
  def login(confluence_url, user=None, api_entrypoint='confluence2'):
    """Prompts the user to log in to confluence, and returns a Confluence object.

    confluence_url: Base url of wiki, e.g. https://confluence.atlassian.com/
    user: Username
    api_entrypoint: one of 'confluence1' or None (Confluence 3.x) [default],
                           'confluence2' (Confluence 4.x or 5.x)

    raises ConfluenceError if login is unsuccessful.
    """
    server = ServerProxy(confluence_url + '/rpc/xmlrpc')
    user = user or getpass.getuser()
    password = getpass.getpass('Please enter confluence password for %s: ' % user)

    if api_entrypoint in (None, 'confluence1'):
      # TODO(???) didn't handle this JSirois review comment:
      #   Can you just switch on in create_html_page?
      #   Alternatively store a lambda here in each branch.
      api = server.confluence1
      fmt = 'markdown'
    elif api_entrypoint == 'confluence2':
      api = server.confluence2
      fmt = 'xhtml'
    else:
      raise ConfluenceError("Don't understand api_entrypoint %s" % api_entrypoint)

    try:
      return Confluence(api, confluence_url, api.login(user, password), fmt)
    except XMLRPCError as e:
      raise ConfluenceError('Failed to log in to %s: %s' % (confluence_url, e))

  @staticmethod
  def get_url(server_url, wiki_space, page_title):
    """ return the url for a confluence page in a given space and with a given
    title. """
    return '%s/display/%s/%s' % (server_url, wiki_space, urllib.quote_plus(page_title))

  def logout(self):
    """Terminates the session and connection to the server.

    Upon completion, the invoking instance is no longer usable to communicate with confluence.
    """
    self._api_entrypoint.logout(self._session_token)

  def getpage(self, wiki_space, page_title):
    """ Fetches a page object.

    Returns None if the page does not exist or otherwise could not be fetched.
    """
    try:
      return self._api_entrypoint.getPage(self._session_token, wiki_space, page_title)
    except XMLRPCError as e:
      log.warn('Failed to fetch page %s: %s' % (page_title, e))
      return None

  def storepage(self, page):
    """Stores a page object, updating the page if it already exists.

    returns the stored page, or None if the page could not be stored.
    """
    try:
      return self._api_entrypoint.storePage(self._session_token, page)
    except XMLRPCError as e:
      log.error('Failed to store page %s: %s' % (page.get('title', '[unknown title]'), e))
      return None

  def removepage(self, page):
    """Deletes a page from confluence.

    raises ConfluenceError if the page could not be removed.
    """
    try:
      self._api_entrypoint.removePage(self._session_token, page)
    except XMLRPCError as e:
      raise ConfluenceError('Failed to delete page: %s' % e)

  def create(self, space, title, content, parent_page=None, **pageoptions):
    """ Create a new confluence page with the given title and content.  Additional page options
    available in the xmlrpc api can be specified as kwargs.

    returns the created page or None if the page could not be stored.
    raises ConfluenceError if a parent page was specified but could not be found.
    """

    pagedef = dict(
      space = space,
      title = title,
      url = Confluence.get_url(self._server_url, space, title),
      content = content,
      contentStatus = 'current',
      current = True
    )
    pagedef.update(**pageoptions)

    if parent_page:
      # Get the parent page id.
      parent_page_obj = self.getpage(space, parent_page)
      if parent_page_obj is None:
        raise ConfluenceError('Failed to find parent page %s in space %s' % (parent_page, space))
      pagedef['parentId'] = parent_page_obj['id']

    # Now create the page
    return self.storepage(pagedef)

  def create_html_page(self, space, title, html, parent_page=None, **pageoptions):
    if self._content_format == 'markdown':
      content = '{html}\n\n%s\n\n{html}' % html
    elif self._content_format == 'xhtml':
      content = '''<ac:macro ac:name="html">
          <ac:plain-text-body><![CDATA[%s]]></ac:plain-text-body>
          </ac:macro>''' % html
    else:
      raise ConfluenceError("Don't know how to convert %s to HTML" % format)
    return self.create(space, title, content, parent_page, **pageoptions)

  def addattachment(self, page, filename):
    """Add an attachment to an existing page.
    Note: this will first read the entire file into memory"""
    mime_type = mimetypes.guess_type(filename, strict=False)[0]
    if not mime_type:
      raise ConfluenceError('Failed to detect MIME type of %s' % filename)

    try:
      with open(filename, 'rb') as f:
        file_data = f.read()

      attachment = dict(fileName=basename(filename), contentType=mime_type)
      return self._api_entrypoint.addAttachment(self._session_token,
                                                page['id'],
                                                attachment,
                                                Binary(file_data))
    except (IOError, OSError) as e:
      log.error('Failed to read data from file %s: %s' % (filename, str(e)))
      return None
    except XMLRPCError as e:
      log.error('Failed to add file attachment %s to page: %s' %
          (filename, page.get('title', '[unknown title]')))
      return None

########NEW FILE########
__FILENAME__ = lru_cache
# Taken from http://code.activestate.com/recipes/578078
# A 2.6.x backport of Python 3.3 functools.lru_cache
#
# Modified from 4=>2 length indents for code consistency.
#
# Added an on_eviction parameter that gets applied to each element
# when it is evicted from the cache, e.g. file.close(...).

from collections import namedtuple
from functools import update_wrapper
from threading import Lock

_CacheInfo = namedtuple("CacheInfo", ["hits", "misses", "maxsize", "currsize"])

def lru_cache(maxsize=100, typed=False, on_eviction=lambda x:x):
  """Least-recently-used cache decorator.

  If *maxsize* is set to None, the LRU features are disabled and the cache
  can grow without bound.

  If *typed* is True, arguments of different types will be cached separately.
  For example, f(3.0) and f(3) will be treated as distinct calls with
  distinct results.

  Arguments to the cached function must be hashable.

  View the cache statistics named tuple (hits, misses, maxsize, currsize) with
  f.cache_info().  Clear the cache and statistics with f.cache_clear().
  Access the underlying function with f.__wrapped__.

  See:  http://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used
  """

  # Users should only access the lru_cache through its public API:
  #       cache_info, cache_clear, and f.__wrapped__
  # The internals of the lru_cache are encapsulated for thread safety and
  # to allow the implementation to change (including a possible C version).
  def decorating_function(user_function):
    cache = dict()
    stats = [0, 0]                  # make statistics updateable non-locally
    HITS, MISSES = 0, 1             # names for the stats fields
    kwd_mark = (object(),)          # separate positional and keyword args
    cache_get = cache.get           # bound method to lookup key or return None
    _len = len                      # localize the global len() function
    lock = Lock()                   # because linkedlist updates aren't threadsafe
    root = []                       # root of the circular doubly linked list
    nonlocal_root = [root]                  # make updateable non-locally
    root[:] = [root, root, None, None]      # initialize by pointing to self
    PREV, NEXT, KEY, RESULT = 0, 1, 2, 3    # names for the link fields

    def make_key(args, kwds, typed, tuple=tuple, sorted=sorted, type=type):
      # helper function to build a cache key from positional and keyword args
      key = args
      if kwds:
        sorted_items = tuple(sorted(kwds.items()))
        key += kwd_mark + sorted_items
      if typed:
        key += tuple(type(v) for v in args)
        if kwds:
          key += tuple(type(v) for k, v in sorted_items)
      return key

    if maxsize == 0:
      def wrapper(*args, **kwds):
        # no caching, just do a statistics update after a successful call
        result = user_function(*args, **kwds)
        stats[MISSES] += 1
        return result

    elif maxsize is None:
      def wrapper(*args, **kwds):
        # simple caching without ordering or size limit
        key = make_key(args, kwds, typed) if kwds or typed else args
        result = cache_get(key, root)   # root used here as a unique not-found sentinel
        if result is not root:
          stats[HITS] += 1
          return result
        result = user_function(*args, **kwds)
        cache[key] = result
        stats[MISSES] += 1
        return result

    else:
      def wrapper(*args, **kwds):
        # size limited caching that tracks accesses by recency
        key = make_key(args, kwds, typed) if kwds or typed else args
        with lock:
          link = cache_get(key)
          if link is not None:
            # record recent use of the key by moving it to the front of the list
            root, = nonlocal_root
            link_prev, link_next, key, result = link
            link_prev[NEXT] = link_next
            link_next[PREV] = link_prev
            last = root[PREV]
            last[NEXT] = root[PREV] = link
            link[PREV] = last
            link[NEXT] = root
            stats[HITS] += 1
            return result
        result = user_function(*args, **kwds)
        with lock:
          root = nonlocal_root[0]
          if _len(cache) < maxsize:
            # put result in a new link at the front of the list
            last = root[PREV]
            link = [last, root, key, result]
            cache[key] = last[NEXT] = root[PREV] = link
          else:
            # use root to store the new key and result
            root[KEY] = key
            root[RESULT] = result
            cache[key] = root
            # empty the oldest link and make it the new root
            root = nonlocal_root[0] = root[NEXT]
            evicted = cache.pop(root[KEY])
            on_eviction(evicted[RESULT])
            root[KEY] = None
            root[RESULT] = None
          stats[MISSES] += 1
        return result

    def cache_info():
      """Report cache statistics"""
      with lock:
        return _CacheInfo(stats[HITS], stats[MISSES], maxsize, len(cache))

    def cache_clear():
      """Clear the cache and cache statistics"""
      with lock:
        for value in cache.values():
          on_eviction(value[RESULT])
        cache.clear()
        root = nonlocal_root[0]
        root[:] = [root, root, None, None]
        stats[:] = [0, 0]

    wrapper.__wrapped__ = user_function
    wrapper.cache_info = cache_info
    wrapper.cache_clear = cache_clear
    return update_wrapper(wrapper, user_function)

  return decorating_function

########NEW FILE########
__FILENAME__ = threads
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import threading
from functools import wraps


def __gettid():
  """Wrapper for the gettid() system call on Linux systems

  This is a lightweight, fail-fast function to obtain the thread ID of the thread from which it is
  called.  Unfortunately, there's not currently any means of obtaining this information on Linux
  other than through a direct system call. See man 2 gettid() for more details.

  This function can be called directly, but the more common usage pattern is through use of the
  identify_thread decorator on the run() function of a threading.Thread-derived object.

  Returns:
    on success, an int representing the thread ID of the calling thread, as returned from the
      gettid() system call. In the main thread of a process, this should be equal to the process
      ID (e.g. as returned by getpid())
    -1 on any failure (bad platform, error accessing ctypes/libraries, actual system call failure)

  """
  try:
    import platform
    if not platform.system().startswith('Linux'):
      raise ValueError
    syscalls = {
      'i386':   224,   # unistd_32.h: #define __NR_gettid 224
      'x86_64': 186,   # unistd_64.h: #define __NR_gettid 186
    }
    import ctypes
    tid = ctypes.CDLL('libc.so.6').syscall(syscalls[platform.machine()])
  except:
    tid = -1
  return tid


def identify_thread(instancemethod):
  """Simple decorator to expose Linux thread-IDs on an object.

  On Linux, each thread (aka light-weight process) is represented by a unique thread ID, an integer
  in the same namespace as process IDs. (This is distinct from the opaque identifier provided by the
  pthreads interface, which is essentially useless outside the context of the pthreads library.)
  This decorator provides a means to expose this thread ID on Python objects - most likely,
  subclasses of threading.Thread. The benefit of this is that operating-system level process
  information (for example, anything exposed through /proc on Linux) can then be correlated directly
  to Python thread objects.

  The means for retrieving the thread ID is extremely nonportable - specifically, it will only work
  on i386 and x86_64 Linux systems. However, including this decorator more generally should be safe
  and not break any cross-platform code - it will just result in an 'UNKNOWN' thread ID.

  This decorator can be used to wrap any instance method (and technically also class methods). To be
  truly useful, though, it should be used to wrap the run() function of a class utilising the Python
  threading API (i.e. a derivative of threading.Thread)

  Example usage:
    >>> import threading, time
    >>> from twitter.common.decorators import identify_thread
    >>> class MyThread(threading.Thread):
    ...   def __init__(self):
    ...     threading.Thread.__init__(self)
    ...     do_some_other_init()
    ...     self.daemon = True
    ...   @identify_thread
    ...   def run(self):
    ...     while True:
    ...       do_something_awesome()
    ...
    >>> thread1, thread2, thread3 = MyThread(), MyThread(), MyThread()
    >>> thread1.start(), thread2.start(), thread3.start()
    (None, None, None)
    >>> time.sleep(0.1)
    >>> for thread in (thread1, thread2, thread3):
    ...   print '%s => %s' % (thread.name, thread.__thread_id)
    ...
    Thread-1 => 19767
    Thread-2 => 19768
    Thread-3 => 19769
    >>> import os; os.system('ps -L -p %d -o pid,ppid,tid,thcount,cmd' % os.getpid())
      PID  PPID   TID THCNT CMD
    19764 19760 19764     4 /usr/bin/python2.6 /tmp/tmpSW3VIC
    19764 19760 19767     4 /usr/bin/python2.6 /tmp/tmpSW3VIC
    19764 19760 19768     4 /usr/bin/python2.6 /tmp/tmpSW3VIC
    19764 19760 19769     4 /usr/bin/python2.6 /tmp/tmpSW3VIC

  Note that there will be a non-zero delay between when the thread is started and when the thread ID
  attribute (self.__thread_id) is available.

  """
  @wraps(instancemethod)
  def identified(self, *args, **kwargs):
    tid = __gettid()
    if tid == -1:
      self.__thread_id = 'UNKNOWN'
    else:
      self.__thread_id = tid
      if isinstance(self, threading.Thread):
        self.setName('%s [TID=%d]' % (self.name, self.__thread_id))
    return instancemethod(self, *args, **kwargs)

  return identified

########NEW FILE########
__FILENAME__ = chroot
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Alec Thomas, Brian Wickman'

import contextlib
import copy
import errno
import os
import shutil
import tempfile
import zipfile

from . import safe_mkdir


class Chroot(object):
  """
    A chroot of files overlayed from one directory to another directory.

    Files may be tagged when added in order to keep track of multiple overlays in
    the chroot.
  """
  class ChrootException(Exception): pass

  class ChrootTaggingException(Exception):
    def __init__(self, filename, orig_tag, new_tag):
      Exception.__init__(self,
        "Trying to add %s to fileset(%s) but already in fileset(%s)!" % (
          filename, new_tag, orig_tag))

  def __init__(self, chroot_base, name=None):
    """
      chroot_base = directory for the creation of the target chroot.
      name = if specified, create the chroot in a temporary directory underneath
        chroot_base with 'name' as the prefix, otherwise create the chroot directly
        into chroot_base
    """
    self.root = None
    try:
      safe_mkdir(chroot_base)
    except:
      raise Chroot.ChrootException('Unable to create chroot in %s' % chroot_base)
    if name is not None:
      self.chroot = tempfile.mkdtemp(dir=chroot_base, prefix='%s.' % name)
    else:
      self.chroot = chroot_base
    self.filesets = {}

  def set_relative_root(self, root):
    """
      Make all source paths relative to this root path.
    """
    self.root = root

  def clone(self, into=None):
    into = into or tempfile.mkdtemp()
    new_chroot = Chroot(into)
    new_chroot.root = self.root
    for label, fileset in self.filesets.items():
      for fn in fileset:
        new_chroot.link(os.path.join(self.chroot, self.root or '', fn),
                        fn, label=label)
    return new_chroot

  def path(self):
    """The path of the chroot."""
    return self.chroot

  def _check_tag(self, fn, label):
    for fs_label, fs in self.filesets.items():
      if fn in fs and fs_label != label:
        raise Chroot.ChrootTaggingException(fn, fs_label, label)

  def _tag(self, fn, label):
    self._check_tag(fn, label)
    if label not in self.filesets:
      self.filesets[label] = set()
    self.filesets[label].add(fn)

  def _mkdir_for(self, path):
    dirname = os.path.dirname(os.path.join(self.chroot, path))
    safe_mkdir(dirname)

  def _rootjoin(self, path):
    return os.path.join(self.root or '', path)

  def copy(self, src, dst, label=None):
    """
      Copy file from {root}/source to {chroot}/dest with optional label.

      May raise anything shutil.copyfile can raise, e.g.
        IOError(Errno 21 'EISDIR')

      May raise ChrootTaggingException if dst is already in a fileset
      but with a different label.
    """
    self._tag(dst, label)
    self._mkdir_for(dst)
    shutil.copyfile(self._rootjoin(src), os.path.join(self.chroot, dst))

  def link(self, src, dst, label=None):
    """
      Hard link file from {root}/source to {chroot}/dest with optional label.

      May raise anything os.link can raise, e.g.
        IOError(Errno 21 'EISDIR')

      May raise ChrootTaggingException if dst is already in a fileset
      but with a different label.
    """
    self._tag(dst, label)
    self._mkdir_for(dst)
    abs_src = self._rootjoin(src)
    abs_dst = os.path.join(self.chroot, dst)
    try:
      os.link(abs_src, abs_dst)
    except OSError as e:
      if e.errno == errno.EEXIST:
        # File already exists, skip
        pass
      elif e.errno == errno.EXDEV:
        # Hard link across devices, fall back on copying
        shutil.copyfile(abs_src, abs_dst)
      else:
        raise

  def write(self, data, dst, label=None, mode='wb'):
    """
      Write data to {chroot}/dest with optional label.

      Has similar exceptional cases as Chroot.copy
    """

    self._tag(dst, label)
    self._mkdir_for(dst)
    with open(os.path.join(self.chroot, dst), mode) as wp:
      wp.write(data)

  def touch(self, dst, label=None):
    """
      Perform 'touch' on {chroot}/dest with optional label.

      Has similar exceptional cases as Chroot.copy
    """
    self.write('', dst, label, mode='a')

  def get(self, label):
    """Get all files labeled with 'label'"""
    return self.filesets.get(label, set())

  def files(self):
    """Get all files in the chroot."""
    all_files = set()
    for label in self.filesets:
      all_files.update(self.filesets[label])
    return all_files

  def labels(self):
    return self.filesets.keys()

  def __str__(self):
    return 'Chroot(%s {fs:%s})' % (self.chroot,
      ' '.join('%s' % foo for foo in self.filesets.keys()))

  def delete(self):
    shutil.rmtree(self.chroot)

  def zip(self, filename, mode='wb'):
    with contextlib.closing(zipfile.ZipFile(filename, mode)) as zf:
      for f in sorted(self.files()):
        zf.write(os.path.join(self.chroot, f), arcname=f, compress_type=zipfile.ZIP_DEFLATED)


class RelativeChroot(Chroot):
  """
    A chroot of files overlayed from one directory to another directory, but with all
    source copies relative to a particular root directory.
  """
  def __init__(self, root, chroot_base, name=None):
    """
      root = source directory for files
      chroot_base = directory for the creation of the target chroot.
      name = if specified, create the chroot in a temporary directory underneath
        chroot_base with 'name' as the prefix, otherwise create the chroot directly
        into chroot_base
    """
    Chroot.__init__(self, chroot_base, name)
    self.set_relative_root(root)

########NEW FILE########
__FILENAME__ = fileset
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from functools import reduce
import fnmatch
import glob
import os
import re

from twitter.common.lang import Compatibility


def fnmatch_translate_extended(pat):
  """
     A modified version of fnmatch.translate to match zsh semantics more closely:
       '*' matches one or more characters instead of zero or more
       '**' is equivalent to '*'
       '**/' matches one or more directories.
     E.g. src/**/*.py => match all files ending with .py in any subdirectory of src/
  """
  i, n = 0, len(pat)
  res = ''
  while i < n:
    c = pat[i]
    i += 1
    if c == '*':
      if pat[i:i+2] == '*/':
        res += '([^/]+/)*'
        i += 2
      elif pat[i:i+1] == '*':
        res += '([^/]+)'
        i += 1
      else:
        res += '([^/]+)'
    elif c == '?':
      res += '.'
    elif c == '[':
      j = i
      if j < n and pat[j] == '!':
        j += 1
      if j < n and pat[j] == ']':
        j += 1
      while j < n and pat[j] != ']':
        j += 1
      if j >= n:
        res += '\\['
      else:
        stuff = pat[i:j].replace('\\', '\\\\')
        i = j + 1
        if stuff[0] == '!':
          stuff = '^' + stuff[1:]
        elif stuff[0] == '^':
          stuff = '\\' + stuff
        res += '[' + stuff + ']'
    else:
      res += re.escape(c)
  return res + '\Z(?ms)'


class Fileset(object):
  """
    An iterable, callable object that will gather up a set of files lazily when iterated over or
    called.  Supports unions with iterables, other Filesets and individual items using the ^ and +
    operators as well as set difference using the - operator.
  """

  @classmethod
  def walk(cls, path=None, allow_dirs=False, follow_links=False):
    """Walk the directory tree starting at path, or os.curdir if None.  If
       allow_dirs=False, iterate only over files.  If allow_dirs=True,
       iterate over both files and directories.  If follow_links=True symlinked
       directories will be traversed.
    """
    path = path or os.curdir
    for root, dirs, files in os.walk(path, followlinks=follow_links):
      if allow_dirs:
        for dirname in dirs:
          base_dir = os.path.relpath(os.path.normpath(os.path.join(root, dirname)), path)
          yield base_dir
          yield base_dir + os.sep
      for filename in files:
        yield os.path.relpath(os.path.normpath(os.path.join(root, filename)), path)

  @classmethod
  def globs(cls, *globspecs, **kw):
    """Returns a Fileset that combines the lists of files returned by
       glob.glob for each globspec.  File names starting with '.' are not
       returned unless explicitly globbed.  For example, ".*" matches
       ".bashrc" but "*" does not, mirroring the semantics of 'ls' without
       '-a'.

       Walks the current working directory by default, can be overrided with
       the 'root' keyword argument.
    """
    root = kw.pop('root', os.curdir)
    def relative_glob(globspec):
      for fn in glob.glob(os.path.join(root, globspec)):
        yield os.path.relpath(fn, root)
    def combine(files, globspec):
      return files ^ set(relative_glob(globspec))
    return cls(lambda: reduce(combine, globspecs, set()))

  @classmethod
  def _do_rglob(cls, matcher, root, **kw):
    for path in cls.walk(root, **kw):
      if matcher(path):
        yield path

  @classmethod
  def rglobs(cls, *globspecs, **kw):
    """Returns a Fileset that contains the union of all files matched by the
       globspecs applied at each directory beneath the root.  By default the
       root is the current working directory, but can be overridden with the
       'root' keyword argument.

       File names starting with '.' are not returned unless explicitly globbed.
       For example, ".*" matches ".bashrc" but "*" does not, mirroring the
       semantics of 'ls' without '-a'.
    """
    root = kw.pop('root', os.curdir)

    def matcher(path):
      for globspec in globspecs:
        # Ignore hidden files when globbing wildcards.
        if not (globspec.startswith('*') and os.path.basename(path).startswith('.')):
          if fnmatch.fnmatch(path, globspec):
            return True
      return False

    return cls(lambda: set(cls._do_rglob(matcher, allow_dirs=False, root=root, **kw)))

  @classmethod
  def zglobs(cls, *globspecs, **kw):
    """Returns a Fileset that matches zsh-style globs, including '**/' for recursive globbing.

       By default searches from the current working directory.  Can be overridden
       with the 'root' keyword argument.  File names starting with '.' are not
       returned unless explicitly globbed.  For example, ".*" matches ".bashrc" but
       "*" does not, mirroring the semantics of 'ls' without '-a'.
    """
    root = kw.pop('root', os.curdir)
    patterns = [(os.path.basename(spec).startswith('*'),
                 re.compile(fnmatch_translate_extended(spec))) for spec in globspecs]

    def matcher(path):
      for no_hidden, pattern in patterns:
        # Ignore hidden files when globbing wildcards.
        if not (no_hidden and os.path.basename(path).startswith('.')):
          if pattern.match(path):
            return True
      return False

    return cls(lambda: set(cls._do_rglob(matcher, allow_dirs=True, root=root, **kw)))

  def __init__(self, callable_):
    self._callable = callable_

  def __call__(self, *args, **kwargs):
    return self._callable(*args, **kwargs)

  def __iter__(self):
    return iter(self())

  def __add__(self, other):
    return self ^ other

  def __xor__(self, other):
    def union():
      if callable(other):
        return self() ^ other()
      elif isinstance(other, set):
        return self() ^ other
      elif isinstance(other, Compatibility.string):
        raise TypeError('Unsupported operand type (%r) for ^: %r and %r' %
                        (type(other), self, other))
      else:
        try:
          return self() ^ set(iter(other))
        except TypeError:
          return self().add(other)
    return Fileset(union)

  def __sub__(self, other):
    def subtract():
      if callable(other):
        return self() - other()
      elif isinstance(other, set):
        return self() - other
      elif isinstance(other, Compatibility.string):
        raise TypeError('Unsupported operand type (%r) for -: %r and %r' %
                        (type(other), self, other))
      else:
        try:
          return self() - set(iter(other))
        except TypeError:
          return self().remove(other)
    return Fileset(subtract)

########NEW FILE########
__FILENAME__ = lock
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'John Sirois'

import os

from . import lock_file, touch, unlock_file


class Lock(object):
  """A co-operative inter-process file lock."""

  @staticmethod
  def unlocked():
    """Creates a Lock that is already released."""
    return Lock(None)

  @staticmethod
  def acquire(path, onwait=None):
    """Attempts to lock the given path which need not exist ahead of time.

    By default acquire blocks as long as needed for the lock to be released if already held.

    If an onwait function is supplied, it will be passed the lock owner's pid when the lock cannot
    be acquired immediately.  In this case the onwait function should return True if it wishes to
    block on acquisition of the Lock.  Otherwise None will be returned as a signal to acquire's
    caller that the lock failed.
    """

    touch(path)
    lock_fd = lock_file(path, blocking=False)
    if not lock_fd:
      blocking = True
      with open(path, 'r') as fd:
        pid = int(fd.read().strip())
        if onwait:
          blocking = onwait(pid)
      if not blocking:
        return None
      lock_fd = lock_file(path, blocking=blocking)

    lock_fd.truncate(0)
    lock_fd.write('%d\n' % os.getpid())
    lock_fd.flush()
    return Lock(lock_fd)

  def __init__(self, lock_fd):
    self._lock_fd = lock_fd

  def is_unlocked(self):
    """Checks whether or not this lock object is currently actively holding a lock."""
    return self._lock_fd is None

  def release(self):
    """Releases this lock if held and returns True; otherwise, returns False to indicate the lock
    was already released.
    """

    if self._lock_fd:
      unlock_file(self._lock_fd, close=True)
      self._lock_fd = None
      return True
    else:
      return False

########NEW FILE########
__FILENAME__ = tail
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Brian Wickman'

import os
import errno
import time


def _tail_lines(fd, linesback=10):
  if fd is None:
    return

  # Contributed to Python Cookbook by Ed Pascoe (2003)
  avgcharsperline = 75

  while True:
    try:
      fd.seek(int(-1 * avgcharsperline * linesback), 2)
    except IOError:
      fd.seek(0)

    atstart = fd.tell() == 0

    lines = fd.read().splitlines()
    if atstart or len(lines) > (linesback + 1):
      break

    avgcharsperline = avgcharsperline * 1.3

  if len(lines) > linesback:
    start = len(lines) - linesback - 1
  else:
    start = 0

  return lines[start:start+linesback]


def wait_until_opened(filename, forever=True, clock=time):
  while True:
    try:
      return open(filename, 'r')
    except OSError as e:
      if e.errno == errno.ENOENT:
        if forever:
          clock.sleep(1)
        else:
          return None
      else:
        raise


def tail(filename, lines=10):
  with open(filename, 'r') as fp:
    for line in _tail_lines(fp, lines):
      yield line


def tail_f(filename, forever=True, include_last=False, clock=time):
  fd = wait_until_opened(filename, forever, clock)

  # wind back to near the end of the file...
  last_lines = _tail_lines(fd, 10)

  while True:
    if fd is None:
      return

    where = fd.tell()

    if last_lines:
      yield last_lines.pop(0)
      continue
    else:
      line = fd.readline()

    if line:
      yield line
    else:
      # check health of the file descriptor.
      fd_results = os.fstat(fd.fileno())
      try:
        st_results = None
        st_results = os.stat(filename)
      except OSError as e:
        if e.errno == errno.ENOENT:
          fd = wait_until_opened(filename, forever, clock)
          continue
        else:
          raise

      # file changed from underneath us, reopen
      if fd_results.st_ino != st_results.st_ino:
        fd.close()
        fd = wait_until_opened(filename, forever, clock)
        continue

      if st_results.st_size < where:
        # file truncated, rewind
        fd.seek(0)
      else:
        # our buffer has not yet caught up, wait.
        clock.sleep(1)
        fd.seek(where)

########NEW FILE########
__FILENAME__ = pingpong
from functools import partial
import time
import urllib2

from twitter.common import log
from twitter.common.concurrent import defer
from twitter.common.http import HttpServer
from twitter.common.metrics import (
    AtomicGauge,
    LambdaGauge,
    Observable)
from twitter.common.quantity import Amount, Time


class PingPongServer(Observable):
  PING_DELAY = Amount(1, Time.SECONDS)

  def __init__(self, target_host, target_port, clock=time):
    self._clock = clock
    self._target = (target_host, target_port)
    self._pings = AtomicGauge('pings')
    self.metrics.register(self._pings)

  def send_request(self, endpoint, message, ttl):
    url_base = 'http://%s:%d' % self._target
    try:
      urllib2.urlopen('%s/%s/%s/%d' % (url_base, endpoint, message, ttl)).read()
    except Exception as e:
      log.error('Failed to query %s: %s' % (url_base, e))

  @HttpServer.route('/ping/:message')
  @HttpServer.route('/ping/:message/:ttl')
  def ping(self, message, ttl=60):
    self._pings.increment()
    log.info('Got ping (ttl=%s): %s' % (message, ttl))
    ttl = int(ttl) - 1
    if ttl > 0:
      defer(partial(self.send_request, 'ping', message, ttl), delay=self.PING_DELAY,
          clock=self._clock)

########NEW FILE########
__FILENAME__ = pingpong_main
import time

from twitter.common import app, log
from twitter.common.app.modules.http import RootServer
from twitter.common.log.options import LogOptions
from twitter.common.metrics import RootMetrics

from twitter.common.examples.pingpong import PingPongServer


app.add_option('--target_host', default='localhost',
               help='The target host to send pingpong requests.')
app.add_option('--target_port', default=12345, type='int',
               help='The target port to send pingpong requests.')


app.configure('twitter.common.app.modules.http', enable=True)


def main(args, options):
  pingpong = PingPongServer(options.target_host, options.target_port)
  RootServer().mount_routes(pingpong)
  RootMetrics().register_observable('pingpong', pingpong)

  try:
    time.sleep(2**20)
  except KeyboardInterrupt:
    log.info('Shutting down.')


LogOptions.set_disk_log_level('NONE')
LogOptions.set_stderr_log_level('google:DEBUG')
app.main()

########NEW FILE########
__FILENAME__ = hdfs
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'tdesai'

import os
import subprocess
import sys

from twitter.common.contextutil import environment_as, temporary_file
from twitter.common.quantity import Amount, Data
from twitter.common.string import ScanfParser
from twitter.common.util.command_util import CommandUtil


class HDFSHelper(object):
  """
  This Class provides a set of functions for hadoop operations. 
  NOTE: This class assumes a local hadoop client on the path.
  """
  class InternalError(Exception): pass

  PARSER = ScanfParser('%(mode)s %(dirents)s %(user)s %(group)s %(filesize)d '
            '%(year)d-%(month)d-%(day)d %(hour)d:%(minute)d')

  def __init__(self, config, command_class=CommandUtil, heap_limit=Amount(256, Data.MB)):
    """heap_limit is the maximum heap that should be allocated to the hadoop process,
    defined using twitter.common.quantity.Data."""
    if not os.path.isdir(config):
      raise ValueError("hadoop requires root of a config tree")
    self._config = config
    self._cmd_class = command_class
    if heap_limit is None:
      raise ValueError('The hadoop heap_limit must not be specified as "None".')
    self._heap_limit = heap_limit

  @property
  def config(self):
    return self._config

  def _call(self, cmd, *args, **kwargs):
    """Runs hadoop fs command  with the given command and args.
    Checks the result of the call by default but this can be disabled with check=False.
    """
    cmd = ['hadoop', '--config', self._config, 'dfs', cmd] + list(args)
    heapsize = str(int(self._heap_limit.as_(Data.MB)))
    with environment_as(HADOOP_HEAPSIZE=heapsize):
      if kwargs.get('check'):
        return self._cmd_class.check_call(cmd)
      elif kwargs.get('return_output'):
        return self._cmd_class.execute_and_get_output(cmd)
      elif kwargs.get('supress_output'):
        return self._cmd_class.execute_suppress_stdout(cmd)
      else:
        return self._cmd_class.execute(cmd)

  def get(self, src, dst):
    """
    Copy file(s) in hdfs to local path (via proxy if necessary).
    NOTE: If src matches multiple files, make sure dst is a directory!
    """
    if isinstance(src, list):
      hdfs_src = " ".join(src)
    else:
      hdfs_src = src
    return self._call('-get', hdfs_src, dst)

  def put(self, src, dst):
    """
    Copy the local file src to a hadoop path dst.
    """
    abs_src = os.path.expanduser(src)
    assert os.path.exists(abs_src), 'File does not exist, cannot copy: %s' % abs_src
    return self._do_put(abs_src, dst)

  def _do_put(self, source, dst):
    """
    Put the local file in to HDFS
    """
    if isinstance(dst, list):
      hdfs_dst = " ".join(dst)
    else:
      hdfs_dst = dst
    if not self._call('-test', '-e', hdfs_dst, check=False):
      self._call('-rm', '-skipTrash', hdfs_dst)
    return self._call('-put', source, hdfs_dst)

  def exists(self, path, flag='-e'):
    """
    Checks if the path exists in hdfs
    Returns true if it exists or else
    Returns false
    """
    try:
      return self._call("-test", flag, path) == 0
    except subprocess.CalledProcessError:
      return False

  def cat(self, remote_file_pattern, local_file=sys.stdout):
    """
    Cat hdfs file to local
    """
    return self._call("-cat", remote_file_pattern, also_output_to_file=local_file)

  def _ls(self, path, is_dir=False, is_recursive=False):
    """
    Return list of [hdfs_full_path, filesize]
    Raises exception when the hadoop ls command returns error
    """
    hdfs_cmd = '-lsr' if is_recursive else '-ls'
    (exit_code, ls_result) = self._call(hdfs_cmd, path, return_output=True)
    if exit_code != 0:
      raise self.InternalError("Error occurred. %s.Check logs for details" % ls_result)
    file_list = []
    if ls_result is None:
      return file_list
    lines = ls_result.splitlines()
    for line in lines:
      if line == "" or line.startswith("Found"):
        continue

      seg = line.split(None, 7)
      if len(seg) < 8:
        raise self.InternalError("Invalid hdfs -ls output. [%s]" % line)

      filename = seg[-1]
      try:
        metadata = self.PARSER.parse(' '.join(seg[0:7]))
      except ScanfParser.ParseError as e:
        raise self.InternalError('Unable to parse hdfs output: %s' % e)
      #seg[0] example: drwxrwx---
      if metadata.mode.startswith('d') != is_dir:
        continue

      file_list.append([filename, metadata.filesize])
    return file_list

  def ls(self, path, is_dir=False):
    """
    Returns list of [hdfs_full_path, filesize]
    If is_dir is true returns only the toplevel directories.
    """
    return self._ls(path, is_dir, False)

  def lsr(self, path, is_dir=False):
    """
    Returns list of [hdfs_full_path, filesize] in recursive manner
    If is_dir is true returns only the directories.
    """
    return self._ls(path, is_dir, True)

  def read(self, filename):
    """
      Return the contents of filename, or None if an error occurred.
    """
    with temporary_file() as fp:
      os.unlink(fp.name)
      if self._call("-copyToLocal", filename, fp.name) == 0:
        with open(fp.name) as f:
          return f.read()
      else:
        return None

  def write(self, filename, text):
    """
    Write will write the contents in the text to the filename given
    The file will be overwritten if it already exists
    """
    self._call("-rm", filename)
    with temporary_file() as fp:
      fp.write(text)
      fp.flush()
      return self._call('-copyFromLocal', fp.name, filename)

  def mkdir(self, path):
    """
    Mkdir will create a directory. If already present, it will return an error
    """
    return self._call("-mkdir", path)

  def mkdir_suppress_err(self, path):
    """
    Creates a directory if it does not exists
    """
    if not self.exists(path):
      return self.mkdir(path)

  def rm(self, filename):
    """
    Removes a file.
    """
    return self._call("-rm", filename, suppress_output=True)

  def cp(self, src, dest):
    """
    Copies a src file to dest
    """
    return self._call("-cp", src, dest, suppress_output=True)

  def copy_from_local(self, local, remote):
    """
    Copies the file from local to remote
    """
    return self._call("-copyFromLocal", local, remote, suppress_output=True)

  def copy_to_local(self, remote, local):
    """
    Copies the file from remote to local
    """
    return self._call("-copyToLocal", remote, local, suppress_output=True)

########NEW FILE########
__FILENAME__ = diagnostics
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pstats
import sys
import threading
import traceback

try:
  import cStringIO as StringIO
except ImportError:
  import StringIO

try:
  from twitter.common import app
  HAS_APP = True
except ImportError:
  HAS_APP = False

from .server import HttpServer, route


class DiagnosticsEndpoints(object):
  """
    Export the thread stacks of the running process.
  """
  UNHEALTHY = threading.Event()

  @classmethod
  def generate_stacks(cls):
    threads = dict((th.ident, th) for th in threading.enumerate())
    tb = []
    for thread_id, stack in sys._current_frames().items():
      tb.append("\n\n# Thread%s: %s (%s, %d)" % (
        ' (daemon)' if threads[thread_id].daemon else '',
        threads[thread_id].__class__.__name__, threads[thread_id].name, thread_id))
      for filename, lineno, name, line in traceback.extract_stack(stack):
        tb.append('  File: "%s", line %d, in %s' % (filename, lineno, name))
        if line:
          tb.append("    %s" % (line.strip()))
    return "\n".join(tb)

  @route("/threads")
  def handle_threads(self):
    HttpServer.set_content_type('text/plain; charset=iso-8859-1')
    return self.generate_stacks()

  @route("/profile")
  def handle_profile(self):
    HttpServer.set_content_type('text/plain; charset=iso-8859-1')
    if HAS_APP and app.profiler() is not None:
      output_stream = StringIO.StringIO()
      stats = pstats.Stats(app.profiler(), stream=output_stream)
      stats.sort_stats('time', 'name')
      stats.print_stats()
      return output_stream.getvalue()
    else:
      return 'Profiling is disabled'

  @route("/health")
  def handle_health(self):
    return 'UNHEALTHY' if self.UNHEALTHY.is_set() else 'OK'

########NEW FILE########
__FILENAME__ = plugin
from abc import abstractmethod

from twitter.common.lang import Interface


class Plugin(Interface):
  @property
  def name(self):
    """The name of the plugin."""
    return self.__class__.__name__

  @property
  def api(self):
    # This Plugin is the duck-typed Bottle Plugin interface v2.
    return 2

  def setup(self, app):
    pass

  @abstractmethod
  def apply(self, callback, route):
    """Given the Bottle callback and Route object, return a (possibly)
       decorated version of the original callback function, e.g. a
       version that profiles the endpoint.

       For more information see:
         http://bottlepy.org/docs/stable/plugindev.html
    """

  def close(self):
    pass


########NEW FILE########
__FILENAME__ = echo
from __future__ import print_function
from functools import wraps
import pprint

from twitter.common.http.plugin import Plugin

from ..server import request


class EchoHeaders(Plugin):
  """An example Plugin that prints to stdout request information as it comes in."""

  def apply(self, callback, route):
    @wraps(callback)
    def wrapped_callback(*args, **kw):
      print('path: %s' % request.path)
      print('method: %s' % request.method)
      print('cookies: ', end='')
      pprint.pprint(dict(request.cookies.items()))
      print('query: ', end='')
      pprint.pprint(dict(request.query.items()))
      print('params: ', end='')
      pprint.pprint(dict(request.params.items()))
      print('headers: ', end='')
      pprint.pprint(dict(request.headers.items()))
      return callback(*args, **kw)
    return wrapped_callback

########NEW FILE########
__FILENAME__ = kerberos
from __future__ import absolute_import
from functools import wraps

from twitter.common import log
from twitter.common.contextutil import environment_as
from twitter.common.http import request, response
from twitter.common.http.plugin import Plugin

from bottle import HTTPResponse
import kerberos


class Kerberized(Plugin):
  """HTTPServer plugin for kerberos auth."""
  DEFAULT_AUTH_FAIL = 'kerberos auth failed'
  DEFAULT_AUTH_ERR = 'error during kerberos auth'
  DEFAULT_CONTENT_TYPE = 'text/plain'
  AUTH_HEADER = 'HTTP_AUTHORIZATION'

  def __init__(self,
               keytab,
               service='HTTP',
               fail_response=None,
               err_response=None,
               content_type=None):
    """Params

       keytab           path to kerberos keytab (e.g. '/etc/krb5.tab')
       service          kerberos service name (optional, defaults to HTTP)
       fail_response    response body to send on auth failures (optional)
       err_response     response body to send on auth errors (optional)
       content_type     content-type for fail/err responses, e.g. to support json APIs (optional)
    """

    self._keytab = keytab
    self._service = service
    self._fail_response = fail_response or self.DEFAULT_AUTH_FAIL
    self._err_response = err_response or self.DEFAULT_AUTH_ERR
    self._content_type = content_type or self.DEFAULT_CONTENT_TYPE

  def parse_auth_header(self, line):
    return line.split(' ', 1)

  def auth_error(self):
    resp = HTTPResponse(self._err_response, status=500)
    resp.set_header('Content-Type', self._content_type)
    return resp

  def auth_fail(self, gss_context=None):
    resp = HTTPResponse(self._fail_response, status=401)
    resp.set_header('Content-Type', self._content_type)
    resp.set_header(
      'WWW-Authenticate',
      'Negotiate' + (' ' + gss_context if gss_context else '')
    )
    return resp

  def check_result(self, result, success=1):
    return result == success

  def authorize(self, req, app, app_args, app_kwargs):
    """Perform a Kerberos authentication handshake with the KDC."""
    http_auth = req.environ.get(self.AUTH_HEADER)
    if not http_auth:
      log.info('kerberos: rejecting non-authed request from %s', req.environ.get('REMOTE_ADDR'))
      return self.auth_fail()

    log.debug('kerberos: processing auth: %s', http_auth)
    auth_type, auth_key = self.parse_auth_header(http_auth)

    if auth_type == 'Negotiate':
      # Initialize a kerberos context.
      try:
        result, context = kerberos.authGSSServerInit(self._service)
        log.debug('kerberos: authGSSServerInit(%s) -> %s, %s', self._service, result, context)
      except kerberos.GSSError as e:
        log.warning('kerberos: GSSError during init: %s', e)
        result, context = 0, None

      if not self.check_result(result):
        log.warning('kerberos: bad result from authGSSServerInit(%s): %s', self._service, result)
        return self.auth_error()

      # Process the next challenge step and retrieve the response.
      gss_key = None
      try:
        result = kerberos.authGSSServerStep(context, auth_key)
        log.debug('kerberos: authGSSServerStep(%s, %s) -> %s', context, auth_key, result)

        gss_key = kerberos.authGSSServerResponse(context)
        log.debug('kerberos: authGSSServerResponse(%s) -> %s', context, gss_key)
      except kerberos.GSSError as e:
        log.warning('kerberos: GSSError(%s)', e)
        result = 0

      if not self.check_result(result):
        return self.auth_fail(gss_key)

      # Retrieve the user id and add it to the request environment.
      username = kerberos.authGSSServerUserName(context)
      req.environ['REMOTE_USER'] = username
      log.info('kerberos: authenticated user %s from %s', username, req.environ.get('REMOTE_ADDR'))

      # Pass on the GSS response in the Bottle response.
      response.set_header('WWW-Authenticate', 'Negotiate ' + str(gss_key))

      # Clean up.
      kerberos.authGSSServerClean(context)

      return app(*app_args, **app_kwargs)
    else:
      return self.auth_fail()

  def apply(self, app, route):
    """Main entry-point for bottle plugins."""
    @wraps(app, assigned=())
    def wrapped_app(*args, **kwargs):
      with environment_as(KRB5_KTNAME=self._keytab):
        return self.authorize(request, app, args, kwargs)
    return wrapped_app

  def __call__(self, f):
    """Support usage as a route handler decorator to limit scope to individual routes. e.g.

       @Kerberized(keytab=keytab_path)
       @HttpServer.route('/blah')
       def blah_handler(self):
         return 'kerberized'
    """
    return self.apply(f, None)

########NEW FILE########
__FILENAME__ = server
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import copy
import os
import threading
import types

import bottle

__all__ = (
    'abort',
    'HttpServer',
    'mako_view',
    'redirect',
    'request',
    'response',
    'route',
    'static_file',
    'view',
)


class HttpServer(object):
  """
    Wrapper around bottle to make class-bound servers a little easier
    to write.

    Three illustrative examples:

    Basic encapsulated server:
      from twitter.common.http import HttpServer, route

      class MyServer(HttpServer):
        @route("/hello")
        @route("/hello/:first")
        @route("/hello/:first/:last")
        def hello(self, first='Zaphod', last='Beeblebrox'):
          return 'Hello, %s %s!' % (first, last)

      server = MyServer()
      server.run('localhost', 8888)

    Using a mixin pattern:
      class HelloMixin(object):
        @route("/hello")
        def hello(self):
          return 'Hello!'

      class GoodbyeMixin(object):
        @route("/goodbye")
        def goodbye(self):
          return 'Goodbye!'

      # mixin directly by subclassing
      class MyServerUno(HttpServer, HelloMixin, GoodbyeMixin):
        pass

      # or instead mixin dynamically
      class MyServerDos(HttpServer):
        pass
      server = MyServerDos()
      server.mount_routes(HelloMixin())
      server.mount_routes(GoodbyeMixin())

    Plugin handling:
      Bottle supports plugins.  You can manually specify the plugin for a route with the 'apply'
      keyword argument:

      class DiagnosticsEndpoints(object):
        @route('/vars', apply=[TimerPlugin()])
        def vars(self):
          return self.metrics.sample()

      but if you'd like to have a plugin apply to all methods routed within a particular class,
      you can set the class attribute 'plugins':

      class DiagnosticsEndpoints(object):
        plugins = [TimerPlugin()]
        skip_plugins = []

        @route('/vars')
        def vars(self):
          return self.metrics.sample()

        @route('/ping', apply=[BasicAuth(require_group='mesos')])
        def ping(self):
          return 'pong'

        @route('/health', skip=['TimerPlugin'])
          return 'ok'

      This attribute will be mixed-in after plugins specified on a per route basis.  You may also
      specify 'skip_plugins' at the class-level or 'skip' at the route-level which is a list of
      plugins/plugin names to not apply to the routes.

      This also makes it possible to mix-in authentication classes, e.g.

      class AuthenticateEverything(object):
        plugins = [BasicAuth()]

      class MyApplication(AuthenticateEverything):
        @route('/list/:name')
        def list_by_name(self, name):
          ...
  """

  ROUTES_ATTRIBUTE = '__routes__'
  VIEW_ATTRIBUTE = '__view__'
  ERROR_ATTRIBUTE = '__errors__'

  abort = staticmethod(bottle.abort)
  request = Request = bottle.request
  response = Response = bottle.response
  redirect = staticmethod(bottle.redirect)
  static_file = staticmethod(bottle.static_file)

  @classmethod
  def route(cls, *args, **kwargs):
    """Route a request to a callback.  For the route format, see:
       http://bottlepy.org/docs/dev/tutorial.html#request-routing"""
    # Annotates the callback function with a set of applicable routes rather than registering
    # the route with the global application.  This allows us to mount routes at instantiation
    # time rather than at route declaration time.
    def annotated(function):
      if not hasattr(function, cls.ROUTES_ATTRIBUTE):
        setattr(function, cls.ROUTES_ATTRIBUTE, [])
      getattr(function, cls.ROUTES_ATTRIBUTE).append((args, kwargs))
      return function
    return annotated

  @classmethod
  def view(cls, *args, **kwargs):
    """Postprocess the output of this method with a view.  For more information see:
       http://bottlepy.org/docs/dev/tutorial.html#templates"""
    # Annotates the callback function with a set of applicable views a la HttpServer.route above.
    def annotated(function):
      setattr(function, cls.VIEW_ATTRIBUTE, (args, kwargs))
      return function
    return annotated

  @classmethod
  def error(cls, error_code):
    def annotated(function):
      if not hasattr(function, cls.ERROR_ATTRIBUTE):
        setattr(function, cls.ERROR_ATTRIBUTE, [])
      getattr(function, cls.ERROR_ATTRIBUTE).append(error_code)
      return function
    return annotated

  @classmethod
  def mako_view(cls, *args, **kwargs):
    """Helper function for annotating mako-specific views."""
    kwargs.update(template_adapter=bottle.MakoTemplate)
    return cls.view(*args, **kwargs)

  @classmethod
  def set_content_type(cls, header_value):
    cls.response.content_type = header_value

  def __init__(self):
    self._app = bottle.Bottle()
    self._hostname = None
    self._port = None
    self._mounts = set()
    self.mount_routes(self)

  # Delegate to the underlying Bottle application
  def __getattr__(self, attr):
    return getattr(self._app, attr)

  @classmethod
  def source_name(cls, class_or_instance):
    return getattr(class_or_instance, '__name__', class_or_instance.__class__.__name__)

  def _bind_method(self, class_instance, method_name):
    """
      Delegate class_instance.method_name to self.method_name
    """
    if not hasattr(class_instance, method_name):
      raise ValueError('No method %s.%s exists for bind_method!' % (
        self.source_name(class_instance), method_name))
    if isinstance(getattr(class_instance, method_name), types.MethodType):
      method_self = getattr(class_instance, method_name).im_self
      if method_self is None:
        # I attempted to allow for an unbound class pattern but failed.  The Python interpreter
        # allows for types.MethodType(cls.f, cls(), cls) to bind properly, but (cls.f, self, cls)
        # cannot unless self is in the cls MRO chain which is not guaranteed if cls just derives
        # from a vanilla object.
        raise TypeError('Cannot mount methods from an unbound class.')
      self._mounts.add(method_self)
      setattr(self, method_name, getattr(class_instance, method_name))

  @classmethod
  def _apply_plugins(cls, class_instance, kw):
    plugins = kw.get('apply', [])
    skiplist = kw.get('skip', [])
    class_plugins = getattr(class_instance, 'plugins', [])
    class_skiplist = getattr(class_instance, 'skiplist', [])
    kw.update(apply=plugins + class_plugins, skip=skiplist + class_skiplist)
    return kw

  def mount_routes(self, class_instance):
    """
      Mount the routes from another class instance.

      The routes must be added to the class via the HttpServer.route annotation and not directly
      from the bottle.route decorator.
    """
    for callback_name in dir(class_instance):
      callback = getattr(class_instance, callback_name)
      if hasattr(callback, self.ROUTES_ATTRIBUTE) or hasattr(callback, self.ERROR_ATTRIBUTE):
        # Bind the un-annotated callback to this class
        self._bind_method(class_instance, callback_name)
        # Apply view annotations
        if hasattr(callback, self.VIEW_ATTRIBUTE):
          args, kw = getattr(callback, self.VIEW_ATTRIBUTE)
          callback = bottle.view(*args, **kw)(callback)
          setattr(self, callback_name, callback)
        # Apply route annotations
        for args, kw in getattr(callback, self.ROUTES_ATTRIBUTE, ()):
          kw = self._apply_plugins(class_instance, copy.deepcopy(kw))
          kw.update(callback=callback)
          self._app.route(*args, **kw)
        for error_code in getattr(callback, self.ERROR_ATTRIBUTE, ()):
          self._app.error(error_code)(callback)

  @property
  def app(self):
    """
      Return the bottle app object associated with this HttpServer instance.
    """
    return self._app

  @property
  def hostname(self):
    return self._hostname

  @property
  def port(self):
    return self._port

  def run(self, hostname, port, server='wsgiref'):
    """
      Start a webserver on hostname & port.
    """
    self._hostname = hostname
    self._port = port
    self._app.run(host=hostname, port=port, server=server)

  def __str__(self):
    return 'HttpServer(%s, mixins: %s)' % (
        '%s:%s' (self.hostname, self.port) if self.hostname else 'unbound',
        ', '.join(self.source_name(instance) for instance in self._mounts))


abort = HttpServer.abort
mako_view = HttpServer.mako_view
redirect = HttpServer.redirect
request = HttpServer.request
response = HttpServer.response
route = HttpServer.route
static_file = HttpServer.static_file
view = HttpServer.view

########NEW FILE########
__FILENAME__ = attribute_info
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import sys
from .java_types import *
from .class_flags import ClassFlags
from . import signature_parser

class AttributeInfo(object):
  """
    Encapsulate the attribute_info class.
    http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#43817

    attribute_info {
      u2 attribute_name_index;
      u4 attribute_length;
      u1 info[attribute_length];
    }
  """
  def __init__(self, data, constants):
    self._parse_header(data, constants)

  def _parse_header(self, data, constants):
    self._attribute_name_index = u2(data[0:2]).get()
    self._attribute_name       = constants[self._attribute_name_index]
    self._attribute_length     = u4(data[2:6]).get()
    self._size                 = 6 + self._attribute_length
    self._info_data            = data[6:self._size]

  def name(self):
    return self._attribute_name

  def size(self):
    """Total size of the attribute_info blob."""
    return self._size

  def parsed_name(self):
    return self._attribute_name

  def bytes(self):
    """Attribute-specific data for subclasses."""
    return self._info_data

  def __str__(self):
    return 'AttributeInfo(name:%s, size=%d)' % (self._attribute_name, self.size())

class Code(AttributeInfo):
  """
    Code_attribute {
      u2 attribute_name_index;
      u4 attribute_length;
      u2 max_stack;
      u2 max_locals;
      u4 code_length;
      u1 code[code_length];
      u2 exception_table_length;
      {
        u2 start_pc;
        u2 end_pc;
        u2 handler_pc;
        u2 catch_type;
     } exception_table[exception_table_length];
     u2 attributes_count;
     attribute_info attributes[attributes_count];
  }
  """
  @staticmethod
  def name():
    return 'Code'

  def __init__(self, data, constants):
    AttributeInfo.__init__(self, data, constants)
    bytes = self.bytes()

    (max_stack, max_locals, code_length), bytes = JavaNativeType.parse(bytes, u2, u2, u4)
    self._code_length = code_length

    bytecode = bytes[0:code_length]
    bytes = bytes[code_length:]
    (exception_table_length,), bytes = JavaNativeType.parse(bytes, u2)

    # gobble up stuff
    for k in range(exception_table_length):
      _, bytes = JavaNativeType.parse(bytes, u2, u2, u2, u2)

    (attributes_count,), bytes = JavaNativeType.parse(bytes, u2)
    attributes = []
    offset = 0
    for k in range(attributes_count):
      attribute = Attribute.parse(bytes[offset:], constants)
      offset += attribute.size()
      attributes.append(attribute)
    self._attributes = attributes

  def __str__(self):
    output = 'Code(length:%s)' % self._code_length
    if self._attributes:
      output += '\n'
      attrs = []
      for attr in self._attributes:
        attrs.append('        %s: %s' % (attr.name(), attr))
      output += '\n'.join(attrs)
    return output

class SourceFile(AttributeInfo):
  """
    http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#79868
    SourceFile_attribute {
      u2 attribute_name_index;
      u4 attribute_length;
      u2 sourcefile_index;
    }
  """
  @staticmethod
  def name():
    return 'SourceFile'

  def __init__(self, data, constants):
    AttributeInfo.__init__(self, data, constants)
    bytes = self.bytes()
    self._sourcefile_index     = u2(bytes[0:2]).get()
    self._sourcefile           = constants[self._sourcefile_index]

  def __str__(self):
    return 'SourceFile(file:%s)' % self._sourcefile

class Exceptions(AttributeInfo):
  """
    http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#3129
    Exceptions_attribute {
      u2 attribute_name_index;
      u4 attribute_length;
      u2 number_of_exceptions;
      u2 exception_index_table[number_of_exceptions];
    }
  """
  @staticmethod
  def name():
    return 'Exceptions'

  def __init__(self, data, constants):
    AttributeInfo.__init__(self, data, constants)
    bytes = self.bytes()

    self._number_of_exceptions = u2(bytes[0:2]).get()
    self._exceptions = []
    for index in range(self._number_of_exceptions):
      constant_index = u2(bytes[2*(index+1):]).get()
      self._exceptions.append(constants[constant_index](constants))

  def __str__(self):
    if self._exceptions:
      return 'throws %s' % ' '.join('%s' % s for s in self._exceptions)
    else:
      return ''

class Signature(AttributeInfo):
  """
    Signature_attribute {
      u2 attribute_name_index;
      u4 attribute_length;
      u2 signature_index
    }
  """
  @staticmethod
  def name():
    return 'Signature'

  def __init__(self, data, constants):
    AttributeInfo.__init__(self, data, constants)
    bytes = self.bytes()

    self._signature_index = u2(bytes[0:2]).get()
    self._signature = constants[self._signature_index]
    self._parsed = None
    self._parse_signature()

  def _parse_signature(self):
    class_signature, _ = signature_parser.ClassSignature.match(self._signature.bytes())
    if class_signature:
      self._parsed = class_signature
      return

    method_signature, _ = signature_parser.MethodTypeSignature.match(self._signature.bytes())
    if method_signature:
      self._parsed = method_signature

  def __str__(self):
    return 'Signature(%s)' % (
      self._parsed)

class InnerClassFlags(object):
  """http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#75734
  """
  ACC_PUBLIC	 = 0x0001
  ACC_PRIVATE	 = 0x0002
  ACC_PROTECTED	 = 0x0004
  ACC_STATIC	 = 0x0008
  ACC_FINAL	 = 0x0010
  ACC_INTERFACE	 = 0x0200
  ACC_ABSTRACT	 = 0x0400
  ACC_SYNTHETIC	 = 0x1000
  ACC_ANNOTATION = 0x2000
  ACC_ENUM	 = 0x4000

  MASK = ACC_PUBLIC   | ACC_PRIVATE   | ACC_PROTECTED  | \
         ACC_STATIC   | ACC_FINAL     | ACC_INTERFACE  | \
         ACC_ABSTRACT | ACC_SYNTHETIC | ACC_ANNOTATION | \
         ACC_ENUM

  def __init__(self, flags):
    self._flags = flags
    if flags ^ (flags & InnerClassFlags.MASK) != 0:
      print >> sys.stderr, "Invalid InnerClassFlags mask!! Extra bits: %s" % (
        flags ^ (flags & InnerClassFlags.MASK))

  def public(self):
    return self._flags & InnerClassFlags.ACC_PUBLIC

  def private(self):
    return self._flags & InnerClassFlags.ACC_PRIVATE

  def protected(self):
    return self._flags & InnerClassFlags.ACC_PROTECTED

  def static(self):
    return self._flags & InnerClassFlags.ACC_STATIC

  def final(self):
    return self._flags & InnerClassFlags.ACC_FINAL

  def interface(self):
    return self._flags & InnerClassFlags.ACC_INTERFACE

  def abstract(self):
    return self._flags & InnerClassFlags.ACC_ABSTRACT

  def synthetic(self):
    return self._flags & InnerClassFlags.ACC_SYNTHETIC

  def annotation(self):
    return self._flags & InnerClassFlags.ACC_ANNOTATION

  def enum(self):
    return self._flags & InnerClassFlags.ACC_ENUM

  def __str__(self):
    verbs = []
    if self.public(): verbs.append('public')
    if self.private(): verbs.append('private')
    if self.protected(): verbs.append('protected')
    if self.static(): verbs.append('static')
    if self.final(): verbs.append('final')
    if self.interface(): verbs.append('interface')
    if self.abstract(): verbs.append('abstract')
    if self.synthetic(): verbs.append('synthetic')
    if self.annotation(): verbs.append('annotation')
    if self.enum(): verbs.append('enum')
    return ' '.join(verbs)

class InnerClass(object):
  def __init__(self, data, constants):
    (inner_class_info_index, outer_class_info_index,
     inner_name_index, inner_class_flags), data = JavaNativeType.parse(data, u2, u2, u2, u2)

    debug = """
    print 'constant pool size, inner, outer, name, flags = %s, %s, %s, %s, %s => %s' % (
      len(constants),
      inner_class_info_index,
      outer_class_info_index,
      inner_name_index,
      inner_class_flags,
      InnerClassFlags(inner_class_flags))
    """

    self._inner_class = constants[inner_class_info_index]
    if outer_class_info_index < len(constants):
      self._outer_class = constants[outer_class_info_index]
    else:
      print >> sys.stderr, 'WARNING: Malformed InnerClass(outer_class_info_index)!'
      self._outer_class = None
    if inner_name_index < len(constants):
      self._inner_name = constants[inner_name_index]
    else:
      print >> sys.stderr, 'WARNING: Malformed InnerClass(inner_name)!'
      self._inner_name = None
    self._inner_class_access_flags = InnerClassFlags(inner_class_flags)

    if self._inner_class is not None:
      self._inner_class = self._inner_class(constants)
    if self._outer_class is not None:
      self._outer_class = self._outer_class(constants)
    if self._inner_name is not None:
      self._inner_name = self._inner_name(constants)
    else:
      self._inner_name = 'Anonymous'

  def __str__(self):
    return '%s %s::%s %s' % (
      self._inner_class_access_flags,
      self._outer_class,
      self._inner_class,
      self._inner_name)

class InnerClasses(AttributeInfo):
  """
    http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#79996
    InnerClasses_attribute {
      u2 attribute_name_index;
      u4 attribute_length;
      ------
      u2 number_of_classes;
      {  u2 inner_class_info_index;
         u2 outer_class_info_index;
         u2 inner_name_index;
         u2 inner_class_access_flags;
      } classes[number_of_classes];
    }
  """
  @staticmethod
  def name():
    return 'InnerClasses'

  def __init__(self, data, constants):
    AttributeInfo.__init__(self, data, constants)
    bytes = self.bytes()

    self._number_of_classes = u2(bytes[0:2]).get()
    self._classes = []
    offset = 2
    for index in range(self._number_of_classes):
      klass = InnerClass(data[offset:], constants)
      self._classes.append(klass)
      offset += 4 * u2.size()

  def __str__(self):
    return '{\n%s\n}' % ('\n  '.join('%s' % s for s in self._classes))

class Attribute(object):
  """
    Factory for producing AttributeInfos.
  """

  _KNOWN_ATTRIBUTE_MAP = {
    SourceFile.name(): SourceFile,
    Signature.name(): Signature,
    Exceptions.name(): Exceptions,
    Code.name(): Code
    # InnerClasses.name(): InnerClasses
  }

  @staticmethod
  def parse(data, constants):
    """Parse the Attribute_info

      @data: The data stream from which to deserialize the blob
      @constants: The constant pool of the class file.
    """
    attribute_name_index = u2(data[0:2]).get()
    attribute_name       = constants[attribute_name_index]

    attribute_class = Attribute._KNOWN_ATTRIBUTE_MAP.get(attribute_name.bytes(), None)
    if attribute_class is not None:
      return attribute_class(data, constants)
    else:
      return AttributeInfo(data, constants)

########NEW FILE########
__FILENAME__ = jar_inspect
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
from zipfile import ZipFile

from twitter.common import app, log
from twitter.common.java.class_file import ClassFile

app.set_option('log_to_stderr', 'INFO')

def main(args):
  log.debug('main got args: %s' % args)
  for arg in args:
    if arg.endswith('.jar'):
      zp = ZipFile(arg)
      for f in zp.filelist:
        if f.filename.endswith('.class'):
          print
          print
          print '%s' % f.filename,
          foo = ClassFile(zp.read(f.filename))
          print ' => methods:%s, interfaces:%s' % (
            len(foo.methods()),
            len(foo.interfaces()))
          print foo
      zp.close()

app.main()

########NEW FILE########
__FILENAME__ = java_inspect
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common import app
from twitter.common.java.class_file import ClassFile

def main(args):
  for arg in args:
    cf = ClassFile.from_file(arg)
    print os.path.abspath(arg)
    print cf
    print

app.main()
########NEW FILE########
__FILENAME__ = class_file
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from .java_types import *
from .class_flags import ClassFlags
from .constant import (
  Constant, LongConstant, DoubleConstant, ClassConstant,
  FieldrefConstant, InterfaceMethodrefConstant, MethodrefConstant)
from .field_info import FieldInfo
from .method_info import MethodInfo
from .attribute_info import Attribute
from .signature_parser import PackageSpecifier

from hashlib import md5

class ClassDecoders:
  @staticmethod
  def decode_constant_pool(data, count):
    # 0th entry is a sentinel, since constants are indexed 1..constant_pool_size
    constants = [None]
    offset = 0
    skip = False
    for k in range(1, count):
      if skip:
        skip = False
        continue
      constant = Constant.parse(data[offset:])
      constants.append(constant)
      offset += constant.size()
      # special cases for Long/Double constants!
      # http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#1348
      if isinstance(constant, (LongConstant, DoubleConstant)):
        constants.append(None)  # append a sentinel
        skip = True
    return constants, data[offset:]

  @staticmethod
  def decode_interfaces(data, count, constants):
    interfaces, data = JavaNativeType.parse(data, *[u2]*count)
    interfaces = map(lambda offset: constants[offset], interfaces)
    return interfaces, data

  @staticmethod
  def decode_fields(data, count, constants):
    fields = []
    offset = 0
    for k in range(count):
      field = FieldInfo(data[offset:], constants)
      offset += field.size()
      fields.append(field)
    return fields, data[offset:]

  @staticmethod
  def decode_methods(data, count, constants):
    methods = []
    offset = 0
    for k in range(count):
      method = MethodInfo(data[offset:], constants)
      offset += method.size()
      methods.append(method)
    return methods, data[offset:]

  @staticmethod
  def decode_attributes(data, count, constants):
    attributes = []
    offset = 0
    for k in range(count):
      attribute = Attribute.parse(data[offset:], constants)
      offset += attribute.size()
      attributes.append(attribute)
    return attributes, data[offset:]

class ClassFile(object):
  """Wrapper for a .class file.
  """

  _LINKAGE_CONSTANT_TYPES = (
    FieldrefConstant,
    InterfaceMethodrefConstant,
    MethodrefConstant)

  def __init__(self, data):
    self._data = data
    self._decode()
    self._track_dependencies()

  def _linkage_constants(self):
    return [
      c for c in self._constant_pool
      if isinstance(c, self._LINKAGE_CONSTANT_TYPES)]

  def linkage_signature(self):
    cs = [c(self._constant_pool) for c in self._linkage_constants()]
    m = md5()
    m.update('\n'.join(sorted(cs)))
    return m.hexdigest()

  def _track_dependencies(self):
    self._external_references = set(
      c(self._constant_pool) for c in self._linkage_constants())

  @staticmethod
  def from_fp(fp):
    return ClassFile(fp.read())

  @staticmethod
  def from_file(filename):
    with open(filename, 'rb') as fp:
      return ClassFile.from_fp(fp)

  def _decode(self):
    data = self._data

    (self._magic, self._minor_version, self._major_version), data = \
      JavaNativeType.parse(data, u4, u2, u2)
    assert self._magic == 0xCAFEBABE

    # constant pool
    (self._constant_pool_count,), data = JavaNativeType.parse(data, u2)
    self._constant_pool, data = ClassDecoders.decode_constant_pool(
      data, self._constant_pool_count)

    (access_flags, this_class, super_class), data = JavaNativeType.parse(data, u2, u2, u2)
    self._access_flags  = ClassFlags(access_flags)
    self._this_class    = self._constant_pool[this_class]
    self._super_class   = self._constant_pool[super_class]

    # interfaces
    (self._interfaces_count,), data = JavaNativeType.parse(data, u2)
    self._interfaces, data = ClassDecoders.decode_interfaces(
      data, self._interfaces_count, self._constant_pool)

    # fields
    (self._fields_count,), data = JavaNativeType.parse(data, u2)
    self._fields, data = ClassDecoders.decode_fields(
      data, self._fields_count, self._constant_pool)

    # methods
    (self._methods_count,), data = JavaNativeType.parse(data, u2)
    self._methods, data = ClassDecoders.decode_methods(
      data, self._methods_count, self._constant_pool)

    # attributes
    (self._attributes_count,), data = JavaNativeType.parse(data, u2)
    self._attributes, data = ClassDecoders.decode_attributes(
      data, self._attributes_count, self._constant_pool)

  def version(self):
    return self._major_version, self._minor_version

  def methods(self):
    return self._methods

  def attributes(self):
    return self._attributes

  def interfaces(self):
    return self._interfaces

  def fields(self):
    return self._fields

  def this_class(self):
    return self._this_class(self._constant_pool)

  def super_class(self):
    return self._super_class(self._constant_pool)

  def constants(self):
    return self._constant_pool

  def constant(self, index):
    return self._constant_pool[index](self._constant_pool)

  def access_flags(self):
    return self._access_flags

  def __str__(self):
    const = self._constant_pool
    output = []
    output.append("class version: (%d, %d)" % (self._major_version, self._minor_version))
    output.append("this: %s" % self._this_class(const))
    output.append("super: %s" % self._super_class(const))
    output.append("access flags: %s" % self._access_flags)
    if self._interfaces:
      output.append("interfaces: ")
      for interface in self._interfaces:
        output.append("  %s" % interface(const))
    if self._fields:
      output.append("fields: ")
      for field in self._fields:
        output.append("  %s" % field)
    if self._methods:
      output.append("methods: ")
      for method in self._methods:
        output.append("  %s" % method)
    if self._attributes:
      output.append("attributes: ")
      for attribute in self._attributes:
        output.append("  %s" % attribute)
    if self._external_references:
      output.append("external references: ")
      for ref in self._external_references:
        output.append("  %s" % ref)
    output.append("linkage signature: \n  %s" % self.linkage_signature())
    return '\n'.join(output)

########NEW FILE########
__FILENAME__ = class_flags
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from .java_types import *

class ClassFlags(object):
  """http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#75734
  """
  ACC_PUBLIC	 = 0x0001
  ACC_FINAL	 = 0x0010
  ACC_SUPER	 = 0x0020
  ACC_INTERFACE	 = 0x0200
  ACC_ABSTRACT	 = 0x0400

  def __init__(self, flags):
    self._flags = flags

  def public(self):
    return self._flags & ClassFlags.ACC_PUBLIC

  def final(self):
    return self._flags & ClassFlags.ACC_FINAL

  def super_(self):
    return self._flags & ClassFlags.ACC_SUPER

  def interface(self):
    return self._flags & ClassFlags.ACC_INTERFACE

  def abstract(self):
    return self._flags & ClassFlags.ACC_ABSTRACT

  def __str__(self):
    verbs = []
    if self.public(): verbs.append('public')
    if self.final(): verbs.append('final')
    if self.super_(): verbs.append('super')
    if self.interface(): verbs.append('interface')
    if self.abstract(): verbs.append('abstract')
    return ' '.join(verbs)

########NEW FILE########
__FILENAME__ = constant
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from .util import javaify
from .java_types import *

"""
  Parse constants as defined in
  http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#20080
"""

class ConstantBase(object):
  def __call__(self, constants):
    return 'AnonymousConstant()'

  def parse(self, data):
    elements, _ = JavaNativeType.parse(data, *self.__class__.TYPES)
    return elements

  def size(self):
    return sum(map(lambda typ: typ.size(), self.__class__.TYPES))

class ClassConstant(ConstantBase):
  """
    u1 tag
    u2 name_index
  """
  TYPES = [u1, u2]

  def __init__(self, data):
    self._tag = u1(data[0:1]).get()
    self._name_index = u2(data[1:3]).get()

  def __call__(self, constants):
    return str(constants[self._name_index].bytes())

class FieldrefConstant(ConstantBase):
  """
    u1 tag
    u2 class_index
    u2 name_and_type_index
  """

  TYPES = [u1, u2, u2]

  def __init__(self, data):
    self._tag, self._class_index, self._name_and_type_index = self.parse(data)

  def __call__(self, constants):
    return '%s.%s' % (
      constants[self._class_index](constants),
      constants[self._name_and_type_index](constants))

class MethodrefConstant(ConstantBase):
  """
    u1 tag
    u2 class_index
    u2 name_and_type_index
  """

  TYPES = [u1, u2, u2]

  def __init__(self, data):
    self._tag, self._class_index, self._name_and_type_index = self.parse(data)

  def __call__(self, constants):
    return '%s.%s' % (
      constants[self._class_index](constants),
      constants[self._name_and_type_index](constants))

class InterfaceMethodrefConstant(ConstantBase):
  """
    u1 tag
    u2 class_index
    u2 name_and_type_index
  """

  TYPES = [u1, u2, u2]

  def __init__(self, data):
    self._tag, self._class_index, self._name_and_type_index = self.parse(data)

  def __call__(self, constants):
    return '%s.%s' % (
      constants[self._class_index](constants),
      constants[self._name_and_type_index](constants))

class StringConstant(ConstantBase):
  """
    u1 tag
    u2 string_index
  """
  TYPES = [u1, u2]

  def __init__(self, data):
    self._tag, self._string_index = self.parse(data)

class IntegerConstant(ConstantBase):
  """
    u1 tag
    u4 bytes
  """
  TYPES = [u1, u4]

  def __init__(self, data):
    self._tag, self._bytes = self.parse(data)

class FloatConstant(ConstantBase):
  """
    u1 tag
    u4 bytes
  """
  TYPES = [u1, u4]

  def __init__(self, data):
    self._tag, self._bytes = self.parse(data)

class LongConstant(ConstantBase):
  """
    u1 tag
    u4 high_bytes
    u4 low_bytes
  """
  TYPES = [u1, u4, u4]

  def __init__(self, data):
    self._tag, self._high_bytes, self._low_bytes = self.parse(data)

class DoubleConstant(ConstantBase):
  """
    u1 tag
    u4 high_bytes
    u4 low_bytes
  """
  TYPES = [u1, u4, u4]

  def __init__(self, data):
    self._tag, self._high_bytes, self._low_bytes = self.parse(data)

class NameAndTypeConstant(ConstantBase):
  """
    u1 tag
    u2 name_index
    u2 descriptor_index
  """
  TYPES = [u1, u2, u2]

  def __init__(self, data):
    self._tag, self._name_index, self._descriptor_index = self.parse(data)

  def size(self):
    return u1.size() + u2.size() + u2.size()

  def __call__(self, constants):
    return '%s.%s' % (
      constants[self._name_index].bytes(),
      constants[self._descriptor_index].bytes())

class Utf8Constant(ConstantBase):
  """
    u1 tag
    u2 length
    u1 bytes[length]
  """
  def __init__(self, data):
    (self._tag, self._length), data = JavaNativeType.parse(data, u1, u2)
    self._bytes = data[0:self._length]

  def size(self):
    return u1.size() + u2.size() + self._length

  def bytes(self):
    return self._bytes

  def __str__(self):
    return self._bytes

class Constant(object):
  # http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#1221
  CONSTANT_Class              = 7
  CONSTANT_Fieldref           = 9
  CONSTANT_Methodref          = 10
  CONSTANT_InterfaceMethodref = 11
  CONSTANT_String             = 8
  CONSTANT_Integer            = 3
  CONSTANT_Float              = 4
  CONSTANT_Long               = 5
  CONSTANT_Double             = 6
  CONSTANT_NameAndType        = 12
  CONSTANT_Utf8               = 1

  _BASE_TYPES = {
    CONSTANT_Class: ClassConstant,
    CONSTANT_Fieldref: FieldrefConstant,
    CONSTANT_Methodref: MethodrefConstant,
    CONSTANT_InterfaceMethodref: InterfaceMethodrefConstant,
    CONSTANT_String: StringConstant,
    CONSTANT_Integer: IntegerConstant,
    CONSTANT_Float: FloatConstant,
    CONSTANT_Long: LongConstant,
    CONSTANT_Double: DoubleConstant,
    CONSTANT_NameAndType: NameAndTypeConstant,
    CONSTANT_Utf8: Utf8Constant
  }

  @staticmethod
  def parse(data):
    print('parse data[0] = %s' % data[0])
    tag = u1(data[0]).get()
    constant = Constant._BASE_TYPES[tag](data)
    return constant

########NEW FILE########
__FILENAME__ = field_info
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from .java_types import *
from .attribute_info import Attribute
from .signature_parser import BaseType
from . import util

_UNPARSED = (None, 0)

class FieldInfoFlags(object):
  """http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#88358
  """

  ACC_PUBLIC	 = 0x0001
  ACC_PRIVATE	 = 0x0002
  ACC_PROTECTED	 = 0x0004
  ACC_STATIC	 = 0x0008
  ACC_FINAL	 = 0x0010
  ACC_VOLATILE	 = 0x0040
  ACC_TRANSIENT	 = 0x0080

  def __init__(self, data):
    self._flags = u2(data).get()

  def public(self):
    return self._flags & FieldInfoFlags.ACC_PUBLIC

  def private(self):
    return self._flags & FieldInfoFlags.ACC_PRIVATE

  def protected(self):
    return self._flags & FieldInfoFlags.ACC_PROTECTED

  def static(self):
    return self._flags & FieldInfoFlags.ACC_STATIC

  def final(self):
    return self._flags & FieldInfoFlags.ACC_FINAL

  def volatile(self):
    return self._flags & FieldInfoFlags.ACC_VOLATILE

  def transient(self):
    return self._flags & FieldInfoFlags.ACC_TRANSIENT

  def __str__(self):
    verbs = []
    if self.public(): verbs.append('public')
    if self.private(): verbs.append('private')
    if self.protected(): verbs.append('protected')
    if self.static(): verbs.append('static')
    if self.final(): verbs.append('final')
    if self.volatile(): verbs.append('volatile')
    if self.transient(): verbs.append('transient')
    return ' '.join(verbs)

class ObjectType(object):
  @staticmethod
  def match(data):
    if data[0] == 'L':
      eof = data.find(';')
      return data[1:eof], eof + 1
    else:
      return _UNPARSED

class ArrayType(object):
  @staticmethod
  def match(data):
    if data[0] == '[':
      component, offset = ComponentType.match(data[1:])
      return component+'[]', offset + 1
    else:
      return _UNPARSED

class ComponentType(object):
  @staticmethod
  def match(data):
    return FieldType.match(data)

class FieldDescriptor(object):
  @staticmethod
  def match(data):
    return FieldType.match(data)

class FieldType(object):
  """http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#1170

    FieldType:
      BaseType
      ObjectType
      ArrayType

    FieldDescriptor:
      FieldType

    ComponentType:
      FieldType

    BaseType: 'B' | 'C' | 'D' | 'F' | 'I' | 'J' | 'S' | 'Z'

    ObjectType:
      L <classname> ;

    ArrayType:
      [ ComponentType
  """
  @staticmethod
  def match(data):
    base_type, offset = BaseType.match(data)
    if offset: return base_type, offset
    object_type, offset = ObjectType.match(data)
    if offset: return object_type, offset
    array_type, offset = ArrayType.match(data)
    if offset: return array_type, offset
    return _UNPARSED

class FieldInfo(object):
  def __init__(self, data, constants):
    self._access_flags = FieldInfoFlags(data[0:2])
    (self._name_index, self._descriptor_index, self._attributes_count), data = \
      JavaNativeType.parse(data[2:], u2, u2, u2)
    self._name = constants[self._name_index] # synthesized
    self._descriptor = constants[self._descriptor_index] # synthesized
    self._parsed_descriptor, _ = FieldDescriptor.match(self._descriptor.bytes())
    self._attributes = []
    offset = 0
    for k in range(self._attributes_count):
      attribute = Attribute.parse(data[offset:], constants)
      offset += attribute.size()
      self._attributes.append(attribute)
    self._size = offset + 8

  def size(self):
    return self._size

  def __str__(self):
    base = '%s %s %s' % (
      self._access_flags,
      util.javaify(self._parsed_descriptor),
      self._name)
    if self._attributes:
      for attr in self._attributes:
        base += '\n    %s: %s' % (attr.name(), attr)
      base += '\n'
    return base

########NEW FILE########
__FILENAME__ = java_types
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import struct

class JavaNativeType(object):
  class ParseException(Exception): pass

  def __init__(self, data):
    pass

  def __call__(self):
    return self._value

  def value(self):
    return self._value

  def get(self):
    return self.value()

  @staticmethod
  def size():
    raise Exception("Unimplemented!")

  @staticmethod
  def parse(data, *type_args):
    offset = 0
    parsed_types = []
    total_size = 0
    for t in type_args:
      if not issubclass(t, JavaNativeType):
        raise JavaNativeType.ParseException("Not a valid JavaNativeType: %s" % t)
      total_size += t.size()
    if total_size > len(data):
      raise JavaNativeType.ParseException("Not enough data to deserialize %s" % repr(type_args))
    for t in type_args:
      parsed_type = t(data[slice(offset, offset + t.size())]).value()
      parsed_types.append(parsed_type)
      offset += t.size()
    return parsed_types, data[total_size:]

class u1(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack('>B', data[0:1])[0]

  @staticmethod
  def size():
    return 1

class u2(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack(">H", data[0:2])[0]

  @staticmethod
  def size():
    return 2

class s2(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack(">h", data[0:2])[0]

  @staticmethod
  def size():
    return 2

class u4(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack(">L", data[0:4])[0]

  @staticmethod
  def size():
    return 4

class s4(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack(">l", data[0:4])[0]

  @staticmethod
  def size():
    return 4

class s8(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack(">q", data[0:8])[0]

  @staticmethod
  def size():
    return 8

class f4(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack(">f", data[0:4])[0]

  @staticmethod
  def size():
    return 4

class f8(JavaNativeType):
  def __init__(self, data):
    JavaNativeType.__init__(self, data)
    self._value = struct.unpack(">d", data[0:8])[0]

  @staticmethod
  def size():
    return 8

########NEW FILE########
__FILENAME__ = method_info
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from .util import javaify
from .java_types import *
from .attribute_info import Attribute
from .field_info import FieldType

class MethodInfoFlags(object):
  """http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#75568
  """

  ACC_PUBLIC	   = 0x0001
  ACC_PRIVATE	   = 0x0002
  ACC_PROTECTED	   = 0x0004
  ACC_STATIC	   = 0x0008
  ACC_FINAL	   = 0x0010
  ACC_SYNCHRONIZED = 0x0020
  ACC_NATIVE       = 0x0100
  ACC_ABSTRACT     = 0x0400
  ACC_STRICT       = 0x0800

  def __init__(self, data):
    self._flags = u2(data).get()

  def public(self):
    return self._flags & MethodInfoFlags.ACC_PUBLIC

  def private(self):
    return self._flags & MethodInfoFlags.ACC_PRIVATE

  def protected(self):
    return self._flags & MethodInfoFlags.ACC_PROTECTED

  def static(self):
    return self._flags & MethodInfoFlags.ACC_STATIC

  def final(self):
    return self._flags & MethodInfoFlags.ACC_FINAL

  def synchronized(self):
    return self._flags & MethodInfoFlags.ACC_SYNCHRONIZED

  def native(self):
    return self._flags & MethodInfoFlags.ACC_NATIVE

  def abstract(self):
    return self._flags & MethodInfoFlags.ACC_ABSTRACT

  def strict(self):
    return self._flags & MethodInfoFlags.ACC_STRICT

  def __str__(self):
    verbs = []
    if self.public(): verbs.append('public')
    if self.private(): verbs.append('private')
    if self.protected(): verbs.append('protected')
    if self.static(): verbs.append('static')
    if self.final(): verbs.append('final')
    if self.synchronized(): verbs.append('synchronized')
    if self.native(): verbs.append('native')
    if self.abstract(): verbs.append('abstract')
    if self.strict(): verbs.append('strict')
    return ' '.join(verbs)

class MethodDescriptor(object):
  """http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#1170

    MethodDescriptor:
      ( ParameterDescriptor* ) ReturnDescriptor

    ParameterDescriptor:
      FieldType

    ReturnDescriptor:
      FieldType
      V
  """

  @staticmethod
  def match(data):
    parameters = []
    assert data[0] == '('
    index = 1
    while True:
      descriptor, offset = ParameterDescriptor.match(data[index:])
      if offset == 0: break
      parameters.append(descriptor)
      index += offset
    assert data[index] == ')', 'data[%s] is actually: %s, full: %s' % (index, data[index], data)
    return_descriptor, offset = ReturnDescriptor.match(data[index+1:])
    return '%s %%s(%s)' % (
      javaify(return_descriptor),
      ', '.join(javaify(p) for p in parameters) if parameters else 'void'), index+1+offset

class ParameterDescriptor(object):
  @staticmethod
  def match(data):
    return FieldType.match(data)

class ReturnDescriptor(object):
  @staticmethod
  def match(data):
    field_type, offset = FieldType.match(data)
    if offset: return field_type, offset
    if data[0] == 'V': return 'void', 1
    return None, 0

class MethodInfo(object):
  def __init__(self, data, constants):
    self._access_flags      = MethodInfoFlags(data[0:2])
    (self._name_index, self._descriptor_index, self._attributes_count), data = \
      JavaNativeType.parse(data[2:], u2, u2, u2)
    self._name              = constants[self._name_index] # synthesized
    self._descriptor        = constants[self._descriptor_index] # synthesized
    self._parsed_descriptor, _ = MethodDescriptor.match(self._descriptor.bytes())
    self._attributes = []
    offset = 0
    for k in range(self._attributes_count):
      attribute = Attribute.parse(data[offset:], constants)
      offset += attribute.size()
      self._attributes.append(attribute)
    self._size = offset + 8

  def size(self):
    return self._size

  def __str__(self):
    output = []
    sig = ''
    if self._access_flags:
      sig += '%s ' % self._access_flags
    sig += (self._parsed_descriptor % self._name)
    output.append(sig)
    for attr in self._attributes:
      if attr.name() == 'Signature':
        output.append('    %s: %s' % (attr.name(), attr._parsed))
      else:
        output.append('    %s: %s' % (attr.name(), attr))
    return '\n'.join('%s' % o for o in output)

########NEW FILE########
__FILENAME__ = attribute_buffer
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from abc import abstractproperty
import struct

from twitter.common.lang import Interface


class AttributeBuffer(Interface):
  """Provider a simple attribute proxy over a binary stream."""
  BIG_ENDIAN = 0
  LITTLE_ENDIAN = 1

  @abstractproperty
  def data(self):
    """Return the data stream of this attribute buffer."""

  @abstractproperty
  def attributes(self):
    """Return a map of attribute name => (struct format, byte range)"""

  @abstractproperty
  def endianness(self):
    """Return the endianness of the buffers."""

  def unpack(self, format_str, data_range):
    """Unpack a stream from the underlying buffer data_range using format_str."""
    new_format_str = ''.join(
        ('%c%c' % ('>' if self.endianness is self.BIG_ENDIAN else '<', format)
        for format in format_str))
    try:
      value = struct.unpack(new_format_str, self.data[data_range])
    except struct.error as e:
      raise ValueError('Possibly corrupt data buffer: %s' % e)
    if len(format_str) > 1:
      return value
    else:
      return value[0]

  def __getattr__(self, attribute):
    if attribute not in self.attributes:
      return self.__getattribute__(attribute)
    format_str, data_range = self.attributes[attribute]
    return self.unpack(format_str, data_range)


class SimpleAttributeBuffer(AttributeBuffer):
  ATTRIBUTES = {}

  def __init__(self, data, endianness):
    self._data, self._endianness = data, endianness

  @property
  def data(self):
    return self._data

  @property
  def endianness(self):
    return self._endianness

  @property
  def attributes(self):
    return self.ATTRIBUTES

########NEW FILE########
__FILENAME__ = jammystat
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import glob

from twitter.common import app

from twitter.common.java.perfdata import PerfData


app.add_option(
    '-f',
    dest='filename',
    default=None,
    help='Filename to load hsperfdata from.')


app.add_option(
    '--hsperfdata_root',
    dest='hsperfdata_root',
    default='/tmp',
    help='Root directory to search for hsperfdata files.')


app.add_option(
    '-l',
    dest='list',
    default=False,
    action='store_true',
    help='List pids.')


app.add_option(
    '-p',
    dest='pid',
    default=None,
    type=int,
    help='PID to load hsperfdata from.')



def file_provider():
  options = app.get_options()
  def provider():
    with open(options.filename, 'rb') as fp:
      return fp.read()
  return provider


def list_pids():
  options = app.get_options()
  pattern = os.path.join(options.hsperfdata_root, 'hsperfdata_*', '*')
  for path in glob.glob(pattern):
    root, pid = os.path.split(path)
    dirname = os.path.basename(root)
    role = dirname[len('hsperfdata_'):]
    yield path, role, int(pid)


def print_pids():
  for path, role, pid in list_pids():
    print('role %s pid %d path %s' % (role, pid, path))


def pid_provider():
  options = app.get_options()
  for path, _, pid in list_pids():
    if pid == options.pid:
      break
  else:
    app.error('Could not find pid %s' % options.pid)
  def loader():
    with open(path, 'rb') as fp:
      return fp.read()
  return loader


def main(args, options):
  if len(args) > 0:
    app.error('Must provide hsperfdata via -f/-p')

  if options.list:
    print_pids()
    return

  perfdata = None
  if options.filename:
    perfdata = PerfData.get(file_provider())
  elif options.pid:
    perfdata = PerfData.get(pid_provider())

  if perfdata is None:
    app.error('No hsperfdata provider specified!')

  perfdata.sample()
  for key in sorted(perfdata):
    print('%s: %s' % (key, perfdata[key]))


app.main()

########NEW FILE########
__FILENAME__ = perfdata2
import struct

from twitter.common.java.perfdata.attribute_buffer import SimpleAttributeBuffer
from twitter.common.java.perfdata.constants import TypeCode, Units, Variability


class PerfDataEntryHeader2(SimpleAttributeBuffer):
  """From v2_0/PerfDataBuffer.java:

   * typedef struct {
   *   jint entry_length;         // entry length in bytes
   *   jint name_offset;          // offset to entry name, relative to start
   *                              // of entry
   *   jint vector_length;        // length of the vector. If 0, then scalar.
   *   jbyte data_type;           // JNI field descriptor type
   *   jbyte flags;               // miscellaneous attribute flags
   *                              // 0x01 - supported
   *   jbyte data_units;          // unit of measure attribute
   *   jbyte data_variability;    // variability attribute
   *   jbyte data_offset;         // offset to data item, relative to start
   *                              // of entry.
   * } PerfDataEntry;
  """
  ATTRIBUTES = {
    'entry_length':  ('i', slice( 0,  4)),
    'name_offset':   ('i', slice( 4,  8)),
    'vector_length': ('i', slice( 8, 12)),
    'data_type':     ('b', slice(12, 13)),
    'flags':         ('b', slice(13, 14)),
    'data_units':    ('b', slice(14, 15)),
    'data_var':      ('b', slice(15, 16)),
    'data_offset':   ('i', slice(16, 20)),
  }
  LENGTH = 20


class PerfDataBuffer2Prologue(SimpleAttributeBuffer):
  ATTRIBUTES = {
    'accessible':      ('b', slice( 7,  8)),
    'prologue_used':   ('i', slice( 8, 12)),
    'overflow_offset': ('i', slice(12, 16)),
    'mtime':           ('q', slice(16, 24)),
    'entry_offset':    ('i', slice(24, 28)),
    'num_entries':     ('i', slice(28, 32))
  }


class PerfData2Format(object):
  def __init__(self, endianness=SimpleAttributeBuffer.LITTLE_ENDIAN):
    self._endianness = endianness

  def __call__(self, data):
    prologue = PerfDataBuffer2Prologue(data, self._endianness)

    if not prologue.accessible:
      return {}

    monitor_map = {}
    start_offset = prologue.entry_offset
    parsed_entries = 0

    def more_entries():
      return start_offset + PerfDataEntryHeader2.LENGTH < len(data)

    while more_entries() and parsed_entries < prologue.num_entries:
      entry = PerfDataEntryHeader2(
          data[start_offset:start_offset + PerfDataEntryHeader2.LENGTH], self._endianness)

      name_start = start_offset + entry.name_offset
      name_end = data.find('\x00', name_start)
      name = data[name_start:name_end]

      try:
        code = TypeCode.to_code(chr(entry.data_type))
      except KeyError:
        raise ValueError('Failed to figure out type of: %s' % name)
      variability = entry.data_var
      data_start = start_offset + entry.data_offset

      if entry.vector_length == 0:
        if code != TypeCode.LONG:
          raise ValueError('Unexpected monitor type: %d' % code)
        value = struct.unpack(
            '>q' if self._endianness is SimpleAttributeBuffer.BIG_ENDIAN else '<q',
            data[data_start:data_start + 8])[0]
        monitor_map[name] = (entry.data_units, value)
      else:
        if code != TypeCode.BYTE or entry.data_units != Units.STRING or (
            variability not in (Variability.CONSTANT, Variability.VARIABLE)):
          raise ValueError('Unexpected vector monitor: code:%s units:%s variability:%s' % (
              code, entry.data_units, variability))
        monitor_map[name] = (entry.data_units,
            data[data_start:data_start + entry.vector_length].rstrip('\r\n\x00'))

      start_offset += entry.entry_length
      parsed_entries += 1

    return self._postprocess(monitor_map)

  @classmethod
  def _postprocess(cls, monitor_map):
    """Given a monitor map of name = (unit, value), return a normalized set of
       counters."""
    frequency = 1.0 * monitor_map['sun.os.hrt.frequency'][1]

    def ticks_to_duration(value):
      return value / frequency

    def produce_value(unit, value):
      if unit == Units.TICKS:
        return ticks_to_duration(value)
      else:
        return value

    return dict((key, produce_value(value[0], value[1])) for key, value in monitor_map.items())

########NEW FILE########
__FILENAME__ = constants
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

class Units(object):
  INVALID = 0
  NONE = 1
  BYTES = 2
  TICKS = 3
  EVENTS = 4
  STRING = 5
  HERTZ = 6


class Variability(object):
  INVALID = 0
  CONSTANT = 1
  MONOTONIC = 2
  VARIABLE = 3


class TypeCode(object):
  BOOLEAN = 0
  CHAR = 1
  FLOAT = 2
  DOUBLE = 3
  BYTE = 4
  SHORT = 5
  INT = 6
  LONG = 7
  OBJECT = 8
  ARRAY = 9
  VOID = 10

  MAP = {
    'Z': (bool, BOOLEAN),
    'C': (str, CHAR),
    'F': (float, FLOAT),
    'D': (float, DOUBLE),
    'B': (int, BYTE),
    'S': (int, SHORT),
    'I': (int, INT),
    'J': (int, LONG),
    'L': (str, OBJECT),
    '[': (str, ARRAY),
    'V': (str, VOID),
  }

  @classmethod
  def to_code(cls, b):
    try:
      return cls.MAP[b][1]
    except KeyError:
      raise ValueError('Unknown TypeCode: %r' % b)

########NEW FILE########
__FILENAME__ = signature_parser
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# TODO(wickman)  This is horribly broken in Python 3.x for the following reason:
#
# 2.x:
# >>> b'\n'[0:]
# '\n'
# >>> b'\n'[0]
# '\n'
#
# 3.x:
# >>> b'\n'[0:]
# b'\n'
# >>> b'\n'[0]
# 10
#
# Fix it!


_UNPARSED = (None, 0)

def _list_if_none(variable):
  if variable is not None:
    return variable
  else:
    return []

class ParseException(Exception):
  pass

class BaseType(object):
  """http://java.sun.com/docs/books/jvms/second_edition/html/ClassFile.doc.html#84645
  """

  _CHAR_MAP = {
    'B': "byte",       # _signed byte
    'C': "char",       # Unicode character
    'D': "double",     # double-precision floating-point value
    'F': "float",      # single-precision floating-point value
    'I': "int",        # integer
    'J': "long",       # long integer
# Handled by ClassSignature
#   'L': "reference",  # an instance of class <classname>
    'S': "short",      # signed short
    'Z': "boolean",    # true or false
# Handled by ArraySignature
#   '[': "reference",  # one array dimension
  }

  @staticmethod
  def match(data):
    if data[0] in BaseType._CHAR_MAP:
      return BaseType._CHAR_MAP[data[0]], 1
    else:
      #print '  BaseType(_UNPARSED @ %d, data = %s)' % (0, data)
      return _UNPARSED

# ------- signature parsing ---------

class ClassSignature(object):
  """
    ClassSignature:
      [FormalTypeParameters] SuperclassSignature SuperinterfaceSignature*
  """
  @staticmethod
  def match(data):
    offset = 0
    ftp, bytes_read = FormalTypeParameters.match(data[offset:])
    offset += bytes_read
    scs, bytes_read = ClassTypeSignature.match(data[offset:])
    if scs is None:
      return _UNPARSED
    offset += bytes_read
    super_sigs = []
    while offset < len(data):
      sis, bytes_read = ClassTypeSignature.match(data[offset:])
      if sis is None:
        break
      offset += bytes_read
      super_sigs.append(sis)
    return ClassSignature(ftp, scs, super_sigs), offset

  def __init__(self, ftp=None, scs=None, sis=None):
    self._formal_type_parameters = _list_if_none(ftp)
    self._superclass_signature = scs
    self._superinterface_signatures = _list_if_none(sis)

  def __str__(self):
    output = []
    if self._formal_type_parameters:
      output.append('<%s>' % ', '.join('%s' % s for s in self._formal_type_parameters))
    output.append('CLASS extends %s' % self._superclass_signature)
    if self._superinterface_signatures:
      output.append('implements %s' % ', '.join('%s' % s for s in self._superinterface_signatures))

    return 'ClassSignature(%s)' % ' '.join(output)

class ClassTypeSignature(object):
  """
    'L' PackageSpecifier* SimpleClassTypeSignature ClassTypeSignatureSuffix* ';'

    This grammar is totally incorrect
       "L" {Ident "/"} Ident OptTypeArguments {"." Ident OptTypeArguments} ";".
             ^ package specifier

  """
  @staticmethod
  def match(data):
    if data[0] != 'L':
      return _UNPARSED
    offset = 1

    package_class, bytes_read = PackageSpecifier.match(data[offset:])
    if package_class is None:
      return _UNPARSED
    offset += bytes_read

    package_arguments, bytes_read = TypeArguments.match(data[offset:])
    offset += bytes_read

    suffixes = []
    while data[offset] != ';':
      suffix, bytes_read = ClassTypeSignatureSuffix.match(data[offset:])
      if not suffix:
        return _UNPARSED
      suffixes.append(suffix)
      offset += bytes_read
    return ClassTypeSignature(package_class, package_arguments, suffixes), offset + 1

  def __init__(self, package=None, package_arguments=None, suffixes=None):
    self._package = package
    self._arguments = _list_if_none(package_arguments)
    self._suffixes = _list_if_none(suffixes)

  def __str__(self):
    output = ['%s' % self._package]
    if self._arguments:
      output.append('<')
      output.append(', '.join('%s' % s for s in self._arguments))
      output.append('>')
    if self._suffixes:
      output.append('suffixes')
      output.extend(self._suffixes)
    return '%s' % (''.join('%s' % s for s in output))

class Identifier(object):
  """
    Names of methods, fields and local variables are stored as unqualified
    names.  Unqualified names must not contain the characters '.', ';', '['
    or '/'.  Method names are further constrained so that, with the
    exception of the special method names (3.9) <init> and <clinit>, they
    must not contain the characters '<' or '>'.

    Possible RHS:
      ':'
      '/'

  """
  @staticmethod
  def match(data):
    _BAD_CHARACTERS = ('.', ';', '[', '/', '<', '>', # documented
                      ':')                           # inferred
    if data.startswith('<init>'):
      return Identifier('<init>'), len('<init>')
    elif data.startswith('<clinit>'):
      return Identifier('<clinit>'), len('<clinit>')
    offset = 0
    while data[offset] not in _BAD_CHARACTERS:
      offset += 1
    if offset > 0:
      return Identifier(data[0:offset]), offset
    else:
      return _UNPARSED

  def __init__(self, ident):
    self._identifier = ident

  def __str__(self):
    return '%s' % self._identifier

class ClassBound(object):
  """
    ':' [ FieldTypeSignature ]
  """
  @staticmethod
  def match(data):
    if data[0] != ':':
      return _UNPARSED
    offset = 1
    fieldsig, bytes_read = FieldTypeSignature.match(data[offset:])
    return ClassBound(fieldsig), offset + bytes_read

  def __init__(self, signature=None):
    self._signature = signature

  def __str__(self):
    # an empty signature means infer java.lang.Object.
    if self._signature is None:
      return 'java.lang.Object'
    else:
      return '%s' % self._signature

class InterfaceBound(object):
  """
    ':' FieldTypeSignature
  """
  @staticmethod
  def match(data):
    if data[0] != ':':
      return _UNPARSED
    offset = 1
    fieldsig, bytes_read = FieldTypeSignature.match(data[offset:])
    return InterfaceBound(fieldsig), offset + bytes_read

  def __init__(self, signature=None):
    self._signature = signature

  def __str__(self):
    return '%s' % self._signature

class FieldTypeSignature(object):
  """
    FieldTypeSignature:
      ClassTypeSignature
      ArrayTypeSignature
      TypeVariableSignature
  """
  @staticmethod
  def match(data):
    cls, offset = ClassTypeSignature.match(data)
    if cls is not None:
      return cls, offset

    array, offset = ArrayTypeSignature.match(data)
    if array is not None:
      return array, offset

    typev, offset = TypeVariableSignature.match(data)
    if typev is not None:
      return typev, offset

    return _UNPARSED

class PackageSpecifier(object):
  """
    Identifier '/' PackageSpecifier*
  """
  @staticmethod
  def match(data):
    package_sequence = []
    offset = 0
    while True:
      ident, bytes_read = Identifier.match(data[offset:])
      if ident is None:
        if len(package_sequence) == 0:
          return _UNPARSED
      else:
        offset += bytes_read
        package_sequence.append(ident)

      if data[offset] != '/':
        return PackageSpecifier(package_sequence), offset
      else:
        offset += 1

  def __init__(self, package_id):
    self._package = package_id

  def parent(self):
    return PackageSpecifier(self._package[0:-1])

  def leaf(self):
    return self._package[-1]

  def __str__(self):
    #return 'PackageSpecifier(%s)' % ('.'.join('%s' % foo._identifier for foo in self._package))
    return '.'.join('%s' % foo._identifier for foo in self._package)

class SimpleClassTypeSignature(object):
  """
    Identifier [ TypeArguments ]
  """
  @staticmethod
  def match(data):
    ident, offset = Identifier.match(data)
    if ident is None:
      return _UNPARSED
    type_args, bytes_read = TypeArguments.match(data[offset:])
    return SimpleClassTypeSignature(ident, type_args), offset + bytes_read

  def __init__(self, identifier, type_args=None):
    self._identifier = identifier
    self._type_arguments = _list_if_none(type_args)

  def __str__(self):
    appendix = ''
    if self._type_arguments:
      appendix = '<%s>' % ', '.join('%s' % s for s in self._type_arguments)
    return 'SimpleClassTypeSignature(%s%s)' % (
      self._identifier, appendix)

class ClassTypeSignatureSuffix(object):
  """
    '.' SimpleClassTypeSignature
  """
  @staticmethod
  def match(data):
    if data[0] != '.':
      return _UNPARSED
    scts, bytes_read = SimpleClassTypeSignature.match(data[1:])
    if scts is None:
      return _UNPARSED
    return ClassTypeSignatureSuffix(scts), bytes_read + 1

  def __init__(self, signature):
    self._signature = signature

  def __str__(self):
    return '.%s' % self._signature

class TypeVariableSignature(object):
  """
    'T' Identifier ';'
  """
  @staticmethod
  def match(data):
    if data[0] != 'T':
      return _UNPARSED
    offset = 1
    ident, bytes_read = Identifier.match(data[offset:])
    offset += bytes_read
    if data[offset] != ';':
      return _UNPARSED
    return TypeVariableSignature(ident), offset + 1

  def __init__(self, identifier):
    self._identifier = identifier

  def __str__(self):
    return '<%s>' % self._identifier

class TypeArguments(object):
  """
    '<' TypeArgument+ '>'
  """
  @staticmethod
  def match(data):
    if data[0] != '<':
      return _UNPARSED
    offset = 1

    type_args = []
    while data[offset] != '>':
      type_arg, bytes_read = TypeArgument.match(data[offset:])
      if type_arg is None:
        break
      type_args.append(type_arg)
      offset += bytes_read
    if len(type_args) == 0:
      return _UNPARSED
    return type_args, offset + 1

  def __init__(self, arguments):
    self._arguments = arguments


class TypeArgument(object):
  """
    [ WildcardIndicator ] FieldTypeSignature
    '*'
  """
  @staticmethod
  def match(data):
    if data[0] == '*':
      return TypeArgument(data[0], sig=None), 1

    offset = 0
    wildcard, bytes_read = WildcardIndicator.match(data[offset:])
    offset += bytes_read
    field_signature, bytes_read = FieldTypeSignature.match(data[offset:])
    if field_signature is None:
      return _UNPARSED
    return TypeArgument(wildcard, field_signature), offset + bytes_read

  def __init__(self, wildcard_indicator=None, sig=None):
    self._wildcard = wildcard_indicator
    self._signature = sig
    if self._wildcard == '*':
      assert self._signature is None

  def __str__(self):
    if self._wildcard == '*':
      return '?'
    elif self._wildcard:
      if self._wildcard == '+':
        return '? extends %s' % self._signature
      else:
        return '%s extends ?' % self._signature
    else:
      return '%s' % self._signature

class WildcardIndicator(object):
  """
    '+'
    '-'
  """
  @staticmethod
  def match(data):
    if data[0] == '+' or data[0] == '-':
      return data[0], 1
    return _UNPARSED

class ArrayTypeSignature(object):
  """
    '[' TypeSignature
  """
  @staticmethod
  def match(data):
    if data[0] != '[':
      return _UNPARSED
    offset = 1
    sig, bytes_read = TypeSignature.match(data[offset:])
    if sig is None:
      return _UNPARSED
    return ArrayTypeSignature(sig), offset + bytes_read

  def __init__(self, sig):
    self._signature = sig

  def __str__(self):
    return '[]%s' % self._signature

class VoidSignature(object):
  @staticmethod
  def match(data):
    if data[0] == 'V':
      return VoidSignature(), 1
    return _UNPARSED


  def __init__(self):
    pass

  def __str__(self):
    return 'void'

class TypeSignature(object):
  """
    TypeSignature:
       FieldTypeSignature
       BaseType
  """
  @staticmethod
  def match(data):
    offset = 0
    sig, bytes_read = FieldTypeSignature.match(data[offset:])
    if sig: return TypeSignature(sig), offset + bytes_read
    base, bytes_read = BaseType.match(data[offset:])
    if base: return TypeSignature(base), offset + bytes_read
    return _UNPARSED

  def __init__(self, sig):
    self._signature = sig

  def __str__(self):
    return '%s' % self._signature

class MethodTypeSignature(object):
  """
    [FormalTypeParameters] '(' TypeSignature* ')' ReturnType ThrowsSignature*

    For example:

    <T:Ljava/lang/Object;> ( Ljava/lang/Class<+TT;>; ) Lcom/twitter/common/base/Supplier<TT;>;
    ----------------------   -----------------------   ---------------------------------------
    FormalTypeParameters        TypeSignature*         ReturnType
  """
  @staticmethod
  def match(data):
    formal_type, offset = FormalTypeParameters.match(data)
    if data[offset] != '(':
      return _UNPARSED
    offset += 1
    type_sigs = []
    while True:
      type_sig, bytes_read = TypeSignature.match(data[offset:])
      if type_sig is not None:
        type_sigs.append(type_sig)
        offset += bytes_read
        continue
      else:
        if data[offset] != ')':
          return _UNPARSED
        else:
          break
    offset += 1
    return_type, bytes_read = ReturnType.match(data[offset:])
    if return_type is None:
      return _UNPARSED
    offset += bytes_read
    throws = []
    while offset < len(data):
      throw_sig, bytes_read = ThrowsSignature.match(data[offset:])
      if throw_sig is None:
        break
      throws.append(throw_sig)
      offset += bytes_read
    return MethodTypeSignature(formal_type, type_sigs, return_type, throws), offset

  def __init__(self, formal_type=None, type_sigs=None, return_type=None, throws=None):
    self._formal_type = _list_if_none(formal_type)
    self._type_signatures = _list_if_none(type_sigs)
    self._return_type = return_type
    self._throws = _list_if_none(throws)

  def __str__(self):
    output = ''
    if self._formal_type:
      output += ('<%s> ' % ', '.join('%s' % s for s in self._formal_type))
    output += ('%s' % self._return_type)
    output += (' METHOD')
    if self._type_signatures:
      output += ('(%s)' % ', '.join('%s' % s for s in self._type_signatures))
    if self._throws:
      output += (' throws %s' % ', '.join('%s' % s for s in self._throws))
    return 'MethodTypeSignature(%s)' % output

class ReturnType(object):
  """
    TypeSignature
    VoidSignature
  """
  @staticmethod
  def match(data):
    offset = 0
    type_sig, bytes_read = TypeSignature.match(data)
    if type_sig: return ReturnType(type_sig), bytes_read
    void_sig, bytes_read = VoidSignature.match(data)
    if void_sig: return ReturnType(void_sig), bytes_read
    return _UNPARSED

  def __init__(self, sig):
    self._signature = sig

  def __str__(self):
    return '%s' % self._signature

class ThrowsSignature(object):
  """
    '^' ClassTypeSignature
    '^' TypeVariableSignature
  """
  @staticmethod
  def match(data):
    offset = 0
    if data[offset] != '^':
      return _UNPARSED
    offset += 1
    cls_sig, bytes_read = ClassTypeSignature.match(data[offset:])
    if cls_sig: return cls_sig, bytes_read
    type_sig, bytes_read = TypeVariableSignature.match(data[offset:])
    if type_sig: return type_sig, bytes_read
    return _UNPARSED

class FormalTypeParameter(object):
  """
    Identifier ClassBound InterfaceBound*
  """
  @staticmethod
  def match(data):
    offset = 0
    identifier, bytes_read = Identifier.match(data[offset:])
    if identifier is None:
      return _UNPARSED
    else:
      pass
    offset += bytes_read

    class_bound, bytes_read = ClassBound.match(data[offset:])
    if class_bound is None:
      return _UNPARSED
    offset += bytes_read
    interfaces = []
    while True:
      interface, bytes_read = InterfaceBound.match(data[offset:])
      if interface is None:
        break
      offset += bytes_read
      interfaces.append(interface)
    return FormalTypeParameter(identifier, class_bound, interfaces), offset

  def __init__(self, identifier, class_bound, interface_bound=None):
    self._identifier = identifier
    self._class_bound = class_bound
    self._interface_bound = _list_if_none(interface_bound)

  def __str__(self):
    appendix = ''
    if self._interface_bound:
      appendix = 'interfaces:%s' % self._interface_bound
    return '%s extends %s%s' % (
      self._identifier,
      self._class_bound, appendix)

class FormalTypeParameters(object):
  """
    FormalTypeParameters:
      '<' FormalTypeParameter+ '>'
  """
  @staticmethod
  def match(data):
    offset = 0
    if data[offset] != '<': return _UNPARSED
    offset += 1
    parameters = []
    while data[offset] != '>':
      parameter, bytes_read = FormalTypeParameter.match(data[offset:])
      if parameter is None:
        return _UNPARSED
      parameters.append(parameter)
      offset += bytes_read
    # raise exception if len(parameters) == 0
    if len(parameters) == 0:
      raise ParseException('FormalTypeParameters requires >= 1 FormalTypeParameter')
    return parameters, offset + 1

########NEW FILE########
__FILENAME__ = util
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

def javaify(klz):
  return klz.replace('/', '.')

########NEW FILE########
__FILENAME__ = jira
import base64
import getpass
import json
import textwrap
import urllib
import urllib2
import urlparse

from contextlib import contextmanager

from twitter.common import log


class JiraError(Exception):
  '''Indicates a problem performing an action with JIRA.'''

  def __init__(self, cause=None, message=None):
    self._cause = cause
    self._message = message

  def __str__(self):
    msg = 'JIRA request failed'
    if self._message:
      msg += ', message: %s' % self._message
    if self._cause:
      msg += ' due to %s' % self._cause
    return msg


class Jira(object):
  '''Interface for interacting with JIRA.

     Currently only works for situations where the user may be prompted for
     credentials.
  '''

  RESOLVE_NAMES = (
    'Resolve',
    'Resolve Issue',
  )

  def __init__(self, server_url, api_base='/rest/api/2/', user=None, password=None):
    self._base_url = urlparse.urljoin(server_url, api_base)
    self._user = user or getpass.getuser()
    #Set the password if initialized or Lazily request password later when request is made.
    self._pass = password

  def _getpass(self):
    if not self._pass:
      self._pass = getpass.getpass('Please enter JIRA password for %s: ' % self._user)
    return self._pass

  def comment(self, issue, comment):
    with self._api_call_guard():
      self.api_call('issue/%s/comment' % issue, {'body': comment})

  def get_transitions(self, issue):
    return self.api_call('issue/%s/transitions' % issue)

  def _get_resolve_transition_id(self, issue):
    '''Find the transition id to resolve the issue'''
    try:
      transitions = json.loads(self.get_transitions(issue))['transitions']
    except (KeyError, ValueError) as e:
      raise JiraError(cause=e, message='Transitions list did not have the expected JSON format')

    for transition in transitions:
      if transition['name'] in self.RESOLVE_NAMES:
        return transition['id']

    raise JiraError(message=textwrap.dedent('''
    Could not find the id of the JIRA \'Resolve\' transition, here were the
    available transitions:
    %s
    ''' % (transitions)))

  def resolve(self, issue, comment=None):
    transition_id = self._get_resolve_transition_id(issue)

    data = {
      'fields': {'resolution': {'name': 'Fixed'}},
      'transition': {'id': transition_id}
    }
    if comment:
      data['update'] = {'comment': [{'add': {'body': comment}}]}

    with self._api_call_guard(http_error_msg='Transition failed, is the bug already closed?'):
      self.api_call('issue/%s/transitions' % issue, data)

  # create a new issue using project key and issuetype names, i.e. TEST, Incident
  def create_issue(self, project, issue_type, summary, description=None, **kw):
    data = {
      'fields': {
        'project': {'key': project},
        'issuetype': {'name': issue_type},
        'summary': summary,
        'description': description
      }
    }
    data['fields'].update(kw)

    with self._api_call_guard():
      return self.api_call('issue', data)

  def fetch_issue_fields(self, project_key, issue_type):
    data = {
      "projectKeys": project_key,
      "issuetypeIds": issue_type,
      "expand": "projects.issuetypes.fields"
    }
    qs = urllib.urlencode(data)
    endpoint = '%s?%s' % ('issue/createmeta', qs)

    with self._api_call_guard():
      return self.api_call(endpoint)

  def get_issue(self, issue):
    '''Returns the data for the given issue'''
    endpoint = 'issue/%s' % (issue)

    with self._api_call_guard():
      return self.api_call(endpoint)

  def get_link_types(self):
    '''Returns a list of the available link types'''
    link_types = []

    with self._api_call_guard():
      try:
        link_types_data = json.loads(self.api_call('issueLinkType'))
      except (KeyError, ValueError) as e:
        raise JiraError(cause=e, message='Transitions list did not have the expected JSON format')

    for link_type in link_types_data['issueLinkTypes']:
      link_types.append(link_type['name'])

    return link_types

  def add_link(self, inward_issue, outward_issue, comment=None, link_type='Related'):
    '''Add a link between the inward_issue and the outward_issue'''
    link_types = self.get_link_types()

    if not link_type in link_types:
      raise JiraError(message="Error: Link type of '%s' not valid: " % link_type)

    data = {
      'type': {'name': link_type},
      'inwardIssue': {'key': inward_issue},
      'outwardIssue': {'key': outward_issue},
    }

    if comment:
      data['comment'] = {'body': comment}

    with self._api_call_guard():
      return self.api_call('issueLink', data)

  def remove_link(self, link_id):
    '''Remove a link specified by by the link ID. The link ID can be found with get_issue_links()'''
    endpoint = 'issueLink/%s' % (link_id)

    with self._api_call_guard():
      self.api_call(endpoint, send_delete=True)

  def get_issue_links(self, issue):
    '''Returns the links section of an issue'''
    issue_data = self.get_issue(issue)
    issue_parsed = json.loads(issue_data)

    return issue_parsed['fields']['issuelinks']

  def add_watcher(self, issue, watcher):
    '''Adds the given watcher to the given issue'''
    endpoint = 'issue/%s/watchers' % (issue)

    with self._api_call_guard():
      return self.api_call(endpoint, watcher)

  def remove_watcher(self, issue, watcher):
    '''Removes the given watcher from the given issue'''
    endpoint = 'issue/%s/watchers?username=%s' % (issue, watcher)

    with self._api_call_guard():
      return self.api_call(endpoint, send_delete=True)

  def transition(self, issue, transition_name, comment=None, **kw):
    '''Transition an issue from one state to another

    issue -- The issue key to appy the transition to (e.g. SEARCH-1000)
    transition_name -- The name of the transition state in the JIRA web UI (e.g. "Start Progress")
    comment -- Adds a comment during the transition
    **kw -- Allows for optional fields to be passed during the transition
    '''
    transitions_json = self.get_transitions(issue)

    transitions = json.loads(transitions_json)
    for transition in transitions['transitions']:
      if transition['name'] == transition_name:
        endpoint = 'issue/%s/transitions' % (issue)
        data = {'transition': {'id': transition['id']}}

        if comment:
          data['update'] = {'comment': [{'add': {'body': 'Deploy completed'}}]}

        if kw:
          data['fields'] = kw

        with self._api_call_guard():
          return self.api_call(endpoint, data)

    raise JiraError(message="Transition '%s' not currently available for this issue" %
                      (transition_name))

  @contextmanager
  def _api_call_guard(self, http_error_msg=None, url_error_msg=None):
    try:
      yield
    except urllib2.HTTPError as e:
      raise JiraError(cause=e, message=http_error_msg)
    except urllib2.URLError as e:
      raise JiraError(cause=e, message=url_error_msg)

  def api_call(self, endpoint, post_json=None, authorization=None, send_delete=False):
    url = urlparse.urljoin(self._base_url, endpoint)
    headers = {'User-Agent': 'twitter.common.jira'}
    base64string = authorization or base64.b64encode('%s:%s' % (self._user, self._getpass()))
    headers['Authorization'] = 'Basic %s' % base64string
    log.debug(headers)
    data = json.dumps(post_json) if post_json else None
    if data:
      headers['Content-Type'] = 'application/json'
    request = urllib2.Request(url, data, headers)
    if send_delete:
      request.get_method = lambda: 'DELETE'
    return urllib2.urlopen(request).read()

########NEW FILE########
__FILENAME__ = lockable
import threading
from functools import wraps

class Lockable(object):
  def __init__(self):
    self.__lock = threading.RLock()

  @staticmethod
  def sync(method):
    @wraps(method)
    def wrapper(self, *args, **kw):
      with self.__lock:
        return method(self, *args, **kw)
    return wrapper

  @property
  def lock(self):
    return self.__lock

########NEW FILE########
__FILENAME__ = base
def format_message(record):
  try:
    record_message = '%s' % (record.msg % record.args)
  except TypeError:
    record_message = record.msg
  return record_message

########NEW FILE########
__FILENAME__ = glog
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import logging
import socket
import sys
import time

from twitter.common.log.formatters.base import format_message


class GlogFormatter(logging.Formatter):
  """
    Format a log in Google style format:
    [DIWEF]mmdd hh:mm:ss.uuuuuu pid file:line] msg
  """
  SCHEME = 'google'
  LOG_LINE_FORMAT = "[DIWEF]mmdd hh:mm:ss.uuuuuu pid file:line] msg"

  LEVEL_MAP = {
    logging.FATAL: 'F',
    logging.ERROR: 'E',
    logging.WARN:  'W',
    logging.INFO:  'I',
    logging.DEBUG: 'D'
  }

  @classmethod
  def logfile_preamble(cls):
    return ''.join('%s\n' % line for line in [
      'Log file created at: %s' % time.strftime('%Y/%m/%d %H:%M:%S', time.localtime()),
      'Running on machine: %s' % socket.gethostname(),
      cls.LOG_LINE_FORMAT,
      'Command line: %s' % ' '.join(sys.argv)])

  def __init__(self):
    logging.Formatter.__init__(self)

  def format(self, record):
    try:
      level = GlogFormatter.LEVEL_MAP[record.levelno]
    except:
      level = '?'
    date = time.localtime(record.created)
    date_usec = (record.created - int(record.created)) * 1e6
    record_message = '%c%02d%02d %02d:%02d:%02d.%06d %s %s:%d] %s' % (
       level, date.tm_mon, date.tm_mday, date.tm_hour, date.tm_min, date.tm_sec, date_usec,
       record.process if record.process is not None else '?????',
       record.filename,
       record.lineno,
       format_message(record))
    record.getMessage = lambda: record_message
    return logging.Formatter.format(self, record)

########NEW FILE########
__FILENAME__ = plain
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import logging
from twitter.common.log.formatters.base import format_message

class PlainFormatter(logging.Formatter):
  """
    Format a log in a plainer style:
    type] msg
  """
  SCHEME = 'plain'

  LEVEL_MAP = {
    logging.FATAL: 'FATAL',
    logging.ERROR: 'ERROR',
    logging.WARN:  ' WARN',
    logging.INFO:  ' INFO',
    logging.DEBUG: 'DEBUG'
  }

  def __init__(self):
    logging.Formatter.__init__(self)

  def format(self, record):
    try:
      level = PlainFormatter.LEVEL_MAP[record.levelno]
    except:
      level = '?????'
    record_message = '%s] %s' % (level, format_message(record))
    record.getMessage = lambda: record_message
    return logging.Formatter.format(self, record)

########NEW FILE########
__FILENAME__ = handlers
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from logging import Handler

try:
  from scribe import scribe
  from thrift.protocol import TBinaryProtocol
  from thrift.transport import TTransport, TSocket
  _SCRIBE_PRESENT = True
except ImportError:
  _SCRIBE_PRESENT = False


class ScribeHandler(Handler):
  """logging.Handler interface for Scribe."""
  class ScribeHandlerException(Exception):
    pass

  def __init__(self, *args, **kwargs):
    """logging.Handler interface for Scribe.

    Params:
    buffer: If True, buffer messages when scribe is unavailable. If False, drop on floor.
    category: Scribe category for logging events.
    host: Scribe host.
    port: Scribe port.
    """
    if not _SCRIBE_PRESENT:
      raise self.ScribeHandlerException(
        "Could not initialize ScribeHandler: Scribe modules not present.")
    self._buffer_enabled = kwargs.pop("buffer")
    self._category = kwargs.pop("category")
    self._client = None
    self._host = kwargs.pop("host")
    self._log_buffer = []
    self._port = kwargs.pop("port")
    self._transport = None
    Handler.__init__(self, *args, **kwargs)

  @property
  def messages_pending(self):
    """Return True if there are messages in the buffer."""
    return bool(self._log_buffer)

  @property
  def client(self):
    """Scribe client object."""
    if not self._client:
      protocol = TBinaryProtocol.TBinaryProtocol(trans=self.transport,
                                                 strictRead=False,
                                                 strictWrite=False)
      self._client = scribe.Client(iprot=protocol, oprot=protocol)
    return self._client

  @property
  def transport(self):
    """Scribe transport object."""
    if not self._transport:
      socket = TSocket.TSocket(host=self._host, port=self._port)
      self._transport = TTransport.TFramedTransport(socket)
    return self._transport

  def close(self):
    """Flushes any remaining messages in the queue."""
    if self.messages_pending:
      try:
        self.scribe_write(self._log_buffer)
      except self.ScribeHandlerException:
        pass
    Handler.close(self)

  def emit(self, record):
    """Emit a record via Scribe."""
    fmt_record = self.format(record)
    self._log_buffer.append(scribe.LogEntry(category=self._category, message=fmt_record))
    try:
      self.scribe_write(self._log_buffer)
    except self.ScribeHandlerException:
      if not self._buffer_enabled:
        self._log_buffer = []
    else:
      self._log_buffer = []

  def scribe_write(self, messages):
    """Sends a list of messages to scribe.

    Params:
    messages: List of scribe.LogEntry objects.

    Raises:
    ScribeHandlerException on timeouts and connection errors.
    """
    try:
      self.transport.open()
      result = self.client.Log(messages)
      if result != scribe.ResultCode.OK:
        raise self.ScribeHandlerException('Scribe message submission failed')
    except TTransport.TTransportException as err:
      raise self.ScribeHandlerException('Could not connect to scribe host=%s:%s error=%s'
                                        % (self._host, self._port, err))
    finally:
      self.transport.close()

########NEW FILE########
__FILENAME__ = initialize
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""
Interface to Glog-style formatter.

import twitter.common.log

if not using twitter.common.app:
  for __main__:
    log = twitter.common.log.init('my_binary_name')
otherwise init will be called automatically on app.main()

for library/endpoint:
  from twitter.common import log

log.info('info baby')
log.debug('debug baby')
log.fatal('oops fatal!')

Will log to my_binary_name.{INFO,WARNING,ERROR,...} into log_dir using the
Google logging format.

See twitter.com.log.options for customizations.
"""

from __future__ import print_function

import getpass
import logging
import os
from socket import gethostname
import sys
import time

from twitter.common.log.formatters import glog, plain
from twitter.common.log.handlers import ScribeHandler
from twitter.common.log.options import LogOptions
from twitter.common.dirutil import safe_mkdir


class GenericFilter(logging.Filter):
  def __init__(self, levelfn=lambda record_level: True):
    self._levelfn = levelfn
    logging.Filter.__init__(self)

  def filter(self, record):
    return 1 if self._levelfn(record.levelno) else 0


class ProxyFormatter(logging.Formatter):
  class UnknownSchemeException(Exception): pass

  _SCHEME_TO_FORMATTER = {
    glog.GlogFormatter.SCHEME: glog.GlogFormatter(),
    plain.PlainFormatter.SCHEME: plain.PlainFormatter()
  }

  def __init__(self, scheme_fn):
    logging.Formatter.__init__(self)
    self._scheme_fn = scheme_fn

  def preamble(self):
    scheme = self._scheme_fn()
    if scheme not in ProxyFormatter._SCHEME_TO_FORMATTER:
      raise ProxyFormatter.UnknownSchemeException("Unknown logging scheme: %s" % scheme)
    formatter = ProxyFormatter._SCHEME_TO_FORMATTER[scheme]
    if hasattr(formatter, 'logfile_preamble') and callable(formatter.logfile_preamble):
      return formatter.logfile_preamble()

  def format(self, record):
    scheme = self._scheme_fn()
    if scheme not in ProxyFormatter._SCHEME_TO_FORMATTER:
      raise ProxyFormatter.UnknownSchemeException("Unknown logging scheme: %s" % scheme)
    return ProxyFormatter._SCHEME_TO_FORMATTER[scheme].format(record)


_FILTER_TYPES = {
  logging.DEBUG: 'DEBUG',
  logging.INFO: 'INFO',
  logging.WARN: 'WARNING',
  logging.ERROR: 'ERROR',
  logging.FATAL: 'FATAL' # strangely python logging translates this to CRITICAL
}


def print_stderr(message):
  """Emit a message on standard error if logging to stderr is permitted."""
  if LogOptions.stderr_log_level() != LogOptions.LOG_LEVEL_NONE:
    print(message, file=sys.stderr)


def _safe_setup_link(link_filename, real_filename):
  """
    Create a symlink from link_filename to real_filename.
  """
  real_filename = os.path.relpath(real_filename, os.path.dirname(link_filename))

  if os.path.exists(link_filename):
    try:
      os.unlink(link_filename)
    except OSError:
      pass
  try:
    os.symlink(real_filename, link_filename)
  except OSError as e:
    # Typically permission denied.
    pass


class PreambleFileHandler(logging.FileHandler):
  def __init__(self, filename, preamble=None):
    self._preamble = preamble
    logging.FileHandler.__init__(self, filename)

  def _open(self):
    stream = logging.FileHandler._open(self)
    if self._preamble:
      stream.write(self._preamble)
    return stream


def _initialize_disk_logging():
  safe_mkdir(LogOptions.log_dir())


def _setup_aggregated_disk_logging(filebase):
  filename = os.path.join(LogOptions.log_dir(), filebase + '.log')
  formatter = ProxyFormatter(LogOptions.disk_log_scheme)
  file_handler = PreambleFileHandler(filename, formatter.preamble())
  file_handler.setFormatter(formatter)
  file_handler.addFilter(GenericFilter(lambda level: level >= LogOptions.disk_log_level()))
  return [file_handler]


def _setup_disk_logging(filebase):
  handlers = []

  def gen_filter(level):
    return GenericFilter(
      lambda record_level: record_level == level and level >= LogOptions.disk_log_level())

  def gen_link_filename(filebase, level):
    return '%(filebase)s.%(level)s' % {
      'filebase': filebase,
      'level': level,
    }

  hostname = gethostname()
  username = getpass.getuser()
  pid = os.getpid()
  datestring = time.strftime('%Y%m%d-%H%M%S', time.localtime())

  def gen_verbose_filename(filebase, level):
    return '%(filebase)s.%(hostname)s.%(user)s.log.%(level)s.%(date)s.%(pid)s' % {
      'filebase': filebase,
      'hostname': hostname,
      'user': username,
      'level': level,
      'date': datestring,
      'pid': pid
    }

  logroot = LogOptions.log_dir()
  for filter_type, filter_name in _FILTER_TYPES.items():
    formatter = ProxyFormatter(LogOptions.disk_log_scheme)
    filter = gen_filter(filter_type)
    full_filebase = os.path.join(logroot, filebase)
    logfile_link = gen_link_filename(full_filebase, filter_name)
    logfile_full = gen_verbose_filename(full_filebase, filter_name)
    file_handler = PreambleFileHandler(logfile_full, formatter.preamble())
    file_handler.setFormatter(formatter)
    file_handler.addFilter(filter)
    handlers.append(file_handler)
    _safe_setup_link(logfile_link, logfile_full)
  return handlers


def _setup_scribe_logging():
  filter = GenericFilter(lambda r_l: r_l >= LogOptions.scribe_log_level())
  formatter = ProxyFormatter(LogOptions.scribe_log_scheme)
  scribe_handler = ScribeHandler(buffer=LogOptions.scribe_buffer(),
                                 category=LogOptions.scribe_category(),
                                 host=LogOptions.scribe_host(),
                                 port=LogOptions.scribe_port())
  scribe_handler.setFormatter(formatter)
  scribe_handler.addFilter(filter)
  return [scribe_handler]


def _setup_stderr_logging():
  filter = GenericFilter(lambda r_l: r_l >= LogOptions.stderr_log_level())
  formatter = ProxyFormatter(LogOptions.stderr_log_scheme)
  stderr_handler = logging.StreamHandler(sys.stderr)
  stderr_handler.setFormatter(formatter)
  stderr_handler.addFilter(filter)
  return [stderr_handler]


def teardown_disk_logging():
  root_logger = logging.getLogger()
  global _DISK_LOGGERS
  for handler in _DISK_LOGGERS:
    root_logger.removeHandler(handler)
  _DISK_LOGGERS = []


def teardown_scribe_logging():
  root_logger = logging.getLogger()
  global _SCRIBE_LOGGERS
  for handler in _SCRIBE_LOGGERS:
    root_logger.removeHandler(handler)
  _SCRIBE_LOGGERS = []


def teardown_stderr_logging():
  root_logger = logging.getLogger()
  global _STDERR_LOGGERS
  for handler in _STDERR_LOGGERS:
    root_logger.removeHandler(handler)
  _STDERR_LOGGERS = []


_SCRIBE_LOGGERS = []
_STDERR_LOGGERS = []
_DISK_LOGGERS = []


def init(filebase=None):
  """
    Sets up default stderr logging and, if filebase is supplied, sets up disk logging using:
      {--log_dir}/filebase.{INFO,WARNING,...}

    If '--log_simple' is specified, logs are written into a single file:
      {--log_dir}/filebase.log
  """
  logging._acquireLock()

  # set up permissive logger
  root_logger = logging.getLogger()
  root_logger.setLevel(logging.DEBUG)

  # clear existing handlers
  teardown_scribe_logging()
  teardown_stderr_logging()
  teardown_disk_logging()
  for handler in root_logger.handlers:
    root_logger.removeHandler(handler)

  # setup INFO...FATAL handlers
  if filebase:
    _initialize_disk_logging()
    initializer = _setup_aggregated_disk_logging if LogOptions.simple() else _setup_disk_logging
    for handler in initializer(filebase):
      root_logger.addHandler(handler)
      _DISK_LOGGERS.append(handler)

  if LogOptions._is_scribe_logging_required():
    try:
      for handler in _setup_scribe_logging():
        root_logger.addHandler(handler)
        _SCRIBE_LOGGERS.append(handler)
    except ScribeHandler.ScribeHandlerException as err:
      print_stderr(err)

  for handler in _setup_stderr_logging():
    root_logger.addHandler(handler)
    _STDERR_LOGGERS.append(handler)

  logging._releaseLock()

  if len(_DISK_LOGGERS) > 0:
    print_stderr('Writing log files to disk in %s' % LogOptions.log_dir())
  if len(_SCRIBE_LOGGERS) > 0:
    print_stderr('Sending log messages to scribe host=%s:%d category=%s'
          % (LogOptions.scribe_host(), LogOptions.scribe_port(), LogOptions.scribe_category()))

  return root_logger

########NEW FILE########
__FILENAME__ = options
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""
Glog log system global options.

Exports module-level options such as --log_dir and --stderr_log_level, but may be
overridden locally before calling log.init().
"""

from __future__ import print_function

import logging
import optparse
import sys


_DISK_LOG_LEVEL_OPTION = 'twitter_common_log_disk_log_level'


_DEFAULT_LOG_OPTS = {
  'twitter_common_log_stderr_log_level': 'ERROR',
  _DISK_LOG_LEVEL_OPTION: 'INFO',
  'twitter_common_log_log_dir': '/var/tmp',
  'twitter_common_log_simple': False,
  'twitter_common_log_scribe_buffer': False,
  'twitter_common_log_scribe_host': 'localhost',
  'twitter_common_log_scribe_log_level': 'NONE',
  'twitter_common_log_scribe_port': 1463,
  'twitter_common_log_scribe_category': 'python_default'
}


try:
  from twitter.common import app
  HAVE_APP = True
except ImportError:
  from collections import namedtuple
  DefaultLogOpts = namedtuple('DefaultLogOpts', _DEFAULT_LOG_OPTS.keys())
  class AppDefaultProxy(object):
    def __init__(self):
      self._opts = DefaultLogOpts(**_DEFAULT_LOG_OPTS)
    def get_options(self):
      return self._opts
    def set_option(self, key, value, force=False):
      opts = self._opts._asdict()
      if force or key not in opts:
        opts[key] = value
  app = AppDefaultProxy()
  HAVE_APP = False


class LogOptionsException(Exception): pass


class LogOptions(object):
  _LOG_LEVEL_NONE_KEY = 'NONE'
  LOG_LEVEL_NONE = 100

  _LOG_LEVELS = {
    'DEBUG':             logging.DEBUG,
    'INFO':              logging.INFO,
    'WARN':              logging.WARN,
    'FATAL':             logging.FATAL,
    'ERROR':             logging.ERROR,
    _LOG_LEVEL_NONE_KEY: LOG_LEVEL_NONE
  }

  _LOG_SCHEMES = [
    'google',
    'plain'
  ]

  _STDERR_LOG_LEVEL = None
  _STDOUT_LOG_SCHEME = None
  _DISK_LOG_LEVEL = None
  _DISK_LOG_SCHEME = None
  _LOG_DIR = None
  _SIMPLE = None
  _SCRIBE_BUFFER = None
  _SCRIBE_HOST = None
  _SCRIBE_LOG_LEVEL = None
  _SCRIBE_LOG_SCHEME = None
  _SCRIBE_PORT = None
  _SCRIBE_CATEGORY = None

  @staticmethod
  def _parse_loglevel(log_level, scheme='google'):
    level = None
    components = log_level.split(':')
    if len(components) == 1:
      level = components[0]
    elif len(components) == 2:
      scheme, level = components[0], components[1]
    else:
      raise LogOptionsException("Malformed log level: %s" % log_level)

    if level in LogOptions._LOG_LEVELS:
      level = LogOptions._LOG_LEVELS[level]
    else:
      raise LogOptionsException("Unknown log level: %s" % level)

    if scheme not in LogOptions._LOG_SCHEMES:
      raise LogOptionsException("Unknown log scheme: %s" % scheme)

    return (scheme, level)

  @staticmethod
  def loglevel_name(log_level):
    """
      Return the log level name of the given log_level (integer), or None if it has no name.
    """
    for name, value in LogOptions._LOG_LEVELS.items():
      if value == log_level:
        return name

  @staticmethod
  def _valid_loglevel(log_level):
    try:
      LogOptions._parse_loglevel(log_level)
      return True
    except:
      return False

  @staticmethod
  def _is_scribe_logging_required():
    return LogOptions._LOG_LEVEL_NONE_KEY != app.get_options().twitter_common_log_scribe_log_level

  @staticmethod
  def disable_scribe_logging():
    """
      Disables scribe logging altogether.
    """
    app.set_option("_SCRIBE_LOG_LEVEL", LogOptions._LOG_LEVEL_NONE_KEY, force=True)

  @staticmethod
  def set_scribe_category(category):
    """
      Set the scribe category for logging. Must be called before log.init() for
      changes to take effect.
    """
    LogOptions._SCRIBE_CATEGORY = category

  @staticmethod
  def scribe_category():
    """
      Get the current category used when logging to the scribe daemon.
    """
    if LogOptions._SCRIBE_CATEGORY is None:
      LogOptions._SCRIBE_CATEGORY = app.get_options().twitter_common_log_scribe_category
    return LogOptions._SCRIBE_CATEGORY

  @staticmethod
  def set_scribe_log_level(log_level):
    """
      Set the log level for scribe.
    """
    LogOptions._SCRIBE_LOG_SCHEME, LogOptions._SCRIBE_LOG_LEVEL = (
      LogOptions._parse_loglevel(log_level, scheme='google')
    )

  @staticmethod
  def scribe_log_level():
    """
      Get the current scribe_log_level (in logging units specified by logging module.)
    """
    if LogOptions._SCRIBE_LOG_LEVEL is None:
      LogOptions.set_scribe_log_level(app.get_options().twitter_common_log_scribe_log_level)
    return LogOptions._SCRIBE_LOG_LEVEL

  @staticmethod
  def scribe_log_scheme():
    """
      Get the current scribe log scheme.
    """
    if LogOptions._SCRIBE_LOG_SCHEME is None:
      LogOptions.set_scribe_log_level(app.get_options().twitter_common_log_scribe_log_level)
    return LogOptions._SCRIBE_LOG_SCHEME

  @staticmethod
  def set_scribe_buffer(buffer_enabled):
    """
      Set buffer for scribe logging. Must be called before log.init() for
      changes to take effect.
    """
    LogOptions._SCRIBE_BUFFER = buffer_enabled

  @staticmethod
  def scribe_buffer():
    """
      Get the current buffer setting for scribe logging.
    """
    if LogOptions._SCRIBE_BUFFER is None:
      LogOptions._SCRIBE_BUFFER = app.get_options().twitter_common_log_scribe_buffer
    return LogOptions._SCRIBE_BUFFER

  @staticmethod
  def set_scribe_host(host):
    """
      Set the scribe host for logging. Must be called before log.init() for
      changes to take effect.
    """
    LogOptions._SCRIBE_HOST = host

  @staticmethod
  def scribe_host():
    """
      Get the current host running the scribe daemon.
    """
    if LogOptions._SCRIBE_HOST is None:
      LogOptions._SCRIBE_HOST = app.get_options().twitter_common_log_scribe_host
    return LogOptions._SCRIBE_HOST

  @staticmethod
  def set_scribe_port(port):
    """
      Set the scribe port for logging. Must be called before log.init() for
      changes to take effect.
    """
    LogOptions._SCRIBE_PORT = port

  @staticmethod
  def scribe_port():
    """
      Get the current port used to connect to the scribe daemon.
    """
    if LogOptions._SCRIBE_PORT is None:
      LogOptions._SCRIBE_PORT = app.get_options().twitter_common_log_scribe_port
    return LogOptions._SCRIBE_PORT

  @staticmethod
  def set_stderr_log_level(log_level):
    """
      Set the log level for stderr.
    """
    LogOptions._STDOUT_LOG_SCHEME, LogOptions._STDERR_LOG_LEVEL = (
        LogOptions._parse_loglevel(log_level, scheme='plain'))

  @staticmethod
  def stderr_log_level():
    """
      Get the current stderr_log_level (in logging units specified by logging module.)
    """
    if LogOptions._STDERR_LOG_LEVEL is None:
      LogOptions.set_stderr_log_level(app.get_options().twitter_common_log_stderr_log_level)
    return LogOptions._STDERR_LOG_LEVEL

  @staticmethod
  def stderr_log_scheme():
    """
      Get the current stderr log scheme.
    """
    if LogOptions._STDOUT_LOG_SCHEME is None:
      LogOptions.set_stderr_log_level(app.get_options().twitter_common_log_stderr_log_level)
    return LogOptions._STDOUT_LOG_SCHEME

  # old deprecated version of these functions.
  set_stdout_log_level = set_stderr_log_level
  stdout_log_level = stderr_log_level
  stdout_log_scheme = stderr_log_scheme

  @staticmethod
  def _is_disk_logging_required():
    return LogOptions._LOG_LEVEL_NONE_KEY != getattr(app.get_options(), _DISK_LOG_LEVEL_OPTION)

  @staticmethod
  def disable_disk_logging():
    """
      Disables disk logging altogether.
    """
    app.set_option(_DISK_LOG_LEVEL_OPTION, LogOptions._LOG_LEVEL_NONE_KEY, force=True)

  @staticmethod
  def set_disk_log_level(log_level):
    """
      Set the log level for disk.
    """
    LogOptions._DISK_LOG_SCHEME, LogOptions._DISK_LOG_LEVEL = (
        LogOptions._parse_loglevel(log_level, scheme='google'))

  @staticmethod
  def disk_log_level():
    """
      Get the current disk_log_level (in logging units specified by logging module.)
    """
    if LogOptions._DISK_LOG_LEVEL is None:
      LogOptions.set_disk_log_level(app.get_options().twitter_common_log_disk_log_level)
    return LogOptions._DISK_LOG_LEVEL

  @staticmethod
  def disk_log_scheme():
    """
      Get the current disk log scheme.
    """
    if LogOptions._DISK_LOG_SCHEME is None:
      LogOptions.set_disk_log_level(app.get_options().twitter_common_log_disk_log_level)
    return LogOptions._DISK_LOG_SCHEME

  @staticmethod
  def set_log_dir(dir):
    """
      Set the logging dir for disk logging.  Must be called before log.init() for
      changes to take effect.
    """
    LogOptions._LOG_DIR = dir

  @staticmethod
  def log_dir():
    """
      Get the current directory into which logs will be written.
    """
    if LogOptions._LOG_DIR is None:
      LogOptions._LOG_DIR = app.get_options().twitter_common_log_log_dir
    return LogOptions._LOG_DIR

  @staticmethod
  def set_simple(value):
    """
      Enable/disable simple logging mode.  Must be called before log.init().
    """
    LogOptions._SIMPLE = bool(value)

  @staticmethod
  def simple():
    """
      Whether or not simple logging should be used.
    """
    if LogOptions._SIMPLE is None:
      LogOptions._SIMPLE = app.get_options().twitter_common_log_simple
    return LogOptions._SIMPLE

  @staticmethod
  def _disk_options_callback(option, opt, value, parser):
    try:
      LogOptions.set_disk_log_level(value)
    except LogOptionsException as e:
      raise optparse.OptionValueError('Failed to parse option: %s' % e)
    parser.values.twitter_common_log_disk_log_level = value

  @staticmethod
  def _scribe_options_callback(option, opt, value, parser):
    try:
      LogOptions.set_scribe_log_level(value)
    except LogOptionsException as e:
      raise optparse.OptionValueError('Failed to parse option: %s' % e)
    parser.values.twitter_common_log_scribe_log_level = value

  __log_to_stderr_is_set = False
  __log_to_stdout_is_set = False
  @staticmethod
  def _stderr_options_callback(option, opt, value, parser):
    if LogOptions.__log_to_stdout_is_set:
      raise optparse.OptionValueError(
        '--log_to_stdout is an obsolete flag that was replaced by --log_to_stderr. '
        'Use only --log_to_stderr.')
    LogOptions.__log_to_stderr_is_set = True
    try:
      LogOptions.set_stderr_log_level(value)
    except LogOptionsException as e:
      raise optparse.OptionValueError('Failed to parse option: %s' % e)
    parser.values.twitter_common_log_stderr_log_level = value

  @staticmethod
  def _stdout_options_callback(option, opt, value, parser):
    if LogOptions.__log_to_stderr_is_set:
      raise optparse.OptionValueError(
        '--log_to_stdout is an obsolete flag that was replaced by --log_to_stderr. '
        'Use only --log_to_stderr.')
    LogOptions.__log_to_stdout_is_set = True
    print('--log_to_stdout is an obsolete flag that was replaced by '
          '--log_to_stderr. Use --log_to_stderr instead.', file=sys.stderr)
    try:
      LogOptions.set_stderr_log_level(value)
    except LogOptionsException as e:
      raise optparse.OptionValueError('Failed to parse option: %s' % e)
    parser.values.twitter_common_log_stderr_log_level = value


_LOGGING_HELP = """The level at which logging to %%s [default: %%%%default].
Takes either LEVEL or scheme:LEVEL, where LEVEL is one
of %s and scheme is one of %s.
""" % (repr(LogOptions._LOG_LEVELS.keys()), repr(LogOptions._LOG_SCHEMES))


if HAVE_APP:
  app.add_option('--log_to_stdout',
                 callback=LogOptions._stdout_options_callback,
                 default=_DEFAULT_LOG_OPTS['twitter_common_log_stderr_log_level'],
                 type='string',
                 action='callback',
                 metavar='[scheme:]LEVEL',
                 dest='twitter_common_log_stderr_log_level',
                 help='OBSOLETE - legacy flag, use --log_to_stderr instead.')

  app.add_option('--log_to_stderr',
                 callback=LogOptions._stderr_options_callback,
                 default=_DEFAULT_LOG_OPTS['twitter_common_log_stderr_log_level'],
                 type='string',
                 action='callback',
                 metavar='[scheme:]LEVEL',
                 dest='twitter_common_log_stderr_log_level',
                 help=_LOGGING_HELP % 'stderr')

  app.add_option('--log_to_disk',
                 callback=LogOptions._disk_options_callback,
                 default=_DEFAULT_LOG_OPTS['twitter_common_log_disk_log_level'],
                 type='string',
                 action='callback',
                 metavar='[scheme:]LEVEL',
                 dest='twitter_common_log_disk_log_level',
                 help=_LOGGING_HELP % 'disk')

  app.add_option('--log_dir',
                 type='string',
                 default=_DEFAULT_LOG_OPTS['twitter_common_log_log_dir'],
                 metavar='DIR',
                 dest='twitter_common_log_log_dir',
                 help='The directory into which log files will be generated [default: %default].')

  app.add_option('--log_simple',
                 default=_DEFAULT_LOG_OPTS['twitter_common_log_simple'],
                 action='store_true',
                 dest='twitter_common_log_simple',
                 help='Write a single log file rather than one log file per log level '
                      '[default: %default].')

  app.add_option('--log_to_scribe',
              callback=LogOptions._scribe_options_callback,
              default=_DEFAULT_LOG_OPTS['twitter_common_log_scribe_log_level'],
              type='string',
              action='callback',
              metavar='[scheme:]LEVEL',
              dest='twitter_common_log_scribe_log_level',
              help=_LOGGING_HELP % 'scribe')

  app.add_option('--scribe_category',
              type='string',
              default=_DEFAULT_LOG_OPTS['twitter_common_log_scribe_category'],
              metavar='CATEGORY',
              dest='twitter_common_log_scribe_category',
              help="The category used when logging to the scribe daemon. [default: %default].")

  app.add_option('--scribe_buffer',
              action='store_true',
              default=_DEFAULT_LOG_OPTS['twitter_common_log_scribe_buffer'],
              dest='twitter_common_log_scribe_buffer',
              help="Buffer messages when scribe is unavailable rather than dropping them. [default: %default].")

  app.add_option('--scribe_host',
              type='string',
              default=_DEFAULT_LOG_OPTS['twitter_common_log_scribe_host'],
              metavar='HOST',
              dest='twitter_common_log_scribe_host',
              help="The host running the scribe daemon. [default: %default].")

  app.add_option('--scribe_port',
              type='int',
              default=_DEFAULT_LOG_OPTS['twitter_common_log_scribe_port'],
              metavar='PORT',
              dest='twitter_common_log_scribe_port',
              help="The port used to connect to the scribe daemon. [default: %default].")

########NEW FILE########
__FILENAME__ = parsers
from datetime import datetime, timedelta

from twitter.common.lang import total_ordering

# TODO(wickman) Do something that won't break if this is running over NYE?
_CURRENT_YEAR = str(datetime.now().year)


class Level(object):
  DEBUG   = 0
  INFO    = 10
  WARNING = 20
  ERROR   = 30
  FATAL   = 40


@total_ordering
class Line(object):
  __slots__ = (
    'raw',
    'level',
    'datetime',
    'pid',
    'source',
    'message'
  )

  @classmethod
  def parse(cls, line):
    """parses a line and returns Line if successfully parsed, ValueError/None otherwise."""
    raise NotImplementedError

  @staticmethod
  def parse_order(line, *line_parsers):
    """Given a text line and any number of Line implementations, return the first that matches
       or None if no lines match."""
    for parser in line_parsers:
      try:
        return parser.parse(line)
      except ValueError:
        continue

  def __init__(self, raw, level, dt, pid, source, message):
    (self.raw, self.level, self.datetime, self.pid, self.source, self.message) = (
        raw, level, dt, pid, source, message)

  def extend(self, lines):
    extension = '\n'.join(lines)
    return self.__class__('\n'.join([self.raw, extension]), self.level, self.datetime, self.pid,
        self.source, '\n'.join([self.message, extension]))

  def __lt__(self, other):
    return self.datetime < other.datetime

  def __gt__(self, other):
    return self.datetime > other.datetime

  def __eq__(self, other):
    if not isinstance(other, self.__class__):
      return False
    return (self.datetime == other.datetime and
            self.level == other.level and
            self.pid == other.pid and
            self.source == other.source and
            self.message == other.message)

  def __str__(self):
    return self.raw


class GlogLine(Line):
  LEVEL_MAP = {
    'I': Level.INFO,
    'W': Level.WARNING,
    'E': Level.ERROR,
    'F': Level.FATAL,
    'D': Level.DEBUG
  }

  @classmethod
  def split_time(cls, line):
    if len(line) == 0:
      raise ValueError
    if line[0] not in 'IWEFD':
      raise ValueError
    sline = line[1:].split(' ')
    if len(sline) < 2:
      raise ValueError
    t = datetime.strptime(''.join([_CURRENT_YEAR, sline[0], ' ', sline[1]]), '%Y%m%d %H:%M:%S.%f')
    return cls.LEVEL_MAP[line[0]], t, sline[2:]

  @classmethod
  def parse(cls, line):
    level, dt, rest = cls.split_time(line)
    pid, source, message = rest[0], rest[1], ' '.join(rest[2:])
    return cls(line, level, dt, pid, source, message)


class ZooLine(Line):
  LEVEL_MAP = {
    "ZOO_INVALID": 0,
    "ZOO_ERROR": Level.ERROR,
    "ZOO_WARN": Level.WARNING,
    "ZOO_INFO": Level.INFO,
    "ZOO_DEBUG": Level.DEBUG
  }

  @classmethod
  def parse(cls, line):
    sline = line.split(':')
    if len(sline) < 6:
      raise ValueError
    t = datetime.strptime(':'.join(sline[0:3]), '%Y-%m-%d %H:%M:%S,%f')
    pid = sline[3]
    ssource = sline[4].split('@')
    level = cls.LEVEL_MAP.get(ssource[0], 0)
    source = '@'.join(ssource[1:])
    return cls(line, level, t, pid, source, ':'.join(sline[5:]))

########NEW FILE########
__FILENAME__ = reader
from collections import deque
from datetime import datetime, timedelta
import errno
from io import BytesIO, FileIO
import os

from twitter.common.lang import Compatibility

from .parsers import Line


class Buffer(object):
  CHUNKSIZE = 65536

  @classmethod
  def maybe_filelike(cls, filename_or_filelike):
    if isinstance(filename_or_filelike, Compatibility.string):
      return FileIO(filename_or_filelike)
    else:
      return filename_or_filelike

  @classmethod
  def reset(cls, fp):
    try:
      fp.seek(0, os.SEEK_CUR)
    except IOError:
      return False
    except OSError as e:
      if e.errno == errno.ESPIPE:
        return False
      raise

  def __init__(self, filename_or_filelike, infinite=False):
    self._fp = self.maybe_filelike(filename_or_filelike)
    self._buffer = deque()
    self._tail = None
    self._infinite = infinite

  def next(self):
    last_chunk = self.CHUNKSIZE
    while len(self._buffer) == 0 and last_chunk == self.CHUNKSIZE:
      tail_add = self._fp.read(self.CHUNKSIZE)
      if tail_add:
        self._tail = self._tail + tail_add if self._tail else tail_add
        self._flatten_tail()
      last_chunk = len(tail_add)
    if self._buffer:
      return self._buffer.popleft()
    if last_chunk != self.CHUNKSIZE and self._infinite:
      self.reset(self._fp)
    if last_chunk != self.CHUNKSIZE and self._tail is not None and not self._infinite:
      rv = self._tail
      self._tail = None
      return rv

  def _flatten_tail(self):
    flattened = self._tail.split('\n')
    if len(flattened) > 1:
      self._buffer.extend(flattened[0:-1])
      self._tail = flattened[-1]


class Stream(object):
  class EOF(object): pass

  def __init__(self, filename_or_filelike, parsers, infinite=False):
    """
      Given a filelike-object and a set of Line-derived parsers (e.g. GlogLine, ZooLine),
      generate Line objects from the stream.  If infinite=True, continue after hitting
      EOF.
    """
    self._buffer = Buffer(filename_or_filelike, infinite)
    self._head = None
    self._tail = []
    self._infinite = infinite
    self._parsers = parsers

  def _full_head(self):
    return self._head.extend(self._tail) if self._tail else self._head

  def _handle_end(self):
    if self._infinite:
      return
    else:
      if self._head:
        rv = self._full_head()
        self._head, self._tail = None, []
        return rv
      else:
        return self.EOF

  def next(self):
    while True:
      line = self._buffer.next()
      if line is None:
        return self._handle_end()
      tail = Line.parse_order(line, *self._parsers)
      if tail is not None:
        if self._head:
          rv = self._full_head()
          self._head, self._tail = tail, []
          return rv
        else:
          self._head, self._tail = tail, []
      else:
        self._tail.append(line)


class StreamMuxer(object):
  """
    Multiplexes a set of streams into a single stream.
  """
  def __init__(self, streams):
    """
      Takes a set of (stream, label) pairs.
    """
    streams = list(streams)
    self._labels = dict(streams)
    self._refresh = set(stream for (stream, _) in streams)
    self._heads = set()

  def _collect(self):
    discard = set()
    for stream in self._refresh:
      line = stream.next()
      if line is Stream.EOF:
        discard.add(stream)
      elif line is not None:
        discard.add(stream)
        self._heads.add((line, stream))
    self._refresh -= discard

  def _pop(self):
    try:
      minimum = min(self._heads)
      self._heads.discard(minimum)
      return minimum
    except ValueError:
      pass

  def next(self):
    """
      Returns (label, Line) pairs as they're available, None if nothing is available or Stream.EOF
      if all streams have terminated.
    """
    self._collect()
    if not self._heads and not self._refresh:
      return Stream.EOF
    minimum = self._pop()
    if minimum:
      line, stream = minimum
      self._refresh.add(stream)
      return (self._labels[stream], line)

########NEW FILE########
__FILENAME__ = tracer
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from contextlib import contextmanager
import os
import sys
import threading
import time


__all__ = ('Tracer',)


class Trace(object):
  __slots__ = ('msg', 'verbosity', 'parent', 'children', '_clock', '_start', '_stop')
  def __init__(self, msg, parent=None, verbosity=1, clock=time):
    self.msg = msg
    self.verbosity = verbosity
    self.parent = parent
    if parent is not None:
      parent.children.append(self)
    self.children = []
    self._clock = clock
    self._start = self._clock.time()
    self._stop = None

  def stop(self):
    self._stop = self._clock.time()

  def duration(self):
    assert self._stop is not None
    return self._stop - self._start


class Tracer(object):
  """
    A multi-threaded tracer.
  """
  @classmethod
  def env_filter(cls, env_variable):
    def predicate(verbosity):
      try:
        env_verbosity = int(os.environ.get(env_variable, -1))
      except ValueError:
        env_verbosity = -1
      return verbosity <= env_verbosity
    return predicate

  def __init__(self, predicate=None, output=sys.stderr, clock=time):
    """
      If predicate specified, it should take a "verbosity" integer and determine whether
      or not to log, e.g.

        def predicate(verbosity):
          try:
            return verbosity < int(os.environ.get('APP_VERBOSITY', 0))
          except ValueError:
            return False

      output defaults to sys.stderr, but can take any file-like object.
    """
    self._predicate = predicate or (lambda verbosity: True)
    self._length = None
    self._output = output
    self._isatty = getattr(output, 'isatty', False) and output.isatty()
    self._lock = threading.RLock()
    self._local = threading.local()
    self._clock = clock

  def should_log(self, V):
    return self._predicate(V)

  def log(self, msg, V=0, end='\n'):
    if not self.should_log(V):
      return
    if not self._isatty and end == '\r':
      # force newlines if we're not a tty
      end = '\n'
    trailing_whitespace = ''
    with self._lock:
      if self._length and self._length > len(msg):
        trailing_whitespace = ' ' * (self._length - len(msg))
      self._output.write(msg + trailing_whitespace + end)
      self._output.flush()
      self._length = len(msg) if end == '\r' else 0

  def print_trace_snippet(self):
    parent = self._local.parent
    parent_verbosity = parent.verbosity
    if not self.should_log(parent_verbosity):
      return
    traces = []
    while parent:
      if self.should_log(parent.verbosity):
        traces.append(parent.msg)
      parent = parent.parent
    self.log(' :: '.join(reversed(traces)), V=parent_verbosity, end='\r')

  def print_trace(self, indent=0, node=None):
    node = node or self._local.parent
    with self._lock:
      self.log(' ' * indent + ('%s: %.1fms' % (node.msg, 1000.0 * node.duration())),
               V=node.verbosity)
      for child in node.children:
        self.print_trace(indent=indent + 2, node=child)

  @contextmanager
  def timed(self, msg, V=0):
    if getattr(self._local, 'parent', None) is None:
      self._local.parent = Trace(msg, verbosity=V, clock=self._clock)
    else:
      parent = self._local.parent
      self._local.parent = Trace(msg, parent=parent, verbosity=V, clock=self._clock)
    self.print_trace_snippet()
    yield
    self._local.parent.stop()
    if self._local.parent.parent is not None:
      self._local.parent = self._local.parent.parent
    else:
      self.print_trace()
      self._local.parent = None


def main(args):
  import random

  tracer = Tracer(output=open(args[0], 'w')) if len(args) > 0 else Tracer()

  def process(name):
    with tracer.timed(name):
      with tracer.timed('acquiring'):
        with tracer.timed('downloading'):
          time.sleep(3 * random.random())
          if random.random() > 0.66:
            tracer.log('%s failed downloading!' % name)
            return
        with tracer.timed('unpacking'):
          time.sleep(1 * random.random())
      with tracer.timed('configuring'):
        time.sleep(0.5 * random.random())
      with tracer.timed('building'):
        time.sleep(5.0 * random.random())
        if random.random() > 0.66:
          tracer.log('%s failed building!' % name)
          return
      with tracer.timed('installing'):
        time.sleep(2.0 * random.random())

  workers = [threading.Thread(target=process, args=('worker %d' % k,)) for k in range(5)]
  for worker in workers:
    worker.start()
  for worker in workers:
    worker.join()


if __name__ == '__main__':
  main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = gauge
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================


from twitter.common.lang import Compatibility


# Duck-typing helpers
def gaugelike(obj):
  return hasattr(obj, 'read') and callable(obj.read)

def namable(obj):
  return hasattr(obj, 'name') and callable(obj.name)

def namablegauge(obj):
  return gaugelike(obj) and namable(obj)


# Typed gauges.
class Gauge(object):
  """
    A readable gauge that exports a value.
  """
  def __init__(self, value):
    self._value = value

  def read(self):
    return self._value


class NamedGauge(Gauge):
  """
    Named gauge (gauge that exports name() method.)
  """
  def __init__(self, name, value=None):
    if not isinstance(name, str):
      raise TypeError('NamedGauge must be named by a string, got %s' % type(name))
    self._name = name
    Gauge.__init__(self, value)

  def name(self):
    return self._name


class MutableGauge(Gauge):
  """
    Mutable gauge.
  """
  def __init__(self, value=None):
    import threading
    self._lock = threading.Lock()
    Gauge.__init__(self, value)

  def read(self):
    with self.lock():
      return self._value

  def write(self, value):
    with self.lock():
      self._value = value
      return self._value

  def lock(self):
    return self._lock


class Label(NamedGauge):
  """
    A generic immutable key-value Gauge.  (Not specifically strings, but that's
    the intention.)
  """
  def __init__(self, name, value):
    NamedGauge.__init__(self, name, value)


class LambdaGauge(NamedGauge):
  def __init__(self, name, fn):
    import threading
    if not callable(fn):
      raise TypeError("A LambdaGauge must be supplied with a callable, got %s" % type(fn))
    NamedGauge.__init__(self, name, fn)
    self._lock = threading.Lock()

  def read(self):
    with self._lock:
      return self._value()


class MutatorGauge(NamedGauge, MutableGauge):
  def __init__(self, name, value=None):
    NamedGauge.__init__(self, name)
    MutableGauge.__init__(self, value)


class AtomicGauge(NamedGauge, MutableGauge):
  """
    Something akin to AtomicLong. Basically a MutableGauge but with
    atomic add, increment, decrement.
  """
  def __init__(self, name, initial_value=0):
    if not isinstance(initial_value, Compatibility.integer):
      raise TypeError('AtomicGauge must be initialized with an integer.')
    NamedGauge.__init__(self, name)
    MutableGauge.__init__(self, initial_value)

  def add(self, delta):
    """
      Add delta to metric and return updated metric.
    """
    if not isinstance(delta, Compatibility.integer):
      raise TypeError('AtomicGauge.add must be called with an integer.')
    with self.lock():
      self._value += delta
      return self._value

  def increment(self):
    """
      Increment metric and return updated metric.
    """
    return self.add(1)

  def decrement(self):
    """
      Decrement metric and return updated metric.
    """
    return self.add(-1)

########NEW FILE########
__FILENAME__ = metrics
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================


from twitter.common.lang import Compatibility, Singleton
from .gauge import (
  Gauge,
  MutatorGauge,
  NamedGauge,
  namablegauge)


class Observable(object):
  """
    A trait providing a metric namespace for an object.

    Classes should mix-in Observable and register metrics against self.metrics.

    Application owners can then register observable objects into a metric
    space or the root metrics, e.g. via
        RootMetrics().register_observable('object_namespace', my_object)
  """
  @property
  def metrics(self):
    """
      Returns a Metric namespace for this object.
    """
    if not hasattr(self, '_observable_metrics'):
      self._observable_metrics = Metrics()
    return self._observable_metrics


class MetricProvider(object):
  def sample(self):
    """
      Returns a dictionary
        string (metric) => sample (number)
    """
    raise NotImplementedError


class MetricRegistry(object):
  def scope(self, name):
    """
      Returns a (potentially memoized) child scope with a given name.
    """
    raise NotImplementedError

  def register(self, gauge):
    """
      Register a gauge (mapper from name => sample) with this registry.
    """
    raise NotImplementedError

  def unregister(self, name):
    """
      Unregister a name from the registry.
    """
    raise NotImplementedError

  def mutator(self, name):
    """
      Return a mutator function of the gauge associated with name.
    """
    raise NotImplementedError


class Metrics(MetricRegistry, MetricProvider):
  """
    Metric collector.
  """

  class Error(Exception): pass

  @classmethod
  def coerce_value(cls, value):
    if isinstance(value, Compatibility.numeric + Compatibility.string + (bool,)):
      return value
    elif value is None:
      return value
    elif isinstance(value, list):
      return [cls.coerce_value(v) for v in value]
    elif isinstance(value, dict):
      return dict((cls.coerce_value(k), cls.coerce_value(v)) for (k, v) in value.items())
    else:
      return str(value)

  @classmethod
  def coerce_metric(cls, metric_tuple):
    name, value = metric_tuple
    try:
      return (name, cls.coerce_value(value.read()))
    except ValueError:
      return None

  def __init__(self):
    self._metrics = {}
    self._children = {}

  def scope(self, name):
    if not isinstance(name, Compatibility.string):
      raise TypeError('Scope names must be strings, got: %s' % type(name))
    if name not in self._children:
      self._children[name] = Metrics()
    return self._children[name]

  def register_observable(self, name, observable):
    if not isinstance(name, Compatibility.string):
      raise TypeError('Scope names must be strings, got: %s' % type(name))
    if not isinstance(observable, Observable):
      raise TypeError('observable must be an Observable, got: %s' % type(observable))
    self._children[name] = observable.metrics

  def unregister_observable(self, name):
    if not isinstance(name, Compatibility.string):
      raise TypeError('Unregister takes a string name!')
    return self._children.pop(name, None)

  def register(self, gauge):
    if isinstance(gauge, Compatibility.string):
      gauge = MutatorGauge(gauge)
    if not isinstance(gauge, NamedGauge) and not namablegauge(gauge):
      raise TypeError('Must register either a string or a Gauge-like object! Got %s' % gauge)
    self._metrics[gauge.name()] = gauge
    return gauge

  def unregister(self, name):
    if not isinstance(name, Compatibility.string):
      raise TypeError('Unregister takes a string name!')
    return self._metrics.pop(name, None)

  @classmethod
  def sample_name(cls, scope_name, sample_name):
    return '.'.join([scope_name, sample_name])

  def sample(self):
    samples = dict(filter(None, map(self.coerce_metric, self._metrics.items())))
    for scope_name, scope in self._children.items():
      samples.update((self.sample_name(scope_name, sample_name), sample_value)
                     for (sample_name, sample_value) in scope.sample().items())
    return samples


class CompoundMetrics(MetricProvider):
  def __init__(self, *providers):
    if not all(isinstance(provider, MetricProvider) for provider in providers):
      raise TypeError('CompoundMetrics must take a collection of MetricProviders')
    self._providers = providers

  def sample(self):
    root_sample = {}
    for provider in self._providers:
      root_sample.update(provider.sample())
    return root_sample


class MemoizedMetrics(MetricProvider):
  def __init__(self, provider):
    if not isinstance(provider, MetricProvider):
      raise TypeError('MemoizedMetrics must take a MetricProvider')
    self._provider = provider
    self._sample = {}

  def sample(self):
    self._sample = self._provider.sample()
    return self._sample

  @property
  def memoized_sample(self):
    return self._sample


class RootMetrics(Metrics, Singleton):
  """
    Root singleton instance of the metrics.
  """

  _INIT = False

  def __init__(self):
    if not RootMetrics._INIT:
      Metrics.__init__(self)
      RootMetrics._INIT = True

  # For testing.
  def clear(self):
    Metrics.__init__(self)

########NEW FILE########
__FILENAME__ = rate
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import time

from twitter.common.quantity import Amount, Time
from .gauge import NamedGauge, gaugelike, namablegauge

class Rate(NamedGauge):
  """
    Gauge that computes a windowed rate.
  """
  @staticmethod
  def of(gauge, name = None, window = None, clock = None):
    kw = {}
    if window: kw.update(window = window)
    if clock: kw.update(clock = clock)
    if name:
      if not gaugelike(gauge):
        raise TypeError('Rate.of must take a Gauge-like object!  Got %s' % type(gauge))
      return Rate(name, gauge, **kw)
    else:
      if not namablegauge(gauge):
        raise TypeError('Rate.of must take a namable Gauge-like object if no name specified!')
      return Rate(gauge.name(), gauge, **kw)

  def __init__(self, name, gauge, window = Amount(1, Time.SECONDS), clock = time):
    """
      Create a gauge using name as a base for a <name>_per_<window> sampling gauge.

        name: The base name of the gauge.
        gauge: The gauge to sample
        window: The window over which the samples should be measured (default 1 second.)
    """
    self._clock = clock
    self._gauge = gauge
    self._samples = []
    self._window = window
    NamedGauge.__init__(self, '%s_per_%s%s' % (name, window.amount(), window.unit()))

  def filter(self, newer_than=None):
    """
      Filter the samples to only contain elements in the window.
    """
    if newer_than is None:
      newer_than = self._clock.time() - self._window.as_(Time.SECONDS)
    self._samples = [sample for sample in self._samples if sample[0] >= newer_than]

  def read(self):
    now = self._clock.time()
    self.filter(now - self._window.as_(Time.SECONDS))
    new_sample = self._gauge.read()
    self._samples.insert(0, (now, new_sample))
    if len(self._samples) == 1:
      return 0
    last_sample = self._samples[-1]
    dy = new_sample - last_sample[1]
    dt = now - last_sample[0]
    return 0 if dt == 0 else dy / dt

########NEW FILE########
__FILENAME__ = sampler
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import json
import os
import time
import threading

try:
  from twitter.common import log
except ImportError:
  log = None

from twitter.common.exceptions import ExceptionalThread
from twitter.common.quantity import Amount, Time

from .metrics import MetricProvider


class SamplerBase(ExceptionalThread):
  def __init__(self, period, clock):
    self._stop = threading.Event()
    self._period = period
    self._clock = clock
    ExceptionalThread.__init__(self)
    self.daemon = True

  def stop(self):
    self._stop.set()

  def is_stopped(self):
    return self._stop.is_set()

  def iterate(self):
    raise NotImplementedError

  def run(self):
    while True:
      self._clock.sleep(self._period.as_(Time.SECONDS))
      if self.is_stopped():
        break
      self.iterate()


class MetricSampler(SamplerBase, MetricProvider):
  """
    A thread that periodically samples from a MetricProvider and caches the
    samples.
  """
  def __init__(self, provider, period=Amount(1, Time.SECONDS), clock=time):
    self._provider = provider
    self._last_sample = self._provider.sample()
    self._lock = threading.Lock()
    SamplerBase.__init__(self, period, clock)
    self.daemon = True

  def sample(self):
    with self._lock:
      return self._last_sample

  def iterate(self):
    new_sample = self._provider.sample()
    with self._lock:
      self._last_sample = new_sample


class DiskMetricWriter(SamplerBase):
  """
    Takes a MetricProvider and periodically samples its values to disk in JSON format.
  """

  def __init__(self, provider, filename, period=Amount(15, Time.SECONDS), clock=time):
    self._provider = provider
    self._filename = filename
    SamplerBase.__init__(self, period, clock)
    self.daemon = True

  def iterate(self):
    with open(self._filename, 'w') as fp:
      json.dump(self._provider.sample(), fp)


class DiskMetricReader(SamplerBase, MetricProvider):
  """
    Given an input JSON file, periodically reads the contents from disk and exports it
    using the MetricProvider interface.
  """

  def __init__(self, filename, period=Amount(15, Time.SECONDS), clock=time):
    self._filename = filename
    self._sample = {}
    self._lock = threading.Lock()
    SamplerBase.__init__(self, period, clock)
    self.daemon = True

  def sample(self):
    with self._lock:
      return self._sample

  @property
  def age(self):
    try:
      return time.time() - os.path.getmtime(self._filename)
    except (IOError, OSError):
      return 0

  def iterate(self):
    with self._lock:
      try:
        with open(self._filename, 'r') as fp:
          self._sample = json.load(fp)
      except (IOError, OSError, ValueError) as e:
        if log:
          log.warn('Failed to collect sample: %s' % e)

########NEW FILE########
__FILENAME__ = socks
"""
  A SOCKS HTTPConnection handler.  Adapted from https://gist.github.com/e000/869791
"""

from __future__ import absolute_import

import socket

import socks

try:
  from http.client import HTTPConnection
  import urllib.request as urllib_request
except ImportError:
  from httplib import HTTPConnection
  import urllib2 as urllib_request

__all__ = ('opener',)


class SocksiPyConnection(HTTPConnection):
  def __init__(self, proxytype,
                     proxyaddr,
                     proxyport=None,
                     rdns=True,
                     username=None,
                     password=None,
                     *args,
                     **kwargs):
    self._proxyargs = (proxytype, proxyaddr, proxyport, rdns, username, password)
    HTTPConnection.__init__(self, *args, **kwargs)

  def connect(self):
    self.sock = socks.socksocket()
    self.sock.setproxy(*self._proxyargs)

    # Most Python variants use socket._GLOBAL_DEFAULT_TIMEOUT as the socket timeout.
    # Unfortunately this is an object() sentinel, and sock.settimeout requires a float.
    # What were they thinking?
    if not hasattr(socket, '_GLOBAL_DEFAULT_TIMEOUT') or (
        self.timeout != socket._GLOBAL_DEFAULT_TIMEOUT):
      self.sock.settimeout(self.timeout)

    # SocksiPy has this gem:
    #   if type(self.host) != type('')
    # which breaks should it get a host in unicode form in 2.x.  sigh.
    self.sock.connect((str(self.host), self.port))


class SocksiPyHandler(urllib_request.HTTPHandler):
  def __init__(self, *args, **kwargs):
   self._args = args
   self._kw = kwargs
   urllib_request.HTTPHandler.__init__(self)

  def build_connection(self, host, port=None, strict=None, timeout=None):
    return SocksiPyConnection(*self._args, host=host, port=port, strict=strict,
        timeout=timeout, **self._kw)

  def http_open(self, req):
    return self.do_open(self.build_connection, req)


def urllib_opener(proxy_host, proxy_port, proxy_type=socks.PROXY_TYPE_SOCKS5, **kw):
  """
    Construct a proxied urllib opener via the SOCKS proxy at proxy_host:proxy_port.

    proxy_type may be socks.PROXY_TYPE_SOCKS4 or socks.PROXY_TYPE_SOCKS5, by
    default the latter.  the remaining keyword arguments will be passed to SocksiPyHandler,
    e.g. rdns, username, password.
  """
  return urllib_request.build_opener(SocksiPyHandler(proxy_type, proxy_host, proxy_port, **kw))

########NEW FILE########
__FILENAME__ = tunnel
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import atexit
import errno
import socket
import subprocess
import time

from twitter.common import log
from twitter.common.quantity import Amount, Time

__all__ = (
  'TunnelHelper',
)


def safe_kill(po):
  """
    Given a Popen object, safely kill it without an unexpected exception.
  """
  try:
    po.kill()
  except OSError as e:
    if e.errno != errno.ESRCH:
      raise
  po.wait()


# TODO(wickman) Add mox tests for this.
class TunnelHelper(object):
  """ Class to initiate an SSH or SOCKS tunnel to a remote host through a tunnel host.

  The ssh binary must be on the PATH.
  """
  TUNNELS = {}
  PROXIES = {}
  MIN_RETRY = Amount(5, Time.MILLISECONDS)
  MAX_INTERVAL = Amount(1, Time.SECONDS)
  WARN_THRESHOLD = Amount(10, Time.SECONDS)
  DEFAULT_TIMEOUT = Amount(5, Time.MINUTES)

  class TunnelError(Exception): pass

  @classmethod
  def log(cls, msg):
    log.debug('%s: %s' % (cls.__name__, msg))

  @classmethod
  def get_random_port(cls):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.bind(('localhost', 0))
    _, port = s.getsockname()
    s.close()
    return port

  @classmethod
  def acquire_host_pair(cls, host, port=None):
    port = port or cls.get_random_port()
    return host, port

  @classmethod
  def wait_for_accept(cls, port, tunnel_popen, timeout):
    total_time = Amount(0, Time.SECONDS)
    sleep = cls.MIN_RETRY
    warned = False  # Did we log a warning that shows we're waiting for the tunnel?

    while total_time < timeout and tunnel_popen.returncode is None:
      try:
        accepted_socket = socket.create_connection(('localhost', port), timeout=5.0)
        accepted_socket.close()
        return True
      except socket.error:
        total_time += sleep
        time.sleep(sleep.as_(Time.SECONDS))

        # Increase sleep exponentially until MAX_INTERVAL is reached
        sleep = min(sleep * 2, cls.MAX_INTERVAL)

        if total_time > cls.WARN_THRESHOLD and not warned:
          log.warn('Still waiting for tunnel to be established after %s (timeout is %s)' % (
              total_time, cls.DEFAULT_TIMEOUT))
          warned = True

        tunnel_popen.poll()  # needed to update tunnel_popen.returncode
    if tunnel_popen.returncode is not None:
      cls.log('SSH returned prematurely with code %s' % str(tunnel_popen.returncode))
    else:
      cls.log('timed out initializing tunnel')
    return False

  @classmethod
  def create_tunnel(
      cls,
      remote_host,
      remote_port,
      tunnel_host,
      tunnel_port=None,
      timeout=DEFAULT_TIMEOUT):

    """
      Create or retrieve a memoized SSH tunnel to the remote host & port, using
      tunnel_host:tunnel_port as the tunneling server.
    """
    tunnel_key = (remote_host, remote_port)
    if tunnel_key in cls.TUNNELS:
      return 'localhost', cls.TUNNELS[tunnel_key][0]
    tunnel_host, tunnel_port = cls.acquire_host_pair(tunnel_host, tunnel_port)
    cls.log('opening connection to %s:%s via %s:%s' %
        (remote_host, remote_port, tunnel_host, tunnel_port))
    ssh_cmd_args = ('ssh', '-q', '-N', '-T', '-L',
        '%d:%s:%s' % (tunnel_port, remote_host, remote_port), tunnel_host)
    ssh_popen = subprocess.Popen(ssh_cmd_args, stdin=subprocess.PIPE)
    cls.TUNNELS[tunnel_key] = tunnel_port, ssh_popen
    if not cls.wait_for_accept(tunnel_port, ssh_popen, timeout):
      raise cls.TunnelError('Could not establish tunnel to %s via %s' % (remote_host, tunnel_host))
    cls.log('session established')
    return 'localhost', tunnel_port

  @classmethod
  def create_proxy(cls, proxy_host, proxy_port=None, timeout=DEFAULT_TIMEOUT):
    """
      Create or retrieve a memoized SOCKS proxy using the specified proxy host:port
    """
    if proxy_host in cls.PROXIES:
      return 'localhost', cls.PROXIES[proxy_host][0]
    proxy_host, proxy_port = cls.acquire_host_pair(proxy_host, proxy_port)
    cls.log('opening SOCKS proxy connection through %s:%s' % (proxy_host, proxy_port))
    ssh_cmd_args = ('ssh', '-q', '-N', '-T', '-D', str(proxy_port), proxy_host)
    ssh_popen = subprocess.Popen(ssh_cmd_args, stdin=subprocess.PIPE)
    cls.PROXIES[proxy_host] = (proxy_port, ssh_popen)
    if not cls.wait_for_accept(proxy_port, ssh_popen, timeout):
      raise cls.TunnelError('Could not establish proxy via %s' % proxy_host)
    cls.log('session established')
    return 'localhost', proxy_port

  @classmethod
  def cancel_tunnel(cls, remote_host, remote_port):
    """
      Cancel the SSH tunnel to (remote_host, remote_port) if it exists.
    """
    _, po = cls.TUNNELS.pop((remote_host, remote_port), (None, None))
    if po:
      safe_kill(po)


@atexit.register
def _cleanup():
  for _, po in TunnelHelper.TUNNELS.values():
    safe_kill(po)
  for _, po in TunnelHelper.PROXIES.values():
    safe_kill(po)

########NEW FILE########
__FILENAME__ = twitter_option
from copy import copy
from datetime import datetime
from optparse import Option, OptionValueError

def _check_date(option, opt, value):
  try:
    return datetime.strptime(value, '%Y-%m-%d').date()
  except ValueError:
    raise OptionValueError('Value for %s not a valid date in format YYYY-MM-DD' % option)

class TwitterOption(Option):
  TYPES = Option.TYPES + ('date',)
  TYPE_CHECKER = copy(Option.TYPE_CHECKER)
  TYPE_CHECKER['date'] = _check_date

########NEW FILE########
__FILENAME__ = process_handle
from abc import abstractmethod

from twitter.common.lang import Interface
from twitter.common.string import ScanfParser

class ProcessHandle(Interface):
  """
    ProcessHandle interface.  Methods that must be exposed by whatever process
    monitoring mechanism you use.
  """
  @abstractmethod
  def cpu_time(self):
    """
      Total cpu time of this process.
    """

  @abstractmethod
  def wall_time(self):
    """
      Total wall time this process has been up.
    """

  @abstractmethod
  def pid(self):
    """
      PID of the process.
    """

  @abstractmethod
  def ppid(self):
    """
      Parent PID of the process.
    """

  @abstractmethod
  def user(self):
    """
      The owner of the process.
    """

  @abstractmethod
  def cwd(self):
    """
      The current working directory of the process.
    """

  @abstractmethod
  def cmdline(self):
    """
      The full command line of the process.
    """
    raise NotImplementedError



class ProcessHandleParser(ScanfParser):
  """
    Given:
      attrs: list of attribute names
      type_map: map of attribute name to attribute type (%d/%u/etc format converter)
      handlers: optional set of postprocessing callbacks that take (attribute, value)
    Process a line from one of the process information sources (e.g. ps, procfs.)
  """
  def parse(self, line):
    d = {}
    try:
      so = ScanfParser.parse(self, ' '.join(line.split()), True)
      for attr, value in zip(self._attrs, so.ungrouped()):
        d[attr] = self._handlers[attr](attr, value) if attr in self._handlers else value
    except ScanfParser.ParseError as e:
      return {}
    return d

  def __init__(self, attrs, type_map, handlers = {}):
    self._attrs = attrs
    self._handlers = handlers
    attr_list = map(type_map.get, attrs)
    ScanfParser.__init__(self, ' '.join(attr_list))


class ProcessHandleParserBase(object):
  """
    Given a provider of process lines, parse them into bundles of attributes that can be
    translated into ProcessHandles.
  """
  def _produce(self):
    raise NotImplementedError

  def _realize(self):
    return self._realize_from_line(self._produce())

  def _realize_from_line(self, line):
    self._exists = False
    if line is None:
      self._attrs = {}
    else:
      self._attrs = self.PARSER.parse(line)
      if self._attrs:
        self._pid = self._attrs['pid']
        self._exists = True

  @classmethod
  def from_line(cls, line):
    proc = cls()
    proc._realize_from_line(line)
    return proc

  @classmethod
  def init_class(cls):
    if not hasattr(cls, 'PARSER'):
      assert hasattr(cls, 'ATTRS')
      assert hasattr(cls, 'TYPE_MAP')
      assert hasattr(cls, 'HANDLERS')
      setattr(cls, 'PARSER', ProcessHandleParser(cls.ATTRS, cls.TYPE_MAP, cls.HANDLERS))
    if not hasattr(cls, 'ALIASES'):
      cls.ALIASES = {}

  def __init__(self, pid=-1):
    self.init_class()
    self._exists = False
    self._pid = pid
    self._attrs = None
    if pid != -1:
      self._realize()

  def exists(self):
    return self._exists

  def get(self, key):
    probe_key = self.ALIASES[key] if key in self.ALIASES else key
    return self._attrs.get(probe_key)

  def refresh(self, line=None):
    return self._realize() if line is None else self._realize_from_line(line)

########NEW FILE########
__FILENAME__ = process_handle_procfs
import errno
import os
import pwd
import time

from .process_handle import ProcessHandleParserBase

class ProcessHandlersProcfs(object):
  BOOT_TIME = None
  @staticmethod
  def boot_time(now=None):
    now = now or time.time()
    if ProcessHandlersProcfs.BOOT_TIME is None:
      try:
        with open("/proc/uptime") as fp:
          uptime, _ = fp.read().split()
        ProcessHandlersProcfs.BOOT_TIME = now - float(uptime)
      except:
        ProcessHandlersProcfs.BOOT_TIME = 0
        pass
    return ProcessHandlersProcfs.BOOT_TIME

  @staticmethod
  def handle_time(_, value):
    return 1.0 * value / os.sysconf('SC_CLK_TCK')

  @staticmethod
  def handle_mem(_, value):
    return value * os.sysconf('SC_PAGESIZE')

  @staticmethod
  def handle_start_time(key, value):
    elapsed_after_system_boot = ProcessHandlersProcfs.handle_time(key, value)
    return time.time() - (ProcessHandlersProcfs.boot_time() + elapsed_after_system_boot)


class ProcessHandleProcfs(ProcessHandleParserBase):
  ATTRS = (
    """pid comm state ppid pgrp session tty_nr tpgid flags minflt cminflt majflt cmajflt utime
       stime cutime cstime priority nice num_threads itrealvalue starttime vsize rss rsslim
       startcode endcode startstack kstkesp kstkeip signal blocked sigignore sigcatch wchan nswap
       cnswap exit_signal processor rt_priority policy""".split())

  TYPE_MAP = {
            "pid":   "%d",         "comm":   "%s",       "state":  "%c",        "ppid":  "%d",
           "pgrp":   "%d",      "session":   "%d",      "tty_nr":  "%d",       "tpgid":  "%d",
          "flags":   "%u",       "minflt":  "%lu",     "cminflt": "%lu",      "majflt": "%lu",
        "cmajflt":  "%lu",        "utime":  "%lu",       "stime": "%lu",      "cutime": "%ld",
         "cstime":  "%ld",     "priority":  "%ld",        "nice": "%ld", "num_threads": "%ld",
    "itrealvalue":  "%ld",    "starttime": "%llu",       "vsize": "%lu",         "rss": "%ld",
         "rsslim":  "%lu",    "startcode":  "%lu",     "endcode": "%lu",  "startstack": "%lu",
        "kstkesp":  "%lu",      "kstkeip":  "%lu",      "signal": "%lu",     "blocked": "%lu",
      "sigignore":  "%lu",     "sigcatch":  "%lu",       "wchan": "%lu",       "nswap": "%lu",
         "cnswap":  "%lu",  "exit_signal":   "%d",   "processor":  "%d", "rt_priority":  "%u",
         "policy":   "%u"
  }

  ALIASES = {
    'vsz': 'vsize',
    'stat': 'state',
  }

  HANDLERS = {
    'utime': ProcessHandlersProcfs.handle_time,
    'stime': ProcessHandlersProcfs.handle_time,
    'cutime': ProcessHandlersProcfs.handle_time,
    'cstime': ProcessHandlersProcfs.handle_time,
    'starttime': ProcessHandlersProcfs.handle_start_time,
    'rss': ProcessHandlersProcfs.handle_mem
  }

  def _produce(self):
    try:
      with open("/proc/%s/stat" % self._pid) as fp:
        return fp.read()
    except IOError as e:
      if e.errno not in (errno.ENOENT, errno.ESRCH):
        raise e

  def cpu_time(self):
    return self.get('utime') + self.get('stime')

  def wall_time(self):
    return self.get('starttime')

  def pid(self):
    return self.get('pid')

  def ppid(self):
    return self.get('ppid')

  def user(self):
    try:
      uid = os.stat('/proc/%s' % self.pid()).st_uid
      try:
        pwd_entry = pwd.getpwuid(uid)
      except KeyError:
        return None
      return pwd_entry.pw_name
    except OSError:
      return None

  def cwd(self):
    try:
      return os.readlink('/proc/%s/cwd' % self.pid())
    except OSError:
      # Likely permission denied or no such file or directory
      return None

  def cmdline(self):
    try:
      with open('/proc/%s/cmdline' % self.pid(), 'r') as infile:
        return infile.read().replace('\0', ' ')
    except OSError:
      # Likely permission denied or no such file or directory
      return None


########NEW FILE########
__FILENAME__ = process_handle_ps
import os
import subprocess
from process_handle import ProcessHandle, ProcessHandleParserBase

class ProcessHandlersPs(object):
  @staticmethod
  def handle_mem(_, value):
    return value * 1024

  @staticmethod
  def handle_elapsed(_, value):
    seconds = 0

    unpack = value.split('-')
    if len(unpack) == 2:
      seconds += int(unpack[0]) * 86400
      unpack = unpack[1]
    else:
      unpack = unpack[0]

    unpack = unpack.split(':')
    mult = 1.0
    for k in range(len(unpack), 0, -1):
      seconds += float(unpack[k-1]) * mult
      mult    *= 60

    return seconds

class ProcessHandlePs(ProcessHandle, ProcessHandleParserBase):
  ATTRS = [ 'user', 'pid', 'ppid', 'pcpu', 'rss', 'vsz', 'stat', 'etime', 'time', 'comm' ]

  TYPE_MAP = {
     'user': '%s',  'pid': '%d', 'ppid': '%d', 'pcpu': '%f', 'rss': '%d', 'vsz': '%d', 'stat': '%s',
    'etime': '%s', 'time': '%s', 'comm': '%s'
  }

  HANDLERS = {
    'rss':    ProcessHandlersPs.handle_mem,
    'vsz':    ProcessHandlersPs.handle_mem,
    'etime':  ProcessHandlersPs.handle_elapsed, # [[dd-]hh:]mm:ss.ds
    'time':   ProcessHandlersPs.handle_elapsed, # [[dd-]hh:]mm:ss.ds
  }

  ALIASES = {
    'starttime': 'etime',
  }

  def _get_process_attrs(self, attrs):
    try:
      data = os.popen('ps -p %s -o %s' % (self._pid, ','.join(attrs))).readlines()
      if len(data) > 1:
        return data[-1]
    except:
      return None

  def _produce(self):
    return self._get_process_attrs(ProcessHandlePs.ATTRS)

  def cpu_time(self):
    return self.get('time') or 0.0

  def wall_time(self):
    return self.get('starttime') or 0.0

  def pid(self):
    return self.get('pid')

  def ppid(self):
    return self.get('ppid')

  def user(self):
    return self.get('user')

  def cwd(self):
    try:
      lsof = subprocess.Popen(('lsof -a -p %s -d cwd -Fn' % self.pid()).split(),
        stdout=subprocess.PIPE, stderr=subprocess.PIPE)
      stdout, _ = lsof.communicate()
      for line in stdout.split('\n'):
        if line.startswith('n'):
          return line[1:]
    except OSError:
      return None

  def cmdline(self):
    # 'comm' is just the base cmd, this returns the cmd with all the arguments.
    # We don't read 'command' on the initial ps call, because the result contains spaces, and
    # our scanf-like parsing code won't read it. This isn't a performance issue in current usage.
    return self._get_process_attrs(['command']).strip()


########NEW FILE########
__FILENAME__ = process_provider
from abc import abstractmethod
from copy import copy
from collections import defaultdict

from twitter.common.lang import Interface

class ProcessProvider(Interface):
  """
    A provider of process handles.  Basically an interface in front of procfs or ps.
  """

  class UnknownPidError(Exception): pass

  def __init__(self):
    self.clear()

  def clear(self, pids=None):
    """
      Clear pids from cached internal state.  If pids is None, clear everything.
    """
    if pids is None:
      self._raw = {}
      self._pids = set()
      self._pid_to_parent = {}
      self._pid_to_children = defaultdict(set)
      self._handles = {}
    else:
      for pid, ppid in self._pid_to_parent.items():
        try:
          self._pid_to_children[ppid].remove(pid)
        except KeyError:
          pass
      for pid in pids:
        self._raw.pop(pid, None)
        self._handles.pop(pid, None)
      self._pids = self._pids - set(pids)

  def _process_lines(self, lines):
    for line in lines:
      pid, ppid = self._translate_line_to_pid_pair(line)
      if pid is None: continue
      self._pids.add(pid)
      self._raw[pid] = line
      self._pid_to_parent[pid] = ppid
      self._pid_to_children[ppid].add(pid)

  def _raise_unless_has_pid(self, pid):
    if pid not in self._pids:
      raise ProcessProvider.UnknownPidError("Do not know about pid %s, call refresh()?" % pid)

  def pids(self):
    """Returns list of pids from the last collection."""
    return copy(self._pids)

  def _calculate_children(self, pid, current_set):
    new_children = copy(self._pid_to_children[pid])
    added = set()
    for new_child in new_children:
      if new_child not in current_set:
        added.add(new_child)
        current_set.add(new_child)
    for new_child in added:
      self._calculate_children(new_child, current_set)

  def children_of(self, pid, all=False):
    """
      By default returns set of pids of direct descendents of pid based upon
      last collection.  If all=True is set, then get all descendents of pid
      recursively.
    """
    self._raise_unless_has_pid(pid)
    if all:
      all_children = set()
      self._calculate_children(pid, all_children)
      return all_children
    else:
      return copy(self._pid_to_children[pid])

  def get_handle(self, pid):
    """Given a pid, return a ProcessHandle based upon the last collection."""
    self._raise_unless_has_pid(pid)
    return self._translate_line_to_handle(self._raw[pid])

  def collect_all(self):
    """Collect data from all processes."""
    self.clear()
    self._process_lines(self._collect_all())

  def collect_set(self, pidset):
    """Collect data from a subset of processes."""
    self.clear(pidset)
    self._process_lines(self._collect_set(pidset))

  # Required by provider implementations.
  @abstractmethod
  def _collect_all(self):
    """Collect and return all process information into unstructured line-based data."""

  @abstractmethod
  def _collect_set(self, pidset):
    """Collect and return a subset of process information into unstructured line-based data."""

  @abstractmethod
  def _translate_line_to_pid_pair(self, line):
    """Given a line, extract a pid and ppid from it."""

  @abstractmethod
  def _translate_line_to_handle(self, line):
    """Given a line, extract a ProcessHandle from it."""

  @staticmethod
  def _platform_compatible():
    """Returns true if this provider is compatible on the current platform."""
    raise NotImplementedError

########NEW FILE########
__FILENAME__ = process_provider_procfs
import os

from .process_handle_procfs import ProcessHandleProcfs
from .process_provider import ProcessProvider

def filter_map(fn, lst):
  return filter(lambda return_value: return_value is not None, map(fn, lst))

class ProcessProvider_Procfs(ProcessProvider):
  """
    ProcessProvider on top of procfs.
  """
  def _collect_all(self):
    def try_int(value):
      try:
        return int(value)
      except ValueError:
        return None
    return self._collect_set(filter_map(try_int, os.listdir('/proc')))

  def _collect_set(self, pidset):
    def try_read_pid(pid):
      try:
        with open('/proc/%s/stat' % pid) as fp:
          return fp.read()
      except IOError:
        return None
    return filter_map(try_read_pid, pidset)

  def _translate_line_to_pid_pair(self, line):
    sline = line.split()
    try:
      return (int(sline[ProcessHandleProcfs.ATTRS.index('pid')]),
              int(sline[ProcessHandleProcfs.ATTRS.index('ppid')]))
    except:
      return None, None

  def _translate_line_to_handle(self, line):
    return ProcessHandleProcfs.from_line(line)

  @staticmethod
  def _platform_compatible():
    # Compatible with any Linux >=2.5.19, but could be easily adapted
    # to much earlier iterations (2.2.x+)
    return os.uname()[0] == 'Linux'

########NEW FILE########
__FILENAME__ = process_provider_ps
import os
from process_handle_ps import ProcessHandlePs
from process_provider import ProcessProvider

class ProcessProvider_PS(ProcessProvider):
  """
    Process provider on top of the "ps" utility.
  """
  def _collect_all(self):
    return os.popen('ps ax -o %s' % (','.join(ProcessHandlePs.ATTRS))).readlines()

  def _collect_set(self, pidset):
    return os.popen('ps -p %s -o %s' % (
      ','.join(map(str, pidset)), ','.join(ProcessHandlePs.ATTRS))).readlines()

  def _translate_line_to_pid_pair(self, line):
    sline = line.split()
    try:
      return (int(sline[ProcessHandlePs.ATTRS.index('pid')]),
              int(sline[ProcessHandlePs.ATTRS.index('ppid')]))
    except:
      return None, None

  def _translate_line_to_handle(self, line):
    return ProcessHandlePs.from_line(line)

  @staticmethod
  def _platform_compatible():
    # Compatible with any Unix flavor with SUSv2+ conformant 'ps'.
    return True

########NEW FILE########
__FILENAME__ = base
from __future__ import absolute_import

from collections import Iterable

from .compatibility import string as compatibility_string

from pkg_resources import Requirement



REQUIRED_ATTRIBUTES = (
    'extras',
    'key',
    'project_name',
    'specs',
)


def quacks_like_req(req):
  return all(hasattr(req, attr) for attr in REQUIRED_ATTRIBUTES)


def maybe_requirement(req):
  if isinstance(req, Requirement) or quacks_like_req(req):
    return req
  elif isinstance(req, compatibility_string):
    return Requirement.parse(req)
  raise ValueError('Unknown requirement %r' % (req,))


def maybe_requirement_list(reqs):
  if isinstance(reqs, (compatibility_string, Requirement)) or quacks_like_req(reqs):
    return [maybe_requirement(reqs)]
  elif isinstance(reqs, Iterable):
    return [maybe_requirement(req) for req in reqs]
  raise ValueError('Unknown requirement list %r' % (reqs,))


def requirement_is_exact(req):
  return req.specs and len(req.specs) == 1 and req.specs[0][0] == '=='

########NEW FILE########
__FILENAME__ = pex
"""
The pex.pex utility builds PEX environments and .pex files specified by
sources, requirements and their dependencies.
"""

from __future__ import print_function

from optparse import OptionParser
import os
import sys

from twitter.common.python.common import safe_delete, safe_mkdtemp
from twitter.common.python.fetcher import Fetcher, PyPIFetcher
from twitter.common.python.installer import EggInstaller, WheelInstaller
from twitter.common.python.interpreter import PythonInterpreter
from twitter.common.python.obtainer import CachingObtainer
from twitter.common.python.package import (
    EggPackage,
    SourcePackage,
    WheelPackage,
)
from twitter.common.python.platforms import Platform
from twitter.common.python.resolver import resolve as requirement_resolver
from twitter.common.python.pex_builder import PEXBuilder
from twitter.common.python.pex import PEX
from twitter.common.python.tracer import Tracer
from twitter.common.python.translator import (
    ChainedTranslator,
    EggTranslator,
    SourceTranslator,
    WheelTranslator,
)


CANNOT_PARSE_REQUIREMENT = 100
CANNOT_DISTILL = 101


def die(msg, error_code=1):
  print(msg, file=sys.stderr)
  sys.exit(error_code)


def log(msg, v=False):
  if v:
    print(msg, file=sys.stderr)


def parse_bool(option, opt_str, _, parser):
  setattr(parser.values, option.dest, not opt_str.startswith('--no'))


def configure_clp():
  usage = (
      '%prog [options]\n\n'
      '%prog builds a PEX (Python Executable) file based on the given specifications: '
      'sources, requirements, their dependencies and other options')

  parser = OptionParser(usage=usage, version='%prog 0.2')

  parser.add_option(
      '--pypi', '--no-pypi',
      dest='pypi',
      default=True,
      action='callback',
      callback=parse_bool,
      help='Whether to use pypi to resolve dependencies; Default: use pypi')

  parser.add_option(
      '--wheel', '--no-wheel',
      dest='use_wheel',
      default=True,
      action='callback',
      callback=parse_bool,
      help='Whether to allow wheel distributions; Default: allow wheels')

  parser.add_option(
      '--build', '--no-build',
      dest='allow_builds',
      default=True,
      action='callback',
      callback=parse_bool,
      help='Whether to allow building of distributions from source; Default: allow builds')

  parser.add_option(
      '--python',
      dest='python',
      default=None,
      help='The Python interpreter to use to build the pex.  Either specify an explicit '
           'path to an interpreter, or specify a binary accessible on $PATH. '
           'Default: Use current interpreter.')

  parser.add_option(
      '--platform',
      dest='platform',
      default=Platform.current(),
      help='The platform for which to build the PEX.  Default: %%default')

  parser.add_option(
      '--zip-safe', '--not-zip-safe',
      dest='zip_safe',
      default=True,
      action='callback',
      callback=parse_bool,
      help='Whether or not the sources in the pex file are zip safe.  If they are '
           'not zip safe, they will be written to disk prior to execution; '
           'Default: zip safe.')

  parser.add_option(
      '--always-write-cache',
      dest='always_write_cache',
      default=False,
      action='store_true',
      help='Always write the internally cached distributions to disk prior to invoking '
           'the pex source code.  This can use less memory in RAM constrained '
           'environments. [Default: %default]')

  parser.add_option(
      '--ignore-errors',
      dest='ignore_errors',
      default=False,
      action='store_true',
      help='Ignore run-time requirement resolution errors when invoking the pex. '
           '[Default: %default]')

  parser.add_option(
      '--inherit-path',
      dest='inherit_path',
      default=False,
      action='store_true',
      help='Inherit the contents of sys.path (including site-packages) running the pex. '
           '[Default: %default]')

  parser.add_option(
      '--cache-dir',
      dest='cache_dir',
      default=os.path.expanduser('~/.pex/build'),
      help='The local cache directory to use for speeding up requirement '
           'lookups; [Default: %default]')

  parser.add_option(
      '-o', '-p', '--output-file', '--pex-name',
      dest='pex_name',
      default=None,
      help='The name of the generated .pex file: Omiting this will run PEX '
           'immediately and not save it to a file.')

  parser.add_option(
      '-e', '--entry-point',
      dest='entry_point',
      default=None,
      help='The entry point for this pex; Omiting this will enter the python '
           'REPL with sources and requirements available for import.  Can be '
           'either a module or EntryPoint (module:function) format.')

  parser.add_option(
      '-r', '--requirement',
      dest='requirements',
      metavar='REQUIREMENT',
      default=[],
      action='append',
      help='requirement to be included; may be specified multiple times.')

  parser.add_option(
      '--repo',
      dest='repos',
      metavar='PATH',
      default=[],
      action='append',
      help='Additional repository path (directory or URL) to look for requirements.')

  parser.add_option(
      '-s', '--source-dir',
      dest='source_dirs',
      metavar='DIR',
      default=[],
      action='append',
      help='Source to be packaged; This <DIR> should be a pip-installable project '
           'with a setup.py.')

  parser.add_option(
      '-v', '--verbosity',
      dest='verbosity',
      default=False,
      action='store_true',
      help='Turn on logging verbosity.')

  return parser


def interpreter_from_options(options):
  interpreter = None
  if options.python:
    if os.path.exists(options.python):
      interpreter = PythonInterpreter.from_binary(options.python)
    else:
      interpreter = PythonInterpreter.from_env(options.python)
    if interpreter is None:
      die('Failed to find interpreter: %s' % options.python)
  else:
    interpreter = PythonInterpreter.get()
  return interpreter


def translator_from_options(options):
  interpreter = interpreter_from_options(options)
  platform = options.platform

  translators = []

  shared_options = dict(install_cache=options.cache_dir, interpreter=interpreter)

  if options.use_wheel:
    installer_impl = WheelInstaller
    translators.append(WheelTranslator(platform=platform, **shared_options))
  else:
    installer_impl = EggInstaller

  translators.append(EggTranslator(platform=platform, **shared_options))

  if options.allow_builds:
    translators.append(SourceTranslator(installer_impl=installer_impl, **shared_options))

  return ChainedTranslator(*translators)


def build_obtainer(options):
  interpreter = interpreter_from_options(options)
  platform = options.platform

  fetchers = [Fetcher(options.repos)]

  if options.pypi:
    fetchers.append(PyPIFetcher())

  translator = translator_from_options(options)

  if options.use_wheel:
    package_precedence = (WheelPackage, EggPackage, SourcePackage)
  else:
    package_precedence = (EggPackage, SourcePackage)

  obtainer = CachingObtainer(
      install_cache=options.cache_dir,
      fetchers=fetchers,
      translators=translator,
      precedence=package_precedence)

  return obtainer


def build_pex(args, options):
  interpreter = interpreter_from_options(options)

  pex_builder = PEXBuilder(
      path=safe_mkdtemp(),
      interpreter=interpreter,
  )

  pex_info = pex_builder.info

  pex_info.zip_safe = options.zip_safe
  pex_info.always_write_cache = options.always_write_cache
  pex_info.ignore_errors = options.ignore_errors
  pex_info.inherit_path = options.inherit_path

  installer = WheelInstaller if options.use_wheel else EggInstaller

  resolveds = requirement_resolver(
      options.requirements,
      obtainer=build_obtainer(options),
      interpreter=interpreter,
      platform=options.platform)

  if resolveds:
    log('Resolved distributions:', v=options.verbosity)

  for pkg in resolveds:
    log('  %s' % pkg, v=options.verbosity)
    pex_builder.add_distribution(pkg)
    pex_builder.add_requirement(pkg.as_requirement())

  for source_dir in options.source_dirs:
    try:
      bdist = installer(source_dir).bdist()
    except installer.Error:
      die('Failed to run installer for %s' % source_dir, CANNOT_DISTILL)
    pex_builder.add_dist_location(bdist)

  if options.entry_point is not None:
    log('Setting entry point to %s' % options.entry_point, v=options.verbosity)
    pex_builder.info.entry_point = options.entry_point
  else:
    log('Creating environment PEX.', v=options.verbosity)

  return pex_builder


def main():
  parser = configure_clp()
  options, args = parser.parse_args()
  verbosity = 5 if options.verbosity else -1

  with Tracer.env_override(
      PEX_VERBOSE=verbosity,
      TWITTER_COMMON_PYTHON_HTTP=verbosity,
      PYTHON_VERBOSE=verbosity):

    pex_builder = build_pex(args, options)

    if options.pex_name is not None:
      log('Saving PEX file to %s' % options.pex_name, v=options.verbosity)
      tmp_name = options.pex_name + '~'
      safe_delete(tmp_name)
      pex_builder.build(tmp_name)
      os.rename(tmp_name, options.pex_name)
      return 0

    if options.platform != Platform.current():
      log('WARNING: attempting to run PEX with differing platform!')

    pex_builder.freeze()

    log('Running PEX file at %s with args %s' % (pex_builder.path(), args), v=options.verbosity)
    pex = PEX(pex_builder.path(), interpreter=pex_builder.interpreter)
    return pex.run(args=list(args))

########NEW FILE########
__FILENAME__ = common
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import atexit
from collections import defaultdict
import contextlib
import errno
import os
import shutil
import sys
import stat
import tempfile
import threading
import zipfile


# See http://stackoverflow.com/questions/2572172/referencing-other-modules-in-atexit
class MktempTeardownRegistry(object):
  def __init__(self):
    self._registry = defaultdict(set)
    self._getpid = os.getpid
    self._lock = threading.RLock()
    self._exists = os.path.exists
    self._rmtree = shutil.rmtree
    atexit.register(self.teardown)

  def __del__(self):
    self.teardown()

  def register(self, path):
    with self._lock:
      self._registry[self._getpid()].add(path)
    return path

  def teardown(self):
    for td in self._registry.pop(self._getpid(), []):
      if self._exists(td):
        self._rmtree(td)


_MKDTEMP_SINGLETON = MktempTeardownRegistry()


@contextlib.contextmanager
def open_zip(path, *args, **kwargs):
  """
    A with-context for zip files.  Passes through positional and kwargs to zipfile.ZipFile.
  """
  with contextlib.closing(zipfile.ZipFile(path, *args, **kwargs)) as zip:
    yield zip


def safe_mkdtemp(**kw):
  """
    Given the parameters to standard tempfile.mkdtemp, create a temporary directory
    that is cleaned up on process exit.
  """
  # proper lock sanitation on fork [issue 6721] would be desirable here.
  return _MKDTEMP_SINGLETON.register(tempfile.mkdtemp(**kw))


def register_rmtree(directory):
  """
    Register an existing directory to be cleaned up at process exit.
  """
  return _MKDTEMP_SINGLETON.register(directory)


def safe_mkdir(directory, clean=False):
  """
    Ensure a directory is present.  If it's not there, create it.  If it is,
    no-op. If clean is True, ensure the directory is empty.
  """
  if clean:
    safe_rmtree(directory)
  try:
    os.makedirs(directory)
  except OSError as e:
    if e.errno != errno.EEXIST:
      raise


def safe_open(filename, *args, **kwargs):
  """
    Open a file safely (ensuring that the directory components leading up to it
    have been created first.)
  """
  safe_mkdir(os.path.dirname(filename))
  return open(filename, *args, **kwargs)


def safe_delete(filename):
  """
    Delete a file safely. If it's not present, no-op.
  """
  try:
    os.unlink(filename)
  except OSError as e:
    if e.errno != errno.ENOENT:
      raise


def safe_rmtree(directory):
  """
    Delete a directory if it's present. If it's not present, no-op.
  """
  if os.path.exists(directory):
    shutil.rmtree(directory, True)


def chmod_plus_x(path):
  """
    Equivalent of unix `chmod a+x path`
  """
  path_mode = os.stat(path).st_mode
  path_mode &= int('777', 8)
  if path_mode & stat.S_IRUSR:
    path_mode |= stat.S_IXUSR
  if path_mode & stat.S_IRGRP:
    path_mode |= stat.S_IXGRP
  if path_mode & stat.S_IROTH:
    path_mode |= stat.S_IXOTH
  os.chmod(path, path_mode)


def chmod_plus_w(path):
  """
    Equivalent of unix `chmod +w path`
  """
  path_mode = os.stat(path).st_mode
  path_mode &= int('777', 8)
  path_mode |= stat.S_IWRITE
  os.chmod(path, path_mode)


def touch(file, times=None):
  """
    Equivalent of unix `touch path`.

    :file The file to touch.
    :times Either a tuple of (atime, mtime) or else a single time to use for both.  If not
           specified both atime and mtime are updated to the current time.
  """
  if times:
    if len(times) > 2:
      raise ValueError('times must either be a tuple of (atime, mtime) or else a single time value '
                       'to use for both.')

    if len(times) == 1:
      times = (times, times)

  with safe_open(file, 'a'):
    os.utime(file, times)


class Chroot(object):
  """
    A chroot of files overlayed from one directory to another directory.

    Files may be tagged when added in order to keep track of multiple overlays in
    the chroot.
  """
  class ChrootException(Exception): pass

  class ChrootTaggingException(Exception):
    def __init__(self, filename, orig_tag, new_tag):
      Exception.__init__(self,
        "Trying to add %s to fileset(%s) but already in fileset(%s)!" % (
          filename, new_tag, orig_tag))

  def __init__(self, chroot_base, name=None):
    """
      chroot_base = directory for the creation of the target chroot.
      name = if specified, create the chroot in a temporary directory underneath
        chroot_base with 'name' as the prefix, otherwise create the chroot directly
        into chroot_base
    """
    self.root = None
    try:
      safe_mkdir(chroot_base)
    except:
      raise Chroot.ChrootException('Unable to create chroot in %s' % chroot_base)
    if name is not None:
      self.chroot = tempfile.mkdtemp(dir=chroot_base, prefix='%s.' % name)
    else:
      self.chroot = chroot_base
    self.filesets = {}

  def set_relative_root(self, root):
    """
      Make all source paths relative to this root path.
    """
    self.root = root

  def clone(self, into=None):
    into = into or tempfile.mkdtemp()
    new_chroot = Chroot(into)
    new_chroot.root = self.root
    for label, fileset in self.filesets.items():
      for fn in fileset:
        new_chroot.link(os.path.join(self.chroot, self.root or '', fn),
                        fn, label=label)
    return new_chroot

  def path(self):
    """The path of the chroot."""
    return self.chroot

  def _check_tag(self, fn, label):
    for fs_label, fs in self.filesets.items():
      if fn in fs and fs_label != label:
        raise Chroot.ChrootTaggingException(fn, fs_label, label)

  def _tag(self, fn, label):
    self._check_tag(fn, label)
    if label not in self.filesets:
      self.filesets[label] = set()
    self.filesets[label].add(fn)

  def _mkdir_for(self, path):
    dirname = os.path.dirname(os.path.join(self.chroot, path))
    safe_mkdir(dirname)

  def _rootjoin(self, path):
    return os.path.join(self.root or '', path)

  def copy(self, src, dst, label=None):
    """
      Copy file from {root}/source to {chroot}/dest with optional label.

      May raise anything shutil.copyfile can raise, e.g.
        IOError(Errno 21 'EISDIR')

      May raise ChrootTaggingException if dst is already in a fileset
      but with a different label.
    """
    self._tag(dst, label)
    self._mkdir_for(dst)
    shutil.copyfile(self._rootjoin(src), os.path.join(self.chroot, dst))

  def link(self, src, dst, label=None):
    """
      Hard link file from {root}/source to {chroot}/dest with optional label.

      May raise anything os.link can raise, e.g.
        IOError(Errno 21 'EISDIR')

      May raise ChrootTaggingException if dst is already in a fileset
      but with a different label.
    """
    self._tag(dst, label)
    self._mkdir_for(dst)
    abs_src = self._rootjoin(src)
    abs_dst = os.path.join(self.chroot, dst)
    try:
      os.link(abs_src, abs_dst)
    except OSError as e:
      if e.errno == errno.EEXIST:
        # File already exists, skip
        pass
      elif e.errno == errno.EXDEV:
        # Hard link across devices, fall back on copying
        shutil.copyfile(abs_src, abs_dst)
      else:
        raise

  def write(self, data, dst, label=None, mode='wb'):
    """
      Write data to {chroot}/dest with optional label.

      Has similar exceptional cases as Chroot.copy
    """

    self._tag(dst, label)
    self._mkdir_for(dst)
    with open(os.path.join(self.chroot, dst), mode) as wp:
      wp.write(data)

  def touch(self, dst, label=None):
    """
      Perform 'touch' on {chroot}/dest with optional label.

      Has similar exceptional cases as Chroot.copy
    """
    self.write('', dst, label, mode='a')

  def get(self, label):
    """Get all files labeled with 'label'"""
    return self.filesets.get(label, set())

  def files(self):
    """Get all files in the chroot."""
    all_files = set()
    for label in self.filesets:
      all_files.update(self.filesets[label])
    return all_files

  def labels(self):
    return self.filesets.keys()

  def __str__(self):
    return 'Chroot(%s {fs:%s})' % (self.chroot,
      ' '.join('%s' % foo for foo in self.filesets.keys()))

  def delete(self):
    shutil.rmtree(self.chroot)

  def zip(self, filename, mode='wb'):
    with contextlib.closing(zipfile.ZipFile(filename, mode)) as zf:
      for f in sorted(self.files()):
        zf.write(os.path.join(self.chroot, f), arcname=f, compress_type=zipfile.ZIP_DEFLATED)

########NEW FILE########
__FILENAME__ = compatibility
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from abc import ABCMeta
from numbers import Integral, Real
from sys import version_info as sys_version_info

# TODO(wickman)  Since the io package is available in 2.6.x, use that instead of
# cStringIO/StringIO
try:
  # CPython 2.x
  from cStringIO import StringIO
except ImportError:
  try:
    # Python 2.x
    from StringIO import StringIO
  except:
    # Python 3.x
    from io import StringIO
    from io import BytesIO

AbstractClass = ABCMeta('AbstractClass', (object,), {})
PY2 = sys_version_info[0] == 2
PY3 = sys_version_info[0] == 3
StringIO = StringIO
BytesIO = BytesIO if PY3 else StringIO

integer = (Integral,)
real = (Real,)
numeric = integer + real
string = (str,) if PY3 else (str, unicode)
bytes = (bytes,)

if PY2:
  def to_bytes(st):
    if isinstance(st, unicode):
      return st.encode('utf-8')
    else:
      return str(st)
else:
  def to_bytes(st):
    return st.encode('utf-8')

if PY3:
  def exec_function(ast, globals_map):
    locals_map = globals_map
    exec(ast, globals_map, locals_map)
    return locals_map
else:
  eval(compile(
"""
def exec_function(ast, globals_map):
  locals_map = globals_map
  exec ast in globals_map, locals_map
  return locals_map
""", "<exec_function>", "exec"))

if PY3:
  from contextlib import contextmanager, ExitStack

  @contextmanager
  def nested(*context_managers):
    enters = []
    with ExitStack() as stack:
      for manager in context_managers:
        enters.append(stack.enter_context(manager))
      yield tuple(enters)

else:
  from contextlib import nested


__all__ = (
  'AbstractClass',
  'BytesIO',
  'PY2',
  'PY3',
  'StringIO',
  'bytes',
  'exec_function',
  'nested',
  'string',
  'to_bytes',
)

########NEW FILE########
__FILENAME__ = environment
from __future__ import absolute_import, print_function

import os
import site
import sys
import uuid

from .common import open_zip, safe_mkdir, safe_rmtree
from .interpreter import PythonInterpreter
from .package import distribution_compatible
from .pex_builder import PEXBuilder
from .pex_info import PexInfo
from .tracer import Tracer
from .util import CacheHelper, DistributionHelper

from pkg_resources import (
    DistributionNotFound,
    Environment,
    find_distributions,
    Requirement,
    WorkingSet,
)


TRACER = Tracer(predicate=Tracer.env_filter('PEX_VERBOSE'),
                prefix='twitter.common.python.environment: ')


class PEXEnvironment(Environment):
  @classmethod
  def force_local(cls, pex, pex_info):
    if pex_info.code_hash is None:
      # Do not support force_local if code_hash is not set. (It should always be set.)
      return pex
    explode_dir = os.path.join(pex_info.zip_unsafe_cache, pex_info.code_hash)
    TRACER.log('PEX is not zip safe, exploding to %s' % explode_dir)
    if not os.path.exists(explode_dir):
      explode_tmp = explode_dir + '.' + uuid.uuid4().hex
      with TRACER.timed('Unzipping %s' % pex):
        try:
          safe_mkdir(explode_tmp)
          with open_zip(pex) as pex_zip:
            pex_files = (x for x in pex_zip.namelist()
                         if not x.startswith(PEXBuilder.BOOTSTRAP_DIR) and
                            not x.startswith(PexInfo.INTERNAL_CACHE))
            pex_zip.extractall(explode_tmp, pex_files)
        except:
          safe_rmtree(explode_tmp)
          raise
      TRACER.log('Renaming %s to %s' % (explode_tmp, explode_dir))
      os.rename(explode_tmp, explode_dir)
    return explode_dir

  @classmethod
  def update_module_paths(cls, new_code_path):
    # Force subsequent imports to come from the .pex directory rather than the .pex file.
    TRACER.log('Adding to the head of sys.path: %s' % new_code_path)
    sys.path.insert(0, new_code_path)
    for name, module in sys.modules.items():
      if hasattr(module, "__path__"):
        module_dir = os.path.join(new_code_path, *name.split("."))
        TRACER.log('Adding to the head of %s.__path__: %s' % (module.__name__, module_dir))
        module.__path__.insert(0, module_dir)

  @classmethod
  def write_zipped_internal_cache(cls, pex, pex_info):
    prefix_length = len(pex_info.internal_cache) + 1
    distributions = []
    with open_zip(pex) as zf:
      # Distribution names are the first element after ".deps/" and before the next "/"
      distribution_names = set(filter(None, (filename[prefix_length:].split('/')[0]
          for filename in zf.namelist() if filename.startswith(pex_info.internal_cache))))
      # Create Distribution objects from these, and possibly write to disk if necessary.
      for distribution_name in distribution_names:
        internal_dist_path = '/'.join([pex_info.internal_cache, distribution_name])
        dist = DistributionHelper.distribution_from_path(os.path.join(pex, internal_dist_path))
        if DistributionHelper.zipsafe(dist) and not pex_info.always_write_cache:
          distributions.append(dist)
          continue
        dist_digest = pex_info.distributions.get(distribution_name) or CacheHelper.zip_hash(
            zf, internal_dist_path)
        target_dir = os.path.join(pex_info.install_cache, '%s.%s' % (
            distribution_name, dist_digest))
        with TRACER.timed('Caching %s into %s' % (dist, target_dir)):
          distributions.append(CacheHelper.cache_distribution(zf, internal_dist_path, target_dir))
    return distributions

  @classmethod
  def load_internal_cache(cls, pex, pex_info):
    """Possibly cache out the internal cache."""
    internal_cache = os.path.join(pex, pex_info.internal_cache)
    with TRACER.timed('Searching dependency cache: %s' % internal_cache):
      if os.path.isdir(pex):
        for dist in find_distributions(internal_cache):
          yield dist
      else:
        for dist in cls.write_zipped_internal_cache(pex, pex_info):
          yield dist

  def __init__(self, pex, pex_info, interpreter=None, **kw):
    self._internal_cache = os.path.join(pex, pex_info.internal_cache)
    self._pex = pex
    self._pex_info = pex_info
    self._activated = False
    self._working_set = None
    self._interpreter = interpreter or PythonInterpreter.get()
    super(PEXEnvironment, self).__init__(
        search_path=sys.path if pex_info.inherit_path else [], **kw)

  def update_candidate_distributions(self, distribution_iter):
    for dist in distribution_iter:
      if self.can_add(dist):
        with TRACER.timed('Adding %s:%s' % (dist, dist.location)):
          self.add(dist)

  def can_add(self, dist):
    return distribution_compatible(dist, self._interpreter, self.platform)

  def activate(self):
    if not self._activated:
      with TRACER.timed('Activating PEX virtual environment'):
        self._working_set = self._activate()
      self._activated = True

    return self._working_set

  def _activate(self):
    self.update_candidate_distributions(self.load_internal_cache(self._pex, self._pex_info))

    if not self._pex_info.zip_safe and os.path.isfile(self._pex):
      self.update_module_paths(self.force_local(self._pex, self._pex_info))

    # TODO(wickman)  Implement dynamic fetchers if pex_info requirements specify dynamic=True
    # or a non-empty repository.
    all_reqs = [Requirement.parse(req) for req, _, _ in self._pex_info.requirements]

    working_set = WorkingSet([])

    with TRACER.timed('Resolving %s' %
        ' '.join(map(str, all_reqs)) if all_reqs else 'empty dependency list'):
      try:
        resolved = working_set.resolve(all_reqs, env=self)
      except DistributionNotFound as e:
        TRACER.log('Failed to resolve a requirement: %s' % e)
        TRACER.log('Current working set:')
        for dist in working_set:
          TRACER.log('  - %s' % dist)
        raise

    for dist in resolved:
      with TRACER.timed('Activating %s' % dist):
        working_set.add(dist)

        if os.path.isdir(dist.location):
          with TRACER.timed('Adding sitedir'):
            site.addsitedir(dist.location)

        dist.activate()

    return working_set

########NEW FILE########
__FILENAME__ = fetcher
from __future__ import absolute_import

from abc import abstractmethod
import random

from .base import maybe_requirement
from .compatibility import AbstractClass


class FetcherBase(AbstractClass):
  """
    A fetcher takes a Requirement and tells us where to crawl to find it.
  """
  @abstractmethod
  def urls(self, req):
    raise NotImplementedError


class Fetcher(FetcherBase):
  def __init__(self, urls):
    self._urls = urls

  def urls(self, _):
    return self._urls


class PyPIFetcher(FetcherBase):
  PYPI_BASE = 'pypi.python.org'

  @classmethod
  def resolve_mirrors(cls, base):
    """Resolve mirrors per PEP-0381."""
    import socket
    def crange(ch1, ch2):
      return [chr(ch) for ch in range(ord(ch1), ord(ch2) + 1)]
    last, _, _ = socket.gethostbyname_ex('last.' + base)
    assert last.endswith(cls.PYPI_BASE)
    last_prefix = last.split('.')[0]
    # TODO(wickman) Is implementing > z really necessary?
    last_prefix = 'z' if len(last_prefix) > 1 else last_prefix[0]
    return ['%c.%s' % (letter, base) for letter in crange('a', last_prefix)]

  def __init__(self, pypi_base=PYPI_BASE, use_mirrors=False):
    self.mirrors = self.resolve_mirrors(pypi_base) if use_mirrors else [pypi_base]

  def urls(self, req):
    req = maybe_requirement(req)
    random_mirror = random.choice(self.mirrors)
    return ['https://%s/simple/%s/' % (random_mirror, req.project_name)]

########NEW FILE########
__FILENAME__ = finders
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""The finders we wish we had in setuptools.

As of setuptools 3.3, the only finder for zip-based distributions is for eggs.  The path-based
finder only searches paths ending in .egg and not in .whl (zipped or unzipped.)

twitter.common.python.finders augments pkg_resources with additional finders to achieve functional
parity between wheels and eggs in terms of findability with find_distributions.

To use: ::
   >>> from twitter.common.python.finders import register_finders
   >>> register_finders()
"""

import os
import pkgutil
import sys
import zipimport

import pkg_resources


if sys.version_info >= (3, 3) and sys.implementation.name == "cpython":
  import importlib._bootstrap as importlib_bootstrap
else:
  importlib_bootstrap = None


class ChainedFinder(object):
  """A utility to chain together multiple pkg_resources finders."""
  @classmethod
  def of(cls, *chained_finder_or_finder):
    finders = []
    for finder in chained_finder_or_finder:
      if isinstance(finder, cls):
        finders.extend(finder.finders)
      else:
        finders.append(finder)
    return cls(finders)

  def __init__(self, finders):
    self.finders = finders

  def __call__(self, importer, path_item, only=False):
    for finder in self.finders:
      for dist in finder(importer, path_item, only=only):
        yield dist

  def __eq__(self, other):
    if not isinstance(other, ChainedFinder):
      return False
    return self.finders == other.finders


# The following methods are somewhat dangerous as pkg_resources._distribution_finders is not an
# exposed API.  As it stands, pkg_resources doesn't provide an API to chain multiple distribution
# finders together.  This is probably possible using importlib but that does us no good as the
# importlib machinery supporting this is only available in Python >= 3.1.

def _get_finder(importer):
  if not hasattr(pkg_resources, '_distribution_finders'):
    return None
  return pkg_resources._distribution_finders.get(importer)


def _add_finder(importer, finder):
  """Register a new pkg_resources path finder that does not replace the existing finder."""

  existing_finder = _get_finder(importer)

  if not existing_finder:
    pkg_resources.register_finder(importer, finder)
  else:
    pkg_resources.register_finder(importer, ChainedFinder.of(existing_finder, finder))


def _remove_finder(importer, finder):
  """Remove an existing finder from pkg_resources."""

  existing_finder = _get_finder(importer)

  if not existing_finder:
    return

  if isinstance(existing_finder, ChainedFinder):
    try:
      existing_finder.finders.remove(finder)
    except ValueError:
      return
    if len(existing_finder.finders) == 1:
      pkg_resources.register_finder(importer, existing_finder.finders[0])
    elif len(existing_finder.finders) == 0:
      pkg_resources.register_finder(importer, pkg_resources.find_nothing)
  else:
    pkg_resources.register_finder(importer, pkg_resources.find_nothing)


class WheelMetadata(pkg_resources.EggMetadata):
  """Metadata provider for zipped wheels."""

  @classmethod
  def _split_wheelname(cls, wheelname):
    split_wheelname = wheelname.split('-')
    return '-'.join(split_wheelname[:-3])

  def _setup_prefix(self):
    path = self.module_path
    old = None
    while path != old:
      if path.lower().endswith('.whl'):
        self.egg_name = os.path.basename(path)
        # TODO(wickman) Test the regression where we have both upper and lower cased package
        # names.
        self.egg_info = os.path.join(path, '%s.dist-info' % self._split_wheelname(self.egg_name))
        self.egg_root = path
        break
      old = path
      path, base = os.path.split(path)


# See https://bitbucket.org/tarek/distribute/issue/274
class FixedEggMetadata(pkg_resources.EggMetadata):
  """An EggMetadata provider that has functional parity with the disk-based provider."""

  @classmethod
  def normalized_elements(cls, path):
    path_split = path.split('/')
    while path_split[-1] in ('', '.'):
      path_split.pop(-1)
    return path_split

  def _fn(self, base, resource_name):
    # super() does not work here as EggMetadata is an old-style class.
    original_fn = pkg_resources.EggMetadata._fn(self, base, resource_name)
    return '/'.join(self.normalized_elements(original_fn))

  def _zipinfo_name(self, fspath):
    fspath = self.normalized_elements(fspath)
    zip_pre = self.normalized_elements(self.zip_pre)
    if fspath[:len(zip_pre)] == zip_pre:
      return '/'.join(fspath[len(zip_pre):])
    assert "%s is not a subpath of %s" % (fspath, self.zip_pre)


def wheel_from_metadata(location, metadata):
  if not metadata.has_metadata(pkg_resources.DistInfoDistribution.PKG_INFO):
    return None

  from email.parser import Parser
  pkg_info = Parser().parsestr(metadata.get_metadata(pkg_resources.DistInfoDistribution.PKG_INFO))
  return pkg_resources.DistInfoDistribution(
      location=location,
      metadata=metadata,
      # TODO(wickman) Is this necessary or will they get picked up correctly?
      project_name=pkg_info.get('Name'),
      version=pkg_info.get('Version'),
      platform=None)


def find_wheels_on_path(importer, path_item, only=False):
  if not os.path.isdir(path_item) or not os.access(path_item, os.R_OK):
    return
  if not only:
    for entry in os.listdir(path_item):
      if entry.lower().endswith('.whl'):
        for dist in pkg_resources.find_distributions(os.path.join(path_item, entry)):
          yield dist


def find_eggs_in_zip(importer, path_item, only=False):
  if importer.archive.endswith('.whl'):
    # Defer to wheel importer
    return
  metadata = FixedEggMetadata(importer)
  if metadata.has_metadata('PKG-INFO'):
    yield pkg_resources.Distribution.from_filename(path_item, metadata=metadata)
  if only:
    return  # don't yield nested distros
  for subitem in metadata.resource_listdir('/'):
    if subitem.endswith('.egg'):
      subpath = os.path.join(path_item, subitem)
      for dist in find_eggs_in_zip(zipimport.zipimporter(subpath), subpath):
        yield dist


def find_wheels_in_zip(importer, path_item, only=False):
  metadata = WheelMetadata(importer)
  dist = wheel_from_metadata(path_item, metadata)
  if dist:
    yield dist


__PREVIOUS_FINDER = None


def register_finders():
  """Register finders necessary for PEX to function properly."""

  # If the previous finder is set, then we've already monkeypatched, so skip.
  global __PREVIOUS_FINDER
  if __PREVIOUS_FINDER:
    return

  # save previous finder so that it can be restored
  previous_finder = _get_finder(zipimport.zipimporter)
  assert previous_finder, 'This appears to be using an incompatible setuptools.'

  # replace the zip finder with our own implementation of find_eggs_in_zip which uses the correct
  # metadata handler, in addition to find_wheels_in_zip
  pkg_resources.register_finder(
      zipimport.zipimporter, ChainedFinder.of(find_eggs_in_zip, find_wheels_in_zip))

  # append the wheel finder
  _add_finder(pkgutil.ImpImporter, find_wheels_on_path)

  if importlib_bootstrap is not None:
    _add_finder(importlib_bootstrap.FileFinder, find_wheels_on_path)

  __PREVIOUS_FINDER = previous_finder


def unregister_finders():
  """Unregister finders necessary for PEX to function properly."""

  global __PREVIOUS_FINDER
  if not __PREVIOUS_FINDER:
    return

  pkg_resources.register_finder(zipimport.zipimporter, __PREVIOUS_FINDER)
  _remove_finder(pkgutil.ImpImporter, find_wheels_on_path)

  if importlib_bootstrap is not None:
    _remove_finder(importlib_bootstrap.FileFinder, find_wheels_on_path)

  __PREVIOUS_FINDER = None

########NEW FILE########
__FILENAME__ = crawler
import contextlib
from functools import partial
import os
import re
import threading

from ..compatibility import PY3
from .http import CachedWeb, Web, FetchError
from .tracer import TRACER

if PY3:
  from queue import Empty, Queue
  from urllib.parse import urlparse, urljoin
else:
  from Queue import Empty, Queue
  from urlparse import urlparse, urljoin


class CrawlerBase(object):
  """Base class for iterators over links."""
  def __init__(self, opener=None, threads=1):
    self._opener = opener
    self._threads = threads

  @property
  def opener(self):
    return self._opener

  def crawl(self, urls, follow_links=False):
    links, seen = set(), set()
    queue = Queue()
    converged = threading.Event()

    def execute():
      while not converged.is_set():
        try:
          url = queue.get(timeout=0.1)
        except Empty:
          continue
        if url not in seen:
          seen.add(url)
          hrefs, rel_hrefs = self.execute(url)
          links.update(hrefs)
          if follow_links:
            for href in rel_hrefs:
              if href not in seen:
                queue.put(href)
        queue.task_done()

    for url in urls:
      queue.put(url)
    for _ in range(self._threads):
      worker = threading.Thread(target=execute)
      worker.daemon = True
      worker.start()
    queue.join()
    converged.set()
    return links

  def execute(self, url):
    """Return (links, follow_links)."""
    raise NotImplementedError


class PageParser(object):
  HREF_RE = re.compile(r"""href=(?:"([^"]*)"|\'([^\']*)\'|([^>\s\n]*))""", re.I | re.S)
  REL_RE = re.compile(r"""<[^>]*\srel\s*=\s*['"]?([^'">]+)[^>]*>""", re.I)
  REL_SKIP_EXTENSIONS = frozenset(['.zip', '.tar', '.tar.gz', '.tar.bz2', '.tgz', '.exe'])
  REL_TYPES = frozenset(['homepage', 'download'])

  @classmethod
  def href_match_to_url(cls, match):
    def pick(group):
      return '' if group is None else group
    return pick(match.group(1)) or pick(match.group(2)) or pick(match.group(3))

  @classmethod
  def rel_links(cls, page):
    """return rel= links that should be scraped, skipping obviously data links."""
    for match in cls.REL_RE.finditer(page):
      href, rel = match.group(0), match.group(1)
      if rel not in cls.REL_TYPES:
        continue
      href_match = cls.HREF_RE.search(href)
      if href_match:
        href = cls.href_match_to_url(href_match)
        parsed_href = urlparse(href)
        if any(parsed_href.path.endswith(ext) for ext in cls.REL_SKIP_EXTENSIONS):
          continue
        yield href

  @classmethod
  def links(cls, page):
    """return all links on a page, including potentially rel= links."""
    for match in cls.HREF_RE.finditer(page):
      yield cls.href_match_to_url(match)


class Crawler(CrawlerBase):
  """Crawl a url for links."""
  DEFAULT_ENCODING = 'iso-8859-1'

  def __init__(self, cache=None, cache_ttl=3600, enable_cache=True, conn_timeout=None, **kw):
    opener = CachedWeb(cache=cache) if enable_cache else Web()
    self._open = partial(opener.open, ttl=cache_ttl) if enable_cache else opener.open
    self._conn_timeout = conn_timeout
    super(Crawler, self).__init__(opener=opener, **kw)

  @classmethod
  def decode_page(cls, infofp):
    hdr = infofp.headers
    # 2.x / 3.x shenanigans
    charset = hdr.getparam('charset') if hasattr(hdr, 'getparam') else hdr.get_param('charset')
    buf = infofp.read()
    try:
      return buf.decode(charset or cls.DEFAULT_ENCODING)
    except ValueError:
      # there is no universal base class for decoding errors
      TRACER.log('Failed to decode %s using %s' % (infofp.url, charset))
      return buf.decode(cls.DEFAULT_ENCODING)

  def _local_execute(self, path):
    try:
      dirents = os.listdir(path)
    except OSError as e:
      TRACER.log('Failed to fetch %s: %s' % (path, e))
      return set(), set()
    def partition(L, pred):
      return filter(lambda v: not pred(v), L), filter(lambda v: pred(v), L)
    return partition([os.path.join(path, fn) for fn in dirents], os.path.isdir)

  def _remote_execute(self, url):
    try:
      with contextlib.closing(self._open(url, conn_timeout=self._conn_timeout)) as index_fp:
        index_content = self.decode_page(index_fp)
    except FetchError as e:
      TRACER.log('Failed to fetch %s: %s' % (url, e))
      return set(), set()
    links = set(urljoin(url, link) for link in PageParser.links(index_content))
    rel_links = set(urljoin(url, link) for link in PageParser.rel_links(index_content))
    return links, rel_links

  def execute(self, url):
    with TRACER.timed('Crawling %s' % url):
      parsed_url = urlparse(url)
      if parsed_url.scheme in ('', 'file'):
        return self._local_execute(parsed_url.path)
      elif parsed_url.scheme in ('http', 'https'):
        return self._remote_execute(url)
      else:
        TRACER.log('Unknown scheme %s, skipping.' % parsed_url.scheme)
        return set(), set()

########NEW FILE########
__FILENAME__ = http
import contextlib
import hashlib
import os
import socket
import struct
import time

from ..common import safe_delete, safe_mkdir, safe_mkdtemp
from ..compatibility import PY2, PY3
from .tracer import TRACER

if PY3:
  from http.client import parse_headers, HTTPException
  from queue import Queue, Empty
  import urllib.error as urllib_error
  import urllib.parse as urlparse
  import urllib.request as urllib_request
  from urllib.request import addinfourl
else:
  from httplib import HTTPMessage, HTTPException
  from Queue import Queue, Empty
  from urllib import addinfourl
  import urllib2 as urllib_request
  import urllib2 as urllib_error
  import urlparse


class Timeout(Exception):
  pass


class FetchError(Exception):
  """
    Error occurred while fetching via HTTP

    We raise this when we catch urllib or httplib errors because we don't want
    to leak those implementation details to callers.
  """


def deadline(fn, *args, **kw):
  """Helper function to prevent fn(*args, **kw) from running more than
     a specified timeout.

     Takes timeout= kwarg in seconds, which defaults to 150ms (0.150)
  """
  DEFAULT_TIMEOUT_SECS = 0.150

  from threading import Thread
  q = Queue(maxsize=1)
  timeout = kw.pop('timeout', DEFAULT_TIMEOUT_SECS)
  class AnonymousThread(Thread):
    def run(self):
      q.put(fn(*args, **kw))
  AnonymousThread().start()
  try:
    return q.get(timeout=timeout)
  except Empty:
    raise Timeout


class Web(object):
  NS_TIMEOUT_SECS = 5.0
  CONN_TIMEOUT = 1.0
  SCHEME_TO_PORT = {
    'ftp': 21,
    'http': 80,
    'https': 443
  }

  def _resolves(self, fullurl):
    try:
      return socket.gethostbyname(fullurl.hostname)
    except socket.gaierror:
      return ''

  def _reachable(self, fullurl, conn_timeout=None):
    port = fullurl.port if fullurl.port else self.SCHEME_TO_PORT.get(fullurl.scheme, 80)
    try:
      conn = socket.create_connection(
          (fullurl.hostname, port), timeout=(conn_timeout or self.CONN_TIMEOUT))
      conn.close()
      return True
    except (socket.error, socket.timeout):
      TRACER.log('Failed to connect to %s within deadline' % urlparse.urlunparse(fullurl))
      return False

  def reachable(self, url, conn_timeout=None):
    """Do we think this URL is reachable?

       If this isn't here, it takes 5-30s to timeout on DNS resolution for
       certain hosts, so we prefetch DNS at a cost of 5-8ms but cap
       resolution at something sane, e.g. 5s.
    """
    fullurl = urlparse.urlparse(url)
    if not fullurl.scheme or not fullurl.netloc:
      return True
    try:
      with TRACER.timed('Resolving %s' % fullurl.hostname, V=2):
        if not deadline(self._resolves, fullurl, timeout=self.NS_TIMEOUT_SECS):
          TRACER.log('Failed to resolve %s' % url)
          return False
    except Timeout:
      TRACER.log('Timed out resolving %s' % fullurl.hostname)
      return False
    with TRACER.timed('Connecting to %s' % fullurl.hostname, V=2):
      return self._reachable(fullurl, conn_timeout=conn_timeout)

  def maybe_local_url(self, url):
    full_url = urlparse.urlparse(url)
    if full_url.scheme == '':
      return 'file://' + os.path.realpath(url)
    return url

  def open(self, url, conn_timeout=None, **kw):
    """
      Wrapper in front of urlopen that more gracefully handles odd network environments.
    """
    url = self.maybe_local_url(url)
    with TRACER.timed('Fetching %s' % url, V=1):
      if not self.reachable(url, conn_timeout=conn_timeout):
        raise FetchError('Could not reach %s within deadline.' % url)
      try:
        return urllib_request.urlopen(url, **kw)
      except (urllib_error.URLError, HTTPException) as exc:
        raise FetchError(exc)


class CachedWeb(object):
  """
    A basic http cache.

    Can act as a failsoft cache: If an object has expired but the fetch fails,
    will fall back to the cached object if failsoft set to True.
  """
  def __init__(self, cache=None, failsoft=True, clock=time, opener=None):
    self._failsoft = failsoft
    self._cache = cache or safe_mkdtemp()
    safe_mkdir(self._cache)
    self._clock = clock
    self._opener = opener or Web()
    super(CachedWeb, self).__init__()

  def __contains__(self, url):
    age = self.age(url)
    return age is not None and age > 0

  def translate_url(self, url):
    return os.path.join(self._cache, hashlib.md5(url.encode('utf8')).hexdigest())

  def translate_all(self, url):
    return ('%(tgt)s %(tgt)s.tmp %(tgt)s.headers %(tgt)s.headers.tmp' % {
        'tgt': self.translate_url(url)
    }).split()

  def age(self, url):
    """Return the age of an object in seconds, or None if object is not in cache."""
    cached_object = self.translate_url(url)
    if not os.path.exists(cached_object):
      return None
    return self._clock.time() - os.path.getmtime(cached_object)

  def expired(self, url, ttl=None):
    age = self.age(url)
    if age is None:
      return True
    if ttl is None:
      return False
    return age > ttl

  def really_open(self, url, conn_timeout=None):
    try:
      return self._opener.open(url, conn_timeout=conn_timeout)
    except urllib_error.HTTPError as fp:
      # HTTPError is a valid addinfourl -- use this instead of raising
      return fp

  def encode_url(self, url, conn_timeout=None):
    target, target_tmp, headers, headers_tmp = self.translate_all(url)
    with contextlib.closing(self.really_open(url, conn_timeout=conn_timeout)) as http_fp:
      # File urls won't have a response code, they'll either open or raise.
      if http_fp.getcode() and http_fp.getcode() != 200:
        raise urllib_error.URLError('Non-200 response code from %s' % url)
      with TRACER.timed('Caching %s' % url, V=2):
        with open(target_tmp, 'wb') as disk_fp:
          disk_fp.write(http_fp.read())
        with open(headers_tmp, 'wb') as headers_fp:
          headers_fp.write(struct.pack('>h', http_fp.code or 0))
          headers_fp.write(str(http_fp.headers).encode('utf8'))
        os.rename(target_tmp, target)
        os.rename(headers_tmp, headers)

  def decode_url(self, url):
    target, _, headers, _ = self.translate_all(url)
    headers_fp = open(headers, 'rb')
    code, = struct.unpack('>h', headers_fp.read(2))
    def make_headers(fp):
      return HTTPMessage(fp) if PY2 else parse_headers(fp)
    return addinfourl(open(target, 'rb'), make_headers(headers_fp), url, code)

  def clear_url(self, url):
    for path in self.translate_all(url):
      safe_delete(path)

  def cache(self, url, conn_timeout=None):
    """cache the contents of a url."""
    try:
      self.encode_url(url, conn_timeout=conn_timeout)
    except urllib_error.URLError:
      self.clear_url(url)
      raise

  def open(self, url, ttl=None, conn_timeout=None):
    """Return a file-like object with the content of the url."""
    expired = self.expired(url, ttl=ttl)
    with TRACER.timed('Opening %s' % ('(cached)' if not expired else '(uncached)'), V=1):
      if expired:
        try:
          self.cache(url, conn_timeout=conn_timeout)
        except (urllib_error.URLError, HTTPException) as exc:
          if not self._failsoft or url not in self:
            raise FetchError(exc)
      return self.decode_url(url)

########NEW FILE########
__FILENAME__ = link
from __future__ import absolute_import

import contextlib
import os
import posixpath

from ..common import safe_mkdir, safe_mkdtemp
from ..compatibility import PY3
from .http import FetchError

if PY3:
  import urllib.parse as urlparse
else:
  import urlparse


class Link(object):
  """An HTTP link."""

  class Error(Exception): pass
  class InvalidLink(Error): pass
  class UnreadableLink(Error): pass

  def __init__(self, url, opener=None):
    self._url = urlparse.urlparse(url)
    self._opener = opener

  def __eq__(self, link):
    return self.__class__ == link.__class__ and self._url == link._url

  def __hash__(self):
    return hash(self.url)

  @property
  def filename(self):
    return posixpath.basename(self._url.path)

  @property
  def url(self):
    return urlparse.urlunparse(self._url)

  @property
  def local(self):
    """Is the url a local file?"""
    return self._url.scheme in ('', 'file')

  @property
  def remote(self):
    """Is the url a remote file?"""
    return self._url.scheme in ('http', 'https')

  def __repr__(self):
    return '%s(%r)' % (self.__class__.__name__, self.url)

  def fh(self, conn_timeout=None):
    if not self._opener:
      raise self.UnreadableLink("Link cannot be read: no opener supplied.")
    return self._opener.open(self.url, conn_timeout=conn_timeout)

  def fetch(self, location=None, conn_timeout=None):
    """Fetches the link returning the local file path.

    :raises UnreadableLink: if the link could not be fetched.
    """
    if self.local and (location is None or os.path.dirname(self._url.path) == location):
      return self._url.path
    location = location or safe_mkdtemp()
    target = os.path.join(location, self.filename)
    if os.path.exists(target):
      return target
    try:
      with contextlib.closing(self.fh(conn_timeout=conn_timeout)) as url_fp:
        safe_mkdir(os.path.dirname(target))
        with open(target, 'wb') as fp:
          fp.write(url_fp.read())
    except (FetchError, IOError) as e:
      raise self.UnreadableLink('Failed to fetch %s to %s: %s' % (self.url, location, e))
    return target

########NEW FILE########
__FILENAME__ = tracer
from ..tracer import Tracer

__all__ = ('TRACER',)

TRACER = Tracer(predicate=Tracer.env_filter('TWITTER_COMMON_PYTHON_HTTP'),
                prefix='twitter.common.python.http: ')

########NEW FILE########
__FILENAME__ = installer
from __future__ import print_function, absolute_import

import os
import subprocess
import sys
import tempfile

from .common import safe_mkdtemp, safe_rmtree
from .interpreter import PythonInterpreter, PythonCapability
from .tracer import TRACER

from pkg_resources import Distribution, PathMetadata

__all__ = (
  'Installer',
  'Packager'
)


def after_installation(function):
  def function_wrapper(self, *args, **kw):
    self._installed = self.run()
    if not self._installed:
      raise Installer.InstallFailure('Failed to install %s' % self._source_dir)
    return function(self, *args, **kw)
  return function_wrapper


class InstallerBase(object):
  SETUP_BOOTSTRAP_HEADER = "import sys"
  SETUP_BOOTSTRAP_MODULE = "sys.path.insert(0, %(path)r); import %(module)s"
  SETUP_BOOTSTRAP_FOOTER = """
__file__ = 'setup.py'
exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))
"""

  class Error(Exception): pass
  class InstallFailure(Error): pass
  class IncapableInterpreter(Error): pass

  def __init__(self, source_dir, strict=True, interpreter=None, install_dir=None):
    """
      Create an installer from an unpacked source distribution in source_dir.

      If strict=True, fail if any installation dependencies (e.g. distribute)
      are missing.
    """
    self._source_dir = source_dir
    self._install_tmp = install_dir or safe_mkdtemp()
    self._installed = None
    self._strict = strict
    self._interpreter = interpreter or PythonInterpreter.get()
    if not self._interpreter.satisfies(self.capability) and strict:
      raise self.IncapableInterpreter('Interpreter %s not capable of running %s' % (
          self._interpreter, self.__class__.__name__))

  def mixins(self):
    """Return a map from import name to requirement to load into setup script prior to invocation.

       May be subclassed.
    """
    return {}

  @property
  def install_tmp(self):
    return self._install_tmp

  def _setup_command(self):
    """the setup command-line to run, to be implemented by subclasses."""
    raise NotImplementedError

  def _postprocess(self):
    """a post-processing function to run following setup.py invocation."""

  @property
  def capability(self):
    """returns the PythonCapability necessary for the interpreter to run this installer."""
    return PythonCapability(self.mixins().values())

  @property
  def bootstrap_script(self):
    bootstrap_modules = []
    for module, requirement in self.mixins().items():
      path = self._interpreter.get_location(requirement)
      if not path:
        assert not self._strict  # This should be caught by validation
        continue
      bootstrap_modules.append(self.SETUP_BOOTSTRAP_MODULE % {'path': path, 'module': module})
    return '\n'.join(
        [self.SETUP_BOOTSTRAP_HEADER] + bootstrap_modules + [self.SETUP_BOOTSTRAP_FOOTER])

  def run(self):
    if self._installed is not None:
      return self._installed

    with TRACER.timed('Installing %s' % self._install_tmp, V=2):
      command = [self._interpreter.binary, '-']
      command.extend(self._setup_command())
      po = subprocess.Popen(command,
          stdin=subprocess.PIPE,
          stdout=subprocess.PIPE,
          stderr=subprocess.PIPE,
          env=self._interpreter.sanitized_environment(),
          cwd=self._source_dir)
      so, se = po.communicate(self.bootstrap_script.encode('ascii'))
      self._installed = po.returncode == 0

    if not self._installed:
      name = os.path.basename(self._source_dir)
      print('**** Failed to install %s. stdout:\n%s' % (name, so.decode('utf-8')), file=sys.stderr)
      print('**** Failed to install %s. stderr:\n%s' % (name, se.decode('utf-8')), file=sys.stderr)
      return self._installed

    self._postprocess()
    return self._installed

  def cleanup(self):
    safe_rmtree(self._install_tmp)


class Installer(InstallerBase):
  """
    Install an unpacked distribution with a setup.py.

    Simple example:
      >>> from twitter.common.python.package import SourcePackage
      >>> from twitter.common.python.http import Web
      >>> tornado_tgz = SourcePackage(
      ...    'http://pypi.python.org/packages/source/t/tornado/tornado-2.3.tar.gz',
      ...    opener=Web())
      >>> tornado_installer = Installer(tornado_tgz.fetch())
      >>> tornado_installer.distribution()
      tornado 2.3 (/private/var/folders/Uh/UhXpeRIeFfGF7HoogOKC+++++TI/-Tmp-/tmpLLe_Ph/lib/python2.6/site-packages)

    You can then take that distribution and activate it:
      >>> tornado_distribution = tornado_installer.distribution()
      >>> tornado_distribution.activate()
      >>> import tornado

    Alternately you can use the EggInstaller to create an egg instead:
      >>> from twitter.common.python.installer import EggInstaller
      >>> EggInstaller(tornado_tgz.fetch()).bdist()
      '/var/folders/Uh/UhXpeRIeFfGF7HoogOKC+++++TI/-Tmp-/tmpufgZOO/tornado-2.3-py2.6.egg'
  """
  def __init__(self, source_dir, strict=True, interpreter=None):
    """
      Create an installer from an unpacked source distribution in source_dir.

      If strict=True, fail if any installation dependencies (e.g. setuptools)
      are missing.
    """
    super(Installer, self).__init__(source_dir, strict=strict, interpreter=interpreter)
    self._egg_info = None
    fd, self._install_record = tempfile.mkstemp()
    os.close(fd)

  def _setup_command(self):
    return ['install',
           '--root=%s' % self._install_tmp,
           '--prefix=',
           '--single-version-externally-managed',
           '--record', self._install_record]

  def _postprocess(self):
    installed_files = []
    egg_info = None
    with open(self._install_record) as fp:
      installed_files = fp.read().splitlines()
      for line in installed_files:
        if line.endswith('.egg-info'):
          assert line.startswith('/'), 'Expect .egg-info to be within install_tmp!'
          egg_info = line
          break

    if not egg_info:
      self._installed = False
      return self._installed

    installed_files = [os.path.relpath(fn, egg_info) for fn in installed_files if fn != egg_info]

    self._egg_info = os.path.join(self._install_tmp, egg_info[1:])
    with open(os.path.join(self._egg_info, 'installed-files.txt'), 'w') as fp:
      fp.write('\n'.join(installed_files))
      fp.write('\n')

    return self._installed

  @after_installation
  def egg_info(self):
    return self._egg_info

  @after_installation
  def root(self):
    egg_info = self.egg_info()
    assert egg_info
    return os.path.realpath(os.path.dirname(egg_info))

  @after_installation
  def distribution(self):
    base_dir = self.root()
    egg_info = self.egg_info()
    metadata = PathMetadata(base_dir, egg_info)
    return Distribution.from_location(base_dir, os.path.basename(egg_info), metadata=metadata)


class DistributionPackager(InstallerBase):
  def mixins(self):
    mixins = super(DistributionPackager, self).mixins().copy()
    mixins.update(setuptools='setuptools>=1')
    return mixins

  def find_distribution(self):
    dists = os.listdir(self.install_tmp)
    if len(dists) == 0:
      raise self.InstallFailure('No distributions were produced!')
    elif len(dists) > 1:
      raise self.InstallFailure('Ambiguous source distributions found: %s' % (' '.join(dists)))
    else:
      return os.path.join(self.install_tmp, dists[0])


class Packager(DistributionPackager):
  """
    Create a source distribution from an unpacked setup.py-based project.
  """
  def _setup_command(self):
    return ['sdist', '--formats=gztar', '--dist-dir=%s' % self._install_tmp]

  @after_installation
  def sdist(self):
    return self.find_distribution()


class EggInstaller(DistributionPackager):
  """
    Create a source distribution from an unpacked setup.py-based project.
  """
  def _setup_command(self):
    return ['bdist_egg', '--dist-dir=%s' % self._install_tmp]

  @after_installation
  def bdist(self):
    return self.find_distribution()


class WheelInstaller(DistributionPackager):
  """
    Create a source distribution from an unpacked setup.py-based project.
  """
  MIXINS = {
      'setuptools': 'setuptools>=2',
      'wheel': 'wheel>=0.17',
  }

  def mixins(self):
    mixins = super(WheelInstaller, self).mixins().copy()
    mixins.update(self.MIXINS)
    return mixins

  def _setup_command(self):
    return ['bdist_wheel', '--dist-dir=%s' % self._install_tmp]

  @after_installation
  def bdist(self):
    return self.find_distribution()

########NEW FILE########
__FILENAME__ = interpreter
"""
twitter.common.python support for interpreter environments.
"""
from __future__ import absolute_import

try:
  from numbers import Integral
except ImportError:
  Integral = (int, long)

from collections import defaultdict
import os
import re
import subprocess
import sys

from .base import maybe_requirement, maybe_requirement_list
from .compatibility import string
from .tracer import Tracer

from pkg_resources import (
    find_distributions,
    Distribution,
    Requirement,
)

TRACER = Tracer(predicate=Tracer.env_filter('PEX_VERBOSE'),
    prefix='twitter.common.python.interpreter: ')


# Determine in the most platform-compatible way possible the identity of the interpreter
# and its known packages.
ID_PY = b"""
import sys

if hasattr(sys, 'pypy_version_info'):
  subversion = 'PyPy'
elif sys.platform.startswith('java'):
  subversion = 'Jython'
else:
  subversion = 'CPython'

print("%s %s %s %s" % (
  subversion,
  sys.version_info[0],
  sys.version_info[1],
  sys.version_info[2]))

setuptools_path = None
try:
  import pkg_resources
except ImportError:
  sys.exit(0)

requirements = {}
for item in sys.path:
  for dist in pkg_resources.find_distributions(item):
    requirements[str(dist.as_requirement())] = dist.location

for requirement_str, location in requirements.items():
  rs = requirement_str.split('==', 2)
  if len(rs) == 2:
    print('%s %s %s' % (rs[0], rs[1], location))
"""


class PythonCapability(list):
  def __init__(self, requirements=None):
    super(PythonCapability, self).__init__(maybe_requirement_list(requirements or []))


class PythonIdentity(object):
  class Error(Exception): pass
  class InvalidError(Error): pass
  class UnknownRequirement(Error): pass

  # TODO(wickman)  Support interpreter-specific versions, e.g. PyPy-2.2.1
  HASHBANGS = {
    'CPython': 'python%(major)d.%(minor)d',
    'Jython': 'jython',
    'PyPy': 'pypy',
  }

  @classmethod
  def get_subversion(cls):
    if hasattr(sys, 'pypy_version_info'):
      subversion = 'PyPy'
    elif sys.platform.startswith('java'):
      subversion = 'Jython'
    else:
      subversion = 'CPython'
    return subversion

  @classmethod
  def get(cls):
    return cls(cls.get_subversion(), sys.version_info[0], sys.version_info[1], sys.version_info[2])

  @classmethod
  def from_id_string(cls, id_string):
    values = id_string.split()
    if len(values) != 4:
      raise cls.InvalidError("Invalid id string: %s" % id_string)
    return cls(str(values[0]), int(values[1]), int(values[2]), int(values[3]))

  @classmethod
  def from_path(cls, dirname):
    interp, version = dirname.split('-')
    major, minor, patch = version.split('.')
    return cls(str(interp), int(major), int(minor), int(patch))

  def __init__(self, interpreter, major, minor, patch):
    for var in (major, minor, patch):
      assert isinstance(var, Integral)
    self._interpreter = interpreter
    self._version = (major, minor, patch)

  @property
  def interpreter(self):
    return self._interpreter

  @property
  def version(self):
    return self._version

  @property
  def requirement(self):
    return self.distribution.as_requirement()

  @property
  def distribution(self):
    return Distribution(project_name=self._interpreter, version='.'.join(map(str, self._version)))

  @classmethod
  def parse_requirement(cls, requirement, default_interpreter='CPython'):
    if isinstance(requirement, Requirement):
      return requirement
    elif isinstance(requirement, string):
      try:
        requirement = Requirement.parse(requirement)
      except ValueError:
        try:
          requirement = Requirement.parse('%s%s' % (default_interpreter, requirement))
        except ValueError:
          raise ValueError('Unknown requirement string: %s' % requirement)
      return requirement
    else:
      raise ValueError('Unknown requirement type: %r' % (requirement,))

  def matches(self, requirement):
    """Given a Requirement, check if this interpreter matches."""
    try:
      requirement = self.parse_requirement(requirement, self._interpreter)
    except ValueError as e:
      raise self.UnknownRequirement(str(e))
    return self.distribution in requirement

  def hashbang(self):
    hashbang_string = self.HASHBANGS.get(self.interpreter, 'CPython') % {
      'major': self._version[0],
      'minor': self._version[1],
      'patch': self._version[2],
    }
    return '#!/usr/bin/env %s' % hashbang_string

  @property
  def python(self):
    # return the python version in the format of the 'python' key for distributions
    # specifically, '2.6', '2.7', '3.2', etc.
    return '%d.%d' % (self.version[0:2])

  def __str__(self):
    return '%s-%s.%s.%s' % (self._interpreter,
      self._version[0], self._version[1], self._version[2])

  def __repr__(self):
    return 'PythonIdentity(%r, %s, %s, %s)' % (
        self._interpreter, self._version[0], self._version[1], self._version[2])

  def __eq__(self, other):
    return all([isinstance(other, PythonIdentity),
                self.interpreter == other.interpreter,
                self.version == other.version])

  def __hash__(self):
    return hash((self._interpreter, self._version))


class PythonInterpreter(object):
  REGEXEN = (
    re.compile(r'jython$'),
    re.compile(r'python$'),
    re.compile(r'python[23].[0-9]$'),
    re.compile(r'pypy$'),
    re.compile(r'pypy-1.[0-9]$'),
  )

  CACHE = {}  # memoize executable => PythonInterpreter

  try:
    # Versions of distribute prior to the setuptools merge would automatically replace
    # 'setuptools' requirements with 'distribute'.  It provided the 'replacement' kwarg
    # to toggle this, but it was removed post-merge.
    COMPATIBLE_SETUPTOOLS = Requirement.parse('setuptools>=1.0', replacement=False)
  except TypeError:
    COMPATIBLE_SETUPTOOLS = Requirement.parse('setuptools>=1.0')

  class Error(Exception): pass
  class IdentificationError(Error): pass
  class InterpreterNotFound(Error): pass

  @classmethod
  def get(cls):
    return cls.from_binary(sys.executable)

  @classmethod
  def all(cls, paths=None):
    if paths is None:
      paths = os.getenv('PATH', '').split(':')
    return cls.filter(cls.find(paths))

  @classmethod
  def _parse_extras(cls, output_lines):
    def iter_lines():
      for line in output_lines:
        try:
          dist_name, dist_version, location = line.split()
        except ValueError:
          raise cls.IdentificationError('Could not identify requirement: %s' % line)
        yield ((dist_name, dist_version), location)
    return dict(iter_lines())

  @classmethod
  def _from_binary_internal(cls, path_extras):
    def iter_extras():
      for item in sys.path + list(path_extras):
        for dist in find_distributions(item):
          if dist.version:
            yield ((dist.key, dist.version), dist.location)
    return cls(sys.executable, PythonIdentity.get(), dict(iter_extras()))

  @classmethod
  def _from_binary_external(cls, binary, path_extras):
    environ = cls.sanitized_environment()
    environ['PYTHONPATH'] = ':'.join(path_extras)
    po = subprocess.Popen(
        [binary],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        env=environ)
    so, _ = po.communicate(ID_PY)
    output = so.decode('utf8').splitlines()
    if len(output) == 0:
      raise cls.IdentificationError('Could not establish identity of %s' % binary)
    identity, extras = output[0], output[1:]
    return cls(
        binary,
        PythonIdentity.from_id_string(identity),
        extras=cls._parse_extras(extras))

  @classmethod
  def expand_path(cls, path):
    if os.path.isfile(path):
      return [path]
    elif os.path.isdir(path):
      return [os.path.join(path, fn) for fn in os.listdir(path)]
    return []

  @classmethod
  def from_env(cls, hashbang):
    """Resolve a PythonInterpreter as /usr/bin/env would.

       :param hashbang: A string, e.g. "python3.3" representing some binary on the $PATH.
    """
    paths = os.getenv('PATH', '').split(':')
    for path in paths:
      for fn in cls.expand_path(path):
        basefile = os.path.basename(fn)
        if hashbang == basefile:
          try:
            return cls.from_binary(fn)
          except Exception as e:
            TRACER.log('Could not identify %s: %s' % (fn, e))

  @classmethod
  def from_binary(cls, binary, path_extras=None):
    path_extras = path_extras or ()
    if binary not in cls.CACHE:
      if binary == sys.executable:
        cls.CACHE[binary] = cls._from_binary_internal(path_extras)
      else:
        cls.CACHE[binary] = cls._from_binary_external(binary, path_extras)
    return cls.CACHE[binary]

  @classmethod
  def find(cls, paths):
    """
      Given a list of files or directories, try to detect python interpreters amongst them.
      Returns a list of PythonInterpreter objects.
    """
    pythons = []
    for path in paths:
      for fn in cls.expand_path(path):
        basefile = os.path.basename(fn)
        if any(matcher.match(basefile) is not None for matcher in cls.REGEXEN):
          try:
            pythons.append(cls.from_binary(fn))
          except Exception as e:
            TRACER.log('Could not identify %s: %s' % (fn, e))
            continue
    return pythons

  @classmethod
  def filter(cls, pythons):
    """
      Given a map of python interpreters in the format provided by PythonInterpreter.find(),
      filter out duplicate versions and versions we would prefer not to use.

      Returns a map in the same format as find.
    """
    good = []

    MAJOR, MINOR, SUBMINOR = range(3)
    def version_filter(version):
      return (version[MAJOR] == 2 and version[MINOR] >= 6 or
              version[MAJOR] == 3 and version[MINOR] >= 2)

    all_versions = set(interpreter.identity.version for interpreter in pythons)
    good_versions = filter(version_filter, all_versions)

    for version in good_versions:
      # For each candidate, use the latest version we find on the filesystem.
      candidates = defaultdict(list)
      for interp in pythons:
        if interp.identity.version == version:
          candidates[interp.identity.interpreter].append(interp)
      for interp_class in candidates:
        candidates[interp_class].sort(
            key=lambda interp: os.path.getmtime(interp.binary), reverse=True)
        good.append(candidates[interp_class].pop(0))

    return good

  @classmethod
  def sanitized_environment(cls):
    # N.B. This is merely a hack because sysconfig.py on the default OS X
    # installation of 2.6/2.7 breaks.
    env_copy = os.environ.copy()
    env_copy.pop('MACOSX_DEPLOYMENT_TARGET', None)
    return env_copy

  @classmethod
  def replace(cls, requirement):
    self = cls.get()
    if self.identity.matches(requirement):
      return False
    for pi in cls.all():
      if pi.identity.matches(requirement):
        break
    else:
      raise cls.InterpreterNotFound('Could not find interpreter matching filter!')
    os.execve(pi.binary, [pi.binary] + sys.argv, cls.sanitized_environment())

  def __init__(self, binary, identity, extras=None):
    """Construct a PythonInterpreter.

       You should probably PythonInterpreter.from_binary instead.

       :param binary: The full path of the python binary.
       :param identity: The :class:`PythonIdentity` of the PythonInterpreter.
       :param extras: A mapping from (dist.key, dist.version) to dist.location
                      of the extras associated with this interpreter.
    """
    self._binary = os.path.realpath(binary)
    self._binary_stat = os.stat(self._binary)
    self._extras = extras or {}
    self._identity = identity

  def with_extra(self, key, version, location):
    extras = self._extras.copy()
    extras[(key, version)] = location
    return self.__class__(self._binary, self._identity, extras)

  @property
  def extras(self):
    return self._extras.copy()

  @property
  def binary(self):
    return self._binary

  @property
  def identity(self):
    return self._identity

  @property
  def python(self):
    return self._identity.python

  @property
  def version(self):
    return self._identity.version

  @property
  def version_string(self):
    return str(self._identity)

  def satisfies(self, capability):
    if not isinstance(capability, PythonCapability):
      raise TypeError('Capability must be a PythonCapability, got %s' % type(capability))
    return not any(self.get_location(req) is None for req in capability)

  def get_location(self, req):
    req = maybe_requirement(req)
    for dist, location in self.extras.items():
      dist_name, dist_version = dist
      if req.key == dist_name and dist_version in req:
        return location

  def __hash__(self):
    return hash(self._binary_stat)

  def __eq__(self, other):
    if not isinstance(other, PythonInterpreter):
      return False
    return self._binary_stat == other._binary_stat

  def __lt__(self, other):
    if not isinstance(other, PythonInterpreter):
      return False
    return self.version < other.version

  def __repr__(self):
    return '%s(%r, %r, %r)' % (self.__class__.__name__, self._binary, self._identity, self._extras)

########NEW FILE########
__FILENAME__ = marshaller
try:
  from imp import get_magic
  HAS_MAGIC = True
except ImportError:
  HAS_MAGIC = False

import marshal
import struct
import time

from .compatibility import BytesIO, bytes as compatibility_bytes


class CodeTimestamp(object):
  TIMESTAMP_RANGE = (4, 8)

  @classmethod
  def from_timestamp(timestamp):
    return CodeTimestamp(timestamp)

  @classmethod
  def from_object(pyc_object):
    stamp = time.localtime(
        struct.unpack('I', pyc_object[slice(*CodeTimestamp.TIMESTAMP_RANGE)])[0])
    return CodeTimestamp(stamp)

  def __init__(self, stamp=time.time()):
    self._stamp = stamp

  def to_object(self):
    return struct.pack('I', self._stamp)


class CodeMarshaller(object):
  class InvalidCode(Exception): pass

  if HAS_MAGIC:
    MAGIC = struct.unpack('I', get_magic())[0]
  MAGIC_RANGE = (0, 4)
  TIMESTAMP_RANGE = (4, 8)

  @staticmethod
  def from_pyc(pyc):
    if not HAS_MAGIC:
      raise CodeMarshaller.InvalidCode('Interpreter cannot unmarshal .pyc!')
    if not isinstance(pyc, compatibility_bytes) and not hasattr(pyc, 'read'):
      raise CodeMarshaller.InvalidCode(
          "CodeMarshaller.from_pyc expects a code or file-like object!")
    if not isinstance(pyc, compatibility_bytes):
      pyc = pyc.read()
    pyc_magic = struct.unpack('I', pyc[slice(*CodeMarshaller.MAGIC_RANGE)])[0]
    if pyc_magic != CodeMarshaller.MAGIC:
      raise CodeMarshaller.InvalidCode("Bad magic number!  Got 0x%X" % pyc_magic)
    stamp = time.localtime(struct.unpack('I', pyc[slice(*CodeMarshaller.TIMESTAMP_RANGE)])[0])
    try:
      code = marshal.loads(pyc[8:])
    except ValueError as e:
      raise CodeMarshaller.InvalidCode("Unmarshaling error! %s" % e)
    return CodeMarshaller(code, stamp)

  @staticmethod
  def from_py(py, filename):
    stamp = int(time.time())
    code = compile(py.replace('\r\n', '\n').replace('\r', '\n'), filename, 'exec')
    return CodeMarshaller(code, stamp)

  def __init__(self, code, stamp):
    self._code = code
    self._stamp = stamp

  @property
  def code(self):
    return self._code

  def to_pyc(self):
    sio = BytesIO()
    sio.write(struct.pack('I', CodeMarshaller.MAGIC))
    sio.write(struct.pack('I', self._stamp))
    sio.write(marshal.dumps(self._code))
    return sio.getvalue()

########NEW FILE########
__FILENAME__ = obtainer
import itertools
import os
import shutil
import time
import uuid

from .base import requirement_is_exact
from .common import safe_mkdtemp
from .fetcher import PyPIFetcher, Fetcher
from .http import Crawler
from .package import (
     EggPackage,
     Package,
     SourcePackage,
     WheelPackage,
)
from .platforms import Platform
from .tracer import TRACER
from .translator import ChainedTranslator, Translator


class Obtainer(object):
  """
    A requirement obtainer.

    An Obtainer takes a Crawler, a list of Fetchers (which take requirements
    and tells us where to look for them) and a list of Translators (which
    translate egg or source packages into usable distributions) and turns them
    into a cohesive requirement pipeline.

    >>> from twitter.common.python.http import Crawler
    >>> from twitter.common.python.obtainer import Obtainer
    >>> from twitter.common.python.fetcher import PyPIFetcher
    >>> from twitter.common.python.resolver import Resolver
    >>> from twitter.common.python.translator import Translator
    >>> obtainer = Obtainer(Crawler(), [PyPIFetcher()], [Translator.default()])
    >>> resolver = Resolver(obtainer)
    >>> distributions = resolver.resolve(['ansicolors', 'elementtree', 'mako', 'markdown', 'psutil',
    ...                                   'pygments', 'pylint', 'pytest'])
    >>> for d in distributions: d.activate()
  """
  DEFAULT_PACKAGE_PRECEDENCE = (
      WheelPackage,
      EggPackage,
      SourcePackage,
  )

  @classmethod
  def default(cls, platform=Platform.current(), interpreter=None):
    translator = Translator.default(platform=platform, interpreter=interpreter)
    return cls(translators=translator)

  @classmethod
  def package_type_precedence(cls, package, precedence=DEFAULT_PACKAGE_PRECEDENCE):
    for rank, package_type in enumerate(reversed(precedence)):
      if isinstance(package, package_type):
        return rank
    # If we do not recognize the package, it gets lowest precedence
    return -1

  @classmethod
  def package_precedence(cls, package, precedence=DEFAULT_PACKAGE_PRECEDENCE):
    return (package.version, cls.package_type_precedence(package, precedence=precedence))

  def __init__(self, crawler=None,
                     fetchers=None,
                     translators=None,
                     precedence=DEFAULT_PACKAGE_PRECEDENCE):
    self._crawler = crawler or Crawler()
    self._fetchers = fetchers or [PyPIFetcher()]
    if isinstance(translators, (list, tuple)):
      self._translator = ChainedTranslator(*translators)
    else:
      self._translator = translators or Translator.default()
    self._precedence = precedence

  def _translate_href(self, href):
    package = Package.from_href(href, opener=self._crawler.opener)
    # Restrict this to a package found in the package precedence list, so that users of
    # obtainers can restrict which distribution formats they support.
    if any(isinstance(package, package_type) for package_type in self._precedence):
      return package

  def _iter_unordered(self, req):
    url_iterator = itertools.chain.from_iterable(fetcher.urls(req) for fetcher in self._fetchers)
    for package in filter(None, map(self._translate_href, self._crawler.crawl(url_iterator))):
      if package.satisfies(req):
        yield package

  def _sort(self, package_list):
    key = lambda package: self.package_precedence(package, self._precedence)
    return sorted(package_list, key=key, reverse=True)

  def _translate_from(self, obtain_set):
    for package in obtain_set:
      dist = self._translator.translate(package)
      if dist:
        return dist

  def iter(self, req):
    """Return a list of packages that satisfy the requirement in best match order."""
    for package in self._sort(self._iter_unordered(req)):
      yield package

  def obtain(self, req_or_package):
    """Given a requirement or package, return a distribution satisfying that requirement."""
    if isinstance(req_or_package, Package):
      return self._translate_from([req_or_package])
    with TRACER.timed('Obtaining %s' % req_or_package):
      return self._translate_from(self.iter(req_or_package))


class CachingObtainer(Obtainer):
  def __init__(self, *args, **kw):
    self.__ttl = kw.pop('ttl', 3600)
    self.__install_cache = kw.pop('install_cache', None) or safe_mkdtemp()
    super(CachingObtainer, self).__init__(*args, **kw)
    self.__cache_obtainer = Obtainer(
        crawler=self._crawler,
        fetchers=[Fetcher([self.__install_cache])],
        translators=self._translator,
        precedence=self._precedence,
    )

  @property
  def ttl(self):
    return self.__ttl

  @property
  def install_cache(self):
    return self.__install_cache

  def _has_expired_ttl(self, dist):
    now = time.time()
    return now - os.path.getmtime(dist.location) >= self.__ttl

  def _dist_can_be_used(self, dist, requirement):
    return requirement_is_exact(requirement) or not self._has_expired_ttl(dist)

  def _set_cached_dist(self, dist):
    target_location = os.path.join(self.__install_cache, os.path.basename(dist.location))
    if os.path.exists(target_location):
      return
    target_tmp = target_location + uuid.uuid4().get_hex()
    shutil.copyfile(dist.location, target_tmp)
    os.rename(target_tmp, target_location)

  def iter(self, req):
    cached_dist = self._translate_from(self.__cache_obtainer.iter(req))
    if cached_dist and self._dist_can_be_used(cached_dist, req):
      for package in self.__cache_obtainer.iter(req):
        yield package
      return
    for package in super(CachingObtainer, self).iter(req):
      yield package

  def obtain(self, req_or_package):
    dist = super(CachingObtainer, self).obtain(req_or_package)
    if dist:
      self._set_cached_dist(dist)
    return dist

########NEW FILE########
__FILENAME__ = orderedset
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================
# OrderedSet recipe referenced in the Python standard library docs (bottom):
#     http://docs.python.org/library/collections.html
#
# Copied from recipe code found here: http://code.activestate.com/recipes/576694/ with small
# modifications
#

import collections


class OrderedSet(collections.MutableSet):
  KEY, PREV, NEXT = range(3)

  def __init__(self, iterable=None):
    self.end = end = []
    end += [None, end, end]         # sentinel node for doubly linked list
    self.map = {}                   # key --> [key, prev, next]
    if iterable is not None:
      self |= iterable

  def __len__(self):
    return len(self.map)

  def __contains__(self, key):
    return key in self.map

  def add(self, key):
    if key not in self.map:
      end = self.end
      curr = end[self.PREV]
      curr[self.NEXT] = end[self.PREV] = self.map[key] = [key, curr, end]

  def update(self, iterable):
    for key in iterable:
      self.add(key)

  def discard(self, key):
    if key in self.map:
      key, prev, next = self.map.pop(key)
      prev[self.NEXT] = next
      next[self.PREV] = prev

  def __iter__(self):
    end = self.end
    curr = end[self.NEXT]
    while curr is not end:
      yield curr[self.KEY]
      curr = curr[self.NEXT]

  def __reversed__(self):
    end = self.end
    curr = end[self.PREV]
    while curr is not end:
      yield curr[self.KEY]
      curr = curr[self.PREV]

  def pop(self, last=True):
    if not self:
      raise KeyError('set is empty')
    key = next(reversed(self)) if last else next(iter(self))
    self.discard(key)
    return key

  def __repr__(self):
    if not self:
      return '%s()' % (self.__class__.__name__,)
    return '%s(%r)' % (self.__class__.__name__, list(self))

  def __eq__(self, other):
    if isinstance(other, OrderedSet):
      return len(self) == len(other) and list(self) == list(other)
    return set(self) == set(other)

  def __del__(self):
    self.clear()                    # remove circular references

########NEW FILE########
__FILENAME__ = package
import contextlib
import os
import tarfile
import zipfile

from .base import maybe_requirement
from .common import safe_mkdtemp
from .http.link import Link
from .interpreter import PythonInterpreter
from .pep425 import PEP425, PEP425Extras
from .platforms import Platform

from pkg_resources import (
    EGG_NAME,
    parse_version,
    safe_name,
)


class Package(Link):
  """Base class for named Python binary packages (e.g. source, egg, wheel)."""

  # The registry of concrete implementations
  _REGISTRY = set()

  @classmethod
  def register(cls, package_type):
    """Register a concrete implementation of a Package to be recognized by twitter.common.python."""
    if not issubclass(package_type, cls):
      raise TypeError('package_type must be a subclass of Package.')
    cls._REGISTRY.add(package_type)

  @classmethod
  def from_href(cls, href, **kw):
    """Convert from a url to Package.

    :param href: The url to parse
    :type href: string
    :returns: A Package object if a valid concrete implementation exists, otherwise None.
    """
    for package_type in cls._REGISTRY:
      try:
        return package_type(href, **kw)
      except package_type.InvalidLink:
        continue

  @property
  def name(self):
    return NotImplementedError

  @property
  def raw_version(self):
    return NotImplementedError

  @property
  def version(self):
    return parse_version(self.raw_version)

  def satisfies(self, requirement):
    """Determine whether this package matches the requirement.

    :param requirement: The requirement to compare this Package against
    :type requirement: string or :class:`pkg_resources.Requirement`
    :returns: True if the package matches the requirement, otherwise False
    """
    requirement = maybe_requirement(requirement)
    link_name = safe_name(self.name).lower()
    if link_name != requirement.key:
      return False
    return self.raw_version in requirement

  def compatible(self, identity, platform=Platform.current()):
    """Is this link compatible with the given :class:`PythonIdentity` identity and platform?

    :param identity: The Python identity (e.g. CPython 2.7.5) against which compatibility
    should be checked.
    :type identity: :class:`PythonIdentity`
    :param platform: The platform against which compatibility should be checked.  If None, do not
    check platform compatibility.
    :type platform: string or None
    """
    raise NotImplementedError


class SourcePackage(Package):
  """A Package representing an uncompiled/unbuilt source distribution."""

  EXTENSIONS = {
    '.tar': (tarfile.TarFile.open, tarfile.ReadError),
    '.tar.gz': (tarfile.TarFile.open, tarfile.ReadError),
    '.tar.bz2': (tarfile.TarFile.open, tarfile.ReadError),
    '.tgz': (tarfile.TarFile.open, tarfile.ReadError),
    '.zip': (zipfile.ZipFile, zipfile.BadZipfile)
  }

  @classmethod
  def split_fragment(cls, fragment):
    """A heuristic used to split a string into version name/fragment:

       >>> split_fragment('pysolr-2.1.0-beta')
       ('pysolr', '2.1.0-beta')
       >>> split_fragment('cElementTree-1.0.5-20051216')
       ('cElementTree', '1.0.5-20051216')
       >>> split_fragment('pil-1.1.7b1-20090412')
       ('pil', '1.1.7b1-20090412')
       >>> split_fragment('django-plugin-2-2.3')
       ('django-plugin-2', '2.3')
    """
    def likely_version_component(enumerated_fragment):
      return sum(bool(v and v[0].isdigit()) for v in enumerated_fragment[1].split('.'))
    fragments = fragment.split('-')
    if len(fragments) == 1:
      return fragment, ''
    max_index, _ = max(enumerate(fragments), key=likely_version_component)
    return '-'.join(fragments[0:max_index]), '-'.join(fragments[max_index:])

  def __init__(self, url, **kw):
    super(SourcePackage, self).__init__(url, **kw)

    for ext, class_info in self.EXTENSIONS.items():
      if self.filename.endswith(ext):
        self._archive_class = class_info
        fragment = self.filename[:-len(ext)]
        break
    else:
      raise self.InvalidLink('%s does not end with any of: %s' % (
          self.filename, ' '.join(self.EXTENSIONS)))
    self._name, self._raw_version = self.split_fragment(fragment)

  @property
  def name(self):
    return safe_name(self._name)

  @property
  def raw_version(self):
    return safe_name(self._raw_version)

  @classmethod
  def first_nontrivial_dir(cls, path):
    files = os.listdir(path)
    if len(files) == 1 and os.path.isdir(os.path.join(path, files[0])):
      return cls.first_nontrivial_dir(os.path.join(path, files[0]))
    else:
      return path

  def _unpack(self, filename, location=None):
    path = location or safe_mkdtemp()
    archive_class, error_class = self._archive_class
    try:
      with contextlib.closing(archive_class(filename)) as package:
        package.extractall(path=path)
    except error_class:
      raise self.UnreadableLink('Could not read %s' % self.url)
    return self.first_nontrivial_dir(path)

  def fetch(self, location=None, conn_timeout=None):
    """Fetch and unpack this source target into the location.

    :param location: The location into which the archive should be unpacked.  If None, a temporary
    ephemeral directory will be created.
    :type location: string or None
    :param conn_timeout: A connection timeout for the fetch.  If None, a default is used.
    :type conn_timeout: float or None
    :returns: The assumed root directory of the package.
    """
    target = super(SourcePackage, self).fetch(conn_timeout=conn_timeout)
    return self._unpack(target, location)

  # SourcePackages are always compatible as they can be translated to a distribution.
  def compatible(self, identity, platform=Platform.current()):
    return True


class EggPackage(Package):
  """A Package representing a built egg."""

  def __init__(self, url, **kw):
    super(EggPackage, self).__init__(url, **kw)
    filename, ext = os.path.splitext(self.filename)
    if ext.lower() != '.egg':
      raise self.InvalidLink('Not an egg: %s' % filename)
    matcher = EGG_NAME(filename)
    if not matcher:
      raise self.InvalidLink('Could not match egg: %s' % filename)

    self._name, self._raw_version, self._py_version, self._platform = matcher.group(
        'name', 'ver', 'pyver', 'plat')

    if self._raw_version is None or self._py_version is None:
      raise self.InvalidLink('url with .egg extension but bad name: %s' % url)

  def __hash__(self):
    return hash((self.name, self.version, self.py_version, self.platform))

  @property
  def name(self):
    return safe_name(self._name)

  @property
  def raw_version(self):
    return safe_name(self._raw_version)

  @property
  def py_version(self):
    return self._py_version

  @property
  def platform(self):
    return self._platform

  def compatible(self, identity, platform=Platform.current()):
    if not Platform.version_compatible(self.py_version, identity.python):
      return False
    if not Platform.compatible(self.platform, platform):
      return False
    return True


class WheelPackage(Package):
  """A Package representing a built wheel."""

  def __init__(self, url, **kw):
    super(WheelPackage, self).__init__(url, **kw)
    filename, ext = os.path.splitext(self.filename)
    if ext.lower() != '.whl':
      raise self.InvalidLink('Not a wheel: %s' % filename)
    try:
      self._name, self._raw_version, self._py_tag, self._abi_tag, self._arch_tag = (
          filename.split('-'))
    except ValueError:
      raise self.InvalidLink('Wheel filename malformed.')
    # See https://github.com/pypa/pip/issues/1150 for why this is unavoidable.
    self._name.replace('_', '-')
    self._raw_version.replace('_', '-')
    self._supported_tags = frozenset(self._iter_tags())

  @property
  def name(self):
    return self._name

  @property
  def raw_version(self):
    return self._raw_version

  def _iter_tags(self):
    for py in self._py_tag.split('.'):
      for abi in self._abi_tag.split('.'):
        for arch in self._arch_tag.split('.'):
          for real_arch in PEP425Extras.platform_iterator(arch):
            yield (py, abi, real_arch)

  def compatible(self, identity, platform=Platform.current()):
    for tag in PEP425.iter_supported_tags(identity, platform):
      if tag in self._supported_tags:
        return True
    return False


Package.register(SourcePackage)
Package.register(EggPackage)
Package.register(WheelPackage)


def distribution_compatible(dist, interpreter=None, platform=None):
  """Is this distribution compatible with the given interpreter/platform combination?

  :param interpreter: The Python interpreter against which compatibility should be checked.  If None
  specified, the current interpreter is used.
  :type identity: :class:`PythonInterpreter` or None
  :param platform: The platform against which compatibility should be checked.  If None, the current
  platform will be used
  :type platform: string or None
  :returns: True if the distribution is compatible, False if it is unrecognized or incompatible.
  """
  interpreter = interpreter or PythonInterpreter.get()
  platform = platform or Platform.current()

  package = Package.from_href(dist.location)
  if not package:
    return False
  return package.compatible(interpreter.identity, platform=platform)

########NEW FILE########
__FILENAME__ = pep425
"""PEP425 handling for twitter.common.python

PEP425 (http://legacy.python.org/dev/peps/pep-0425/) describes a tagging system used to determine
whether or not a distribution's platform is compatible with the current platform.  It is the
tagging system used to describe platform compatibility for wheel files.
"""

from .platforms import Platform

from pkg_resources import get_supported_platform


class PEP425Extras(object):
  """Extensions to platform handling beyond PEP425."""

  @classmethod
  def is_macosx_platform(cls, platform):
    return platform.startswith('macosx')

  @classmethod
  def parse_macosx_tag(cls, platform_tag):
    invalid_tag = ValueError('invalid macosx tag: %s' % platform_tag)
    if not cls.is_macosx_platform(platform_tag):
      raise invalid_tag
    segments = platform_tag.split('_', 3)
    if len(segments) != 4:
      raise invalid_tag
    if segments[0] != 'macosx':
      raise invalid_tag
    try:
      major, minor = int(segments[1]), int(segments[2])
      platform = segments[3]
    except ValueError:
      raise invalid_tag
    return major, minor, platform

  @classmethod
  def iter_compatible_osx_platforms(cls, supported_platform):
    platform_major, platform_minor, platform = cls.parse_macosx_tag(supported_platform)
    platform_equivalents = set(Platform.MACOSX_PLATFORM_COMPATIBILITY.get(platform, ()))
    platform_equivalents.add(platform)
    for minor in range(platform_minor, -1, -1):
      for binary_compat in platform_equivalents:
        yield 'macosx_%s_%s_%s' % (platform_major, minor, binary_compat)

  @classmethod
  def platform_iterator(cls, platform):
    """Iterate over all compatible platform tags of a supplied platform tag.

       :param platform: the platform tag to iterate over
    """
    if cls.is_macosx_platform(platform):
      for plat in cls.iter_compatible_osx_platforms(platform):
        yield plat
    else:
      yield platform


class PEP425(object):
  INTERPRETER_TAGS = {
    'CPython': 'cp',
    'Jython': 'jy',
    'PyPy': 'pp',
    'IronPython': 'ip',
  }

  @classmethod
  def get_implementation_tag(cls, interpreter_subversion):
    return cls.INTERPRETER_TAGS.get(interpreter_subversion)

  @classmethod
  def get_version_tag(cls, interpreter_version):
    return ''.join(map(str, interpreter_version[:2]))

  @classmethod
  def translate_platform_to_tag(cls, platform):
    return platform.replace('.', '_').replace('-', '_')

  @classmethod
  def get_platform_tag(cls):
    return cls.translate_platform_to_tag(get_supported_platform())

  # TODO(wickman) This implementation is technically incorrect but we need to be able to
  # predict the supported tags of an interpreter that may not be on this machine or
  # of a different platform.  Alternatively we could store the manifest of supported tags
  # of a targeted platform in a file to be more correct.
  @classmethod
  def _iter_supported_tags(cls, impl, version, platform):
    """Given a set of tags, iterate over supported tags.

    :param impl: Python implementation tag e.g. cp, jy, pp.
    :param version: E.g. '26', '33'
    :param platform: Platform as from :function:`pkg_resources.get_supported_platform`,
    for example 'linux-x86_64' or 'macosx-10.4-x86_64'.
    :returns: Iterator over (pyver, abi, platform) tuples.
    """
    # Predict soabi for reasonable interpreters.  This is technically wrong but essentially right.
    abis = []
    if impl == 'cp' and version.startswith('3'):
      abis.extend(['cp%sm' % version, 'abi3'])

    major_version = int(version[0])
    minor_versions = []
    for minor in range(int(version[1]), -1, -1):
      minor_versions.append('%d%d' % (major_version, minor))

    platforms = list(PEP425Extras.platform_iterator(cls.translate_platform_to_tag(platform)))

    # interpreter specific
    for p in platforms:
      for abi in abis:
        yield ('%s%s' % (impl, version), abi, p)

    # everything else
    for p in platforms + ['any']:
      for i in ('py', impl):
        yield ('%s%d' % (i, major_version), 'none', p)
        for minor_version in minor_versions:
          yield ('%s%s' % (i, minor_version), 'none', p)

  @classmethod
  def iter_supported_tags(cls, identity, platform=get_supported_platform()):
    """Iterate over the supported tag tuples of this interpreter.

    :param identity: python interpreter identity over which tags should iterate.
    :type identity: :class:`PythonIdentity`
    :param platform: python platform over which tags should iterate, by default the current
                     platform.
    :returns: Iterator over valid PEP425 tag tuples.
    """
    impl_tag = cls.get_implementation_tag(identity.interpreter)
    vers_tag = cls.get_version_tag(identity.version)
    tag_iterator = cls._iter_supported_tags(impl_tag, vers_tag, platform)
    for tag in tag_iterator:
      yield tag

########NEW FILE########
__FILENAME__ = pex
from __future__ import absolute_import, print_function

from contextlib import contextmanager
from distutils import sysconfig
import os
from site import USER_SITE
import sys
import traceback

from .common import safe_mkdir
from .compatibility import exec_function
from .environment import PEXEnvironment
from .interpreter import PythonInterpreter
from .orderedset import OrderedSet
from .pex_info import PexInfo
from .tracer import Tracer

import pkg_resources
from pkg_resources import EntryPoint, find_distributions


TRACER = Tracer(predicate=Tracer.env_filter('PEX_VERBOSE'), prefix='twitter.common.python.pex: ')


class DevNull(object):
  def __init__(self):
    pass

  def write(self, *args, **kw):
    pass


class PEX(object):
  """
    PEX, n. A self-contained python environment.
  """
  class Error(Exception): pass
  class NotFound(Error): pass

  @staticmethod
  def start_coverage():
    try:
      import coverage
      cov = coverage.coverage(auto_data=True, data_suffix=True,
        data_file='.coverage.%s' % os.environ['PEX_COVERAGE'])
      cov.start()
    except ImportError:
      sys.stderr.write('Could not bootstrap coverage module!\n')

  @classmethod
  def clean_environment(cls, forking=False):
    os.unsetenv('MACOSX_DEPLOYMENT_TARGET')
    if not forking:
      for key in filter(lambda key: key.startswith('PEX_'), os.environ):
        os.unsetenv(key)

  def __init__(self, pex=sys.argv[0], interpreter=None):
    self._pex = pex
    self._pex_info = PexInfo.from_pex(self._pex)
    self._env = PEXEnvironment(self._pex, self._pex_info)
    self._interpreter = interpreter or PythonInterpreter.get()

  @property
  def info(self):
    return self._pex_info

  def entry(self):
    """
      Return the module spec of the entry point of this PEX.  None if there
      is no entry point for this environment.
    """
    if 'PEX_MODULE' in os.environ:
      TRACER.log('PEX_MODULE override detected: %s' % os.environ['PEX_MODULE'])
      return os.environ['PEX_MODULE']
    entry_point = self._pex_info.entry_point
    if entry_point:
      TRACER.log('Using prescribed entry point: %s' % entry_point)
      return str(entry_point)

  @classmethod
  def _extras_paths(cls):
    standard_lib = sysconfig.get_python_lib(standard_lib=True)
    try:
      makefile = sysconfig.parse_makefile(sysconfig.get_makefile_filename())
    except (AttributeError, IOError):
      # This is not available by default in PyPy's distutils.sysconfig or it simply is
      # no longer available on the system (IOError ENOENT)
      makefile = {}
    extras_paths = filter(None, makefile.get('EXTRASPATH', '').split(':'))
    for path in extras_paths:
      yield os.path.join(standard_lib, path)

  @classmethod
  def _site_libs(cls):
    try:
      from site import getsitepackages
      site_libs = set(getsitepackages())
    except ImportError:
      site_libs = set()
    site_libs.update([sysconfig.get_python_lib(plat_specific=False),
                      sysconfig.get_python_lib(plat_specific=True)])
    return site_libs

  @classmethod
  def minimum_sys_modules(cls, site_libs):
    new_modules = {}

    for module_name, module in sys.modules.items():
      if any(path.startswith(site_lib) for path in getattr(module, '__path__', ())
          for site_lib in site_libs):
        TRACER.log('Scrubbing %s from sys.modules' % module)
      else:
        new_modules[module_name] = module

    return new_modules

  @classmethod
  def minimum_sys_path(cls, site_libs):
    site_distributions = OrderedSet()
    for path_element in sys.path:
      if any(path_element.startswith(site_lib) for site_lib in site_libs):
        TRACER.log('Inspecting path element: %s' % path_element, V=2)
        site_distributions.update(dist.location for dist in find_distributions(path_element))

    user_site_distributions = OrderedSet(dist.location for dist in find_distributions(USER_SITE))

    for path in site_distributions:
      TRACER.log('Scrubbing from site-packages: %s' % path)
    for path in user_site_distributions:
      TRACER.log('Scrubbing from user site: %s' % path)

    scrub_paths = site_distributions | user_site_distributions
    scrubbed_sys_path = list(OrderedSet(sys.path) - scrub_paths)
    scrub_from_importer_cache = filter(
      lambda key: any(key.startswith(path) for path in scrub_paths),
      sys.path_importer_cache.keys())
    scrubbed_importer_cache = dict((key, value) for (key, value) in sys.path_importer_cache.items()
      if key not in scrub_from_importer_cache)
    return scrubbed_sys_path, scrubbed_importer_cache

  @classmethod
  def minimum_sys(cls):
    """Return the minimum sys necessary to run this interpreter, a la python -S.

    :returns: (sys.path, sys.path_importer_cache, sys.modules) tuple of a
    bare python installation.
    """
    site_libs = set(cls._site_libs())
    for site_lib in site_libs:
      TRACER.log('Found site-library: %s' % site_lib)
    for extras_path in cls._extras_paths():
      TRACER.log('Found site extra: %s' % extras_path)
      site_libs.add(extras_path)
    site_libs = set(os.path.normpath(path) for path in site_libs)

    sys_modules = cls.minimum_sys_modules(site_libs)
    sys_path, sys_path_importer_cache = cls.minimum_sys_path(site_libs)

    return sys_path, sys_path_importer_cache, sys_modules

  @classmethod
  @contextmanager
  def patch_pkg_resources(cls, working_set):
    """Patch pkg_resources given a new working set."""
    def patch(working_set):
      pkg_resources.working_set = working_set
      pkg_resources.require = working_set.require
      pkg_resources.iter_entry_points = working_set.iter_entry_points
      pkg_resources.run_script = pkg_resources.run_main = working_set.run_script
      pkg_resources.add_activation_listener = working_set.subscribe

    old_working_set = pkg_resources.working_set
    patch(working_set)
    try:
      yield
    finally:
      patch(old_working_set)

  @classmethod
  @contextmanager
  def patch_sys(cls):
    """Patch sys with all site scrubbed."""
    def patch_dict(old_value, new_value):
      old_value.clear()
      old_value.update(new_value)

    def patch_all(path, path_importer_cache, modules):
      sys.path[:] = path
      patch_dict(sys.path_importer_cache, path_importer_cache)
      patch_dict(sys.modules, modules)

    old_sys_path, old_sys_path_importer_cache, old_sys_modules = (
        sys.path[:], sys.path_importer_cache.copy(), sys.modules.copy())
    new_sys_path, new_sys_path_importer_cache, new_sys_modules = cls.minimum_sys()

    patch_all(new_sys_path, new_sys_path_importer_cache, new_sys_modules)

    try:
      yield
    finally:
      patch_all(old_sys_path, old_sys_path_importer_cache, old_sys_modules)

  def execute(self, args=()):
    """Execute the PEX.

    This function makes assumptions that it is the last function called by
    the interpreter.
    """

    entry_point = self.entry()

    try:
      with self.patch_sys():
        working_set = self._env.activate()
        if 'PEX_COVERAGE' in os.environ:
          PEX.start_coverage()
        TRACER.log('PYTHONPATH contains:')
        for element in sys.path:
          TRACER.log('  %c %s' % (' ' if os.path.exists(element) else '*', element))
        TRACER.log('  * - paths that do not exist or will be imported via zipimport')
        with self.patch_pkg_resources(working_set):
          if entry_point and 'PEX_INTERPRETER' not in os.environ:
            self.execute_entry(entry_point, args)
          else:
            self.execute_interpreter()
    except Exception:
      # Catch and print any exceptions before we tear things down in finally, then
      # reraise so that the exit status is reflected correctly.
      traceback.print_exc()
      raise
    finally:
      # squash all exceptions on interpreter teardown -- the primary type here are
      # atexit handlers failing to run because of things such as:
      #   http://stackoverflow.com/questions/2572172/referencing-other-modules-in-atexit
      if 'PEX_TEARDOWN_VERBOSE' not in os.environ:
        sys.stderr = DevNull()
        sys.excepthook = lambda *a, **kw: None

  @classmethod
  def execute_interpreter(cls):
    force_interpreter = 'PEX_INTERPRETER' in os.environ
    # TODO(wickman) Apparently os.unsetenv doesn't work on Windows
    os.unsetenv('PEX_INTERPRETER')
    TRACER.log('%s, dropping into interpreter' % (
        'PEX_INTERPRETER specified' if force_interpreter else 'No entry point specified'))
    if sys.argv[1:]:
      try:
        with open(sys.argv[1]) as fp:
          ast = compile(fp.read(), fp.name, 'exec', flags=0, dont_inherit=1)
      except IOError as e:
        print("Could not open %s in the environment [%s]: %s" % (sys.argv[1], sys.argv[0], e))
        sys.exit(1)
      sys.argv = sys.argv[1:]
      old_name = globals()['__name__']
      try:
        globals()['__name__'] = '__main__'
        exec_function(ast, globals())
      finally:
        globals()['__name__'] = old_name
    else:
      import code
      code.interact()

  @classmethod
  def execute_entry(cls, entry_point, args=None):
    if args:
      sys.argv = args
    runner = cls.execute_pkg_resources if ":" in entry_point else cls.execute_module

    if 'PEX_PROFILE' not in os.environ:
      runner(entry_point)
    else:
      import pstats, cProfile
      profile_output = os.environ['PEX_PROFILE']
      safe_mkdir(os.path.dirname(profile_output))
      cProfile.runctx('runner(entry_point)', globals=globals(), locals=locals(),
                      filename=profile_output)
      try:
        entries = int(os.environ.get('PEX_PROFILE_ENTRIES', 1000))
      except ValueError:
        entries = 1000
      pstats.Stats(profile_output).sort_stats(
          os.environ.get('PEX_PROFILE_SORT', 'cumulative')).print_stats(entries)

  @staticmethod
  def execute_module(module_name):
    import runpy
    runpy.run_module(module_name, run_name='__main__')

  @staticmethod
  def execute_pkg_resources(spec):
    entry = EntryPoint.parse("run = {0}".format(spec))
    runner = entry.load(require=False)  # trust that the environment is sane
    runner()

  def cmdline(self, args=()):
    """
      The commandline to run this environment.

      Optional arguments:
        binary: The binary to run instead of the entry point in the environment
        interpreter_args: Arguments to be passed to the interpreter before, e.g. '-E' or
          ['-m', 'pylint.lint']
        args: Arguments to be passed to the application being invoked by the environment.
    """
    cmds = [self._interpreter.binary]
    cmds.append(self._pex)
    cmds.extend(args)
    return cmds

  def run(self, args=(), with_chroot=False, blocking=True, setsid=False,
          stdin=sys.stdin, stdout=sys.stdout, stderr=sys.stderr):
    """
      Run the PythonEnvironment in an interpreter in a subprocess.

      with_chroot: Run with cwd set to the environment's working directory [default: False]
      blocking: If true, return the return code of the subprocess.
                If false, return the Popen object of the invoked subprocess.
    """
    import subprocess
    self.clean_environment(forking=True)

    cmdline = self.cmdline(args)
    TRACER.log('PEX.run invoking %s' % ' '.join(cmdline))
    process = subprocess.Popen(cmdline, cwd=self._pex if with_chroot else os.getcwd(),
                               preexec_fn=os.setsid if setsid else None,
                               stdin=stdin, stdout=stdout, stderr=stderr)
    return process.wait() if blocking else process

########NEW FILE########
__FILENAME__ = pex_bootstrapper
import contextlib
import os
import zipfile

__all__ = ('bootstrap_pex',)


def pex_info_name(entry_point):
  """Return the PEX-INFO for an entry_point"""
  return os.path.join(entry_point, 'PEX-INFO')


def is_compressed(entry_point):
  return os.path.exists(entry_point) and not os.path.exists(pex_info_name(entry_point))


def read_pexinfo_from_directory(entry_point):
  with open(pex_info_name(entry_point), 'rb') as fp:
    return fp.read()


def read_pexinfo_from_zip(entry_point):
  with contextlib.closing(zipfile.ZipFile(entry_point)) as zf:
    return zf.read('PEX-INFO')


def read_pex_info_content(entry_point):
  """Return the raw content of a PEX-INFO."""
  if is_compressed(entry_point):
    return read_pexinfo_from_zip(entry_point)
  else:
    return read_pexinfo_from_directory(entry_point)


def get_pex_info(entry_point):
  """Return the PexInfo object for an entry point."""
  from . import pex_info

  pex_info_content = read_pex_info_content(entry_point)
  if pex_info_content:
    return pex_info.PexInfo.from_json(pex_info_content)
  raise ValueError('Invalid entry_point: %s' % entry_point)


# TODO(wickman) Remove once resolved:
#   https://bitbucket.org/pypa/setuptools/issue/154/build_zipmanifest-results-should-be
def monkeypatch_build_zipmanifest():
  import pkg_resources
  if not hasattr(pkg_resources, 'build_zipmanifest'):
    return
  old_build_zipmanifest = pkg_resources.build_zipmanifest
  def memoized_build_zipmanifest(archive, memo={}):
    if archive not in memo:
      memo[archive] = old_build_zipmanifest(archive)
    return memo[archive]
  pkg_resources.build_zipmanifest = memoized_build_zipmanifest


def bootstrap_pex(entry_point):
  from .finders import register_finders
  monkeypatch_build_zipmanifest()
  register_finders()

  from . import pex
  pex.PEX(entry_point).execute()

########NEW FILE########
__FILENAME__ = pex_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import absolute_import

import logging
import os
import tempfile

from .compatibility import to_bytes
from .common import chmod_plus_x, open_zip, safe_mkdir, Chroot
from .interpreter import PythonInterpreter
from .marshaller import CodeMarshaller
from .pex_info import PexInfo
from .tracer import TRACER
from .util import CacheHelper, DistributionHelper

from pkg_resources import (
    DefaultProvider,
    ZipProvider,
    get_provider,
)


BOOTSTRAP_ENVIRONMENT = b"""
import os
import sys

__entry_point__ = None
if '__file__' in locals() and __file__ is not None:
  __entry_point__ = os.path.dirname(__file__)
elif '__loader__' in locals():
  from zipimport import zipimporter
  from pkgutil import ImpLoader
  if hasattr(__loader__, 'archive'):
    __entry_point__ = __loader__.archive
  elif isinstance(__loader__, ImpLoader):
    __entry_point__ = os.path.dirname(__loader__.get_filename())

if __entry_point__ is None:
  sys.stderr.write('Could not launch python executable!\\n')
  sys.exit(2)

sys.path[0] = os.path.abspath(sys.path[0])
sys.path.insert(0, os.path.abspath(os.path.join(__entry_point__, '.bootstrap')))

from _twitter_common_python.pex_bootstrapper import bootstrap_pex
bootstrap_pex(__entry_point__)
"""


class PEXBuilder(object):
  class InvalidDependency(Exception): pass
  class InvalidExecutableSpecification(Exception): pass

  BOOTSTRAP_DIR = ".bootstrap"

  def __init__(self, path=None, interpreter=None, chroot=None, pex_info=None):
    self._chroot = chroot or Chroot(path or tempfile.mkdtemp())
    self._pex_info = pex_info or PexInfo.default()
    self._frozen = False
    self._interpreter = interpreter or PythonInterpreter.get()
    self._logger = logging.getLogger(__name__)

  @property
  def interpreter(self):
    return self._interpreter

  def chroot(self):
    return self._chroot

  def clone(self, into=None):
    chroot_clone = self._chroot.clone(into=into)
    return PEXBuilder(chroot=chroot_clone, interpreter=self._interpreter,
                      pex_info=self._pex_info.copy())

  def path(self):
    return self.chroot().path()

  @property
  def info(self):
    return self._pex_info

  @info.setter
  def info(self, value):
    if not isinstance(value, PexInfo):
      raise TypeError('PEXBuilder.info must be a PexInfo!')
    self._pex_info = value

  def add_source(self, filename, env_filename):
    self._chroot.link(filename, env_filename, "source")
    if filename.endswith('.py'):
      env_filename_pyc = os.path.splitext(env_filename)[0] + '.pyc'
      with open(filename) as fp:
        pyc_object = CodeMarshaller.from_py(fp.read(), env_filename)
      self._chroot.write(pyc_object.to_pyc(), env_filename_pyc, 'source')

  def add_resource(self, filename, env_filename):
    self._chroot.link(filename, env_filename, "resource")

  def add_requirement(self, req, dynamic=False, repo=None):
    self._pex_info.add_requirement(req, repo=repo, dynamic=dynamic)

  def set_entry_point(self, entry_point):
    self.info.entry_point = entry_point

  def add_dist_location(self, bdist):
    dist = DistributionHelper.distribution_from_path(bdist)
    self.add_distribution(dist)
    self.add_requirement(dist.as_requirement(), dynamic=False, repo=None)

  def add_egg(self, egg):
    return self.add_dist_location(egg)

  def _add_dist_dir(self, path, dist_name):
    for root, _, files in os.walk(path):
      for f in files:
        filename = os.path.join(root, f)
        relpath = os.path.relpath(filename, path)
        target = os.path.join(self._pex_info.internal_cache, dist_name, relpath)
        self._chroot.link(filename, target)
    return CacheHelper.dir_hash(path)

  def _add_dist_zip(self, path, dist_name):
    with open_zip(path) as zf:
      for name in zf.namelist():
        if name.endswith('/'):
          continue
        target = os.path.join(self._pex_info.internal_cache, dist_name, name)
        self._chroot.write(zf.read(name), target)
      return CacheHelper.zip_hash(zf)

  def _prepare_code_hash(self):
    self._pex_info.code_hash = CacheHelper.pex_hash(self._chroot.path())

  def add_distribution(self, dist, dist_name=None):
    dist_name = dist_name or os.path.basename(dist.location)

    if os.path.isdir(dist.location):
      dist_hash = self._add_dist_dir(dist.location, dist_name)
    else:
      dist_hash = self._add_dist_zip(dist.location, dist_name)

    # add dependency key so that it can rapidly be retrieved from cache
    self._pex_info.add_distribution(dist_name, dist_hash)

  def set_executable(self, filename, env_filename=None):
    if env_filename is None:
      env_filename = os.path.basename(filename)
    if self._chroot.get("executable"):
      raise PEXBuilder.InvalidExecutableSpecification(
          "Setting executable on a PEXBuilder that already has one!")
    self._chroot.link(filename, env_filename, "executable")
    entry_point = env_filename
    entry_point.replace(os.path.sep, '.')
    self._pex_info.entry_point = entry_point.rpartition('.')[0]

  # TODO(wickman) Consider changing this behavior to put the onus on the consumer
  # of twitter.common.python to write the pex sources correctly.
  def _prepare_inits(self):
    relative_digest = self._chroot.get("source")
    init_digest = set()
    for path in relative_digest:
      split_path = path.split(os.path.sep)
      for k in range(1, len(split_path)):
        sub_path = os.path.sep.join(split_path[0:k] + ['__init__.py'])
        if sub_path not in relative_digest and sub_path not in init_digest:
          self._chroot.write("__import__('pkg_resources').declare_namespace(__name__)",
              sub_path)
          init_digest.add(sub_path)

  def _prepare_manifest(self):
    self._chroot.write(self._pex_info.dump().encode('utf-8'), PexInfo.PATH, label='manifest')

  def _prepare_main(self):
    self._chroot.write(BOOTSTRAP_ENVIRONMENT, '__main__.py', label='main')

  # TODO(wickman) Ideally we unqualify our setuptools dependency and inherit whatever is
  # bundled into the environment so long as it is compatible (and error out if not.)
  #
  # As it stands, we're picking and choosing the pieces we think we need, which means
  # if there are bits of setuptools imported from elsewhere they may be incompatible with
  # this.
  def _prepare_bootstrap(self):
    """
      Write enough of distribute into the .pex .bootstrap directory so that
      we can be fully self-contained.
    """
    wrote_setuptools = False
    setuptools = DistributionHelper.distribution_from_path(
        self._interpreter.get_location('setuptools'),
        name='setuptools')

    if setuptools is None:
      raise RuntimeError('Failed to find setuptools while building pex!')

    for fn, content_stream in DistributionHelper.walk_data(setuptools):
      if fn == 'pkg_resources.py' or fn.startswith('_markerlib'):
        self._chroot.write(content_stream.read(), os.path.join(self.BOOTSTRAP_DIR, fn), 'resource')
        wrote_setuptools = True

    if not wrote_setuptools:
      raise RuntimeError(
          'Failed to extract pkg_resources from setuptools.  Perhaps pants was linked with an '
          'incompatible setuptools.')

    libraries = (
      'twitter.common.python',
      'twitter.common.python.http',
    )

    for name in libraries:
      dirname = name.replace('twitter.common.python', '_twitter_common_python').replace('.', '/')
      provider = get_provider(name)
      if not isinstance(provider, DefaultProvider):
        mod = __import__(name, fromlist=['wutttt'])
        provider = ZipProvider(mod)
      for fn in provider.resource_listdir(''):
        if fn.endswith('.py'):
          self._chroot.write(provider.get_resource_string(name, fn),
            os.path.join(self.BOOTSTRAP_DIR, dirname, fn), 'resource')

  def freeze(self):
    if self._frozen:
      return
    self._prepare_inits()
    self._prepare_code_hash()
    self._prepare_manifest()
    self._prepare_bootstrap()
    self._prepare_main()
    self._frozen = True

  def build(self, filename):
    self.freeze()
    try:
      os.unlink(filename + '~')
      self._logger.warn('Previous binary unexpectedly exists, cleaning: %s' % (filename + '~'))
    except OSError:
      # The expectation is that the file does not exist, so continue
      pass
    if os.path.dirname(filename):
      safe_mkdir(os.path.dirname(filename))
    with open(filename + '~', 'ab') as pexfile:
      assert os.path.getsize(pexfile.name) == 0
      pexfile.write(to_bytes('%s\n' % self._interpreter.identity.hashbang()))
    self._chroot.zip(filename + '~', mode='a')
    if os.path.exists(filename):
      os.unlink(filename)
    os.rename(filename + '~', filename)
    chmod_plus_x(filename)

########NEW FILE########
__FILENAME__ = pex_info
from __future__ import absolute_import, print_function

from collections import namedtuple
import json
import os
import sys

from .common import open_zip
from .orderedset import OrderedSet

PexRequirement = namedtuple('PexRequirement', 'requirement repo dynamic')
PexPlatform = namedtuple('PexPlatform', 'interpreter version strict')


class PexInfo(object):
  """
    PEX metadata.

    # Build metadata:
    build_properties: BuildProperties # (key-value information about the build system)
    code_hash: str                    # sha1 hash of all names/code in the archive
    distributions: {dist_name: str}   # map from distribution name (i.e. path in
                                      # the internal cache) to its cache key (sha1)

    # Environment options
    pex_root: ~/.pex                   # root of all pex-related files
    entry_point: string                # entry point into this pex
    zip_safe: True, default False      # is this pex zip safe?
    inherit_path: True, default False  # should this pex inherit site-packages + PYTHONPATH?
    ignore_errors: True, default False # should we ignore inability to resolve dependencies?
    always_write_cache: False          # should we always write the internal cache to disk first?
                                       # this is useful if you have very large dependencies that
                                       # do not fit in RAM constrained environments
    requirements: list                 # list of PexRequirement tuples:
                                       #   [requirement, repository, dynamic]
  """

  PATH = 'PEX-INFO'
  INTERNAL_CACHE = '.deps'

  @classmethod
  def make_build_properties(cls):
    from .interpreter import PythonInterpreter
    from pkg_resources import get_platform

    pi = PythonInterpreter.get()
    return {
      'class': pi.identity.interpreter,
      'version': pi.identity.version,
      'platform': get_platform(),
    }

  @classmethod
  def default(cls):
    pex_info = {
      'requirements': [],
      'distributions': {},
      'always_write_cache': False,
      'build_properties': cls.make_build_properties(),
    }
    return cls(info=pex_info)

  @classmethod
  def from_pex(cls, pex):
    if os.path.isfile(pex):
      with open_zip(pex) as zf:
        pex_info = zf.read(cls.PATH)
    else:
      with open(os.path.join(pex, cls.PATH)) as fp:
        pex_info = fp.read()
    return cls.from_json(pex_info)

  @classmethod
  def from_json(cls, content):
    if isinstance(content, bytes):
      content = content.decode('utf-8')
    return PexInfo(info=json.loads(content))

  @classmethod
  def debug(cls, msg):
    if 'PEX_VERBOSE' in os.environ:
      print('PEX: %s' % msg, file=sys.stderr)

  def __init__(self, info=None):
    if info is not None and not isinstance(info, dict):
      raise ValueError('PexInfo can only be seeded with a dict, got: '
                       '%s of type %s' % (info, type(info)))
    self._pex_info = info or {}
    self._distributions = self._pex_info.get('distributions', {})
    self._requirements = OrderedSet(
        PexRequirement(*req) for req in self._pex_info.get('requirements', []))
    self._repositories = OrderedSet(self._pex_info.get('repositories', []))
    self._indices = OrderedSet(self._pex_info.get('indices', []))

  @property
  def build_properties(self):
    return self._pex_info.get('build_properties', {})

  @build_properties.setter
  def build_properties(self, value):
    if not isinstance(value, dict):
      raise TypeError('build_properties must be a dictionary!')
    self._pex_info['build_properties'] = self.make_build_properties()
    self._pex_info['build_properties'].update(value)

  @property
  def zip_safe(self):
    if 'PEX_FORCE_LOCAL' in os.environ:
      self.debug('PEX_FORCE_LOCAL forcing zip_safe to False')
      return False
    return self._pex_info.get('zip_safe', True)

  @zip_safe.setter
  def zip_safe(self, value):
    self._pex_info['zip_safe'] = bool(value)

  @property
  def inherit_path(self):
    if 'PEX_INHERIT_PATH' in os.environ:
      self.debug('PEX_INHERIT_PATH override detected')
      return True
    else:
      return self._pex_info.get('inherit_path', False)

  @inherit_path.setter
  def inherit_path(self, value):
    self._pex_info['inherit_path'] = bool(value)

  @property
  def ignore_errors(self):
    return self._pex_info.get('ignore_errors', False)

  @ignore_errors.setter
  def ignore_errors(self, value):
    self._pex_info['ignore_errors'] = bool(value)

  @property
  def code_hash(self):
    return self._pex_info.get('code_hash')

  @code_hash.setter
  def code_hash(self, value):
    self._pex_info['code_hash'] = value

  @property
  def entry_point(self):
    if 'PEX_MODULE' in os.environ:
      self.debug('PEX_MODULE override detected: %s' % os.environ['PEX_MODULE'])
      return os.environ['PEX_MODULE']
    return self._pex_info.get('entry_point')

  @entry_point.setter
  def entry_point(self, value):
    self._pex_info['entry_point'] = value

  def add_requirement(self, requirement, repo=None, dynamic=False):
    self._requirements.add(PexRequirement(str(requirement), repo, dynamic))

  @property
  def requirements(self):
    return self._requirements

  def add_distribution(self, location, sha):
    self._distributions[location] = sha

  def add_repository(self, repository):
    self._repositories.add(repository)

  def add_index(self, index):
    self._indices.add(index)

  @property
  def distributions(self):
    return self._distributions

  @property
  def always_write_cache(self):
    if 'PEX_ALWAYS_CACHE' in os.environ:
      self.debug('PEX_ALWAYS_CACHE override detected: %s' % os.environ['PEX_ALWAYS_CACHE'])
      return True
    return self._pex_info.get('always_write_cache', False)

  @always_write_cache.setter
  def always_write_cache(self, value):
    self._pex_info['always_write_cache'] = bool(value)

  @property
  def pex_root(self):
    pex_root = self._pex_info.get('pex_root', os.path.join('~', '.pex'))
    return os.path.expanduser(os.environ.get('PEX_ROOT', pex_root))

  @pex_root.setter
  def pex_root(self, value):
    self._pex_info['pex_root'] = value

  @property
  def internal_cache(self):
    return self.INTERNAL_CACHE

  @property
  def install_cache(self):
    return os.path.join(self.pex_root, 'install')

  @property
  def zip_unsafe_cache(self):
    return os.path.join(self.pex_root, 'code')

  def dump(self):
    pex_info_copy = self._pex_info.copy()
    pex_info_copy['requirements'] = list(self._requirements)
    return json.dumps(pex_info_copy)

  def copy(self):
    return PexInfo(info=self._pex_info.copy())

########NEW FILE########
__FILENAME__ = platforms
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import absolute_import

import re
import sys

from pkg_resources import compatible_platforms, get_supported_platform


class Platform(object):
  class UnknownPlatformError(Exception):
    def __init__(self, platform):
      super(Platform.UnknownPlatformError, self).__init__('Unknown platform: %s' % platform)

  # It blows my mind this code is not in distutils or distribute.
  MACOSX_VERSION_STRING = re.compile(r"macosx-(\d+)\.(\d+)-(\S+)")
  MACOSX_PLATFORM_COMPATIBILITY = {
    'i386'      : ('i386',),
    'ppc'       : ('ppc',),
    'x86_64'    : ('x86_64',),
    'ppc64'     : ('ppc64',),
    'fat'       : ('i386', 'ppc'),
    'intel'     : ('i386', 'x86_64'),
    'fat3'      : ('i386', 'ppc', 'x86_64'),
    'fat64'     : ('ppc64', 'x86_64'),
    'universal' : ('i386', 'ppc', 'ppc64', 'x86_64')
  }

  @staticmethod
  def current():
    return get_supported_platform()

  @staticmethod
  def python():
    return sys.version[:3]

  @staticmethod
  def compatible(package, platform):
    if package is None or platform is None or package == platform:
      return True
    MAJOR, MINOR, PLATFORM = range(1, 4)
    package_match = Platform.MACOSX_VERSION_STRING.match(package)
    platform_match = Platform.MACOSX_VERSION_STRING.match(platform)
    if not (package_match and platform_match):
      return compatible_platforms(package, platform)
    if package_match.group(MAJOR) != platform_match.group(MAJOR):
      return False
    if int(package_match.group(MINOR)) > int(platform_match.group(MINOR)):
      return False
    package_platform = package_match.group(PLATFORM)
    if package_platform not in Platform.MACOSX_PLATFORM_COMPATIBILITY:
      raise Platform.UnknownPlatformError(package_platform)
    sys_platform = platform_match.group(PLATFORM)
    if sys_platform not in Platform.MACOSX_PLATFORM_COMPATIBILITY:
      raise Platform.UnknownPlatformError(sys_platform)
    package_compatibility = set(Platform.MACOSX_PLATFORM_COMPATIBILITY[package_platform])
    system_compatibility = set(Platform.MACOSX_PLATFORM_COMPATIBILITY[sys_platform])
    return bool(package_compatibility.intersection(system_compatibility))

  @staticmethod
  def version_compatible(package_py_version, py_version):
    return package_py_version is None or py_version is None or package_py_version == py_version

########NEW FILE########
__FILENAME__ = resolver
from __future__ import print_function

from collections import defaultdict

from .base import maybe_requirement_list
from .interpreter import PythonInterpreter
from .obtainer import Obtainer
from .orderedset import OrderedSet
from .package import Package, distribution_compatible
from .platforms import Platform

from pkg_resources import Distribution


class Untranslateable(Exception):
  pass


class Unsatisfiable(Exception):
  pass


class _DistributionCache(object):
  _ERROR_MSG = 'Expected %s but got %s'

  def __init__(self):
    self._translated_packages = {}

  def has(self, package):
    if not isinstance(package, Package):
      raise ValueError(self._ERROR_MSG % (Package, package))
    return package in self._translated_packages

  def put(self, package, distribution):
    if not isinstance(package, Package):
      raise ValueError(self._ERROR_MSG % (Package, package))
    if not isinstance(distribution, Distribution):
      raise ValueError(self._ERROR_MSG % (Distribution, distribution))
    self._translated_packages[package] = distribution

  def get(self, package):
    if not isinstance(package, Package):
      raise ValueError(self._ERROR_MSG % (Package, package))
    return self._translated_packages[package]


def resolve(requirements, obtainer=None, interpreter=None, platform=None):
  """List all distributions needed to (recursively) meet `requirements`

  When resolving dependencies, multiple (potentially incompatible) requirements may be encountered.
  Handle this situation by iteratively filtering a set of potential project
  distributions by new requirements, and finally choosing the highest version meeting all
  requirements, or raise an error indicating unsatisfiable requirements.

  Note: should `pkg_resources.WorkingSet.resolve` correctly handle multiple requirements in the
  future this should go away in favor of using what setuptools provides.

  :returns: List of :class:`pkg_resources.Distribution` instances meeting `requirements`.
  """
  cache = _DistributionCache()
  interpreter = interpreter or PythonInterpreter.get()
  platform = platform or Platform.current()
  obtainer = obtainer or Obtainer.default(platform=platform, interpreter=interpreter)

  requirements = maybe_requirement_list(requirements)
  distribution_set = defaultdict(list)
  requirement_set = defaultdict(list)
  processed_requirements = set()

  def packages(requirement, existing=None):
    if existing is None:
      existing = obtainer.iter(requirement)
    return [package for package in existing
            if package.satisfies(requirement)
            and package.compatible(interpreter.identity, platform)]

  def requires(package, requirement):
    if not cache.has(package):
      dist = obtainer.obtain(package)
      if dist is None:
        raise Untranslateable('Package %s is not translateable.' % package)
      if not distribution_compatible(dist, interpreter, platform):
        raise Untranslateable('Could not get distribution for %s on appropriate platform.' %
            package)
      cache.put(package, dist)
    dist = cache.get(package)
    return dist.requires(extras=requirement.extras)

  while True:
    while requirements:
      requirement = requirements.pop(0)
      requirement_set[requirement.key].append(requirement)
      # TODO(wickman) This is trivially parallelizable
      distribution_list = distribution_set[requirement.key] = packages(
          requirement,
          existing=distribution_set.get(requirement.key))
      if not distribution_list:
        raise Unsatisfiable('Cannot satisfy requirements: %s' % requirement_set[requirement.key])

    # get their dependencies
    for requirement_key, requirement_list in requirement_set.items():
      new_requirements = OrderedSet()
      highest_package = distribution_set[requirement_key][0]
      for requirement in requirement_list:
        if requirement in processed_requirements:
          continue
        new_requirements.update(requires(highest_package, requirement))
        processed_requirements.add(requirement)
      requirements.extend(list(new_requirements))

    if not requirements:
      break

  to_activate = set()
  for distributions in distribution_set.values():
    to_activate.add(cache.get(distributions[0]))
  return to_activate

########NEW FILE########
__FILENAME__ = testing
import contextlib
import os
import random
import tempfile
from textwrap import dedent
import zipfile

from .common import safe_mkdir, safe_mkdtemp, safe_rmtree
from .installer import EggInstaller
from .util import DistributionHelper


@contextlib.contextmanager
def temporary_dir():
  td = tempfile.mkdtemp()
  try:
    yield td
  finally:
    safe_rmtree(td)


@contextlib.contextmanager
def create_layout(*filelist):
  with temporary_dir() as td:
    for fl in filelist:
      for fn in fl:
        with open(os.path.join(td, fn), 'w') as fp:
          fp.write('junk')
    yield td


def random_bytes(length):
  return ''.join(
      map(chr, (random.randint(ord('a'), ord('z')) for _ in range(length)))).encode('utf-8')


@contextlib.contextmanager
def temporary_content(content_map, interp=None, seed=31337):
  """Write content to disk where content is map from string => (int, string).

     If target is int, write int random bytes.  Otherwise write contents of string."""
  random.seed(seed)
  interp = interp or {}
  with temporary_dir() as td:
    for filename, size_or_content in content_map.items():
      safe_mkdir(os.path.dirname(os.path.join(td, filename)))
      with open(os.path.join(td, filename), 'wb') as fp:
        if isinstance(size_or_content, int):
          fp.write(random_bytes(size_or_content))
        else:
          fp.write((size_or_content % interp).encode('utf-8'))
    yield td


def yield_files(directory):
  for root, _, files in os.walk(directory):
    for f in files:
      filename = os.path.join(root, f)
      rel_filename = os.path.relpath(filename, directory)
      yield filename, rel_filename


def write_zipfile(directory, dest, reverse=False):
  with contextlib.closing(zipfile.ZipFile(dest, 'w')) as zf:
    for filename, rel_filename in sorted(yield_files(directory), reverse=reverse):
      zf.write(filename, arcname=rel_filename)
  return dest


PROJECT_CONTENT = {
  'setup.py': dedent('''
      from setuptools import setup

      setup(
          name=%(project_name)r,
          version='0.0.0',
          zip_safe=%(zip_safe)r,
          packages=['my_package'],
          package_data={'my_package': ['package_data/*.dat']},
      )
  '''),
  'my_package/__init__.py': 0,
  'my_package/my_module.py': 'def do_something():\n  print("hello world!")\n',
  'my_package/package_data/resource1.dat': 1000,
  'my_package/package_data/resource2.dat': 1000,
}


@contextlib.contextmanager
def make_distribution(name='my_project', installer_impl=EggInstaller, zipped=False, zip_safe=True):
  interp = {'project_name': name, 'zip_safe': zip_safe}
  with temporary_content(PROJECT_CONTENT, interp=interp) as td:
    installer = installer_impl(td)
    dist_location = installer.bdist()
    if zipped:
      yield DistributionHelper.distribution_from_path(dist_location)
    else:
      with temporary_dir() as td:
        extract_path = os.path.join(td, os.path.basename(dist_location))
        with contextlib.closing(zipfile.ZipFile(dist_location)) as zf:
          zf.extractall(extract_path)
        yield DistributionHelper.distribution_from_path(extract_path)

########NEW FILE########
__FILENAME__ = tracer
from contextlib import contextmanager
import os
import sys
import threading
import time

__all__ = ('Tracer',)


class Trace(object):
  __slots__ = ('msg', 'verbosity', 'parent', 'children', '_clock', '_start', '_stop')
  def __init__(self, msg, parent=None, verbosity=1, clock=time):
    self.msg = msg
    self.verbosity = verbosity
    self.parent = parent
    if parent is not None:
      parent.children.append(self)
    self.children = []
    self._clock = clock
    self._start = self._clock.time()
    self._stop = None

  def stop(self):
    self._stop = self._clock.time()

  def duration(self):
    assert self._stop is not None
    return self._stop - self._start


class Tracer(object):
  """
    A multi-threaded tracer.
  """
  _ENV_OVERRIDES = {}

  @classmethod
  @contextmanager
  def env_override(cls, **override_dict):
    OLD_ENV = cls._ENV_OVERRIDES.copy()
    cls._ENV_OVERRIDES.update(**override_dict)
    yield
    cls._ENV_OVERRIDES = OLD_ENV

  @classmethod
  def env_filter(cls, env_variable):
    def predicate(verbosity):
      try:
        env_verbosity = int(os.environ.get(env_variable, cls._ENV_OVERRIDES.get(env_variable, -1)))
      except ValueError:
        env_verbosity = -1
      return verbosity <= env_verbosity
    return predicate

  def __init__(self, predicate=None, output=sys.stderr, clock=time, prefix=''):
    """
      If predicate specified, it should take a "verbosity" integer and determine whether
      or not to log, e.g.

        def predicate(verbosity):
          try:
            return verbosity < int(os.environ.get('APP_VERBOSITY', 0))
          except ValueError:
            return False

      output defaults to sys.stderr, but can take any file-like object.
    """
    self._predicate = predicate or (lambda verbosity: True)
    self._length = None
    self._output = output
    self._isatty = getattr(output, 'isatty', False) and output.isatty()
    self._lock = threading.RLock()
    self._local = threading.local()
    self._clock = clock
    self._prefix = prefix

  def should_log(self, V):
    return self._predicate(V)

  def log(self, msg, V=0, end='\n'):
    if not self.should_log(V):
      return
    if not self._isatty and end == '\r':
      # force newlines if we're not a tty
      end = '\n'
    trailing_whitespace = ''
    with self._lock:
      if self._length and self._length > (len(self._prefix) + len(msg)):
        trailing_whitespace = ' ' * (self._length - len(msg) - len(self._prefix))
      self._output.write(''.join([self._prefix, msg, trailing_whitespace, end]))
      self._output.flush()
      self._length = (len(self._prefix) + len(msg)) if end == '\r' else 0

  def print_trace_snippet(self):
    parent = self._local.parent
    parent_verbosity = parent.verbosity
    if not self.should_log(parent_verbosity):
      return
    traces = []
    while parent:
      if self.should_log(parent.verbosity):
        traces.append(parent.msg)
      parent = parent.parent
    self.log(' :: '.join(reversed(traces)), V=parent_verbosity, end='\r')

  def print_trace(self, indent=0, node=None):
    node = node or self._local.parent
    with self._lock:
      self.log(' ' * indent + ('%s: %.1fms' % (node.msg, 1000.0 * node.duration())),
               V=node.verbosity)
      for child in node.children:
        self.print_trace(indent=indent + 2, node=child)

  @contextmanager
  def timed(self, msg, V=0):
    if getattr(self._local, 'parent', None) is None:
      self._local.parent = Trace(msg, verbosity=V, clock=self._clock)
    else:
      parent = self._local.parent
      self._local.parent = Trace(msg, parent=parent, verbosity=V, clock=self._clock)
    self.print_trace_snippet()
    yield
    self._local.parent.stop()
    if self._local.parent.parent is not None:
      self._local.parent = self._local.parent.parent
    else:
      self.print_trace()
      self._local.parent = None


TRACER = Tracer(predicate=Tracer.env_filter('PYTHON_VERBOSE'), prefix='twitter.common.python: ')

########NEW FILE########
__FILENAME__ = translator
from __future__ import absolute_import

from abc import abstractmethod
import os
import shutil

from .common import chmod_plus_w, safe_rmtree, safe_mkdir, safe_mkdtemp
from .compatibility import AbstractClass
from .installer import WheelInstaller
from .interpreter import PythonInterpreter
from .package import (
    EggPackage,
    Package,
    SourcePackage,
    WheelPackage,
)
from .platforms import Platform
from .tracer import TRACER
from .util import DistributionHelper


class TranslatorBase(AbstractClass):
  """
    Translate a link into a distribution.
  """
  @abstractmethod
  def translate(self, link):
    pass


class ChainedTranslator(TranslatorBase):
  """
    Glue a sequence of Translators together in priority order.  The first Translator to resolve a
    requirement wins.
  """
  def __init__(self, *translators):
    self._translators = list(filter(None, translators))
    for tx in self._translators:
      if not isinstance(tx, TranslatorBase):
        raise ValueError('Expected a sequence of translators, got %s instead.' % type(tx))

  def translate(self, package):
    for tx in self._translators:
      dist = tx.translate(package)
      if dist:
        return dist


class SourceTranslator(TranslatorBase):
  @classmethod
  def run_2to3(cls, path):
    from lib2to3.refactor import get_fixers_from_package, RefactoringTool
    rt = RefactoringTool(get_fixers_from_package('lib2to3.fixes'))
    with TRACER.timed('Translating %s' % path):
      for root, dirs, files in os.walk(path):
        for fn in files:
          full_fn = os.path.join(root, fn)
          if full_fn.endswith('.py'):
            with TRACER.timed('%s' % fn, V=3):
              try:
                chmod_plus_w(full_fn)
                rt.refactor_file(full_fn, write=True)
              except IOError as e:
                TRACER.log('Failed to translate %s: %s' % (fn, e))

  def __init__(self,
               install_cache=None,
               interpreter=PythonInterpreter.get(),
               platform=Platform.current(),
               use_2to3=False,
               conn_timeout=None,
               installer_impl=WheelInstaller):
    self._interpreter = interpreter
    self._installer_impl = installer_impl
    self._use_2to3 = use_2to3
    self._install_cache = install_cache or safe_mkdtemp()
    safe_mkdir(self._install_cache)
    self._conn_timeout = conn_timeout
    self._platform = platform

  def translate(self, package):
    """From a SourcePackage, translate to a binary distribution."""
    if not isinstance(package, SourcePackage):
      return None

    unpack_path, installer = None, None
    version = self._interpreter.version

    try:
      unpack_path = package.fetch(conn_timeout=self._conn_timeout)
    except package.UnreadableLink as e:
      TRACER.log('Failed to fetch %s: %s' % (package, e))
      return None

    try:
      if self._use_2to3 and version >= (3,):
        with TRACER.timed('Translating 2->3 %s' % package.name):
          self.run_2to3(unpack_path)
      installer = self._installer_impl(
          unpack_path,
          interpreter=self._interpreter,
          strict=(package.name not in ('distribute', 'setuptools')))
      with TRACER.timed('Packaging %s' % package.name):
        try:
          dist_path = installer.bdist()
        except self._installer_impl.InstallFailure:
          return None
        target_path = os.path.join(self._install_cache, os.path.basename(dist_path))
        # TODO: Make this atomic.
        shutil.move(dist_path, target_path)
        target_package = Package.from_href(target_path)
        if not target_package:
          return None
        if not target_package.compatible(self._interpreter.identity, platform=self._platform):
          return None
        return DistributionHelper.distribution_from_path(target_path)
    finally:
      if installer:
        installer.cleanup()
      if unpack_path:
        safe_rmtree(unpack_path)


class BinaryTranslator(TranslatorBase):
  def __init__(self,
               package_type,
               install_cache=None,
               interpreter=PythonInterpreter.get(),
               platform=Platform.current(),
               conn_timeout=None):
    self._package_type = package_type
    self._install_cache = install_cache or safe_mkdtemp()
    self._platform = platform
    self._identity = interpreter.identity
    self._conn_timeout = conn_timeout

  def translate(self, package):
    """From a binary package, translate to a local binary distribution."""
    if not isinstance(package, self._package_type):
      return None
    if not package.compatible(identity=self._identity, platform=self._platform):
      return None
    try:
      bdist = package.fetch(location=self._install_cache, conn_timeout=self._conn_timeout)
    except package.UnreadableLink as e:
      TRACER.log('Failed to fetch %s: %s' % (package, e))
      return None
    return DistributionHelper.distribution_from_path(bdist)


class EggTranslator(BinaryTranslator):
  def __init__(self, **kw):
    super(EggTranslator, self).__init__(EggPackage, **kw)


class WheelTranslator(BinaryTranslator):
  def __init__(self, **kw):
    super(WheelTranslator, self).__init__(WheelPackage, **kw)


class Translator(object):
  @staticmethod
  def default(install_cache=None,
              platform=Platform.current(),
              interpreter=None,
              conn_timeout=None):

    # TODO(wickman) Consider interpreter=None to indicate "universal" packages
    # since the .whl format can support this.
    # Also consider platform=None to require platform-inspecific packages.
    interpreter = interpreter or PythonInterpreter.get()

    shared_options = dict(
        install_cache=install_cache,
        interpreter=interpreter,
        conn_timeout=conn_timeout)

    whl_translator = WheelTranslator(platform=platform, **shared_options)
    egg_translator = EggTranslator(platform=platform, **shared_options)
    source_translator = SourceTranslator(**shared_options)
    return ChainedTranslator(whl_translator, egg_translator, source_translator)

########NEW FILE########
__FILENAME__ = util
from __future__ import absolute_import

import contextlib
import errno
from hashlib import sha1
import os
import shutil
import uuid

from pkg_resources import find_distributions

from .common import safe_open, safe_rmtree
from .finders import register_finders


class DistributionHelper(object):
  @staticmethod
  def walk_data(dist, path='/'):
    """Yields filename, stream for files identified as data in the distribution"""
    for rel_fn in filter(None, dist.resource_listdir(path)):
      full_fn = os.path.join(path, rel_fn)
      if dist.resource_isdir(full_fn):
        for fn, stream in DistributionHelper.walk_data(dist, full_fn):
          yield fn, stream
      else:
        yield full_fn[1:], dist.get_resource_stream(dist._provider, full_fn)

  @staticmethod
  def zipsafe(dist):
    """Returns whether or not we determine a distribution is zip-safe."""
    # zip-safety is only an attribute of eggs.  wheels are considered never
    # zip safe per implications of PEP 427.
    if hasattr(dist, 'egg_info') and dist.egg_info.endswith('EGG-INFO'):
      egg_metadata = dist.metadata_listdir('')
      return 'zip-safe' in egg_metadata and 'native_libs.txt' not in egg_metadata
    else:
      return False

  @classmethod
  def distribution_from_path(cls, path, name=None):
    """Return a distribution from a path.

    If name is provided, find the distribution.  If none is found matching the name,
    return None.  If name is not provided and there is unambiguously a single
    distribution, return that distribution otherwise None.
    """
    # Monkeypatch pkg_resources finders should it not already be so.
    register_finders()
    if name is None:
      distributions = list(find_distributions(path))
      if len(distributions) == 1:
        return distributions[0]
    else:
      for dist in find_distributions(path):
        if dist.project_name == name:
          return dist


class CacheHelper(object):
  @classmethod
  def update_hash(cls, filelike, digest):
    """Update the digest of a single file in a memory-efficient manner."""
    block_size = digest.block_size * 1024
    for chunk in iter(lambda: filelike.read(block_size), b''):
      digest.update(chunk)

  @classmethod
  def hash(cls, path, digest=None, hasher=sha1):
    """Return the digest of a single file in a memory-efficient manner."""
    if digest is None:
      digest = hasher()
    with open(path, 'rb') as fh:
      cls.update_hash(fh, digest)
    return digest.hexdigest()

  @classmethod
  def _compute_hash(cls, names, stream_factory):
    digest = sha1()
    digest.update(''.join(names).encode('utf-8'))
    for name in names:
      with contextlib.closing(stream_factory(name)) as fp:
        cls.update_hash(fp, digest)
    return digest.hexdigest()

  @classmethod
  def zip_hash(cls, zf, prefix=''):
    """Return the hash of the contents of a zipfile, comparable with a cls.dir_hash."""
    prefix_length = len(prefix)
    names = sorted(name[prefix_length:] for name in zf.namelist()
        if name.startswith(prefix) and not name.endswith('.pyc') and not name.endswith('/'))
    def stream_factory(name):
      return zf.open(prefix + name)
    return cls._compute_hash(names, stream_factory)

  @classmethod
  def _iter_files(cls, directory):
    normpath = os.path.normpath(directory)
    for root, _, files in os.walk(normpath):
      for f in files:
        yield os.path.relpath(os.path.join(root, f), normpath)

  @classmethod
  def pex_hash(cls, d):
    """Return a reproducible hash of the contents of a directory."""
    names = sorted(f for f in cls._iter_files(d) if not (f.endswith('.pyc') or f.startswith('.')))
    def stream_factory(name):
      return open(os.path.join(d, name), 'rb')
    return cls._compute_hash(names, stream_factory)

  @classmethod
  def dir_hash(cls, d):
    """Return a reproducible hash of the contents of a directory."""
    names = sorted(f for f in cls._iter_files(d) if not f.endswith('.pyc'))
    def stream_factory(name):
      return open(os.path.join(d, name), 'rb')
    return cls._compute_hash(names, stream_factory)

  @classmethod
  def cache_distribution(cls, zf, source, target_dir):
    """Possibly cache an egg from within a zipfile into target_cache.

       Given a zipfile handle and a filename corresponding to an egg distribution within
       that zip, maybe write to the target cache and return a Distribution."""
    dependency_basename = os.path.basename(source)
    if not os.path.exists(target_dir):
      target_dir_tmp = target_dir + '.' + uuid.uuid4().hex
      for name in zf.namelist():
        if name.startswith(source) and not name.endswith('/'):
          # strip off prefix + '/'
          target_name = os.path.join(dependency_basename, name[len(source) + 1:])
          with contextlib.closing(zf.open(name)) as zi:
            with safe_open(os.path.join(target_dir_tmp, target_name), 'wb') as fp:
              shutil.copyfileobj(zi, fp)
      try:
        os.rename(target_dir_tmp, target_dir)
      except OSError as e:
        if e.errno == errno.ENOTEMPTY:
          safe_rmtree(target_dir_tmp)
        else:
          raise

    dist = DistributionHelper.distribution_from_path(target_dir)
    assert dist is not None, 'Failed to cache distribution %s' % source
    return dist

########NEW FILE########
__FILENAME__ = parse_simple
from optparse import OptionValueError

from twitter.common.lang import Compatibility
from twitter.common.quantity import Data, Time, Amount


class InvalidTime(ValueError):
  def __init__(self, timestring):
    ValueError.__init__(self, "Invalid time span: %s" % timestring)


def parse_time(timestring):
  """
    Parse a time string of the format
      XdYhZmWs (each field optional but must be in that order.)
  """
  if not isinstance(timestring, Compatibility.string):
    raise TypeError('timestring should be of type string')
  BASES = (('d', Time.DAYS), ('h', Time.HOURS), ('m', Time.MINUTES), ('s', Time.SECONDS))
  timestr = timestring.lower()
  total_time = Amount(0, Time.SECONDS)
  for base_char, base in BASES:
    timesplit = timestr.split(base_char)
    if len(timesplit) > 2:
      raise InvalidTime(timestring)
    if len(timesplit) == 2:
      try:
        amount = int(timesplit[0])
      except ValueError:
        raise InvalidTime(timestring)
      total_time = total_time + Amount(amount, base)
      timestr = timesplit[1]
  if len(timestr) != 0:
    raise InvalidTime(timestring)
  return total_time




class InvalidData(ValueError):
  def __init__(self, datastring):
    ValueError.__init__(self, "Invalid size: %s" % datastring)


def parse_data(datastring):
  """
    Parse a data string of the format:
      [integer][unit]
    where unit is in upper/lowercase k, kb, m, mb, g, gb, t, tb
  """
  if not isinstance(datastring, Compatibility.string):
    raise InvalidData('parse_data takes a string, got %s' % type(datastring))

  datastring = datastring.strip().lower()

  try:
    return Amount(int(datastring), Data.BYTES)
  except ValueError:
    pass

  BASES = { 'k': Data.KB,
            'kb': Data.KB,
            'm': Data.MB,
            'mb': Data.MB,
            'g': Data.GB,
            'gb': Data.GB,
            't': Data.TB,
            'tb': Data.TB }
  for base in BASES:
    if datastring.endswith(base):
      try:
        return Amount(int(datastring[:-len(base)]), BASES[base])
      except ValueError as e:
        raise InvalidData('Could not parse amount: %s' % e)
  raise InvalidData('Amount did not have a valid base: %s.  Valid bases: %s' % (
      datastring, ' '.join(BASES)))


def parse_amount_into(parse_function, option_name, default=None):
  def parse_amount_callback(option, opt, value, parser):
    try:
      setattr(parser.values, option_name, parse_function(value or default))
    except Exception as e:
      raise OptionValueError('Failed to parse: %s' % e)
  return parse_amount_callback


def parse_time_into(option_name, default=None):
  """
    An optparse-compatible callback for populating Amounts of Time.
  """
  return parse_amount_into(parse_time, option_name, default=default)


def parse_data_into(option_name, default=None):
  """
    An optparse-compatible callback for populating Amounts of Data.
  """
  return parse_amount_into(parse_data, option_name, default=default)

########NEW FILE########
__FILENAME__ = filelike
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""Encapsulate a file-like object to provide a common interface for RecordIO streams"""

import errno
import os

from twitter.common import log


# TODO(wickman) This needs to be py 3.x friendly.
_VALID_STRINGIO_CLASSES = []

from StringIO import StringIO

# The hoops jumped through here are because StringI and StringO are not
# exposed directly in the stdlib.
_VALID_STRINGIO_CLASSES.append(StringIO)

try:
  from cStringIO import StringIO
  _VALID_STRINGIO_CLASSES.append(type(StringIO())) # cStringIO.StringI
  _VALID_STRINGIO_CLASSES.append(type(StringIO('foo'))) # cStringIO.StringO
except ImportError:
  pass

_VALID_STRINGIO_CLASSES = tuple(_VALID_STRINGIO_CLASSES)


class FileLike(object):
  class Error(Exception): pass

  @staticmethod
  def get(fp):
    if isinstance(fp, _VALID_STRINGIO_CLASSES):
      return StringIOFileLike(fp)
    elif isinstance(fp, file):
      return FileLike(fp)
    elif isinstance(fp, FileLike):
      return fp
    else:
      raise ValueError('Unknown file-like object %s' % fp)

  def __init__(self, fp):
    self._fp = fp

  @property
  def mode(self):
    return self._fp.mode

  def dup(self):
    fd = os.dup(self._fp.fileno())
    try:
      cur_fp = os.fdopen(fd, self._fp.mode)
      cur_fp.seek(0)
    except OSError as e:
      log.error('Failed to duplicate fd on %s, error = %s' % (self._fp.name, e))
      try:
        os.close(fd)
      except OSError as e:
        if e.errno != errno.EBADF:
          log.error('Failed to close duped fd on %s, error = %s' % (self._fp.name, e))
      raise self.Error('Failed to dup %s' % self._fp)
    return FileLike(cur_fp)

  def read(self, length):
    return self._fp.read(length)

  def write(self, data):
    return self._fp.write(data)

  def tell(self):
    return self._fp.tell()

  def seek(self, dest):
    return self._fp.seek(dest)

  def close(self):
    return self._fp.close()

  @property
  def name(self):
    return self._fp.name

  def flush(self):
    try:
      self._fp.flush()
      os.fsync(self._fp.fileno())
    except (IOError, OSError) as e:
      log.error("Failed to fsync on %s! Error: %s" % (self._fp.name, e))


class StringIOFileLike(FileLike):
  def __init__(self, fp):
    self._fp = fp

  @property
  def mode(self):
    return 'r+'

  @property
  def name(self):
    return 'FileLike'

  def flush(self):
    pass

  def dup(self):
    return StringIOFileLike(StringIO(self._fp.read()))

########NEW FILE########
__FILENAME__ = recordio
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""Generic interfaces for record-based IO streams.

This module contains a RecordIO specification which defines interfaces for reading and writing to
sequential record streams using codecs. A record consists of a header (containing the length of the
frame), and a frame containing the encoded data.

To create a new RecordIO implementation, subclass RecordReader, RecordWriter, and Codec.
A basic example, StringRecordIO, is provided.

"""

import errno
from abc import abstractmethod
import os
import struct

from twitter.common import log
from twitter.common.lang import Compatibility, Interface

from .filelike import FileLike


class RecordIO(object):
  class Error(Exception): pass
  class PrematureEndOfStream(Error): pass
  class RecordSizeExceeded(Error): pass
  class InvalidTypeException(Error): pass
  class InvalidFileHandle(Error): pass
  class InvalidArgument(Error): pass
  class InvalidCodec(Error): pass

  RECORD_HEADER_SIZE = 4
  MAXIMUM_RECORD_SIZE = 64 * 1024 * 1024

  class Codec(Interface):
    """
      An encoder/decoder interface for bespoke RecordReader/Writers.
    """
    @abstractmethod
    def encode(self, blob):
      """
        Given: blob in custom format
        Return: serialized byte data
        Raises: InvalidTypeException if a bad blob type is supplied
      """

    @abstractmethod
    def decode(self, blob):
      """
        Given: deserialized byte data
        Return: blob in custom format
        Raises: InvalidTypeException if a bad blob type is supplied
      """

  class _Stream(object):
    """
      Shared initialization functionality for Reader/Writer
    """
    def __init__(self, fp, codec):
      try:
        self._fp = FileLike.get(fp)
      except ValueError as err:
        raise RecordIO.InvalidFileHandle(err)
      if not isinstance(codec, RecordIO.Codec):
        raise RecordIO.InvalidCodec("Codec must be subclass of RecordIO.Codec")
      self._codec = codec

    def close(self):
      """
        Close the underlying filehandle of the RecordIO stream.
      """
      self._fp.close()

  class Reader(_Stream):
    def __init__(self, fp, codec):
      """
        Initialize a Reader from file-like fp, with RecordIO.Codec codec
      """
      RecordIO._Stream.__init__(self, fp, codec)
      if ('w' in self._fp.mode or 'a' in self._fp.mode) and '+' not in self._fp.mode:
        raise RecordIO.InvalidFileHandle(
          'Filehandle supplied to RecordReader does not appear to be readable!')

    def __iter__(self):
      """
      Return an iterator over the entire contents of the underlying file handle.

        May raise:
          RecordIO.Error or subclasses
      """
      try:
        dup_fp = self._fp.dup()
      except self._fp.Error:
        log.error('Failed to dup %r' % self._fp)
        return

      try:
        while True:
          blob = RecordIO.Reader.do_read(dup_fp, self._codec)
          if blob:
            yield blob
          else:
            break
      finally:
        dup_fp.close()

    @staticmethod
    def do_read(fp, decoder):
      """
        Read a single record from the given filehandle and decode using the supplied decoder.

        May raise:
          RecordIO.PrematureEndOfStream if the stream is truncated in the middle of
            an expected message
          RecordIO.RecordSizeExceeded if the message exceeds RecordIO.MAXIMUM_RECORD_SIZE

      """
      # read header
      header = fp.read(RecordIO.RECORD_HEADER_SIZE)
      if len(header) == 0:
        log.debug("%s has no data (current offset = %d)" % (fp.name, fp.tell()))
        # Reset EOF (appears to be only necessary on OS X)
        fp.seek(fp.tell())
        return None
      elif len(header) != RecordIO.RECORD_HEADER_SIZE:
        raise RecordIO.PrematureEndOfStream(
            "Expected %d bytes in header, got %d" % (RecordIO.RECORD_HEADER_SIZE, len(header)))
      blob_len = struct.unpack('>L', header)[0]
      if blob_len > RecordIO.MAXIMUM_RECORD_SIZE:
        raise RecordIO.RecordSizeExceeded("Record exceeds maximum allowable size")

      # read frame
      read_blob = fp.read(blob_len)
      if len(read_blob) != blob_len:
        raise RecordIO.PrematureEndOfStream(
          'Expected %d bytes in frame, got %d' % (blob_len, len(read_blob)))
      return decoder.decode(read_blob)

    def read(self):
      """
        Read a single record from this stream.  Updates the file position on both
        success and failure (unless no data is available, in which case the file
        position is unchanged and None is returned.)

        Returns string blob or None if no data available.

        May raise:
          RecordIO.PrematureEndOfStream if the stream is truncated in the middle of
            an expected message
          RecordIO.RecordSizeExceeded if the message exceeds RecordIO.MAXIMUM_RECORD_SIZE
      """
      return RecordIO.Reader.do_read(self._fp, self._codec)

    def try_read(self):
      """
        Attempt to read a single record from the stream.  Only updates the file position
        if a read was successful.

        Returns string blob or None if no data available.

        May raise:
          RecordIO.RecordSizeExceeded
      """
      pos = self._fp.tell()
      try:
        return self.read()
      except RecordIO.PrematureEndOfStream as e:
        log.debug('Got premature end of stream [%s], skipping - %s' % (self._fp.name, e))
        self._fp.seek(pos)
        return None

  class Writer(_Stream):
    def __init__(self, fp, codec, sync=False):
      """
        Initialize a Writer from the FileLike fp, with RecordIO.Codec codec.

        If sync=True is supplied, then all mutations are fsynced after write, otherwise
        standard filesystem buffering is employed.
      """
      RecordIO._Stream.__init__(self, fp, codec)
      if 'w' not in self._fp.mode and 'a' not in self._fp.mode and '+' not in self._fp.mode:
        raise RecordIO.InvalidFileHandle(
          'Filehandle supplied to RecordWriter does not appear to be writeable!')
      self.set_sync(sync)

    def set_sync(self, value):
      self._sync = bool(value)

    @staticmethod
    def do_write(fp, record, codec, sync=False):
      """
        Write a record to the specified fp using the supplied codec.

        Returns True on success, False on any filesystem failure.
      """
      blob = codec.encode(record)
      header = struct.pack(">L", len(blob))
      try:
        fp.write(header)
        fp.write(blob)
      except (IOError, OSError) as e:
        log.debug("Got exception in write(%s): %s" % (fp.name, e))
        return False
      if sync:
        fp.flush()
      return True

    @staticmethod
    def append(filename, record, codec):
      """
        Given a filename stored in RecordIO format, open the file, append a
        record to it and close.

        Returns True if it succeeds, or False if it fails for any reason.
        Raises IOError, OSError if there is a problem opening filename for appending.
      """
      if not isinstance(codec, RecordIO.Codec):
        raise RecordIO.InvalidCodec("append called with an invalid codec!")
      if not os.path.exists(filename):
        return False
      try:
        fp = None
        with open(filename, "a+") as fp:
          return RecordIO.Writer.do_write(fp, record, codec)
      except (IOError, OSError) as e:
        if fp:
          log.debug("Unexpected exception (%s), but continuing" % e)
          return False
        else:
          raise

    def write(self, blob):
      """
        Append the blob to the current RecordWriter.

        Returns True on success, False on any filesystem failure.
      """
      return RecordIO.Writer.do_write(self._fp, blob, self._codec, sync=self._sync)


class StringCodec(RecordIO.Codec):
  """
    A simple string-based implementation of Codec.

    Performs no actual encoding/decoding; simply verifies that input is a string
  """
  @staticmethod
  def _validate(blob):
    if not isinstance(blob, Compatibility.string):
      raise RecordIO.InvalidTypeException("blob (type=%s) not StringType!" % type(blob))
    return blob

  def encode(self, blob):
    return self._validate(blob)

  def decode(self, blob):
    return self._validate(blob)


class StringRecordReader(RecordIO.Reader):
  """
    Simple RecordReader that deserializes strings.
  """
  def __init__(self, fp):
    RecordIO.Reader.__init__(self, fp, StringCodec())


class StringRecordWriter(RecordIO.Writer):
  """
    Write framed string records to a stream.

    Max record size is 64MB for the sake of sanity.
  """
  def __init__(self, fp):
    RecordIO.Writer.__init__(self, fp, StringCodec())

  @staticmethod
  def append(filename, blob, codec=StringCodec()):
    return RecordIO.Writer.append(filename, blob, codec)


RecordReader = StringRecordReader
RecordWriter = StringRecordWriter

########NEW FILE########
__FILENAME__ = thrift_recordio
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import sys
import inspect

from .recordio import RecordIO

try:
  import thrift.TSerialization as _SER
  _HAS_THRIFT = True
except ImportError:
  _SER = None
  _HAS_THRIFT = False
  print("WARNING: Unable to load thrift in thrift_recordio", file=sys.stderr)


class ThriftRecordIO(object):
  class ThriftUnavailableException(RecordIO.Error): pass
  class ThriftUnsuppliedException(RecordIO.Error): pass
  class InvalidThriftException(RecordIO.InvalidTypeException): pass

  @staticmethod
  def assert_has_thrift():
    if not _HAS_THRIFT:
      raise ThriftRecordIO.ThriftUnavailableException(
        "thrift module not available for serialization!")

  class ThriftCodec(RecordIO.Codec):
    """
      Thrift Codec.

      If no thrift_base is supplied, this codec may be used correctly in
      encode-only mode (i.e. for RecordWriters.)
    """

    def __init__(self, thrift_base=None):
      self._base = thrift_base
      if self._base is not None and not inspect.isclass(self._base):
        raise ThriftRecordIO.InvalidThriftException(
          "ThriftCodec initialized with invalid Thrift base class")

    def encode(self, input):
      return _SER.serialize(input)

    def decode(self, input):
      if self._base is None:
        raise ThriftRecordIO.ThriftUnsuppliedException(
          "ThriftCodec cannot deserialize because no thrift_base supplied!")

      base = self._base()
      try:
        _SER.deserialize(base, input)
      except EOFError:
        raise RecordIO.PrematureEndOfStream("Reached EOF while decoding frame")
      return base


class ThriftRecordReader(RecordIO.Reader):
  """
    RecordReader that deserializes Thrift objects instead of strings.
  """

  def __init__(self, fp, thrift_base):
    """
      Construct a ThriftRecordReader from given file pointer and Thrift class thrift_base

      May raise:
        RecordIO.ThriftUnavailableException if thrift deserialization is unavailable.
        RecordIO.ThriftUnsuppliedException if thrift_base not supplied
    """
    ThriftRecordIO.assert_has_thrift()
    if not thrift_base:
      raise ThriftRecordIO.ThriftUnsuppliedException(
        'Must construct ThriftRecordReader with valid thrift_base!')
    RecordIO.Reader.__init__(self, fp, ThriftRecordIO.ThriftCodec(thrift_base))


class ThriftRecordWriter(RecordIO.Writer):
  """
    RecordWriter that serializes Thrift objects instead of strings.
  """

  def __init__(self, fp):
    """
      Construct a ThiftRecordWriter from given file pointer.

      May raise:
        RecordIO.ThriftUnavailableException if thrift deserialization is unavailable.
    """
    ThriftRecordIO.assert_has_thrift()
    RecordIO.Writer.__init__(self, fp, ThriftRecordIO.ThriftCodec())

  @staticmethod
  def append(filename, input, codec=ThriftRecordIO.ThriftCodec()):
    return RecordIO.Writer.append(filename, input, codec)

########NEW FILE########
__FILENAME__ = resourcepool
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================


"""A generic thread-safe resource pool."""

try:
  from Queue import Queue
except ImportError:
  from queue import Queue

from twitter.common.quantity import Amount, Time


class Resource(object):
  """Wrapper object around an allocated resource from ResourcePool.

  The underlying resource should generally be only accessed through this
  classes context-manager interface.

  Resources should only be accessed by one thread at a time.
  """

  __slots__ = ('_pool', 'resource')

  def __init__(self, pool, resource):
    self._pool = pool
    self.resource = resource

  def __del__(self):
      try:
        if self._pool is not None:
          self.release()
      except:
        pass

  def release(self):
    """Release the underlying resource back into the pool."""
    self._pool.release(self.resource)
    self._pool = None

  def __enter__(self):
    return self.resource

  def __exit__(self, unused_type, unused_val, unused_tb):
    self.release()

  def __repr__(self):
    return 'Resource(%r)' % self.resource


class ResourcePool(object):
  """A generic resource pool.

    >>> class MyResource(object):
    ...   def __init__(self, name):
    ...     self.name = name
    >>> pool = ResourcePool([MyResource('one'), MyResource('two')])
    >>> with pool.acquire() as resource:
      ...   print resource.name
  """

  def __init__(self, resources):
    """Create a new resource pool populated with resources."""
    self._resources = Queue()
    for resource in resources:
      self._resources.put(resource)

  def acquire(self, timeout=None):
    """Acquire a resource.

    This should generally be only accessed through the context-manager
    interface:

      >>> with pool.acquire() as resource:
      ...   print resource.name

    :param timeout: If provided, seconds (or Amount) to wait for a resource before raising
        Queue.Empty. If not provided, blocks indefinitely.

    :returns: Returns a Resource() wrapper object.
    :raises Empty: No resources are available before timeout.
    """
    if timeout is None:
      resource = self._resources.get()
    else:
      if isinstance(timeout, Amount):
        timeout = timeout.as_(Time.SECONDS)
      resource = self._resources.get(True, timeout)
    return Resource(self, resource)

  def release(self, resource):
    """Add a resource to the pool."""
    self._resources.put(resource)

  def empty(self):
    """Check if any resources are available.

    Note: This is a rough guide only. It does not guarantee that acquire()
        will succeed.
    """
    return self._resources.empty()

########NEW FILE########
__FILENAME__ = reviewboard
from __future__ import print_function

__author__ = 'Bill Farner'

import base64
import cookielib
import mimetools
import os
import getpass
import json
import sys
import urllib2
from urlparse import urljoin
from urlparse import urlparse


VERSION = '0.8-precommit'


class APIError(Exception):
  pass


class RepositoryInfo:
  """
  A representation of a source code repository.
  """
  def __init__(self, path=None, base_path=None, supports_changesets=False,
               supports_parent_diffs=False):
    self.path = path
    self.base_path = base_path
    self.supports_changesets = supports_changesets
    self.supports_parent_diffs = supports_parent_diffs
    self.debug('repository info: %s' % self)

  def debug(self, message):
    """
    Does nothing by default but can be oferwritten on an Repository info object
    to print the message to the screen and such.
    """
    pass

  def __str__(self):
    return ('Path: %s, Base path: %s, Supports changesets: %s' %
      (self.path, self.base_path, self.supports_changesets))


class ReviewBoardServer:
  """
  An instance of a Review Board server.
  """
  def __init__(self,
               url,
               cookie_file=None,
               info=None,
               repository=None,
               username=None,
               password=None,
               debug=False):
    self._debug = debug

    # Load the config and cookie files
    if cookie_file is None:
      if 'USERPROFILE' in os.environ:
        homepath = os.path.join(os.environ['USERPROFILE'],
                                'Local Settings', 'Application Data')
      elif 'HOME' in os.environ:
        homepath = os.environ['HOME']
      else:
        homepath = ''

      cookie_file = os.path.join(homepath, '.post-review-cookies.txt')

    if info is None:
      info = RepositoryInfo(path=repository, base_path='/')

    self.url = url
    if self.url[-1] != '/':
      self.url += '/'
    self.info = info
    self.cookie_file = cookie_file
    self.cookie_jar = cookielib.MozillaCookieJar(self.cookie_file)

    if not self.has_valid_cookie() and (not username or not password):
      print('==> Review Board Login Required')
      print('Enter username and password for Review Board at %s' % self.url)
      username = raw_input('Username: ')
      password = getpass.getpass('Password: ')
    self._add_auth_params(username, password)

  def _add_auth_params(self, username, password):
    headers = [('User-agent', 'post-review/' + VERSION)]

    if username and password:
      self.debug('Authorizing as user %s' % username)
      base64string = base64.encodestring('%s:%s' % (username, password))
      base64string = base64string[:-1]
      headers.append(('Authorization', 'Basic %s' % base64string))

    # Set up the HTTP libraries to support all of the features we need.
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(self.cookie_jar))
    opener.addheaders = headers
    urllib2.install_opener(opener)

  def get_url(self, rb_id):
    return '%s/r/%s' % (self.url, rb_id)

  def debug(self, message):
    """
    Prints a debug message, if debug is enabled.
    """
    if self._debug:
      print('[Debug] %s' % message)

  def die(self, msg=None):
    """
    Cleanly exits the program with an error message. Erases all remaining
    temporary files.
    """
    raise Exception(msg)

  def has_valid_cookie(self):
    """
    Load the user's cookie file and see if they have a valid
    'rbsessionid' cookie for the current Review Board server.  Returns
    true if so and false otherwise.
    """
    try:
      parsed_url = urlparse(self.url)
      host = parsed_url[1]
      path = parsed_url[2] or '/'

      # Cookie files don't store port numbers, unfortunately, so
      # get rid of the port number if it's present.
      host = host.split(':')[0]

      self.debug('Looking for "%s %s" cookie in %s' %
                 (host, path, self.cookie_file))
      self.cookie_jar.load(self.cookie_file, ignore_expires=True)

      try:
        cookie = self.cookie_jar._cookies[host][path]['rbsessionid']

        if not cookie.is_expired():
          self.debug('Loaded valid cookie -- no login required')
          return True

        self.debug('Cookie file loaded, but cookie has expired')
      except KeyError:
        self.debug('Cookie file loaded, but no cookie for this server')
    except IOError, error:
      self.debug('Couldn\'t load cookie file: %s' % error)

    return False

  def new_review_request(self, changenum=None, submit_as=None, diff_only=False):
    """
    Creates a review request on a Review Board server, updating an
    existing one if the changeset number already exists.

    If submit_as is provided, the specified user name will be recorded as
    the submitter of the review request (given that the logged in user has
    the appropriate permissions).
    """
    try:
      data = { 'repository_path': self.info.path }

      if changenum:
        data['changenum'] = changenum

      if submit_as:
        self.debug('Submitting the review request as %s' % submit_as)
        data['submit_as'] = submit_as

      rsp = self.api_call('api/review-requests/new/', data)
    except APIError, e:
      rsp, = e.args

      if not diff_only:
        if rsp['err']['code'] == 204: # Change number in use
          self.debug('Review request already exists. Updating it...')
          rsp = self.api_call(
            'api/review-requests/%s/update_from_changenum/' %
            rsp['review_request']['id'])
        else:
          raise e

    self.debug('Review request created')
    return rsp['review_request']

  def set_submitted(self, review_request_id):
    """
    Marks a review request as submitted.
    """
    self.api_call('api/review-requests/%s/' % review_request_id, {
        'status': 'submitted',
        }, method='PUT')

  def set_discarded(self, review_request_id):
    """
    Marks a review request as discarded.
    """
    self.api_call('api/review-requests/%s/' % review_request_id, {
        'status': 'discarded',
        }, method='PUT')

  def send_review_reply(self, review_request_id, message):
    """
    Replies to a review with a message.
    """
    self.api_call('/api/review-requests/%s/reviews/' % review_request_id, {
        'public': True,
        'body_top': message
        }, method='POST')

  def set_review_request_field(self, review_request, field, value):
    """
    Sets a field in a review request to the specified value.
    """
    rid = review_request['id']

    self.debug('Attempting to set field "%s" to "%s" for review request "%s"' %
               (field, value, rid))

    self.api_call('api/review-requests/%s/draft/set/' % rid,
            {field: value})

  def get_review_request(self, rid):
    """
    Returns the review request with the specified ID.
    """
    rsp = self.api_call('api/review-requests/%s/' % rid)
    return rsp['review_request']

  def save_draft(self, review_request):
    """
    Saves a draft of a review request.
    """
    self.api_call('api/review-requests/%s/draft/save/' %
            review_request['id'])
    self.debug('Review request draft saved')

  def upload_diff(self, review_request, diff_content, parent_diff_content):
    """
    Uploads a diff to a Review Board server.
    """
    self.debug('Uploading diff, size: %d' % len(diff_content))

    if parent_diff_content:
      self.debug('Uploading parent diff, size: %d' % len(parent_diff_content))

    fields = {}
    files = {}

    if self.info.base_path:
      fields['basedir'] = self.info.base_path

    files['path'] = {
        'filename': 'diff',
        'content': diff_content
        }

    if parent_diff_content:
      files['parent_diff_path'] = {
          'filename': 'parent_diff',
          'content': parent_diff_content
          }

    self.api_call('api/review-requests/%s/diff/new/' %
                  review_request['id'], fields, files)

  def publish(self, review_request):
    """
    Publishes a review request.
    """
    self.debug('Publishing')
    self.api_call('api/review-requests/%s/publish/' %
                  review_request['id'])

  def fetch_reviews(self, rb_id, start=0, max_results=25):
    """
    Fetches reviews in response to a review request.
    """
    return self.api_call('/api/review-requests/%s/reviews/?start=%s&max-results=%s'
                         % (rb_id, start, max_results))['reviews']

  def process_json(self, data):
    """
    Loads in a JSON file and returns the data if successful. On failure,
    APIError is raised.
    """
    rsp = json.loads(data)

    if rsp['stat'] == 'fail':
      raise APIError, rsp

    return rsp

  def _make_url(self, path):
    """Given a path on the server returns a full http:// style url"""
    url = urljoin(self.url, path)
    if not url.startswith('http'):
      url = 'http://%s' % url
    return url

  def http_request(self, path, fields=None, files=None, headers=None, method=None):
    """
    Executes an HTTP request against the specified path, storing any cookies that
    were set.  By default, if there are no field or files a GET is issued, otherwise a POST is used.
    The HTTP verb can be customized by specifying method.
    """
    if fields:
      debug_fields = fields.copy()
    else:
      debug_fields = {}

    if 'password' in debug_fields:
      debug_fields['password'] = '**************'
    url = self._make_url(path)
    self.debug('HTTP request to %s: %s' % (url, debug_fields))

    headers = headers or {}
    if fields or files:
      content_type, body = self._encode_multipart_formdata(fields, files)
      headers.update({
        'Content-Type': content_type,
        'Content-Length': str(len(body))
      })
      r = urllib2.Request(url, body, headers)
    else:
      r = urllib2.Request(url, headers=headers)

    if method:
      r.get_method = lambda: method

    try:
      return urllib2.urlopen(r).read()
    except urllib2.URLError, e:
      try:
        self.debug(e.read())
      except AttributeError:
        pass

      self.die('Unable to access %s. The host path may be invalid\n%s' %
               (url, e))
    except urllib2.HTTPError, e:
      return self.die('Unable to access %s (%s). The host path may be invalid'
                       '\n%s' % (url, e.code, e.read()))

  def api_call(self, path, fields=None, files=None, method=None):
    """
    Performs an API call at the specified path. By default, if there are no field or files a GET is
    issued, otherwise a POST is used. The HTTP verb can be customized by specifying method.
    """
    return self.process_json(
      self.http_request(path, fields, files, {'Accept': 'application/json'}, method=method))

  def _encode_multipart_formdata(self, fields, files):
    """
    Encodes data for use in an HTTP POST or PUT.
    """
    BOUNDARY = mimetools.choose_boundary()
    content = []

    fields = fields or {}
    files = files or {}

    for key in fields:
      content.append('--' + BOUNDARY + '\r\n')
      content.append('Content-Disposition: form-data; name="%s"\r\n' % key)
      content.append('\r\n')
      content.append(fields[key])
      content.append('\r\n')

    for key in files:
      filename = files[key]['filename']
      value = files[key]['content']
      content.append('--' + BOUNDARY + '\r\n')
      content.append('Content-Disposition: form-data; name="%s"; ' % key)
      content.append('filename="%s"\r\n' % filename)
      content.append('\r\n')
      content.append(value)
      content.append('\r\n')

    content.append('--')
    content.append(BOUNDARY)
    content.append('--\r\n')
    content.append('\r\n')

    content_type = 'multipart/form-data; boundary=%s' % BOUNDARY

    return content_type, ''.join(map(str, content))

  def post_review(self, changenum, diff_content=None,
          parent_diff_content=None, submit_as=None,
          target_groups=None, target_people=None, summary=None,
          branch=None, bugs_closed=None, description=None,
          testing_done=None, rid=None, publish=True):
    """
    Attempts to create a review request on a Review Board server
    and upload a diff. On success, the review request path is displayed.
    """
    try:
      save_draft = False

      if rid:
        review_request = self.get_review_request(rid)
      else:
        review_request = self.new_review_request(changenum, submit_as)

      if target_groups:
        self.set_review_request_field(review_request, 'target_groups',
                                      target_groups)
        save_draft = True

      if target_people:
        self.set_review_request_field(review_request, 'target_people',
                                      target_people)
        save_draft = True

      if summary:
        self.set_review_request_field(review_request, 'summary',
                                      summary)
        save_draft = True

      if branch:
        self.set_review_request_field(review_request, 'branch', branch)
        save_draft = True

      if bugs_closed:
        self.set_review_request_field(review_request, 'bugs_closed',
                                      bugs_closed)
        save_draft = True

      if description:
        self.set_review_request_field(review_request, 'description',
                                      description)
        save_draft = True

      if testing_done:
        self.set_review_request_field(review_request, 'testing_done',
                                      testing_done)
        save_draft = True

      if save_draft:
        self.save_draft(review_request)
    except APIError, e:
      rsp, = e.args
      if rid:
        return self.die('Error getting review request %s: %s (code %s)' %
                        (rid, rsp['err']['msg'], rsp['err']['code']))
      else:
        error_message = 'Error creating review request: %s (code %s)\n' % (rsp['err']['msg'],
                                                                           rsp['err']['code'])
        if rsp['err']['code'] == 105:
          bad_keys = rsp['fields']
          if bad_keys:
            error_message = 'Invalid key-value pairs:\n'
            for key, issues in bad_keys.items():
              error_message += '%s: %s\n' % (key, ', '.join(issues))

        return self.die(error_message)

    if not self.info.supports_changesets:
      try:
        self.upload_diff(review_request, diff_content, parent_diff_content)
      except APIError, e:
        rsp, = e.args
        print('Error uploading diff: %s (%s)' % (rsp['err']['msg'], rsp['err']['code']))
        self.debug(rsp)
        self.die('Your review request still exists, but the diff is not '
                 'attached.')

    if publish:
      self.publish(review_request)

    request_url = 'r/' + str(review_request['id'])
    review_url = urljoin(self.url, request_url)

    if not review_url.startswith('http'):
      review_url = 'http://%s' % review_url

    sys.stderr.write('Review request #%s posted.\n' % review_request['id'])
    sys.stderr.write('\n%s\n' % review_url)

    return 1

  def get_raw_diff(self, review):
    """
    Returns the raw diff for the given reviewboard item.
    """
    return self.http_request('/r/%s/diff/raw/' % review, {})

########NEW FILE########
__FILENAME__ = address
from twitter.common.lang import Compatibility

class Address(object):
  class InvalidFormat(Exception):
    pass

  @staticmethod
  def sanity_check(host, port):
    if not isinstance(host, Compatibility.string):
      raise Address.InvalidFormat('Host must be a string, got %s' % host)
    if not isinstance(port, (int, long)):
      raise Address.InvalidFormat('Port must be an integer, got %s' % port)
    if port <= 0:
      raise Address.InvalidFormat('Port must be a positive integer, got %s' % port)

  @staticmethod
  def from_string(*args, **kw):
    if (kw or len(args) != 1 or not isinstance(args[0], Compatibility.string)
        or not len(args[0].split(':')) == 2):
      raise Address.InvalidFormat('from_string expects "host:port" string.')
    host, port = args[0].split(':')
    try:
      port = int(port)
    except ValueError:
      raise Address.InvalidFormat('Port must be an integer, got %s' % port)
    Address.sanity_check(host, port)
    return Address(host, port)

  @staticmethod
  def from_pair(*args, **kw):
    if (kw or len(args) != 2 or not isinstance(args[0], Compatibility.string)
        or not isinstance(args[1], (int, long))):
      raise Address.InvalidFormat('from_pair expects host, port as input!')
    Address.sanity_check(args[0], args[1])
    return Address(args[0], args[1])

  @staticmethod
  def from_tuple(*args, **kw):
    if kw or len(args) != 1 or len(args[0]) != 2:
      raise Address.InvalidFormat('from_tuple expects (host, port) tuple as input!')
    host, port = args[0]
    Address.sanity_check(host, port)
    return Address(host, port)

  @staticmethod
  def from_address(*args, **kw):
    if kw or len(args) != 1 or not isinstance(args[0], Address):
      raise Address.InvalidFormat('from_address expects an address as input!')
    return Address(args[0].host, args[0].port)

  @staticmethod
  def parse(*args, **kw):
    for parser in [Address.from_string, Address.from_pair, Address.from_address, Address.from_tuple]:
      try:
        return parser(*args, **kw)
      except Address.InvalidFormat:
        continue
    raise Address.InvalidFormat('Could not parse input: args=%s kw=%s' % (
      repr(args), repr(kw)))

  def __init__(self, host, port):
    self._host = host
    self._port = port

  @property
  def host(self):
    return self._host

  @property
  def port(self):
    return self._port

########NEW FILE########
__FILENAME__ = factories
import functools
from thrift.transport import TSocket, TTransport
from thrift.protocol import TBinaryProtocol
from twitter.common.rpc.address import Address


class ConnectionClosable(object):
  """Mixin class for thrift connection closability."""

  def close(self):
    if hasattr(self, '_connection') and self._connection:
      self._connection.close()
    return

  def __enter__(self):
    return self

  def __exit__(self, *args, **kwargs):
    self.close()
    return False


class ConnectionFactory(object):
  def __init__(self, connection_klazz):
    self._connection_klazz = connection_klazz

  def __call__(self, *args, **kw):
    address = Address.parse(*args, **kw)
    return self._connection_klazz(host=address.host, port=address.port)


class TransportFactory(object):
  def __init__(self, transport_klazz):
    self._transport_klazz = transport_klazz

  def __call__(self, connection):
    return self._transport_klazz(connection)


class ProtocolFactory(object):
  def __init__(self, protocol_klazz):
    self._protocol_klazz = protocol_klazz

  def __call__(self, transport):
    return self._protocol_klazz(transport)


class ClientFactory(object):
  def __init__(self, client_iface, connection):
    self._client_iface = client_iface
    self._connection = connection

  def _mixin(self, klazz, mixin):
    if mixin not in klazz.__bases__:
      klazz.__bases__ += (mixin,)
    return klazz

  def __call__(self, protocol):
    # Mix ConnectionClosable class into the client class.
    client_class = getattr(self._client_iface, 'Client')
    client_class = self._mixin(client_class, ConnectionClosable)

    # Instantiate and set _connection object on the instance.
    client = client_class(protocol)
    client._connection = self._connection

    return client


def make_client(client_iface, *args, **kw):
  """
    Ex, basic usage:
      make_client(UserService, 'localhost', 9999)

    Ex, make an SSL socket server (see also make_server)
      make_client(UserService, 'smf1-amk-25-sr1.prod.twitter.com', 9999,
                  connection=TSocket.TSSLServerSocket,
                  ca_certs=...)

    Ex, make a finagle client:
      make_client(UserService, 'localhost', 9999,
                  protocol=TFinagleProtocol)

    And one with a client_id
      make_client(UserService, 'localhost', 9999,
                  protocol=functools.partial(TFinagleProtocol, client_id="test_client"))

    (this is equivalent to
      make_client(UserService, 'localhost', 9999,
                  protocol=TFinagleProtocolWithClientId("test_client")))

    Ex, bind to a unix_socket instead of host/port pair (unix_socket is kwarg to
    TSocket.TSocket):
      make_client(UserService, unix_socket=...opened-fifo...)

    N.B. This can also be used as a contextmanager or with contextlib.closing to
         automatically handle closing of the thrift connection (or manually via close()).

      with make_client(...) as c:
        c.somefunc()
  """
  protocol_class = kw.pop('protocol', TBinaryProtocol.TBinaryProtocolAccelerated)
  transport_class = kw.pop('transport', TTransport.TFramedTransport)
  connection_class = kw.pop('connection', TSocket.TSocket)

  connection = ConnectionFactory(connection_class)(*args, **kw)
  connection.open()

  return ClientFactory(client_iface, connection)(
           ProtocolFactory(protocol_class)(
             TransportFactory(transport_class)(connection)))


def make_client_factory(client_iface):
  return functools.partial(make_client, client_iface)

########NEW FILE########
__FILENAME__ = protocol
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import functools
import threading

from thrift.Thrift import TApplicationException, TMessageType
from thrift.protocol import TBinaryProtocol

from gen.twitter.finagle.thrift.ttypes import (
    ClientId,
    ConnectionOptions,
    RequestHeader,
    ResponseHeader,
    UpgradeReply,
)

from .trace import Trace


def upgrade_protocol_to_finagle(protocol):
  UPGRADE_METHOD = "__can__finagle__trace__v3__"

  def send(protocol):
    protocol.writeMessageBegin(UPGRADE_METHOD, TMessageType.CALL, 0)
    connection_options = ConnectionOptions()
    connection_options.write(protocol)
    protocol.writeMessageEnd()
    protocol.trans.flush()

  def recv(protocol):
    (fname, mtype, rseqid) = protocol.readMessageBegin()
    if fname != UPGRADE_METHOD:
      raise Exception('Unexpected error upgrading Finagle transport!')
    if mtype == TMessageType.EXCEPTION:
      exc = TApplicationException()
      exc.read(protocol)
      protocol.readMessageEnd()
      raise exc
    reply = UpgradeReply()
    reply.read(protocol)
    protocol.readMessageEnd()
    return reply

  send(protocol)
  return recv(protocol)


class TFinagleProtocol(TBinaryProtocol.TBinaryProtocolAccelerated):
  def __init__(self, *args, **kw):
    self._locals = threading.local()
    self._finagle_upgraded = False
    self._client_id = kw.pop('client_id', None)
    self._client_id = ClientId(name=self._client_id) if self._client_id else None
    TBinaryProtocol.TBinaryProtocolAccelerated.__init__(self, *args, **kw)
    try:
      upgrade_protocol_to_finagle(self)
      self._finagle_upgraded = True
    except TApplicationException:
      pass

  def to_request_header(self, trace_id):
    return RequestHeader(trace_id=trace_id.trace_id.value,
                         parent_span_id=trace_id.parent_id.value,
                         span_id=trace_id.span_id.value,
                         sampled=trace_id.sampled,
                         client_id=self._client_id)

  def writeMessageBegin(self, *args, **kwargs):
    if self._finagle_upgraded:
      if not hasattr(self._locals, 'trace'):
        self._locals.trace = Trace()
      trace_id = self._locals.trace.get()
      self.to_request_header(trace_id).write(self)
      with self._locals.trace.push(trace_id):
        return TBinaryProtocol.TBinaryProtocolAccelerated.writeMessageBegin(self, *args, **kwargs)
    else:
      return TBinaryProtocol.TBinaryProtocolAccelerated.writeMessageBegin(self, *args, **kwargs)

  def readMessageBegin(self, *args, **kwargs):
    if self._finagle_upgraded:
      header = ResponseHeader()
      header.read(self)
      self._locals.last_response = header
    return TBinaryProtocol.TBinaryProtocolAccelerated.readMessageBegin(self, *args, **kwargs)


def TFinagleProtocolWithClientId(client_id):
  return functools.partial(TFinagleProtocol, client_id=client_id)

########NEW FILE########
__FILENAME__ = trace
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import copy
from contextlib import contextmanager
import random
import re

class SpanId(object):
  __slots__ = ('_value',)

  HEX_REGEX = re.compile(r'^[a-f0-9]{16}$', re.IGNORECASE)

  class InvalidSpanId(ValueError):
    def __init__(self, value):
      ValueError.__init__(self, 'Invalid SpanId: %s' % repr(value))

  @staticmethod
  def from_value(value):
    if isinstance(value, str):
      if SpanId.HEX_REGEX.match(value):
        return SpanId(int(value, 16))
    elif isinstance(value, (int, long)):
      return SpanId(value)
    elif isinstance(value, SpanId):
      return SpanId(value.value)
    elif value is None:
      return SpanId(None)
    raise SpanId.InvalidSpanId(value)

  def __init__(self, value):
    self._value = value

  @property
  def value(self):
    return self._value

  def __str__(self):
    return 'SpanId(%016x)' % (self._value if self._value is not None else 'Empty')


class TraceId(object):
  @staticmethod
  def rand():
    return random.randint(0, 2**63-1)

  def __init__(self, trace_id, parent_id, span_id, sampled):
    self.trace_id = SpanId.from_value(trace_id)
    self.parent_id = SpanId.from_value(parent_id)
    self.span_id = SpanId.from_value(span_id)
    self.sampled = bool(sampled)

  def next(self):
    return TraceId(self.trace_id, self.span_id, TraceId.rand(), self.sampled)

  def __str__(self):
    return 'TraceId(trace_id = %s, parent_id = %s, span_id = %s, sampled = %s)' % (
      self.trace_id, self.parent_id, self.span_id, self.sampled)


class Trace(object):
  """
    The container of a trace.  Typically stored as a threadlocal on each
    finagle-upgraded protocol.
  """
  def __init__(self, sample_rate=0.001):
    assert 0.0 <= sample_rate <= 1.0
    self._sample_rate = sample_rate
    self._stack = []

  def get(self):
    if len(self._stack) == 0:
      span_id = TraceId.rand()
      trace_id = TraceId(span_id, None, span_id, self.should_sample())
      self._stack.append(trace_id)
    return self._stack[-1]

  @contextmanager
  def push(self, trace_id):
    self._stack.append(trace_id)
    try:
      yield self
    finally:
      self._stack.pop()

  @contextmanager
  def unwind(self):
    trace_id_copy = copy.deepcopy(self._stack[-1])
    try:
      yield self
    finally:
      self._stack[-1] = trace_id_copy

  def pop(self):
    return self._stack.pop()

  def should_sample(self):
    return random.random() < self._sample_rate



########NEW FILE########
__FILENAME__ = transport
"""
  A SASL Thrift transport based upon the pure-sasl library, both implemented by
  @tylhobbs and adapted for twitter.common.  See:

    https://issues.apache.org/jira/browse/THRIFT-1719
    https://issues.apache.org/jira/secure/attachment/12548462/1719-python-sasl.txt
"""

from struct import pack, unpack

from twitter.common.lang import Compatibility
StringIO = Compatibility.BytesIO

from puresasl.client import SASLClient
from thrift.transport.TTransport import (
    CReadableTransport,
    TTransportBase,
    TTransportException)


class TSaslClientTransport(TTransportBase, CReadableTransport):
  """
  A SASL transport based on the pure-sasl library:
      https://github.com/thobbs/pure-sasl
  """

  START = 1
  OK = 2
  BAD = 3
  ERROR = 4
  COMPLETE = 5

  def __init__(self, transport, host, service, mechanism='GSSAPI',
      **sasl_kwargs):
    """
    transport: an underlying transport to use, typically just a TSocket
    host: the name of the server, from a SASL perspective
    service: the name of the server's service, from a SASL perspective
    mechanism: the name of the preferred mechanism to use

    All other kwargs will be passed to the puresasl.client.SASLClient
    constructor.
    """
    self.transport = transport
    self.sasl = SASLClient(host, service, mechanism, **sasl_kwargs)
    self.__wbuf = StringIO()
    self.__rbuf = StringIO()

  def open(self):
    if not self.transport.isOpen():
      self.transport.open()

    self.send_sasl_msg(self.START, self.sasl.mechanism)
    self.send_sasl_msg(self.OK, self.sasl.process() or '')

    while True:
      status, challenge = self.recv_sasl_msg()
      if status == self.OK:
        self.send_sasl_msg(self.OK, self.sasl.process(challenge) or '')
      elif status == self.COMPLETE:
        if not self.sasl.complete:
          raise TTransportException("The server erroneously indicated "
              "that SASL negotiation was complete")
        else:
          break
      else:
        raise TTransportException("Bad SASL negotiation status: %d (%s)"
            % (status, challenge))

  def send_sasl_msg(self, status, body):
    if body is None:
      body = ''
    header = pack(">BI", status, len(body))
    self.transport.write(header + body)
    self.transport.flush()

  def recv_sasl_msg(self):
    header = self.transport.readAll(5)
    status, length = unpack(">BI", header)
    if length > 0:
      payload = self.transport.readAll(length)
    else:
      payload = ""
    return status, payload

  def write(self, data):
    self.__wbuf.write(data)

  def flush(self):
    data = self.__wbuf.getvalue()
    encoded = self.sasl.wrap(data)
    self.transport.write(''.join((pack("!i", len(encoded)), encoded)))
    self.transport.flush()
    self.__wbuf = StringIO()

  def read(self, sz):
    ret = self.__rbuf.read(sz)
    if len(ret) != 0:
      return ret

    self._read_frame()
    return self.__rbuf.read(sz)

  def _read_frame(self):
    header = self.transport.readAll(4)
    length, = unpack('!i', header)
    encoded = self.transport.readAll(length)
    self.__rbuf = StringIO(self.sasl.unwrap(encoded))

  def close(self):
    self.sasl.dispose()
    self.transport.close()

  # based on TFramedTransport
  @property
  def cstringio_buf(self):
    return self.__rbuf

  def cstringio_refill(self, prefix, reqlen):
    # self.__rbuf will already be empty here because fastbinary doesn't
    # ask for a refill until the previous buffer is empty.  Therefore,
    # we can start reading new frames immediately.
    while len(prefix) < reqlen:
      self._read_frame()
      prefix += self.__rbuf.getvalue()
    self.__rbuf = StringIO(prefix)
    return self.__rbuf

########NEW FILE########
__FILENAME__ = tsslsocket
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import socket
import ssl

from thrift.transport.TSSLSocket import TSSLSocket
from thrift.transport.TTransport import TTransportException


class DelayedHandshakeTSSLSocket(TSSLSocket):
  """Monkeypatched TSSLSocket that allows delaying of the initial SSL handshake.

  The purpose of this DelayedHandshakeTSSLSocket is to allow for intermixing other
  transport-layer protocols such as SOCKS.

  Behaves the same as TSSLSocket except it accepts the delay_handshake
  keyword argument.  This defers the SSL handshake until *after* connect.
  """

  def __init__(self, *args, **kw):
    # Curse 2.6.x + PEP-3102
    self.__delay_handshake = kw.pop('delay_handshake', False)
    self.__socket_factory = kw.pop('socket_factory', socket)
    if 'unix_socket' in kw:
      raise ValueError('%s does not support unix_sockets!' % self.__class__.__name__)
    TSSLSocket.__init__(self, *args, **kw)  # thrift does not support super()

  def __do_wrap(self, handshake_on_connect=False):
    # TODO(wickman) Thrift 0.9.1 supports keyfile and certfile.  File a
    # ticket to get delay_handshake added to the Thrift core libs.
    self.handle = ssl.wrap_socket(
        self.handle,
        ssl_version=self.SSL_VERSION,
        do_handshake_on_connect=handshake_on_connect,
        ca_certs=self.ca_certs,
        cert_reqs=self.cert_reqs)

  def open(self):
    try:
      resolved = self._resolveAddr()
      for offset, res in enumerate(resolved):
        sock_family, sock_type, _, _, ip_port = res[0:5]
        self.handle = self.__socket_factory.socket(sock_family, sock_type)
        self.handle.settimeout(self._timeout)
        reraise = (offset == len(resolved) - 1)

        if not self.__delay_handshake:
          self.__do_wrap(handshake_on_connect=True)

        try:
          self.handle.connect(ip_port)
        except self.__socket_factory.error:
          if reraise:
            raise

        if self.__delay_handshake:
          self.__do_wrap(handshake_on_connect=False)
          self.handle.do_handshake()

    except self.__socket_factory.error as e:
      message = 'Could not connect to %s:%d: %s' % (self.host, self.port, e)
      raise TTransportException(type=TTransportException.NOT_OPEN, message=message)

    if self.validate:
      self._validate_cert()

########NEW FILE########
__FILENAME__ = read_write_buffer
import threading

from twitter.common.lang import Compatibility

StringIO = Compatibility.StringIO


class _RWBuf(object):
  """An unbounded read-write buffer.

  Can be used as a file-like object for reading and writing.
  Subclasses implement write functionality."""
  def __init__(self, io):
    self._lock = threading.Lock()
    self._io = io
    self._readpos = 0

  def read(self, size=-1):
    with self._lock:
      self._io.seek(self._readpos)
      ret = self._io.read() if size == -1 else self._io.read(size)
      self._readpos = self._io.tell()
      return ret

  def read_from(self, pos, size=-1):
    with self._lock:
      self._io.seek(pos)
      return self._io.read() if size == -1 else self._io.read(size)

  def write(self, s):
    with self._lock:
      self.do_write(str(s))
      self._io.flush()

  def flush(self):
    with self._lock:
      self._io.flush()

  def close(self):
    self._io.close()

  def do_write(self, s):
    raise NotImplementedError


class InMemoryRWBuf(_RWBuf):
  """An unbounded read-write buffer entirely in memory.

  Can be used as a file-like object for reading and writing. Note that it can't be used in
  situations that require a real file (e.g., redirecting stdout/stderr of subprocess.Popen())."""
  def __init__(self):
    _RWBuf.__init__(self, StringIO())
    self._writepos = 0

  def do_write(self, s):
    self._io.seek(self._writepos)
    self._io.write(s)
    self._writepos = self._io.tell()


class FileBackedRWBuf(_RWBuf):
  """An unbounded read-write buffer backed by a file.

  Can be used as a file-like object for reading and writing the underlying file. Has a fileno,
  so you can redirect stdout/stderr of subprocess.Popen() etc. to this object. This is useful
  when you want to poll the output of long-running subprocesses in a separate thread."""
  def __init__(self, backing_file):
    _RWBuf.__init__(self, open(backing_file, 'a+'))
    self.fileno = self._io.fileno

  def do_write(self, s):
    self._io.write(s)

########NEW FILE########
__FILENAME__ = scanf
import re
from ctypes import (
  c_int,
  c_long,
  c_longlong,
  c_uint,
  c_ulong,
  c_ulonglong,
  c_float,
  c_double,
  c_char,
  c_char_p,
)

from twitter.common.lang import Compatibility


class ScanfResult(object):
  def __init__(self):
    self._dict = {}
    self._list = []

  def groups(self):
    """
      Matched named parameters.
    """
    return self._dict

  def __getattr__(self, key):
    if key in self._dict:
      return self._dict[key]
    else:
      raise AttributeError('Could not find attribute: %s' % key)

  def ungrouped(self):
    """
      Matched unnamed parameters.
    """
    return self._list

  def __iter__(self):
    return iter(self._list)


class ScanfParser(object):
  class ParseError(Exception): pass

  """
    Partial scanf emulator.
  """
  CONVERSIONS = {
     "c": (".", c_char),
     "d": ("[-+]?\d+", c_int),
     "ld": ("[-+]?\d+", c_long),
     "lld": ("[-+]?\d+", c_longlong),
     "f": (r"[-+]?[0-9]*\.?[0-9]*(?:[eE][-+]?[0-9]+)?", c_float),
     "s": ("\S+", c_char_p),
     "u": ("\d+", c_uint),
     "lu": ("\d+", c_ulong),
     "llu": ("\d+", c_ulonglong),
  }

  # ctypes don't do str->int conversion, so must preconvert for non-string types
  PRECONVERSIONS = {
    c_char: str,  # to cover cases like unicode
    c_int: int,
    c_long: int,
    c_longlong: long if Compatibility.PY2 else int,
    c_uint: int,
    c_ulong: int,
    c_ulonglong: long if Compatibility.PY2 else int,
    c_float: float,
    c_double: float
  }

  def _preprocess_format_string(self, string):
    def match_conversion(string, k):
      MAX_CONVERSION_LENGTH = 3
      for offset in range(MAX_CONVERSION_LENGTH, 0, -1):
        k_offset = k + offset
        if string[k:k_offset] in ScanfParser.CONVERSIONS:
          re, converter = ScanfParser.CONVERSIONS[string[k:k_offset]]
          if converter in ScanfParser.PRECONVERSIONS:
            return (re, lambda val: converter(ScanfParser.PRECONVERSIONS[converter](val))), k_offset
          else:
            return (re, converter), k_offset
      raise ScanfParser.ParseError('%s is an invalid format specifier' % (
        string[k]))

    def extract_specifier(string, k):
      if string[k] == '%':
        return '%', None, k+1
      if string[k] == '*':
        def null_apply(scan_object, value):
          pass
        (regex, preconversion), k = match_conversion(string, k+1)
        return '(%s)' % regex, null_apply, k
      if string[k] == '(':
        offset = string[k+1:].find(')')
        if offset == -1:
          raise ScanfParser.ParseError("Unmatched (")
        if offset == 0:
          raise ScanfParser.ParseError("Empty label string")
        name = string[k+1:k+1+offset]
        (regex, preconversion), k = match_conversion(string, k+1+offset+1)
        def dict_apply(scan_object, value):
          scan_object._dict[name] = preconversion(value).value
        return '(%s)' % regex, dict_apply, k
      (regex, preconversion), k = match_conversion(string, k)
      def list_apply(scan_object, value):
        scan_object._list.append(preconversion(value).value)
      return '(%s)' % regex, list_apply, k

    re_str = ""
    k = 0
    applicators = []
    while k < len(string):
      if string[k] == '%' and len(string) > k+1:
        regex, applicator, k = extract_specifier(string, k+1)
        re_str += regex
        if applicator:
          applicators.append(applicator)
      else:
        re_str += re.escape(string[k])
        k += 1
    return re_str, applicators

  def parse(self, line, allow_extra=False):
    """
      Given a line of text, parse it and return a ScanfResult object.
    """
    if not isinstance(line, Compatibility.string):
      raise TypeError("Expected line to be a string, got %s" % type(line))
    sre_match = self._re.match(line)
    if sre_match is None:
      raise ScanfParser.ParseError("Failed to match pattern: %s against %s" % (
        self._re_pattern, line))
    groups = list(sre_match.groups())
    if len(groups) != len(self._applicators):
      raise ScanfParser.ParseError("Did not parse all groups! Missing %d" % (
        len(self._applicators) - len(groups)))
    if sre_match.end() != len(line) and not allow_extra:
      raise ScanfParser.ParseError("Extra junk on the line! '%s'" % (
        line[sre_match.end():]))
    so = ScanfResult()
    for applicator, group in zip(self._applicators, groups):
      applicator(so, group)
    return so

  def __init__(self, format_string):
    """
      Given a format string, construct a parser.

      The format string takes:
        %c %d %u %f %s
        %d and %u take l or ll modifiers
        you can name parameters %(hey there)s and the string value will be keyed by "hey there"
        you can parse but not save parameters by specifying %*f
    """
    if not isinstance(format_string, Compatibility.string):
      raise TypeError('format_string should be a string, instead got %s' % type(format_string))
    self._re_pattern, self._applicators = self._preprocess_format_string(format_string)
    self._re = re.compile(self._re_pattern)

########NEW FILE########
__FILENAME__ = clock
from abc import abstractmethod
import threading
import time

from twitter.common.lang import Interface

class ClockInterface(Interface):

  @abstractmethod
  def time(self):
    pass

  @abstractmethod
  def tick(self, amount):
    pass

  @abstractmethod
  def sleep(self, amount):
    pass


class Handshake(object):
  def __init__(self):
    self._syn_event = threading.Event()
    self._ack_event = threading.Event()

  def syn(self):
    self._syn_event.wait()
    self._ack_event.set()

  def ack(self):
    self._syn_event.set()
    self._ack_event.wait()


class ThreadedClock(ClockInterface):
  def __init__(self, initial_value=0):
    self._time = initial_value
    self._waiters = []  # queue of [stop time, Handshake]

  def time(self):
    return self._time

  def _pop_waiter(self, end):
    times_up = sorted((waiter for waiter in self._waiters if waiter[0] <= end),
                       key=lambda element: element[0])
    if times_up:
      waiter = times_up[0]
      self._waiters.remove(waiter)
      return waiter

  def tick(self, amount):
    # yield thread, in case any others are waiting to sleep() on this clock
    time.sleep(0.1)
    now = self._time
    end = now + amount

    while True:
      waiter = self._pop_waiter(end)
      if not waiter:
        break

      waiter_time, waiter_handshake = waiter
      print('Time now: %s' % self._time)
      self._time = waiter_time
      waiter_handshake.ack()

    print('Time now: %s' % self._time)
    self._time = end

  def sleep(self, amount):
    waiter_end = self._time + amount
    waiter_handshake = Handshake()

    self._waiters.append((waiter_end, waiter_handshake))
    waiter_handshake.syn()

########NEW FILE########
__FILENAME__ = periodic_thread
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import time

from twitter.common.threading.stoppable_thread import StoppableThread


class PeriodicThread(StoppableThread):
  """A thread that runs a target function periodically.

  Note: Don't subclass this to override run(). That won't work. """
  def __init__(self, group=None, target=None, name=None, period_secs=1, args=(), kwargs=None):
    if kwargs is None:
      kwargs = {}

    def _periodic_target():
      target(*args, **kwargs)
      time.sleep(period_secs)

    StoppableThread.__init__(self, group=group, target=_periodic_target, name=name, args=args, kwargs=kwargs)

########NEW FILE########
__FILENAME__ = stoppable_thread

import threading


class StoppableThread(threading.Thread):
  """A thread that can be stopped.

  The target function will be called in a tight loop until the thread is stopped.

  Note: Don't subclass this to override run(). That won't work. """
  def __init__(self, group=None, target=None, name=None, post_target=None, args=(), kwargs=None):
    if kwargs is None:
      kwargs = {}

    def stoppable_target():
      while True:
        target(*args, **kwargs)
        if post_target:
          post_target()
        with self._lock:
          if self._stopped:
            return

    threading.Thread.__init__(self, group=group, target=stoppable_target, name=name, args=args, kwargs=kwargs)
    self._lock = threading.Lock()  # Protects self._stopped.
    self._stopped = False

  def stop(self):
    """Blocks until the thread is joined."""
    with self._lock:
      self._stopped = True
    self.join()

########NEW FILE########
__FILENAME__ = command_util
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

__author__ = 'Tejal Desai'

import os
import subprocess
import sys
import tempfile

from twitter.common.contextutil import temporary_file

try:
  from twitter.common import log
except ImportError:
  import logging as log


class CommandUtil(object):
  """
  This Class provides an wrapper to system command as a string and
  return the output.
  """
  @staticmethod
  def _execute_internal(cmd, log_std_out, log_std_err, log_cmd, also_output_to_file=None,
                        return_output=False):
    """
    Executes the command and returns 0 if successful
    Non-Zero status if command fails or an exception raised
    """
    tmp_filename = None
    if log_std_out or log_std_err:
      tmp_filename = tempfile.mktemp()
      tmp_file = open(tmp_filename, "w")

    if (not log_std_err) or (not log_std_out):
      dev_null_file = open(os.devnull, "w")

    if log_std_out:
      if also_output_to_file:
        std_out_file = open(also_output_to_file, "w")
      else:
        std_out_file = tmp_file
    else:
      std_out_file = dev_null_file

    if log_std_err:
      std_err_file = tmp_file
    else:
      std_err_file = dev_null_file

    if log_cmd:
      log.info("Executing: %s" % " ".join(cmd))
    #Call subprocess.call to run the command.
    try:
      ret = subprocess.call(cmd, stdout=std_out_file, stderr=std_err_file)
      text = None
      #Check if there is some output or error captured to log
      if tmp_filename:
        with open(tmp_filename, 'r') as file_read:
          text = file_read.read()
        os.remove(tmp_filename)
      if text:
        log.info("External output:\n%s" % text)
      return (ret, text) if return_output else ret
    except subprocess.CalledProcessError as exception:
      log.error("Subprocess Exception occurred %s" % exception)
      return (ret, None) if return_output else ret
    except OSError as exception:
      log.error("OS Exception occurred %s" % exception)
      #IF this exception occurs the ret value is not initialized hence return non-zero
      return (1, None) if return_output else 1



  @staticmethod
  def check_call(cmd):
    """
    Calls subprocess.check_call instead of subprocess.call
    """
    return subprocess.check_call(cmd)

  @staticmethod
  def execute(cmd, log_cmd=True, also_output_to_file=None):
    """
    Calls logs output and error if any
    """
    return CommandUtil._execute_internal(cmd, True, True, log_cmd, also_output_to_file)

  @staticmethod
  def execute_suppress_stdout(cmd, log_cmd=True):
    """
    Executes the command and supresses stdout
    """
    return CommandUtil._execute_internal(cmd, False, True, log_cmd)

  @staticmethod
  def execute_suppress_stdout_stderr(cmd, log_cmd=True):
    """
    Executes the command and supresses stdout and stderr
    """
    return CommandUtil._execute_internal(cmd, False, False, log_cmd)

  @staticmethod
  def execute_and_get_output(cmd, log_cmd=True):
    """
    Executes the command and returns the tuple return status and output
    If the subprocess.call raises an exception then returns error code and None as output
    If the command is in error returns the tuple return error code and error message
    return CommandUtil._execute_internal(cmd, True, True, log_cmd, None, True)
    """
    return CommandUtil._execute_internal(cmd, True, False, log_cmd, None, True)

########NEW FILE########
__FILENAME__ = client
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import namedtuple
from functools import wraps
import posixpath
import random
import socket
import sys
import threading
import zookeeper

try:
  from twitter.common import app
  WITH_APP=True
except ImportError:
  WITH_APP=False

try:
  from twitter.common import log
  from twitter.common.log.options import LogOptions
except ImportError:
  import logging as log

from twitter.common.metrics import (
    AtomicGauge,
    LambdaGauge,
    Observable)

try:
  from Queue import Queue, Empty
except ImportError:
  from queue import Queue, Empty

from .constants import Acl, Id


if WITH_APP:
  app.add_option(
      '--zookeeper',
      default='zookeeper.local.twitter.com:2181',
      metavar='HOST:PORT[,HOST:PORT,...]',
      dest='twitter_common_zookeeper_ensemble',
      help='A comma-separated list of host:port of ZooKeeper servers.')
  app.add_option(
      '--zookeeper_timeout',
      type='float',
      default=15.0,
      dest='twitter_common_zookeeper_timeout',
      help='The default timeout (in seconds) for ZK operations.')
  app.add_option(
      '--zookeeper_reconnects',
      type='int',
      default=0,
      dest='twitter_common_zookeeper_reconnects',
      help='The number of permitted reconnections before failing zookeeper (0 = infinite).')
  app.add_option(
      '--zookeeper_log_level',
      dest='twitter_common_zookeeper_log_level_override',
      choices=('NONE', 'DEBUG','INFO','WARN','ERROR','FATAL'),
      default='',
      help='Override the default ZK logging level.')

  class ZookeeperLoggingSubsystem(app.Module):
    # Map the ZK debug log to the same level as stderr logging.
    _ZK_LOG_LEVEL_MAP = {
        log.DEBUG: zookeeper.LOG_LEVEL_DEBUG,
        log.INFO: zookeeper.LOG_LEVEL_INFO,
        log.WARN: zookeeper.LOG_LEVEL_WARN,
        log.ERROR: zookeeper.LOG_LEVEL_ERROR,
        log.FATAL: zookeeper.LOG_LEVEL_ERROR,
    }

    def __init__(self):
      app.Module.__init__(self, __name__, description='Zookeeper logging subsystem.')

    def setup_function(self):
      log_level_override = app.get_options().twitter_common_zookeeper_log_level_override
      self._set_log_level(log_level_override=log_level_override)

    def _set_log_level(self, log_level_override=''):
      stderr_log_level = LogOptions.stderr_log_level()
      # set default level to FATAL.
      # we do this here (instead of add_option) to distinguish when an override is set.
      if stderr_log_level == log.INFO and log_level_override != 'INFO':
        stderr_log_level = log.FATAL
      # default to using stderr logging level, setting override if applicable
      log_level = getattr(log, log_level_override, stderr_log_level)
      # set the logger
      zk_log_level = ZookeeperLoggingSubsystem._ZK_LOG_LEVEL_MAP.get(
          log_level, zookeeper.LOG_LEVEL_ERROR)
      zookeeper.set_debug_level(zk_log_level)


  app.register_module(ZookeeperLoggingSubsystem())


class Perms(object):
  READ = zookeeper.PERM_READ
  WRITE = zookeeper.PERM_WRITE
  CREATE = zookeeper.PERM_CREATE
  DELETE = zookeeper.PERM_DELETE
  ADMIN = zookeeper.PERM_ADMIN
  ALL = zookeeper.PERM_ALL


class Ids(object):
  ANYONE_ID_UNSAFE = Id('world', 'anyone')
  AUTH_IDS = Id('auth', '')


class Acls(object):
  OPEN_ACL_UNSAFE = [Acl(Perms.ALL, Ids.ANYONE_ID_UNSAFE)]
  CREATOR_ALL_ACL = [Acl(Perms.ALL, Ids.AUTH_IDS)]
  READ_ACL_UNSAFE = [Acl(Perms.READ, Ids.ANYONE_ID_UNSAFE)]
  EVERYONE_READ_CREATOR_ALL = [Acl(Perms.ALL, Ids.AUTH_IDS), Acl(Perms.READ, Ids.ANYONE_ID_UNSAFE)]


class ZooDefs(object):
  Acls = Acls
  Ids = Ids
  Perms = Perms


del Acls, Ids, Perms


class ZooKeeper(Observable):
  """A convenience wrapper around the low-level ZooKeeper API.

  Blocks until the initial connection is established, and proxies method calls
  to the corresponding ZK functions, passing the handle.

  Supports both synchronous and asynchronous APIs.

  Syncronous API Notes:

    Synchronous calls will block across connection loss or session
    expiration until reconnected.

  Asynchronous API Notes:

    Asynchronous calls will queue up while the session/connection is
    unhealthy and only be dispatched while zookeeper is healthy.  It is
    still possible for asynchronous calls to fail should the session be
    severed after the call has been successfully dispatched.  In other
    words: don't assume your rc will always be zookeeper.OK.

    Watches will behave as normal assuming successful dispatch.  In general
    when using this wrapper, you should retry your call if your watch is
    fired with EXPIRED_SESSION_STATE and ignore anything else whose state is
    not CONNECTED_STATE.  This wrapper will never re-dispatch calls that
    have been sent to zookeeper without error.
  """

  class Error(Exception): pass
  class ConnectionTimeout(Error): pass
  class InvalidEnsemble(Error): pass
  class Stopped(Error): pass

  # White-list of methods that accept a ZK handle as their first argument
  _ZK_SYNC_METHODS = frozenset([
      'add_auth', 'close', 'create', 'delete', 'exists', 'get', 'get_acl',
      'get_children', 'is_unrecoverable', 'recv_timeout', 'set', 'set2',
      'set_acl', 'set_watcher', 'state',
   ])

  _ZK_ASYNC_METHODS = frozenset([
      'acreate', 'adelete', 'aexists', 'aget', 'aget_acl', 'aget_children', 'aset',
      'aset_acl', 'async'
  ])

  COMPLETION_RETRY = frozenset([
    zookeeper.CONNECTIONLOSS,
    zookeeper.OPERATIONTIMEOUT,
    zookeeper.SESSIONEXPIRED,
    zookeeper.CLOSING,
  ])

  @classmethod
  def expand_ensemble(cls, servers):
    """Expand comma-separated list of host:port to comma-separated, fully-resolved list of ip:port."""
    server_ports = []
    for server_port in servers.split(','):
      server_split = server_port.split(':', 2)
      if len(server_split) == 1:
        server, port = server_split[0], cls.DEFAULT_PORT
      else:
        try:
          server, port = server_split[0], int(server_split[1])
        except ValueError:
          raise cls.InvalidEnsemble('Invalid ensemble string: %s' % server_port)
      try:
        for ip in socket.gethostbyname_ex(server)[2]:
          server_ports.append('%s:%s' % (ip, port))
      except socket.gaierror:
        raise cls.InvalidEnsemble('Could not resolve %s' % server)
    return ','.join(server_ports)

  DEFAULT_TIMEOUT_SECONDS = 30.0
  DEFAULT_ENSEMBLE = 'localhost:2181'
  DEFAULT_PORT = 2181
  DEFAULT_ACL = ZooDefs.Acls.OPEN_ACL_UNSAFE
  MAX_RECONNECTS = 1

  # (is live?, is stopped?) => human readable status
  STATUS_MATRIX = {
    (True, True): 'WTF',
    (True, False): 'OK',
    (False, True): 'STOPPED',
    (False, False): 'CONNECTING'
  }

  class Completion(object):
    def __init__(self, zk, function, *args, **kw):
      self._zk = zk
      self._cid = random.randint(0, sys.maxint - 1)
      self._logger = kw.pop('logger', log.debug)
      @wraps(function)
      def wrapper(zh):
        return function(zh, *args, **kw)
      self._fn = wrapper
      self._logger('Created %s args:(%s) kw:{%s}' % (
        self,
        ', '.join(map(repr, args)),
        ', '.join('%s: %r' % (key, val) for key, val in kw.items())))

    def __str__(self):
      return '%s(id:%s, zh:%s, %s)' % (
          self.__class__.__name__, self._cid, self._zk._zh, self._fn.__name__)

    def __call__(self):
      try:
        self._logger('%s start' % self)
        result = self._fn(self._zk._zh)
        self._logger('%s success' % self)
        return result
      except TypeError as e:
        # Raced; zh now dead, so re-enqueue.
        if self._zk._zh is not None:
          raise
        self._logger('%s raced, re-enqueueing' % self)
        self._zk._add_completion(self._fn)
      except (zookeeper.ConnectionLossException,
              zookeeper.InvalidStateException,
              zookeeper.SessionExpiredException,
              SystemError) as e:
        self._logger('%s excepted (%s), re-enqueueing' % (self, e))
        self._zk._add_completion(self._fn)
      return zookeeper.OK

  # N.B.(wickman) This is code is theoretically racy.  We cannot synchronize
  # events across the zookeeper C event loop, however we do everything in
  # our power to catch transitional latches.  These are almost always
  # exercised in tests and never in practice.
  #
  # TODO(wickman) ConnectionLoss probably does not encapsulate all the
  # exception states that arise on connection loss and/or session
  # expiration.  However, we don't want to blanket catch ZooKeeperException
  # because some things e.g.  get() will raise NoNodeException.  We should
  # partition the exception space in two: behavioral exceptions and, well,
  # exceptional exceptions.
  class BlockingCompletion(Completion):
    def __call__(self):
      while True:
        try:
          self._logger('%s start' % self)
          result = self._fn(self._zk._zh)
          self._logger('%s success' % self)
          return result
        except (zookeeper.ConnectionLossException,
                zookeeper.InvalidStateException,
                zookeeper.SessionExpiredException,
                TypeError) as e:
          # TypeError because we raced on live latch from True=>False when _zh gets reinitialized.
          if isinstance(e, TypeError) and self._zk._zh is not None:
            self._logger('%s excepted, user error' % self)
            raise
          # We had the misfortune of the live latch being set but having a session event propagate
          # before the BlockingCompletion could be executed.
          while not self._zk._stopped.is_set():
            self._logger('%s [live: %s] excepted on connection event: %s' % (
                self, self._zk._live.is_set(), e))
            self._zk._live.wait(timeout=0.1)
            if self._zk._live.is_set():
              break
          if self._zk._stopped.is_set():
            raise ZooKeeper.Stopped('ZooKeeper is stopped.')
        except Exception as e:
          self._logger('%s excepted unexpectedly: %s' % (self, e))
          raise

  def __init__(self,
               servers=None,
               timeout_secs=None,
               watch=None,
               max_reconnects=None,
               authentication=None,
               logger=log.debug):
    """Create new ZooKeeper object.

    Blocks until ZK negotation completes, or the timeout expires. By default
    only tries to connect once.  Use a larger 'max_reconnects' if you want
    to be resilient to things such as DNS outages/changes.

    If watch is set to a function, it is called whenever the global
    zookeeper watch is dispatched using the same function signature, with the
    exception that this object is used in place of the zookeeper handle.

    If authentication is set, it should be a tuple of (scheme, credentials),
    for example, ('digest', 'username:password')
    """

    default_ensemble = self.DEFAULT_ENSEMBLE
    default_timeout = self.DEFAULT_TIMEOUT_SECONDS
    default_reconnects = self.MAX_RECONNECTS
    if WITH_APP:
      options = app.get_options()
      default_ensemble = options.twitter_common_zookeeper_ensemble
      default_timeout = options.twitter_common_zookeeper_timeout
      default_reconnects = options.twitter_common_zookeeper_reconnects
    self._servers = servers or default_ensemble
    self._timeout_secs = timeout_secs or default_timeout
    self._init_count = 0
    self._credentials = authentication
    self._authenticated = threading.Event()
    self._live = threading.Event()
    self._stopped = threading.Event()
    self._completions = Queue()
    self._zh = None
    self._watch = watch
    self._logger = logger
    self._max_reconnects = max_reconnects if max_reconnects is not None else default_reconnects
    self._init_metrics()
    self.reconnect()

  def __del__(self):
    self._safe_close()

  def _log(self, msg):
    self._logger('[zh:%s] %s' % (self._zh, msg))

  def _init_metrics(self):
    self._session_expirations = AtomicGauge('session_expirations')
    self._connection_losses = AtomicGauge('connection_losses')
    self.metrics.register(self._session_expirations)
    self.metrics.register(self._connection_losses)
    self.metrics.register(LambdaGauge('session_id', lambda: self.session_id))
    self.metrics.register(LambdaGauge('live', lambda: int(self._live.is_set())))

  @property
  def session_id(self):
    try:
      session_id, _ = zookeeper.client_id(self._zh)
      return session_id
    except:
      return None

  @property
  def session_expirations(self):
    return self._session_expirations.read()

  @property
  def connection_losses(self):
    return self._connection_losses.read()

  @property
  def live(self):
    return self._live

  def stop(self):
    """Gracefully stop this Zookeeper session."""
    self._log('Shutting down ZooKeeper')
    self._stopped.set()
    self._safe_close()
    self._completions = Queue()  # there is no .clear()

  def restart(self):
    """Stop and restart this Zookeeper session.  Unfinished completions will be retried
       on reconnection."""
    self._safe_close()
    self._stopped.clear()
    self.reconnect()

  def _safe_close(self):
    if self._zh is not None:
      zh, self._zh = self._zh, None
      try:
        zookeeper.close(zh)
      except zookeeper.ZooKeeperException:
        # the session has been corrupted or otherwise disconnected
        pass
      self._live.clear()

  def _add_completion(self, function, *args, **kw):
    self._completions.put(self.Completion(self, function, logger=self._log, *args, **kw))

  def _clear_completions(self):
    while self._live.is_set():
      try:
        completion = self._completions.get_nowait()
        completion()
        self._completions.task_done()
      except Empty:
        return

  def reconnect(self):
    """Attempt to reconnect to ZK."""
    if self._stopped.is_set():
      self._safe_close()
      return

    def safe_close(zh):
      try:
        zookeeper.close(zh)
      except:
        # TODO(wickman) When the SystemError bug is fixed in zkpython, narrow this except clause.
        pass

    def activate():
      self._authenticated.set()
      self._live.set()

    def on_authentication(zh, rc):
      if self._zh != zh:
        safe_close(zh)
        return
      if rc == zookeeper.OK:
        activate()

    def maybe_authenticate():
      if self._authenticated.is_set() or not self._credentials:
        activate()
        return
      try:
        scheme, credentials = self._credentials
        zookeeper.add_auth(self._zh, scheme, credentials, on_authentication)
      except zookeeper.ZooKeeperException as e:
        self._logger('Failed to authenticate: %s' % e)

    def connection_handler(handle, type, state, path):
      if self._zh != handle:
        safe_close(handle)
        return
      if self._stopped.is_set():
        return
      if self._watch:
        self._watch(self, type, state, path)
      if state == zookeeper.CONNECTED_STATE:
        self._logger('Connection started, setting live.')
        maybe_authenticate()
        self._clear_completions()
      elif state == zookeeper.EXPIRED_SESSION_STATE:
        self._logger('Session lost, clearing live state.')
        self._session_expirations.increment()
        self._live.clear()
        self._authenticated.clear()
        self._zh = None
        self._init_count = 0
        self.reconnect()
      else:
        self._logger('Connection lost, clearing live state.')
        self._connection_losses.increment()
        self._live.clear()

    # this closure is exposed for testing only -- in order to simulate session events.
    self._handler = connection_handler

    timeout_ms = int(self._timeout_secs * 1000)
    while True:
      self._safe_close()
      servers = self.expand_ensemble(self._servers)
      self._log('Connecting to ZK hosts at %s' % servers)
      self._zh = zookeeper.init(servers, connection_handler, timeout_ms)
      self._init_count += 1
      self._live.wait(self._timeout_secs + 1)
      if self._live.is_set():
        break
      elif self._max_reconnects > 0 and self._init_count >= self._max_reconnects:
        self._safe_close()
        raise ZooKeeper.ConnectionTimeout('Timed out waiting for ZK connection to %s' % servers)
    self._log('Successfully connected to ZK at %s' % servers)

  def _wrap_sync(self, function_name):
    """Wrap a zookeeper module function in an error-handling completion that injects the
       current zookeeper handle as the first parameter."""
    function = getattr(zookeeper, function_name)
    @wraps(function)
    def _curry(*args, **kwargs):
      return self.BlockingCompletion(self, function, logger=self._log, *args, **kwargs)()
    return _curry

  def _wrap_async(self, function_name):
    """Wrap an asynchronous zookeeper module function in an error-handling
       completion that injects the current zookeeper handle as the first
       parameter and puts it on a completion queue if the current connection
       state is unhealthy."""
    function = getattr(zookeeper, function_name)
    @wraps(function)
    def _curry(*args, **kwargs):
      completion = self.Completion(self, function, logger=self._log, *args, **kwargs)
      if self._live.is_set():
        return completion()
      else:
        # TODO(wickman)  This is racy, should it go from not live => live
        # prior to Queue.put.  Two solutions: a periodic background thread
        # that attempts to empty the completion queue, or use a mutex-protected
        # container for self._live.
        self._completions.put(self.Completion(self, function, logger=self._log, *args, **kwargs))
        return zookeeper.OK  # proxy OK.
    return _curry

  def safe_create(self, path, acl=DEFAULT_ACL):
    child = '/'
    for component in filter(None, path.split('/')):
      child = posixpath.join(child, component)
      try:
        self.create(child, "", acl, 0)
      except zookeeper.NodeExistsException:
        continue
      except zookeeper.NoAuthException:
        if not self.exists(child):
          raise
    return child

  def safe_delete(self, path):
    try:
      if not self.exists(path):
        return True
      for child in self.get_children(path):
        if not self.safe_delete(posixpath.join(path, child)):
          return False
      self.delete(path)
    except zookeeper.ZooKeeperException:
      return False
    return True

  def __getattr__(self, function_name):
    """Proxy to underlying ZK functions."""
    if function_name in ZooKeeper._ZK_SYNC_METHODS:
      return self._wrap_sync(function_name)
    elif function_name in ZooKeeper._ZK_ASYNC_METHODS:
      return self._wrap_async(function_name)
    else:
      raise AttributeError('%r has no attribute %r' % (self, function_name))

  def __str__(self):
    return 'ZooKeeper(status=%s,queued=%d,servers=%r)' % (
      self.STATUS_MATRIX[(self._live.is_set(), self._stopped.is_set())],
      self._completions.qsize(), self._servers)

  def __repr__(self):
    return 'ZooKeeper(servers=%r)' % self._servers

########NEW FILE########
__FILENAME__ = constants
import zookeeper

from .named_value import NamedValue


class Event(NamedValue):
  MAP = {
    0: 'UNKNOWN',
    zookeeper.CREATED_EVENT: 'CREATED',
    zookeeper.DELETED_EVENT: 'DELETED',
    zookeeper.CHANGED_EVENT: 'CHANGED',
    zookeeper.CHILD_EVENT: 'CHILD',
    zookeeper.SESSION_EVENT: 'SESSION',
    zookeeper.NOTWATCHING_EVENT: 'NOTWATCHING'
  }

  @property
  def map(self):
    return self.MAP


class State(NamedValue):
  MAP = {
    0: 'UNKNOWN',
    zookeeper.CONNECTING_STATE: 'CONNECTING',
    zookeeper.ASSOCIATING_STATE: 'ASSOCIATING',
    zookeeper.CONNECTED_STATE: 'CONNECTED',
    zookeeper.EXPIRED_SESSION_STATE: 'EXPIRED_SESSION',
    zookeeper.AUTH_FAILED_STATE: 'AUTH_FAILED',
  }

  @property
  def map(self):
    return self.MAP


class ReturnCode(NamedValue):
  MAP = {
    # Normal
    zookeeper.OK: 'OK',

    # Abnormal
    zookeeper.NONODE: 'NONODE',
    zookeeper.NOAUTH: 'NOAUTH',
    zookeeper.BADVERSION: 'BADVERSION',
    zookeeper.NOCHILDRENFOREPHEMERALS: 'NOCHILDRENFOREPHEMERALS',
    zookeeper.NODEEXISTS: 'NODEEXISTS',
    zookeeper.NOTEMPTY: 'NOTEMPTY',
    zookeeper.SESSIONEXPIRED: 'SESSIONEXPIRED',
    zookeeper.INVALIDCALLBACK: 'INVALIDCALLBACK',
    zookeeper.INVALIDACL: 'INVALIDACL',
    zookeeper.AUTHFAILED: 'AUTHFAILED',
    zookeeper.CLOSING: 'CLOSING',
    zookeeper.NOTHING: 'NOTHING',
    zookeeper.SESSIONMOVED: 'SESSIONMOVED',

    # Exceptional
    zookeeper.SYSTEMERROR: 'SYSTEMERROR',
    zookeeper.RUNTIMEINCONSISTENCY: 'RUNTIMEINCONSISTENCY',
    zookeeper.DATAINCONSISTENCY: 'DATAINCONSISTENCY',
    zookeeper.CONNECTIONLOSS: 'CONNECTIONLOSS',
    zookeeper.MARSHALLINGERROR: 'MARSHALLINGERROR',
    zookeeper.UNIMPLEMENTED: 'UNIMPLEMENTED',
    zookeeper.OPERATIONTIMEOUT: 'OPERATIONTIMEOUT',
    zookeeper.BADARGUMENTS: 'BADARGUMENTS',
    zookeeper.INVALIDSTATE: 'INVALIDSTATE'
  }

  @property
  def map(self):
    return self.MAP


class Id(object):
  def __init__(self, scheme, id_):
    if not isinstance(scheme, str) or not isinstance(id_, str):
      raise ValueError('Scheme and id must be strings!')
    self.scheme = scheme
    self.id = id_


class Acl(dict):
  def __init__(self, perm, id_):
    dict.__init__(self)
    self['perms'] = perm
    self['scheme'] = id_.scheme
    self['id'] = id_.id

########NEW FILE########
__FILENAME__ = group
import functools
import posixpath
import threading
import time

try:
  from twitter.common import log
except ImportError:
  import logging as log

from twitter.common.concurrent import Future
from twitter.common.exceptions import ExceptionalThread
from twitter.common.quantity import Amount, Time
from twitter.common.zookeeper.constants import ReturnCode

from .group_base import (
    Capture,
    GroupBase,
    GroupInterface,
    Membership,
    set_different)

import zookeeper


class Group(GroupBase, GroupInterface):
  """
    An implementation of GroupInterface against CZookeeper.
  """

  def __init__(self, zk, path, acl=None):
    self._zk = zk
    self._path = '/' + '/'.join(filter(None, path.split('/')))  # normalize path
    self._members = {}
    self._member_lock = threading.Lock()
    self._acl = acl or zk.DEFAULT_ACL

  def _prepare_path(self, success):
    class Background(ExceptionalThread):
      BACKOFF = Amount(5, Time.SECONDS)
      def run(_):
        child = '/'
        for component in self._path.split('/')[1:]:
          child = posixpath.join(child, component)
          while True:
            try:
              self._zk.create(child, "", self._acl)
              break
            except zookeeper.NodeExistsException:
              break
            except zookeeper.NoAuthException:
              if self._zk.exists(child):
                break
              else:
                success.set(False)
                return
            except zookeeper.OperationTimeoutException:
              time.sleep(Background.BACKOFF.as_(Time.SECONDS))
              continue
        success.set(True)
    background = Background()
    background.daemon = True
    background.start()

  def info(self, member, callback=None):
    if member == Membership.error():
      raise self.InvalidMemberError('Cannot get info on error member!')

    capture = Capture(callback)

    def do_info():
      self._zk.aget(path, None, info_completion)

    with self._member_lock:
      if member not in self._members:
        self._members[member] = Future()
      member_future = self._members[member]

    member_future.add_done_callback(lambda future: capture.set(future.result()))

    dispatch = False
    with self._member_lock:
      if not member_future.done() and not member_future.running():
        try:
          dispatch = member_future.set_running_or_notify_cancel()
        except:
          pass

    def info_completion(_, rc, content, stat):
      if rc in self._zk.COMPLETION_RETRY:
        do_info()
        return
      if rc == zookeeper.NONODE:
        future = self._members.pop(member, Future())
        future.set_result(Membership.error())
        return
      elif rc != zookeeper.OK:
        return
      self._members[member].set_result(content)

    if dispatch:
      path = posixpath.join(self._path, self.id_to_znode(member.id))
      do_info()

    return capture()

  def join(self, blob, callback=None, expire_callback=None):
    membership_capture = Capture(callback)
    exists_capture = Capture(expire_callback)

    def on_prepared(success):
      if success:
        do_join()
      else:
        membership_capture.set(Membership.error())

    prepare_capture = Capture(on_prepared)

    def do_join():
      self._zk.acreate(posixpath.join(self._path, self.MEMBER_PREFIX),
          blob, self._acl, zookeeper.SEQUENCE | zookeeper.EPHEMERAL, acreate_completion)

    def exists_watch(_, event, state, path):
      if (event == zookeeper.SESSION_EVENT and state == zookeeper.EXPIRED_SESSION_STATE) or (
          event == zookeeper.DELETED_EVENT):
        exists_capture.set()

    def exists_completion(path, _, rc, stat):
      if rc in self._zk.COMPLETION_RETRY:
        self._zk.aexists(path, exists_watch, functools.partial(exists_completion, path))
        return
      if rc == zookeeper.NONODE:
        exists_capture.set()

    def acreate_completion(_, rc, path):
      if rc in self._zk.COMPLETION_RETRY:
        do_join()
        return
      if rc == zookeeper.OK:
        created_id = self.znode_to_id(path)
        membership = Membership(created_id)
        with self._member_lock:
          result_future = self._members.get(membership, Future())
          result_future.set_result(blob)
          self._members[membership] = result_future
        if expire_callback:
          self._zk.aexists(path, exists_watch, functools.partial(exists_completion, path))
      else:
        membership = Membership.error()
      membership_capture.set(membership)

    self._prepare_path(prepare_capture)
    return membership_capture()

  def cancel(self, member, callback=None):
    capture = Capture(callback)

    def do_cancel():
      self._zk.adelete(posixpath.join(self._path, self.id_to_znode(member.id)),
        -1, adelete_completion)

    def adelete_completion(_, rc):
      if rc in self._zk.COMPLETION_RETRY:
        do_cancel()
        return
      # The rationale here is two situations:
      #   - rc == zookeeper.OK ==> we successfully deleted the znode and the membership is dead.
      #   - rc == zookeeper.NONODE ==> the membership is dead, though we may not have actually
      #      been the ones to cancel, or the node never existed in the first place.  it's possible
      #      we owned the membership but it got severed due to session expiration.
      if rc == zookeeper.OK or rc == zookeeper.NONODE:
        future = self._members.pop(member.id, Future())
        future.set_result(Membership.error())
        capture.set(True)
      else:
        capture.set(False)

    do_cancel()
    return capture()

  def monitor(self, membership=frozenset(), callback=None):
    capture = Capture(callback)

    def wait_exists():
      self._zk.aexists(self._path, exists_watch, exists_completion)

    def exists_watch(_, event, state, path):
      if event == zookeeper.SESSION_EVENT and state == zookeeper.EXPIRED_SESSION_STATE:
        wait_exists()
        return
      if event == zookeeper.CREATED_EVENT:
        do_monitor()
      elif event == zookeeper.DELETED_EVENT:
        wait_exists()

    def exists_completion(_, rc, stat):
      if rc == zookeeper.OK:
        do_monitor()

    def do_monitor():
      self._zk.aget_children(self._path, get_watch, get_completion)

    def get_watch(_, event, state, path):
      if event == zookeeper.SESSION_EVENT and state == zookeeper.EXPIRED_SESSION_STATE:
        wait_exists()
        return
      if state != zookeeper.CONNECTED_STATE:
        return
      if event == zookeeper.DELETED_EVENT:
        wait_exists()
        return
      if event != zookeeper.CHILD_EVENT:
        return
      if set_different(capture, membership, self._members):
        return
      do_monitor()

    def get_completion(_, rc, children):
      if rc in self._zk.COMPLETION_RETRY:
        do_monitor()
        return
      if rc == zookeeper.NONODE:
        wait_exists()
        return
      if rc != zookeeper.OK:
        log.warning('Unexpected get_completion return code: %s' % ReturnCode(rc))
        capture.set(set([Membership.error()]))
        return
      self._update_children(children)
      set_different(capture, membership, self._members)

    do_monitor()
    return capture()

  def list(self):
    try:
      return sorted(map(lambda znode: Membership(self.znode_to_id(znode)),
          filter(self.znode_owned, self._zk.get_children(self._path))))
    except zookeeper.NoNodeException:
      return []


class ActiveGroup(Group):
  """
    An implementation of GroupInterface against CZookeeper when iter() and
    monitor() are expected to be called frequently.  Constantly monitors
    group membership and the contents of group blobs.
  """

  def __init__(self, *args, **kwargs):
    super(ActiveGroup, self).__init__(*args, **kwargs)
    self._monitor_queue = []
    self._monitor_members()

  def monitor(self, membership=frozenset(), callback=None):
    capture = Capture(callback)
    if not set_different(capture, membership, self._members):
      self._monitor_queue.append((membership, capture))
    return capture()

  # ---- private api

  def _monitor_members(self):
    def wait_exists():
     self._zk.aexists(self._path, exists_watch, exists_completion)

    def exists_watch(_, event, state, path):
      if event == zookeeper.SESSION_EVENT and state == zookeeper.EXPIRED_SESSION_STATE:
        wait_exists()
        return
      if event == zookeeper.CREATED_EVENT:
        do_monitor()
      elif event == zookeeper.DELETED_EVENT:
        wait_exists()

    def exists_completion(_, rc, stat):
      if rc == zookeeper.OK:
        do_monitor()

    def do_monitor():
      self._zk.aget_children(self._path, membership_watch, membership_completion)

    def membership_watch(_, event, state, path):
      # Connecting state is caused by transient connection loss, ignore
      if state == zookeeper.CONNECTING_STATE:
        return
      if event == zookeeper.DELETED_EVENT:
        wait_exists()
        return
      # Everything else indicates underlying change.
      do_monitor()

    def membership_completion(_, rc, children):
      if rc in self._zk.COMPLETION_RETRY:
        do_monitor()
        return
      if rc == zookeeper.NONODE:
        wait_exists()
        return
      if rc != zookeeper.OK:
        return

      children = [child for child in children if self.znode_owned(child)]
      _, new = self._update_children(children)
      for child in new:
        def devnull(*args, **kw): pass
        self.info(child, callback=devnull)

      monitor_queue = self._monitor_queue[:]
      self._monitor_queue = []
      members = set(Membership(self.znode_to_id(child)) for child in children)
      for membership, capture in monitor_queue:
        if set(membership) != members:
          capture.set(members)
        else:
          self._monitor_queue.append((membership, capture))

    do_monitor()

########NEW FILE########
__FILENAME__ = group_base
from abc import abstractmethod
import posixpath
import threading

from twitter.common.concurrent import Future
from twitter.common.lang import Interface


class Membership(object):
  ERROR_ID = -1

  @staticmethod
  def error():
    return Membership(_id=Membership.ERROR_ID)

  def __init__(self, _id):
    self._id = _id

  @property
  def id(self):
    return self._id

  def __lt__(self, other):
    return self._id < other._id

  def __eq__(self, other):
    if not isinstance(other, Membership):
      return False
    return self._id == other._id

  def __ne__(self, other):
    return not self == other

  def __hash__(self):
    return hash(self._id)

  def __repr__(self):
    if self._id == Membership.ERROR_ID:
      return 'Membership.error()'
    else:
      return 'Membership(%r)' % self._id


class GroupInterface(Interface):
  """
    A group of members backed by immutable blob data.
  """

  @abstractmethod
  def join(self, blob, callback=None, expire_callback=None):
    """
      Joins the Group using the blob.  Returns Membership synchronously if
      callback is None, otherwise returned to callback.  Returns
      Membership.error() on failure to join.

      If expire_callback is provided, it is called with no arguments when
      the membership is terminated for any reason.
    """

  @abstractmethod
  def info(self, membership, callback=None):
    """
      Given a membership, return the blob associated with that member or
      Membership.error() if no membership exists.  If callback is provided,
      this operation is done asynchronously.
    """

  @abstractmethod
  def cancel(self, membership, callback=None):
    """
      Cancel a given membership.  Returns true if/when the membership does not
      exist.  Returns false if the membership exists and we failed to cancel
      it.  If callback is provided, this operation is done asynchronously.
    """

  @abstractmethod
  def monitor(self, membership_set=frozenset(), callback=None):
    """
      Given a membership set, return once the underlying group membership is
      different.  If callback is provided, this operation is done
      asynchronously.
    """

  @abstractmethod
  def list(self):
    """
      Synchronously return the list of underlying members.  Should only be
      used in place of monitor if you cannot afford to block indefinitely.
    """


# TODO(wickman) The right abstraction here is probably IAsyncResult from Kazoo.
# Kill this in favor of the better abstraction.
class Capture(object):
  """
    A Capture is a mechanism to capture a value to be dispatched via a
    callback or blocked upon.  If Capture is supplied with a callback, the
    callback is called once the value is available, in which case
    Capture.__call__() will return immediately.  If no callback has been
    supplied, Capture.__call__() blocks until a value is available.
  """
  def __init__(self, callback=None):
    self._value = None
    self._event = threading.Event()
    self._callback = callback

  def set(self, value=None):
    self._value = value
    self._event.set()
    if self._callback:
      if value is not None:
        self._callback(value)
      else:
        self._callback()
      self._callback = None

  def get(self):
    self._event.wait()
    return self._value

  def __call__(self):
    if self._callback:
      return None
    return self.get()


def set_different(capture, current_members, actual_members):
  current_members = set(current_members)
  actual_members = set(actual_members)
  if current_members != actual_members:
    capture.set(actual_members)
    return True


class GroupBase(object):
  class GroupError(Exception): pass
  class InvalidMemberError(GroupError): pass

  MEMBER_PREFIX = 'member_'

  @classmethod
  def znode_owned(cls, znode):
    return posixpath.basename(znode).startswith(cls.MEMBER_PREFIX)

  @classmethod
  def znode_to_id(cls, znode):
    znode_name = posixpath.basename(znode)
    assert znode_name.startswith(cls.MEMBER_PREFIX)
    return int(znode_name[len(cls.MEMBER_PREFIX):])

  @classmethod
  def id_to_znode(cls, _id):
    return '%s%010d' % (cls.MEMBER_PREFIX, _id)

  def __iter__(self):
    return iter(self._members)

  def __getitem__(self, member):
    return self.info(member)

  def _update_children(self, children):
    """
      Given a new child list [znode strings], return a tuple of sets of Memberships:
        left: the children that left the set
        new: the children that joined the set
    """
    cached_children = set(self._members)
    current_children = set(Membership(self.znode_to_id(znode))
        for znode in filter(self.znode_owned, children))
    new = current_children - cached_children
    left = cached_children - current_children
    for child in left:
      future = self._members.pop(child, Future())
      future.set_result(Membership.error())
    for child in new:
      self._members[child] = Future()
    return left, new

########NEW FILE########
__FILENAME__ = kazoo_cli
from twitter.common import app

from kazoo.client import KazooClient
from twitter.common.zookeeper.group.kazoo_group import KazooGroup


def main():
  import code
  code.interact(local=globals())


app.main()

########NEW FILE########
__FILENAME__ = kazoo_group
from functools import partial
import itertools
import posixpath
import threading

try:
  from twitter.common import log
except ImportError:
  import logging as log

from twitter.common.concurrent import Future

from .group_base import (
    Capture,
    GroupBase,
    GroupInterface,
    Membership,
    set_different)

from kazoo.client import KazooClient
from kazoo.protocol.states import (
    EventType,
    KazooState,
    KeeperState)

import kazoo.security as ksec
import kazoo.exceptions as ke


# TODO(wickman) Put this in twitter.common somewhere?
def partition(items, predicate=bool):
  a, b = itertools.tee((predicate(item), item) for item in items)
  return ([item for pred, item in a if not pred], [item for pred, item in b if pred])


class KazooGroup(GroupBase, GroupInterface):
  """
    An implementation of GroupInterface against Kazoo.
  """
  DISCONNECT_EXCEPTIONS = (ke.ConnectionLoss, ke.OperationTimeoutError, ke.SessionExpiredError)

  @classmethod
  def translate_acl(cls, acl):
    if not isinstance(acl, dict) or any(key not in acl for key in ('perms', 'scheme', 'id')):
      raise TypeError('Expected acl to be Acl-like, got %s' % type(acl))
    return ksec.ACL(acl['perms'], ksec.Id(acl['scheme'], acl['id']))

  @classmethod
  def translate_acl_list(cls, acls):
    if acls is None:
      return acls
    try:
      acls = list(acls)
    except (ValueError, TypeError):
      raise TypeError('ACLs should be a list, got %s' % type(acls))
    if all(isinstance(acl, ksec.ACL) for acl in acls):
      return acls
    else:
      return [cls.translate_acl(acl) for acl in acls]

  def __init__(self, zk, path, acl=None):
    if not isinstance(zk, KazooClient):
      raise TypeError('KazooGroup must be initialized with a KazooClient')
    self._zk = zk
    self.__state = zk.state
    self.__listener_queue = []
    self.__queue_lock = threading.Lock()
    self._zk.add_listener(self.__state_listener)
    self._path = '/' + '/'.join(filter(None, path.split('/')))  # normalize path
    self._members = {}
    self._member_lock = threading.Lock()
    self._acl = self.translate_acl_list(acl)

  def __state_listener(self, state):
    """Process appropriate callbacks on any kazoo state transition."""
    with self.__queue_lock:
      self.__state = state
      self.__listener_queue, triggered = partition(self.__listener_queue,
          lambda element: element[0] == state)
    for _, callback in triggered:
      callback()

  def _once(self, keeper_state, callback):
    """Ensure a callback is called once we reach the given state: either
       immediately, if currently in that state, or on the next transition to
       that state."""
    invoke = False
    with self.__queue_lock:
      if self.__state != keeper_state:
        self.__listener_queue.append((keeper_state, callback))
      else:
        invoke = True
    if invoke:
      callback()

  def __on_connected(self, callback):
    return self.__on_state(callback, KazooState.CONNECTED)

  def __on_expired(self, callback):
    return self.__on_state(callback, KazooState.LOST)

  def info(self, member, callback=None):
    if member == Membership.error():
      raise self.InvalidMemberError('Cannot get info on error member!')

    capture = Capture(callback)

    def do_info():
      self._zk.get_async(path).rawlink(info_completion)

    with self._member_lock:
      member_future = self._members.setdefault(member, Future())

    member_future.add_done_callback(lambda future: capture.set(future.result()))

    dispatch = False
    with self._member_lock:
      if not member_future.done() and not member_future.running():
        try:
          dispatch = member_future.set_running_or_notify_cancel()
        except:
          pass

    def info_completion(result):
      try:
        content, stat = result.get()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, do_info)
        return
      except ke.NoNodeException:
        future = self._members.pop(member, Future())
        future.set_result(Membership.error())
        return
      except ke.KazooException as e:
        log.warning('Unexpected Kazoo result in info: (%s)%s' % (type(e), e))
        future = self._members.pop(member, Future())
        future.set_result(Membership.error())
        return
      self._members[member].set_result(content)

    if dispatch:
      path = posixpath.join(self._path, self.id_to_znode(member.id))
      do_info()

    return capture()

  def join(self, blob, callback=None, expire_callback=None):
    membership_capture = Capture(callback)
    expiry_capture = Capture(expire_callback)

    def do_join():
      self._zk.create_async(
          path=posixpath.join(self._path, self.MEMBER_PREFIX),
          value=blob,
          acl=self._acl,
          sequence=True,
          ephemeral=True,
          makepath=True
      ).rawlink(acreate_completion)

    def do_exists(path):
      self._zk.exists_async(path, watch=exists_watch).rawlink(partial(exists_completion, path))

    def exists_watch(event):
      if event.type == EventType.DELETED:
        expiry_capture.set()

    def expire_notifier():
      self._once(KazooState.LOST, expiry_capture.set)

    def exists_completion(path, result):
      try:
        if result.get() is None:
          expiry_capture.set()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, partial(do_exists, path))

    def acreate_completion(result):
      try:
        path = result.get()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, do_join)
        return
      except ke.KazooException as e:
        log.warning('Unexpected Kazoo result in join: (%s)%s' % (type(e), e))
        membership = Membership.error()
      else:
        created_id = self.znode_to_id(path)
        membership = Membership(created_id)
        with self._member_lock:
          result_future = self._members.get(membership, Future())
          result_future.set_result(blob)
          self._members[membership] = result_future
        if expire_callback:
          self._once(KazooState.CONNECTED, expire_notifier)
          do_exists(path)

      membership_capture.set(membership)

    do_join()
    return membership_capture()

  def cancel(self, member, callback=None):
    capture = Capture(callback)

    def do_cancel():
      self._zk.delete_async(posixpath.join(self._path, self.id_to_znode(member.id))).rawlink(
          adelete_completion)

    def adelete_completion(result):
      try:
        success = result.get()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, do_cancel)
        return
      except ke.NoNodeError:
        success = True
      except ke.KazooException as e:
        log.warning('Unexpected Kazoo result in cancel: (%s)%s' % (type(e), e))
        success = False

      future = self._members.pop(member.id, Future())
      future.set_result(Membership.error())
      capture.set(success)

    do_cancel()
    return capture()

  def monitor(self, membership=frozenset(), callback=None):
    capture = Capture(callback)

    def wait_exists():
      self._zk.exists_async(self._path, exists_watch).rawlink(exists_completion)

    def exists_watch(event):
      if event.state == KeeperState.EXPIRED_SESSION:
        wait_exists()
        return
      if event.type == EventType.CREATED:
        do_monitor()
      elif event.type == EventType.DELETED:
        wait_exists()

    def exists_completion(result):
      try:
        stat = result.get()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, wait_exists)
        return
      except ke.NoNodeError:
        wait_exists()
        return
      except ke.KazooException as e:
        log.warning('Unexpected exists_completion result: (%s)%s' % (type(e), e))
        return

      if stat:
        do_monitor()

    def do_monitor():
      self._zk.get_children_async(self._path, get_watch).rawlink(get_completion)

    def get_watch(event):
      if event.state == KeeperState.EXPIRED_SESSION:
        wait_exists()
        return
      if event.state != KeeperState.CONNECTED:
        return
      if event.type == EventType.DELETED:
        wait_exists()
        return
      if event.type != EventType.CHILD:
        return
      if set_different(capture, membership, self._members):
        return
      do_monitor()

    def get_completion(result):
      try:
        children = result.get()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, do_monitor)
        return
      except ke.NoNodeError:
        wait_exists()
        return
      except ke.KazooException as e:
        log.warning('Unexpected get_completion result: (%s)%s' % (type(e), e))
        capture.set(set([Membership.error()]))
        return
      self._update_children(children)
      set_different(capture, membership, self._members)

    do_monitor()
    return capture()

  def list(self):
    wait_event = threading.Event()
    while True:
      wait_event.clear()
      try:
        try:
          return sorted(Membership(self.znode_to_id(znode))
                        for znode in self._zk.get_children(self._path)
                        if self.znode_owned(znode))
        except ke.NoNodeException:
          return []
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, wait_event.set)
        wait_event.wait()


class ActiveKazooGroup(KazooGroup):
  def __init__(self, *args, **kwargs):
    super(ActiveKazooGroup, self).__init__(*args, **kwargs)
    self._monitor_queue = []
    self._monitor_members()

  def monitor(self, membership=frozenset(), callback=None):
    capture = Capture(callback)
    if not set_different(capture, membership, self._members):
      self._monitor_queue.append((membership, capture))

    return capture()

  def _monitor_members(self):
    def wait_exists():
      self._zk.exists_async(self._path, exists_watch).rawlink(exists_completion)

    def exists_watch(event):
      if event.state == KeeperState.EXPIRED_SESSION:
        wait_exists()
        return
      if event.type == EventType.CREATED:
        do_monitor()
      elif event.type == EventType.DELETED:
        wait_exists()

    def exists_completion(result):
      try:
        stat = result.get()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, wait_exists)
        return
      except ke.NoNodeError:
        wait_exists()
        return
      except ke.KazooException as e:
        log.warning('Unexpected exists_completion result: (%s)%s' % (type(e), e))
        return

      if stat:
        do_monitor()

    def do_monitor():
      self._zk.get_children_async(self._path, get_watch).rawlink(get_completion)

    def get_watch(event):
      if event.state == KeeperState.EXPIRED_SESSION:
        wait_exists()
        return
      if event.state != KeeperState.CONNECTED:
        return
      if event.type == EventType.DELETED:
        wait_exists()
        return

      do_monitor()

    def get_completion(result):
      try:
        children = result.get()
      except self.DISCONNECT_EXCEPTIONS:
        self._once(KazooState.CONNECTED, do_monitor)
        return
      except ke.NoNodeError:
        wait_exists()
        return
      except ke.KazooException as e:
        log.warning('Unexpected get_completion result: (%s)%s' % (type(e), e))
        return

      children = [child for child in children if self.znode_owned(child)]
      _, new = self._update_children(children)
      for child in new:
        def devnull(*args, **kw): pass
        self.info(child, callback=devnull)

      monitor_queue = self._monitor_queue[:]
      self._monitor_queue = []
      members = set(Membership(self.znode_to_id(child)) for child in children)
      for membership, capture in monitor_queue:
        if set(membership) != members:
          capture.set(members)
        else:
          self._monitor_queue.append((membership, capture))

    do_monitor()

########NEW FILE########
__FILENAME__ = kazoo_client
import logging
import threading
import sys

from twitter.common.metrics import (
    AtomicGauge,
    LambdaGauge,
    Observable)

from kazoo.client import KazooClient
from kazoo.protocol.states import KazooState, KeeperState
from kazoo.retry import KazooRetry


DEFAULT_RETRY_MAX_DELAY_SECS = 600


DEFAULT_RETRY_DICT = dict(
    max_tries=None,
    ignore_expire=True,
)


class TwitterKazooClient(KazooClient, Observable):
  @classmethod
  def make(cls, *args, **kw):
    # TODO(jcohen): Consider removing verbose option entirely in favor of just using loglevel.
    verbose = kw.pop('verbose', False)
    async = kw.pop('async', True)

    if verbose:
      loglevel = kw.pop('loglevel', logging.INFO)
    else:
      loglevel = kw.pop('loglevel', sys.maxsize)

    logger = logging.getLogger('kazoo.devnull')
    logger.setLevel(loglevel)
    kw['logger'] = logger

    zk = cls(*args, **kw)
    if async:
      zk.start_async()
      zk.connecting.wait()
    else:
      zk.start()

    return zk

  def __init__(self, *args, **kw):
    if 'connection_retry' not in kw:
      # The default backoff delay limit in kazoo is 3600 seconds, which is generally
      # too conservative for our use cases.  If not supplied by the caller, provide
      # a backoff that will truncate earlier.
      kw['connection_retry'] = KazooRetry(
          max_delay=DEFAULT_RETRY_MAX_DELAY_SECS, **DEFAULT_RETRY_DICT)

    super(TwitterKazooClient, self).__init__(*args, **kw)
    self.connecting = threading.Event()
    self.__session_expirations = AtomicGauge('session_expirations')
    self.__connection_losses = AtomicGauge('connection_losses')
    self.__session_id = LambdaGauge('session_id', lambda: (self._session_id or 0))
    self.metrics.register(self.__session_expirations)
    self.metrics.register(self.__connection_losses)
    self.metrics.register(self.__session_id)
    self.add_listener(self._observable_listener)

  def _observable_listener(self, state):
    if state == KazooState.LOST:
      self.__session_expirations.increment()
    elif state == KazooState.SUSPENDED:
      self.__connection_losses.increment()

  def _session_callback(self, state):
    rc = super(TwitterKazooClient, self)._session_callback(state)
    if state == KeeperState.CONNECTING:
      self.connecting.set()
    return rc

  @property
  def live(self):
    return self._live

########NEW FILE########
__FILENAME__ = named_value
from abc import ABCMeta, abstractproperty
from twitter.common.lang import Compatibility


class NamedValue(object):
  __metaclass__ = ABCMeta

  def __init__(self, value):
    if isinstance(value, int):
      self._value = value if value in self.map else 0
    elif isinstance(value, Compatibility.string):
      self._value = dict((v, k) for (k, v) in self.map.items()).get(value.upper(), 0)
    else:
      raise ValueError('Unknown value: %s' % value)

  @abstractproperty
  def map(self):
    """Returns the map from id => string"""
    pass

  def __str__(self):
    return self.map.get(self._value, 'UNKNOWN')

  def __repr__(self):
    return '%s(%r)' % (self.__class__.__name__, self.map[self._value])

########NEW FILE########
__FILENAME__ = cli
from __future__ import print_function

from datetime import datetime
import time

from twitter.common import app
from twitter.common.zookeeper.client import ZooKeeper
from twitter.common.zookeeper.serverset import ServerSet


def main(args):
  if len(args) != 1:
    app.error('Must supply a serverset path to monitor.')

  def on_join(endpoint):
    print('@ %s += %s' % (datetime.now(), endpoint))

  def on_leave(endpoint):
    print('@ %s -= %s' % (datetime.now(), endpoint))

  ss = ServerSet(ZooKeeper(), args[0], on_join=on_join, on_leave=on_leave)

  while True:
    time.sleep(100)


app.main()

########NEW FILE########
__FILENAME__ = endpoint
import json
from thrift.TSerialization import deserialize as thrift_deserialize

from gen.twitter.thrift.endpoint.ttypes import (
  Endpoint as ThriftEndpoint,
  ServiceInstance as ThriftServiceInstance)

try:
  from twitter.common import log
except ImportError:
  import logging as log

from twitter.common.lang import Compatibility


class Endpoint(object):
  @classmethod
  def unpack_thrift(cls, blob):
    return cls(blob.host, blob.port)

  @classmethod
  def to_dict(cls, endpoint):
    return {
      'host': endpoint.host,
      'port': endpoint.port
    }

  def __init__(self, host, port):
    if not isinstance(host, Compatibility.string):
      raise ValueError('Expected host to be a string!')
    if not isinstance(port, int):
      raise ValueError('Expected port to be an integer!')
    self._host = host
    self._port = port

  def __key(self):
    return (self.host, self.port)

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__key() == other.__key()

  def __hash__(self):
    return hash(self.__key())

  @property
  def host(self):
    return self._host

  @property
  def port(self):
    return self._port

  def __str__(self):
    return '%s:%s' % (self.host, self.port)


class Status(object):
  MAP = {
    0: 'DEAD',
    1: 'STARTING',
    2: 'ALIVE',
    3: 'STOPPING',
    4: 'STOPPED',
    5: 'WARNING'
  }

  @staticmethod
  def from_id(_id):
    if _id not in Status.MAP:
      raise ValueError('from_id got an invalid Status!')
    return Status(Status.MAP.get(_id), _id)

  @staticmethod
  def from_string(string):
    for _id, name in Status.MAP.items():
      if name == string:
        return Status(name, _id)
    raise ValueError('from_string got an invalid Status!')

  from_thrift = from_id

  def __init__(self, name, _id):
    self._name = name
    self._id = _id

  def name(self):
    return self._name

  def __eq__(self, other):
    return self._id == other._id

  def __hash__(self):
    return hash(self._id)

  def __str__(self):
    return self.name()


class ServiceInstance(object):
  class InvalidType(Exception): pass
  class UnknownEndpoint(Exception): pass

  @classmethod
  def unpack(cls, blob):
    try:
      return cls.unpack_json(blob)
    except Exception as e1:
      try:
        return cls.unpack_thrift(blob)
      except Exception as e2:
        log.debug('Failed to deserialize JSON: %s (%s) && Thrift: %s (%s)' % (
          e1, e1.__class__.__name__, e2, e2.__class__.__name__))
        return None

  @classmethod
  def unpack_json(cls, blob):
    blob = json.loads(blob)
    for key in ('status', 'serviceEndpoint', 'additionalEndpoints'):
      if key not in blob:
        raise ValueError('Expected to find %s in ServiceInstance JSON!' % key)
    additional_endpoints = dict((name, Endpoint(value['host'], value['port']))
      for name, value in blob['additionalEndpoints'].items())
    shard = blob.get('shard')
    if shard is not None:
      try:
        shard = int(shard)
      except ValueError:
        log.warn('Failed to deserialize shard from value %r' % shard)
        shard = None
    return cls(
      service_endpoint=Endpoint(blob['serviceEndpoint']['host'], blob['serviceEndpoint']['port']),
      additional_endpoints=additional_endpoints,
      status=Status.from_string(blob['status']),
      shard=shard)

  @classmethod
  def unpack_thrift(cls, blob):
    if not isinstance(blob, ThriftServiceInstance):
      blob = thrift_deserialize(ThriftServiceInstance(), blob)
    additional_endpoints = dict((name, Endpoint.unpack_thrift(value))
      for name, value in blob.additionalEndpoints.items())
    return cls(
      service_endpoint=Endpoint.unpack_thrift(blob.serviceEndpoint),
      additional_endpoints=additional_endpoints,
      status=Status.from_thrift(blob.status),
      shard=blob.shard)

  @classmethod
  def to_dict(cls, service_instance):
    instance = dict(
      serviceEndpoint=Endpoint.to_dict(service_instance.service_endpoint),
      additionalEndpoints=dict((name, Endpoint.to_dict(endpoint))
          for name, endpoint in service_instance.additional_endpoints.items()),
      status=service_instance.status.name()
    )
    if service_instance.shard is not None:
      instance.update(shard=service_instance.shard)
    return instance

  @classmethod
  def pack(cls, service_instance):
    return json.dumps(cls.to_dict(service_instance))

  def __init__(self, service_endpoint, additional_endpoints=None, status='ALIVE', shard=None):
    if not isinstance(service_endpoint, Endpoint):
      raise ValueError('Expected service_endpoint to be an Endpoint, got %r' % service_endpoint)
    self._shard = shard
    self._service_endpoint = service_endpoint
    self._additional_endpoints = additional_endpoints or {}
    if not isinstance(self._additional_endpoints, dict):
      raise ValueError('Additional endpoints must be a dictionary.')
    for name, endpoint in self._additional_endpoints.items():
      if not isinstance(name, Compatibility.string):
        raise ValueError('Expected additional endpoints to be named by strings.')
      if not isinstance(endpoint, Endpoint):
        raise ValueError('Endpoints must be of type Endpoint.')
    if isinstance(status, Compatibility.string):
      self._status = Status.from_string(status)
      if self._status is None:
        raise ValueError('Unknown status: %s' % status)
    elif isinstance(status, Status):
      self._status = status
    else:
      raise ValueError('Status must be of type ServiceInstance.Status or string.')

  @property
  def service_endpoint(self):
    return self._service_endpoint

  @property
  def additional_endpoints(self):
    return self._additional_endpoints

  @property
  def status(self):
    return self._status

  @property
  def shard(self):
    return self._shard

  def __additional_endpoints_string(self):
    return ['%s=>%s' % (key, val) for key, val in self.additional_endpoints.items()]

  def __key(self):
    return (
        self.service_endpoint,
        frozenset(sorted(self.__additional_endpoints_string())),
        self.status,
        self._shard)

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__key() == other.__key()

  def __hash__(self):
    return hash(self.__key())


  def __str__(self):
    return 'ServiceInstance(%s, %saddl: %s, status: %s)' % (
      self.service_endpoint,
      ('shard: %s, ' % self._shard) if self._shard is not None else '',
      ' : '.join(self.__additional_endpoints_string()),
      self.status)

########NEW FILE########
__FILENAME__ = serverset
try:
  from twitter.common import log
except ImportError:
  import logging as log

from twitter.common.zookeeper.group.group_base import GroupInterface

try:
  from twitter.common.zookeeper.client import ZooKeeper
  from twitter.common.zookeeper.group.group import (
      ActiveGroup,
      Group)
  def pick_zkpython_group(zk, on_join, on_leave):
    # The default underlying implementation is Group if no active monitoring
    # is requested of the ServerSet.  If active monitoring is requested by
    # on_join or on_leave, then use ActiveGroup by default, which has better
    # performance on monitor/iter calls.
    if isinstance(zk, ZooKeeper):
      return Group if (on_join is None and on_leave is None) else ActiveGroup
except ImportError as e:
  def pick_zkpython_group(zk, on_join, on_leave):
    return None

try:
  from kazoo.client import KazooClient
  from twitter.common.zookeeper.group.kazoo_group import ActiveKazooGroup, KazooGroup
  def pick_kazoo_group(zk, on_join, on_leave):
    if isinstance(zk, KazooClient):
      return KazooGroup if (on_join is None and on_leave is None) else ActiveKazooGroup
except ImportError as e:
  def pick_kazoo_group(zk, on_join, on_leave):
    return None

GROUP_SELECTORS = [pick_zkpython_group, pick_kazoo_group]

from .endpoint import ServiceInstance


def first(iterable):
  for element in iterable:
    if element:
      return element


def validate_group_implementation(underlying):
  assert issubclass(underlying, GroupInterface), (
    'Underlying group implementation must be a subclass of GroupInterface, got %s'
    % type(underlying))


class ServerSet(object):
  """
    A dynamic set of service endpoints tracked by Zookeeper.
  """

  def __init__(self, zk, path, underlying=None, on_join=None, on_leave=None, **kwargs):
    """
      Construct a ServerSet at :path given zookeeper handle :zh.

      If :underlying is specified, use that as the underlying Group implementation.  Must be a
      subclass of twitter.common.zookeeper.group.GroupInterface.

      If :on_join is specified, it will be called with a ServiceInstance object every time a
      new service joins the ServerSet.  If :on_leave is specified, it will be called with
      a ServiceInstance object every time a server leaves the ServerSet.

      All remaining arguments are passed to the underlying Group implementation.
    """
    underlying = underlying or first(
        pick_group(zk, on_join, on_leave) for pick_group in GROUP_SELECTORS)
    if underlying is None:
      raise ValueError("Couldn't find a suitable group implementation!")

    validate_group_implementation(underlying)

    self._path = path
    self._group = underlying(zk, path, **kwargs)
    def devnull(*args, **kw): pass
    self._on_join = on_join or devnull
    self._on_leave = on_leave or devnull
    self._members = {}
    if on_join or on_leave:
      self._internal_monitor(set(self._members))

  def join(self, endpoint, additional=None, shard=None, callback=None, expire_callback=None):
    """
      Given 'endpoint' (twitter.common.zookeeper.serverset.Endpoint) and an
      optional map 'additional' of string => endpoint (also Endpoint), and an
      optional shard id, create a ServiceInstance and join it into this ServerSet.

      If 'callback' is provided, the join will be done asynchronously and
      'callback' will be called with the Membership object associated with
      the ServiceInstance.  If joining fails, Membership.error() will be
      returned.  If 'callback' is not provided, join will return with this
      information synchronously.

      If 'expire_callback' is provided, it will be called if the membership
      is severed for any reason such as session expiration or malice.
    """
    service_instance = ServiceInstance.pack(ServiceInstance(endpoint, additional, shard=shard))
    return self._group.join(service_instance, callback=callback, expire_callback=expire_callback)

  def cancel(self, membership, callback=None):
    """Cancel membership in the ServerSet."""
    return self._group.cancel(membership, callback=callback)

  def __iter__(self):
    """Iterate over the services (ServiceInstance objects) in this ServerSet."""
    for member in self._group.list():
      try:
        yield ServiceInstance.unpack(self._group.info(member))
      except Exception as e:
        log.warning('Failed to deserialize endpoint: %s' % e)
        continue

  def _internal_monitor(self, members):
    cached = set(self._members)
    new_members = members - cached
    old_members = cached - members

    for service_instance in map(self._members.pop, old_members):
      self._on_leave(service_instance)

    def make_callback(member_id):
      def callback(service_instance):
        try:
          self._members[member_id] = ServiceInstance.unpack(service_instance)
        except Exception as e:
          log.warning('Failed to deserialize endpoint: %s' % e)
          return
        self._on_join(self._members[member_id])

      return callback

    for member_id in new_members:
      self._group.info(member_id, make_callback(member_id))

    self._group.monitor(members, self._internal_monitor)

########NEW FILE########
__FILENAME__ = test_server
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import atexit
import errno
import os
import signal
import socket
import subprocess
import sys
import tempfile
import threading
import time

from twitter.common.contextutil import environment_as
from twitter.common.dirutil import safe_rmtree
from twitter.common.rpc import make_client
from twitter.common.rpc.finagle import TFinagleProtocol

from gen.twitter.common.zookeeper.testing.angrybird import ZooKeeperThriftServer
from gen.twitter.common.zookeeper.testing.angrybird.ttypes import (
  ExpireSessionRequest,
  ResponseCode)

from thrift.transport.TTransport import TTransportException

try:
  import zookeeper
  HAS_ZKPYTHON = True
except ImportError:
  HAS_ZKPYTHON = False


def get_random_port():
  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
  s.bind(('localhost', 0))
  _, port = s.getsockname()
  s.close()
  return port


class ZookeeperServer(object):
  """Manage local temporary instances of ZK for testing.

  Can also be used as a context manager, in which case it will start the first
  ZK server in the cluster and returns its port.
  """

  class Error(Exception): pass
  class InvalidServerId(Error): pass
  class NotStarted(Error): pass

  BUILD_COMMAND = """
    ./pants goal binary --binary-deployjar src/java/com/twitter/common/zookeeper/testing/angrybird
  """
  COMMAND = "java -jar dist/angrybird.jar -thrift_port %(thrift_port)s -zk_port %(zookeeper_port)s"
  BUILT = False
  CONNECT_RETRIES = 5
  CONNECT_BACKOFF_SECS = 6.0
  INITIAL_BACKOFF = 1.0
  _orphaned_pids = set()

  @classmethod
  def build(cls):
    if not cls.BUILT:
      pex_keys = {}
      for key in os.environ:
        if key.startswith('PEX'):
          pex_keys[key] = os.environ[key]
          os.unsetenv(key)
      assert subprocess.call(cls.BUILD_COMMAND.split()) == 0
      for key in pex_keys:
        os.putenv(key, pex_keys[key])
      cls.BUILT = True

  def __init__(self, zookeeper_port=None, thrift_port=None):
    self._service = None
    self._zh = None
    self.thrift_port = thrift_port or get_random_port()
    self.zookeeper_port = zookeeper_port or get_random_port()
    self.build()
    command = self.COMMAND % {
        'thrift_port': self.thrift_port, 'zookeeper_port': self.zookeeper_port}
    self._po = subprocess.Popen(command.split())
    self._orphaned_pids.add(self._po.pid)
    self.angrybird = self.setup_thrift()

  @property
  def zh(self):
    if self._po is None:
      raise self.NotStarted('Cluster has not been started!')
    if not HAS_ZKPYTHON:
      raise self.Error('No Zookeeper client library available!')
    if self._zh is None:
      start_event = threading.Event()
      def alive(zh, event, state, _):
        if event == zookeeper.SESSION_EVENT and state == zookeeper.CONNECTED_STATE:
          start_event.set()
      self._zh = zookeeper.init(self.ensemble, alive)
      start_event.wait()
    return self._zh

  def setup_thrift(self):
    if self._service is None:
      time.sleep(self.INITIAL_BACKOFF)
      for _ in range(self.CONNECT_RETRIES):
        try:
          self._service = make_client(ZooKeeperThriftServer, 'localhost', self.thrift_port,
            protocol=TFinagleProtocol)
          break
        except TTransportException:
          time.sleep(self.CONNECT_BACKOFF_SECS)
          continue
      else:
        raise self.NotStarted('Could not start Zookeeper cluster!')

      serverPortResponse = self._service.getZooKeeperServerPort()
      assert serverPortResponse.responseCode == ResponseCode.OK
      self.zookeeper_port = serverPortResponse.port
    return self._service

  @property
  def ensemble(self):
    if not self.zookeeper_port:
      raise self.NotStarted('Server not started!')
    return 'localhost:%d' % self.zookeeper_port

  def expire(self, session_id=None):
    if session_id is None:
      if self._zh is None:
        raise self.NotStarted('Must specify session id if no client connection available!')
      if not HAS_ZKPYTHON:
        raise self.Error('No Zookeeper client available!')
      session_id, _ = zookeeper.client_id(self._zh)
    expireResponse = self.angrybird.expireSession(ExpireSessionRequest(sessionId=session_id))
    return expireResponse.responseCode == ResponseCode.OK

  def shutdown(self):
    return self.angrybird.shutdown().responseCode == ResponseCode.OK

  def restart(self):
    return self.angrybird.restart().responseCode == ResponseCode.OK

  def start(self):
    return self.angrybird.startup().responseCode == ResponseCode.OK

  def stop(self):
    if self._po is None:
      raise self.NotStarted('Cluster not started!')
    self._po.kill()
    self._orphaned_pids.remove(self._po.pid)
    self._po = None
    self.thrift_port = None
    self.zookeeper_port = None

  def __enter__(self):
    return self

  def __exit__(self, exc_type, exc_value, traceback):
    self.stop()


@atexit.register
def _cleanup_orphans():
  for pid in ZookeeperServer._orphaned_pids:
    try:
      os.kill(pid, signal.SIGKILL)
    except OSError as e:
      if e.errno != errno.ESRCH:
        print('warning: error killing orphaned ZK server %d: %s' % (pid, e), file=sys.stderr)

########NEW FILE########
__FILENAME__ = netrc_util
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import collections
import os

from netrc import netrc as NetrcDb, NetrcParseError

from twitter.pants.tasks.task_error import TaskError


class Netrc(object):

  def __init__(self):
    self._login = collections.defaultdict(lambda: None)
    self._password = collections.defaultdict(lambda: None)

  def getusername(self, repository):
    self._ensure_loaded()
    return self._login[repository]

  def getpassword(self, repository):
    self._ensure_loaded()
    return self._password[repository]

  def _ensure_loaded(self):
    if not self._login and not self._password:
      db = os.path.expanduser('~/.netrc')
      if not os.path.exists(db):
        raise TaskError('A ~/.netrc file is required to authenticate')
      try:
        db = NetrcDb(db)
        for host, value in db.hosts.items():
          auth = db.authenticators(host)
          if auth:
            login, _, password = auth
            self._login[host] = login
            self._password[host] = password
        if len(self._login) == 0:
          raise TaskError('Found no usable authentication blocks for twitter in ~/.netrc')
      except NetrcParseError as e:
        raise TaskError('Problem parsing ~/.netrc: %s' % e)

########NEW FILE########
__FILENAME__ = abbreviate_target_ids
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Ryan Williams'

def abbreviate_target_ids(arr):
  """Map a list of target IDs to shortened versions.

  This method takes a list of strings (e.g. target IDs) and maps them to shortened versions of
  themselves.

  The original strings should consist of '.'-delimited segments, and the abbreviated versions are
  subsequences of these segments such that each string's subsequence is unique from others in @arr.

  For example: ::

     input: [
       'com.twitter.pants.a.b',
       'com.twitter.pants.a.c',
       'com.twitter.pants.d'
     ]

  might return: ::

     {
       'com.twitter.pants.a.b': 'b',
       'com.twitter.pants.a.c': 'c',
       'com.twitter.pants.d': 'd'
     }

  This can be useful for debugging purposes, removing a lot of boilerplate from printed lists of
  target IDs.

  :param arr: List of strings representing target IDs.
  """
  split_keys = [tuple(a.split('.')) for a in arr]

  split_keys_by_subseq = {}

  def subseq_map(arr, subseq_fn=None, result_cmp_fn=None):
    def subseq_map_rec(remaining_arr, subseq, indent=''):
      if not remaining_arr:
        if subseq_fn:
          subseq_fn(arr, subseq)
        return subseq

      next_segment = remaining_arr.pop()
      next_subseq = tuple([next_segment] + list(subseq))

      skip_value = subseq_map_rec(remaining_arr, subseq, indent + '\t')

      add_value = subseq_map_rec(remaining_arr, next_subseq, indent + '\t')

      remaining_arr.append(next_segment)

      if result_cmp_fn:
        if not subseq:
          # Empty subsequence should always lose.
          return add_value
        if result_cmp_fn(skip_value, add_value):
          return skip_value
        return add_value

      return None

    val = subseq_map_rec(list(arr), tuple())
    return val

  def add_subseq(arr, subseq):
    if subseq not in split_keys_by_subseq:
      split_keys_by_subseq[subseq] = set()
    if split_key not in split_keys_by_subseq[subseq]:
      split_keys_by_subseq[subseq].add(arr)

  for split_key in split_keys:
    subseq_map(split_key, add_subseq)

  def return_min_subseqs(subseq1, subseq2):
    collisions1 = split_keys_by_subseq[subseq1]
    collisions2 = split_keys_by_subseq[subseq2]
    return (len(collisions1) < len(collisions2)
            or (len(collisions1) == len(collisions2)
                and len(subseq1) <= len(subseq2)))

  min_subseq_by_key = {}

  for split_key in split_keys:
    min_subseq = subseq_map(split_key, result_cmp_fn=return_min_subseqs)
    if not min_subseq:
      raise Exception("No min subseq found for %s: %s" % (str(split_key), str(min_subseq)))
    min_subseq_by_key['.'.join(str(segment) for segment in split_key)] = '.'.join(min_subseq)

  return min_subseq_by_key


########NEW FILE########
__FILENAME__ = address
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.lang import Compatibility
from twitter.pants.base.build_file import BuildFile


class Address(object):
  """A target address.

  An address is a unique name representing a
  :class:`twitter.pants.base.target.Target`. Its composed of the
  :class:`twitter.pants.base.build_file.BuildFile` plus target name.

  While not their only use, a noteworthy use of addresses is specifying
  target dependencies. For example:

  ::

    some_target(name='mytarget',
      dependencies=[pants('path/to/buildfile:targetname')]
    )

  Where ``path/to/buildfile:targetname`` is the dependent target address.
  """

  @classmethod
  def parse(cls, root_dir, spec, is_relative=True):
    """Parses the given spec into an Address.

    An address spec can be one of:
    1.) the (relative) path of a BUILD file
    2.) the (relative) path of a directory containing a BUILD file child
    3.) either of 1 or 2 with a ':[target name]' suffix
    4.) a bare ':[target name]' indicating the BUILD file to use is the one in the current directory

    If the spec does not have a target name suffix the target name is taken to be the same name
    as the BUILD file's parent directory.  In this way the containing directory name
    becomes the 'default' target name for a BUILD file.

    If there is no BUILD file at the path pointed to, or if there is but the specified target name
    is not defined in the BUILD file, an IOError is raised.
    """

    if spec.startswith(':'):
      spec = '.' + spec
    parts = spec.split(':', 1)
    path = parts[0]
    if is_relative:
      path = os.path.relpath(os.path.abspath(path), root_dir)
    buildfile = BuildFile(root_dir, path)

    name = os.path.basename(os.path.dirname(buildfile.relpath)) if len(parts) == 1 else parts[1]
    return Address(buildfile, name)

  def __init__(self, buildfile, target_name):
    """
    :param BuildFile buildfile: A BuildFile defined in the repo.
    :param string target_name: The name of a target defined in buildfile.
    """
    assert isinstance(buildfile, BuildFile)
    assert isinstance(target_name, Compatibility.string)
    self.buildfile = buildfile
    self.target_name = target_name

  def reference(self, referencing_buildfile_path=None):
    """How to reference this address in a BUILD file."""
    dirname = os.path.dirname(self.buildfile.relpath)
    if referencing_buildfile_path and dirname == os.path.dirname(referencing_buildfile_path):
      return ':%s' % self.target_name
    elif os.path.basename(dirname) != self.target_name:
      return '%s:%s' % (dirname, self.target_name)
    else:
      return dirname

  def __eq__(self, other):
    result = other and (
      type(other) == Address) and (
      self.buildfile.canonical_relpath == other.buildfile.canonical_relpath) and (
      self.target_name == other.target_name)
    return result

  def __hash__(self):
    value = 17
    value *= 37 + hash(self.buildfile.canonical_relpath)
    value *= 37 + hash(self.target_name)
    return value

  def __ne__(self, other):
    return not self.__eq__(other)

  def __repr__(self):
    return "%s:%s" % (self.buildfile, self.target_name)

########NEW FILE########
__FILENAME__ = build_environment
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import sys

from twitter.common import log

from twitter.pants.version import VERSION as _VERSION

from .build_root import BuildRoot


def get_version():
  return _VERSION


def get_buildroot():
  """Returns the pants ROOT_DIR, calculating it if needed."""
  try:
    return BuildRoot().path
  except BuildRoot.NotFoundError as e:
    print(e.message, file=sys.stderr)
    sys.exit(1)


def set_buildroot(path):
  """Sets the pants ROOT_DIR.

  Generally only useful for tests.
  """
  BuildRoot().path = path


from twitter.pants.scm import Scm


_SCM = None


def get_scm():
  """Returns the pants Scm if any."""
  # TODO(John Sirois): Extract a module/class to carry the bootstrap logic.
  global _SCM
  if not _SCM:
    # We know about git, so attempt an auto-configure
    git_dir = os.path.join(get_buildroot(), '.git')
    if os.path.isdir(git_dir):
      from twitter.pants.scm.git import Git
      git = Git(worktree=get_buildroot())
      try:
        log.info('Detected git repository on branch %s' % git.branch_name)
        set_scm(git)
      except git.LocalException:
        pass
  return _SCM


def set_scm(scm):
  """Sets the pants Scm."""
  if scm is not None:
    if not isinstance(scm, Scm):
      raise ValueError('The scm must be an instance of Scm, given %s' % scm)
    global _SCM
    _SCM = scm


########NEW FILE########
__FILENAME__ = build_file
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import marshal
import os
import re

from glob import glob1

from twitter.common.collections import OrderedSet
from twitter.common.python.interpreter import PythonIdentity


class BuildFile(object):
  _CANONICAL_NAME = 'BUILD'
  _PATTERN = re.compile('^%s(\.[a-z]+)?$' % _CANONICAL_NAME)

  @staticmethod
  def _is_buildfile_name(name):
    return BuildFile._PATTERN.match(name)

  @staticmethod
  def scan_buildfiles(root_dir, base_path=None):
    """Looks for all BUILD files under base_path"""

    buildfiles = []
    for root, dirs, files in os.walk(base_path if base_path else root_dir):
      for filename in files:
        if BuildFile._is_buildfile_name(filename):
          buildfile_relpath = os.path.relpath(os.path.join(root, filename), root_dir)
          buildfiles.append(BuildFile(root_dir, buildfile_relpath))
    return OrderedSet(sorted(buildfiles, key=lambda buildfile: buildfile.full_path))

  def __init__(self, root_dir, relpath, must_exist=True):
    """Creates a BuildFile object representing the BUILD file set at the specified path.

    root_dir: The base directory of the project
    relpath: The path relative to root_dir where the BUILD file is found - this can either point
        directly at the BUILD file or else to a directory which contains BUILD files
    must_exist: If True, the specified BUILD file must exist or else an IOError is thrown
    raises IOError if the specified path does not house a BUILD file and must_exist is True
    """

    path = os.path.abspath(os.path.join(root_dir, relpath))
    buildfile = os.path.join(path, BuildFile._CANONICAL_NAME) if os.path.isdir(path) else path

    if os.path.isdir(buildfile):
      raise IOError("%s is a directory" % buildfile)

    if must_exist:
      if not os.path.exists(buildfile):
        raise IOError("BUILD file does not exist at: %s" % buildfile)

      if not BuildFile._is_buildfile_name(os.path.basename(buildfile)):
        raise IOError("%s is not a BUILD file" % buildfile)

      if not os.path.exists(buildfile):
        raise IOError("BUILD file does not exist at: %s" % buildfile)

    self.root_dir = os.path.realpath(root_dir)
    self.full_path = os.path.realpath(buildfile)

    self.name = os.path.basename(self.full_path)
    self.parent_path = os.path.dirname(self.full_path)

    self._bytecode_path = os.path.join(self.parent_path, '.%s.%s.pyc' % (
      self.name, PythonIdentity.get()))

    self.relpath = os.path.relpath(self.full_path, self.root_dir)
    self.canonical_relpath = os.path.join(os.path.dirname(self.relpath), BuildFile._CANONICAL_NAME)

  def exists(self):
    """Returns True if this BuildFile corresponds to a real BUILD file on disk."""
    return os.path.exists(self.full_path)

  def descendants(self):
    """Returns all BUILD files in descendant directories of this BUILD file's parent directory."""

    descendants = BuildFile.scan_buildfiles(self.root_dir, self.parent_path)
    for sibling in self.family():
      descendants.discard(sibling)
    return descendants

  def ancestors(self):
    """Returns all BUILD files in ancestor directories of this BUILD file's parent directory."""

    def find_parent(dir):
      parent = os.path.dirname(dir)
      buildfile = os.path.join(parent, BuildFile._CANONICAL_NAME)
      if os.path.exists(buildfile) and not os.path.isdir(buildfile):
        return parent, BuildFile(self.root_dir, os.path.relpath(buildfile, self.root_dir))
      else:
        return parent, None

    parent_buildfiles = OrderedSet()

    parentdir = os.path.dirname(self.full_path)
    visited = set()
    while parentdir not in visited and self.root_dir != parentdir:
      visited.add(parentdir)
      parentdir, buildfile = find_parent(parentdir)
      if buildfile:
        parent_buildfiles.update(buildfile.family())

    return parent_buildfiles

  def siblings(self):
    """Returns an iterator over all the BUILD files co-located with this BUILD file not including
    this BUILD file itself"""

    for build in glob1(self.parent_path, 'BUILD*'):
      if self.name != build and BuildFile._is_buildfile_name(build):
        siblingpath = os.path.join(os.path.dirname(self.relpath), build)
        if not os.path.isdir(os.path.join(self.root_dir, siblingpath)):
          yield BuildFile(self.root_dir, siblingpath)

  def family(self):
    """Returns an iterator over all the BUILD files co-located with this BUILD file including this
    BUILD file itself.  The family forms a single logical BUILD file composed of the canonical BUILD
    file and optional sibling build files each with their own extension, eg: BUILD.extras."""

    yield self
    for sibling in self.siblings():
      yield sibling

  def code(self):
    """Returns the code object for this BUILD file."""
    if (os.path.exists(self._bytecode_path) and
        os.path.getmtime(self.full_path) <= os.path.getmtime(self._bytecode_path)):
      with open(self._bytecode_path, 'rb') as bytecode:
        return marshal.load(bytecode)
    else:
      with open(self.full_path, 'rb') as source:
        code = compile(source.read(), self.full_path, 'exec')
        with open(self._bytecode_path, 'wb') as bytecode:
          marshal.dump(code, bytecode)
        return code

  def __eq__(self, other):
    result = other and (
      type(other) == BuildFile) and (
      self.full_path == other.full_path)
    return result

  def __hash__(self):
    return hash(self.full_path)

  def __ne__(self, other):
    return not self.__eq__(other)

  def __repr__(self):
    return self.relpath

########NEW FILE########
__FILENAME__ = build_file_aliases
# =================================================================================================
# Copyright 2011 Twitter, Inc.
# -------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =================================================================================================

from twitter.pants.targets.annotation_processor import AnnotationProcessor
from twitter.pants.targets.artifact import Artifact
from twitter.pants.targets.benchmark import Benchmark
from twitter.pants.targets.credentials import Credentials
from twitter.pants.targets.doc import Page, Wiki
from twitter.pants.targets.exclude import Exclude
from twitter.pants.targets.jar_dependency import JarDependency
from twitter.pants.targets.jar_library import JarLibrary
from twitter.pants.targets.java_agent import JavaAgent
from twitter.pants.targets.java_antlr_library import JavaAntlrLibrary
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.java_protobuf_library import JavaProtobufLibrary
from twitter.pants.targets.java_tests import JavaTests
from twitter.pants.targets.java_thrift_library import JavaThriftLibrary
from twitter.pants.targets.jvm_binary import Bundle, JvmApp, JvmBinary
from twitter.pants.targets.pants_target import Pants
from twitter.pants.targets.python_antlr_library import PythonAntlrLibrary
from twitter.pants.targets.python_artifact import PythonArtifact
from twitter.pants.targets.python_binary import PythonBinary
from twitter.pants.targets.python_egg import PythonEgg
from twitter.pants.targets.python_library import PythonLibrary
from twitter.pants.targets.python_requirement import PythonRequirement
from twitter.pants.targets.python_tests import PythonTests, PythonTestSuite
from twitter.pants.targets.python_thrift_library import PythonThriftLibrary
from twitter.pants.targets.repository import Repository
from twitter.pants.targets.resources import Resources
from twitter.pants.targets.scala_library import ScalaLibrary
from twitter.pants.targets.scala_tests import ScalaTests
from twitter.pants.targets.scalac_plugin import ScalacPlugin
from twitter.pants.targets.sources import SourceRoot


# aliases
annotation_processor = AnnotationProcessor
artifact = Artifact
benchmark = Benchmark
bundle = Bundle
credentials = Credentials
dependencies = jar_library = JarLibrary
egg = PythonEgg
exclude = Exclude
fancy_pants = Pants
jar = JarDependency
java_agent = JavaAgent
java_library = JavaLibrary
java_antlr_library = JavaAntlrLibrary
java_protobuf_library = JavaProtobufLibrary
junit_tests = java_tests = JavaTests
java_thrift_library = JavaThriftLibrary
jvm_binary = JvmBinary
jvm_app = JvmApp
page = Page
python_artifact = setup_py = PythonArtifact
python_binary = PythonBinary
python_library = PythonLibrary
python_antlr_library = PythonAntlrLibrary
python_requirement = PythonRequirement
python_thrift_library = PythonThriftLibrary
python_tests = PythonTests
python_test_suite = PythonTestSuite
repo = Repository
resources = Resources
scala_library = ScalaLibrary
scala_specs = scala_tests = ScalaTests
scalac_plugin = ScalacPlugin
source_root = SourceRoot
wiki = Wiki

########NEW FILE########
__FILENAME__ = build_file_context
# =================================================================================================
# Copyright 2011 Twitter, Inc.
# -------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =================================================================================================

from twitter.pants.goal import Context, Goal, Group, Phase
from twitter.pants.targets.pants_target import Pants
from twitter.pants.tasks import Task, TaskError

pants = Pants
goal = Goal
group = Group
phase = Phase

from .build_file_aliases import *
from .build_file_helpers import *
from .config import Config

# TODO(John Sirois): XXX kill
from .build_environment import *

########NEW FILE########
__FILENAME__ = build_file_helpers
# =================================================================================================
# Copyright 2011 Twitter, Inc.
# -------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =================================================================================================

import os

from twitter.pants.targets.annotation_processor import AnnotationProcessor
from twitter.pants.targets.doc import Page
from twitter.pants.targets.java_agent import JavaAgent
from twitter.pants.targets.java_antlr_library import JavaAntlrLibrary
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.java_protobuf_library import JavaProtobufLibrary
from twitter.pants.targets.java_tests import JavaTests
from twitter.pants.targets.java_thrift_library import JavaThriftLibrary
from twitter.pants.targets.jvm_binary import JvmBinary
from twitter.pants.targets.python_antlr_library import PythonAntlrLibrary
from twitter.pants.targets.python_binary import PythonBinary
from twitter.pants.targets.python_library import PythonLibrary
from twitter.pants.targets.python_tests import PythonTests, PythonTestSuite
from twitter.pants.targets.python_thrift_library import PythonThriftLibrary
from twitter.pants.targets.resources import Resources
from twitter.pants.targets.scala_library import ScalaLibrary
from twitter.pants.targets.scala_tests import ScalaTests
from twitter.pants.targets.sources import SourceRoot


def maven_layout(basedir=None):
  """Sets up typical maven project source roots for all built-in pants target types.

  Shortcut for ``source_root('src/main/java', *java targets*)``,
  ``source_root('src/main/python', *python targets*)``, ...

  :param string basedir: Instead of using this BUILD file's directory as
    the base of the source tree, use a subdirectory. E.g., instead of
    expecting to find java files in ``src/main/java``, expect them in
    ``**basedir**/src/main/java``.
  """

  def root(path, *types):
    SourceRoot.register(os.path.join(basedir, path) if basedir else path, *types)

  root('src/main/antlr', JavaAntlrLibrary, Page, PythonAntlrLibrary)
  root('src/main/java', AnnotationProcessor, JavaAgent, JavaLibrary, JvmBinary, Page)
  root('src/main/protobuf', JavaProtobufLibrary, Page)
  root('src/main/python', Page, PythonBinary, PythonLibrary)
  root('src/main/resources', Page, Resources)
  root('src/main/scala', JvmBinary, Page, ScalaLibrary)
  root('src/main/thrift', JavaThriftLibrary, Page, PythonThriftLibrary)

  root('src/test/java', JavaLibrary, JavaTests, Page)
  root('src/test/python', Page, PythonLibrary, PythonTests, PythonTestSuite)
  root('src/test/resources', Page, Resources)
  root('src/test/scala', JavaTests, Page, ScalaLibrary, ScalaTests)

########NEW FILE########
__FILENAME__ = build_invalidator
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import errno
import hashlib
import itertools
import os

from abc import abstractmethod
from collections import namedtuple

from twitter.common.dirutil import safe_mkdir
from twitter.common.lang import Compatibility, Interface

from twitter.pants.base.hash_utils import hash_all
from twitter.pants.fs.fs import safe_filename
from twitter.pants.base.target import Target


# A CacheKey represents some version of a set of targets.
#  - id identifies the set of targets.
#  - hash is a fingerprint of all invalidating inputs to the build step, i.e., it uniquely
#    determines a given version of the artifacts created when building the target set.
#  - num_sources is the number of source files used to build this version of the target set.
#    Needed only for display.
#  - sources is an (optional) list of the source files used to compute this key.
#    Needed only for display.

CacheKey = namedtuple('CacheKey', ['id', 'hash', 'num_sources', 'sources'])


class SourceScope(Interface):
  """Selects sources of a given scope from targets."""

  @abstractmethod
  def select(self, target):
    """Selects source files from the given target and returns them as absolute paths."""

  @abstractmethod
  def valid(self, target):
    """Returns True if the given target can be used with this SourceScope."""


class NoSources(SourceScope):
  """A SourceScope where all targets are valid but no sources are ever selected."""

  def select(self, target):
    return []

  def valid(self, target):
    return True

NO_SOURCES = NoSources()


class DefaultSourceScope(SourceScope):
  """Selects sources from subclasses of TargetWithSources."""

  def __init__(self, recursive, include_buildfile):
    self._recursive = recursive
    self._include_buildfile = include_buildfile

  def select(self, tgt):
    return tgt.expand_files(self._recursive, self._include_buildfile)

  def valid(self, target):
    return hasattr(target, 'expand_files')

TARGET_SOURCES = DefaultSourceScope(recursive=False, include_buildfile=False)
TRANSITIVE_SOURCES = DefaultSourceScope(recursive=True, include_buildfile=False)

# Bump this to invalidate all existing keys in artifact caches across all pants deployments in the world.
# Do this if you've made a change that invalidates existing artifacts, e.g.,  fixed a bug that
# caused bad artifacts to be cached.
GLOBAL_CACHE_KEY_GEN_VERSION = '6'

class CacheKeyGenerator(object):
  """Generates cache keys for versions of target sets."""

  @staticmethod
  def combine_cache_keys(cache_keys):
    """Returns a cache key for a list of target sets that already have cache keys.

    This operation is 'idempotent' in the sense that if cache_keys contains a single key
    then that key is returned.

    Note that this operation is commutative but not associative.  We use the term 'combine' rather
    than 'merge' or 'union' to remind the user of this. Associativity is not a necessary property,
    in practice.
    """
    if len(cache_keys) == 1:
      return cache_keys[0]
    else:
      combined_id = Target.maybe_readable_combine_ids(cache_key.id for cache_key in cache_keys)
      combined_hash = hash_all(sorted(cache_key.hash for cache_key in cache_keys))
      combined_num_sources = sum(cache_key.num_sources for cache_key in cache_keys)
      combined_sources = \
        sorted(list(itertools.chain(*[cache_key.sources for cache_key in cache_keys])))
      return CacheKey(combined_id, combined_hash, combined_num_sources, combined_sources)

  def __init__(self, cache_key_gen_version=None):
    """cache_key_gen_version - If provided, added to all cache keys. Allows you to invalidate all cache
                               keys in a single pants repo, by changing this value in config.
    """
    self._cache_key_gen_version = (cache_key_gen_version or '') + '_' + GLOBAL_CACHE_KEY_GEN_VERSION

  def key_for_target(self, target, sources=TARGET_SOURCES, fingerprint_extra=None):
    """Get a key representing the given target and its sources.

    A key for a set of targets can be created by calling combine_cache_keys()
    on the target's individual cache keys.

    :target: The target to create a CacheKey for.
    :sources: A source scope to select from the target for hashing, defaults to TARGET_SOURCES.
    :fingerprint_extra: A function that accepts a sha hash and updates it with extra fprint data.
    """
    if not fingerprint_extra:
      if not sources or not sources.valid(target):
        raise ValueError('A target needs to have at least one of sources or a '
                         'fingerprint_extra function to generate a CacheKey.')
    if not sources:
      sources = NO_SOURCES

    sha = hashlib.sha1()
    srcs = sorted(sources.select(target))
    actual_srcs = self._sources_hash(sha, srcs)
    if fingerprint_extra:
      fingerprint_extra(sha)
    sha.update(self._cache_key_gen_version)
    return CacheKey(target.id, sha.hexdigest(), len(actual_srcs), actual_srcs)

  def key_for(self, target_id, sources):
    """Get a cache key representing some id and its associated source files.

    Useful primarily in tests. Normally we use key_for_target().
    """
    sha = hashlib.sha1()
    actual_srcs = self._sources_hash(sha, sources)
    return CacheKey(target_id, sha.hexdigest(), len(actual_srcs), actual_srcs)

  def _walk_paths(self, paths):
    """Recursively walk the given paths.

    :returns: Iterable of (relative_path, absolute_path).
    """
    for path in sorted(paths):
      if os.path.isdir(path):
        for dir_name, _, filenames in sorted(os.walk(path)):
          for filename in filenames:
            filename = os.path.join(dir_name, filename)
            yield os.path.relpath(filename, path), filename
      else:
        yield os.path.basename(path), path

  def _sources_hash(self, sha, paths):
    """Update a SHA1 digest with the content of all files under the given paths.

    :returns: The files found under the given paths.
    """
    files = []
    for relative_filename, filename in self._walk_paths(paths):
      with open(filename, "rb") as fd:
        sha.update(Compatibility.to_bytes(relative_filename))
        sha.update(fd.read())
      files.append(filename)
    return files


# A persistent map from target set to cache key, which is a fingerprint of all
# the inputs to the current version of that target set. That cache key can then be used
# to look up build artifacts in an artifact cache.
class BuildInvalidator(object):
  """Invalidates build targets based on the SHA1 hash of source files and other inputs."""

  def __init__(self, root):
    self._root = os.path.join(root, GLOBAL_CACHE_KEY_GEN_VERSION)
    safe_mkdir(self._root)

  def needs_update(self, cache_key):
    """Check if the given cached item is invalid.

    :param cache_key: A CacheKey object (as returned by BuildInvalidator.key_for().
    :returns: True if the cached version of the item is out of date.
    """
    return self._read_sha(cache_key) != cache_key.hash

  def update(self, cache_key):
    """Makes cache_key the valid version of the corresponding target set.

    :param cache_key: A CacheKey object (typically returned by BuildInvalidator.key_for()).
    """
    self._write_sha(cache_key)

  def force_invalidate_all(self):
    """Force-invalidates all cached items."""
    safe_mkdir(self._root, clean=True)

  def force_invalidate(self, cache_key):
    """Force-invalidate the cached item."""
    try:
      os.unlink(self._sha_file(cache_key))
    except OSError as e:
      if e.errno != errno.ENOENT:
        raise

  def existing_hash(self, id):
    """Returns the existing hash for the specified id.

    Returns None if there is no existing hash for this id.
    """
    return self._read_sha_by_id(id)

  def _sha_file(self, cache_key):
    return self._sha_file_by_id(cache_key.id)

  def _sha_file_by_id(self, id):
    return os.path.join(self._root, safe_filename(id, extension='.hash'))

  def _write_sha(self, cache_key):
    with open(self._sha_file(cache_key), 'w') as fd:
      fd.write(cache_key.hash)

  def _read_sha(self, cache_key):
    return self._read_sha_by_id(cache_key.id)

  def _read_sha_by_id(self, id):
    try:
      with open(self._sha_file_by_id(id), 'rb') as fd:
        return fd.read().strip()
    except IOError as e:
      if e.errno != errno.ENOENT:
        raise
      return None  # File doesn't exist.

########NEW FILE########
__FILENAME__ = build_manual
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================


class manual(object):
  """Annotate things that should appear in generated documents"""

  @staticmethod
  def builddict(tags=None):
    """Decorator to mark something that belongs in the BUILD Dictionary doc.

    Use it on a function to mention the function. Use it on a class to
    mention the class; use it on a class' method to mention that method
    within the class' doc. (Default behavior uses the constructor but
    ignores methods. You want to decorate methods that are kosher for
    BUILD files.)

    tags: E.g., tags=["python"] means This thingy should appear in the
          Python section"
    """
    tags = tags or []
    def builddictdecorator(funcorclass):
      funcorclass.builddictdict = {"tags": tags}
      return funcorclass
    return builddictdecorator


def get_builddict_info(funcorclass):
  """Return None if arg doesn't belong in BUILD dictionary, else something"""
  if hasattr(funcorclass, "builddictdict"):
    return getattr(funcorclass, "builddictdict")
  else:
    return None

########NEW FILE########
__FILENAME__ = build_root
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from contextlib import contextmanager

from twitter.common.lang import Singleton


class BuildRoot(Singleton):
  """Represents the global workspace ROOT_DIR.

  By default a pants workspace is defined by a root directory where the workspace configuration
  file - 'pants.ini' - lives.  This can be overridden by exporting 'PANTS_BUILD_ROOT' in the
  environment with the path to the ROOT_DIR or manipulated through this interface.
  """

  class NotFoundError(Exception):
    """Raised when unable to find the current workspace ROOT_DIR."""

  def __init__(self):
    self._root_dir = None

  @property
  def path(self):
    """Returns the ROOT_DIR for the current workspace."""
    if self._root_dir is None:
      if 'PANTS_BUILD_ROOT' in os.environ:
        self._root_dir = os.environ['PANTS_BUILD_ROOT']
      else:
        buildroot = os.path.abspath(os.getcwd())
        while not os.path.exists(os.path.join(buildroot, 'pants.ini')):
          if buildroot != os.path.dirname(buildroot):
            buildroot = os.path.dirname(buildroot)
          else:
            raise self.NotFoundError('Could not find pants.ini!')
        self._root_dir = buildroot
    return self._root_dir

  @path.setter
  def path(self, root_dir):
    """Manually establishes the ROOT_DIR for the current workspace."""
    path = os.path.realpath(root_dir)
    if not os.path.exists(path):
      raise ValueError('Build root does not exist: %s' % root_dir)
    self._root_dir = path

  def reset(self):
    """Clears the last calculated ROOT_DIR for the current workspace."""
    self._root_dir = None

  def __str__(self):
    return 'BuildRoot(%s)' % self._root_dir

  @contextmanager
  def temporary(self, path):
    """A contextmanager that establishes a temporary ROOT_DIR, restoring the prior ROOT_DIR on
    exit."""
    if path is None:
      raise ValueError('Can only temporarily establish a build root given a path.')
    prior = self._root_dir
    self._root_dir = path
    try:
      yield
    finally:
      self._root_dir = prior

########NEW FILE########
__FILENAME__ = config
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

try:
  import ConfigParser
except ImportError:
  import configparser as ConfigParser

import os
import getpass

from twitter.pants.base.build_environment import get_buildroot


class Config(object):
  """
    Encapsulates ini-style config file loading and access additionally supporting recursive variable
    substitution using standard python format strings, ie: %(var_name)s will be replaced with the
    value of var_name.
  """

  DEFAULT_SECTION = ConfigParser.DEFAULTSECT

  class ConfigError(Exception):
    pass

  @staticmethod
  def load(configpath=None, defaults=None):
    """
      Loads a Config from the given path, by default the path to the pants.ini file in the current
      build root directory.  Any defaults supplied will act as if specified in the loaded config
      file's DEFAULT section.  The 'buildroot', invoking 'user' and invoking user's 'homedir' are
      automatically defaulted.
    """
    configpath = configpath or os.path.join(get_buildroot(), 'pants.ini')
    parser = Config.create_parser(defaults=defaults)
    with open(configpath) as ini:
      parser.readfp(ini)
    return Config(parser)

  @staticmethod
  def create_parser(defaults=None):
    """Creates a config parser that supports %([key-name])s value substitution.

    Any defaults supplied will act as if specified in the loaded config file's DEFAULT section and
    be available for substitutions.

    All of the following are seeded with defaults in the config
      user: the current user
      homedir: the current user's home directory
      buildroot: the root of this repo
      pants_bootstrapdir: the global pants scratch space primarily used for caches
      pants_supportdir: pants support files for this repo go here; for example: ivysettings.xml
      pants_distdir: user visible artifacts for this repo go here
      pants_workdir: the scratch space used to for live builds in this repo
    """
    standard_defaults = dict(
      buildroot=get_buildroot(),
      homedir=os.path.expanduser('~'),
      user=getpass.getuser(),
      pants_bootstrapdir=os.path.expanduser('~/.pants.d'),
      pants_workdir=os.path.join(get_buildroot(), '.pants.d'),
      pants_supportdir=os.path.join(get_buildroot(), 'build-support'),
      pants_distdir=os.path.join(get_buildroot(), 'dist')
    )
    if defaults:
      standard_defaults.update(defaults)
    return ConfigParser.SafeConfigParser(standard_defaults)

  def __init__(self, configparser):
    self.configparser = configparser

    # Overrides
    #
    # This feature allows a second configuration file which will override
    # pants.ini to be specified.  The file is currently specified via an env
    # variable because the cmd line flags are parsed after config is loaded.
    #
    # The main use of the extra file is to have different settings based on
    # the environment.  For example, the setting used to compile or locations
    # of caches might be different between a developer's local environment
    # and the environment used to build and publish artifacts (e.g. Jenkins)
    #
    # The files cannot reference each other's values, so make sure each one is
    # internally consistent
    self.overrides_path = os.environ.get('PANTS_CONFIG_OVERRIDE')
    self.overrides_parser = None
    if self.overrides_path is not None:
      self.overrides_path = os.path.join(get_buildroot(), self.overrides_path)
      self.overrides_parser = Config.create_parser()
      with open(self.overrides_path) as o_ini:
        self.overrides_parser.readfp(o_ini, filename=self.overrides_path)

  def getbool(self, section, option, default=None):
    """Equivalent to calling get with expected type string"""
    return self.get(section, option, type=bool, default=default)

  def getint(self, section, option, default=None):
    """Equivalent to calling get with expected type int"""
    return self.get(section, option, type=int, default=default)

  def getfloat(self, section, option, default=None):
    """Equivalent to calling get with expected type float"""
    return self.get(section, option, type=float, default=default)

  def getlist(self, section, option, default=None):
    """Equivalent to calling get with expected type list"""
    return self.get(section, option, type=list, default=default)

  def getdict(self, section, option, default=None):
    """Equivalent to calling get with expected type dict"""
    return self.get(section, option, type=dict, default=default)

  def getdefault(self, option, type=str, default=None):
    """
      Retrieves option from the DEFAULT section if it exists and attempts to parse it as type.
      If there is no definition found, the default value supplied is returned.
    """
    return self.get(Config.DEFAULT_SECTION, option, type, default=default)

  def get(self, section, option, type=str, default=None):
    """
      Retrieves option from the specified section if it exists and attempts to parse it as type.
      If the specified section is missing a definition for the option, the value is looked up in the
      DEFAULT section.  If there is still no definition found, the default value supplied is
      returned.
    """
    return self._getinstance(section, option, type, default=default)

  def get_required(self, section, option, type=str):
    """Retrieves option from the specified section and attempts to parse it as type.
    If the specified section is missing a definition for the option, the value is
    looked up in the DEFAULT section. If there is still no definition found,
    a `ConfigError` is raised.

    :param string section: Section to lookup the option in, before looking in DEFAULT.
    :param string option: Option to retrieve.
    :param type: Type to retrieve the option as.
    :returns: The option as the specified type.
    :raises: :class:`twitter.pants.base.config.Config.ConfigError` if option is not found.
    """
    val = self.get(section, option, type=type)
    if val is None:
      raise Config.ConfigError('Required option %s.%s is not defined.' % (section, option))
    return val

  def has_section(self, section):
    """Return whether or not this config has the section."""
    return self.configparser.has_section(section)

  def _has_option(self, section, option):
    if self.overrides_parser and self.overrides_parser.has_option(section, option):
      return True
    elif self.configparser.has_option(section, option):
      return True
    return False

  def _get_value(self, section, option):
    if self.overrides_parser and self.overrides_parser.has_option(section, option):
      return self.overrides_parser.get(section, option)
    return self.configparser.get(section, option)

  def _getinstance(self, section, option, type, default=None):
    if not self._has_option(section, option):
      return default
    raw_value = self._get_value(section, option)
    if issubclass(type, str):
      return raw_value

    try:
      parsed_value = eval(raw_value, {}, {})
    except SyntaxError as e:
      raise Config.ConfigError('No valid %s for %s.%s: %s\n%s' % (
        type.__name__, section, option, raw_value, e))

    if not isinstance(parsed_value, type):
      raise Config.ConfigError('No valid %s for %s.%s: %s' % (
        type.__name__, section, option, raw_value))

    return parsed_value

########NEW FILE########
__FILENAME__ = double_dag
__author__ = 'Ryan Williams'

from twitter.pants.base.abbreviate_target_ids import abbreviate_target_ids

# This file contains the implementation for a doubly-linked DAG data structure that is useful for dependency analysis.

class DoubleDagNode(object):
  def __init__(self, data):
    self.data = data
    self.parents = set()
    self.children = set()

  def __repr__(self):
    return "Node(%s)" % self.data.id


class DoubleDag(object):
  """This implementation of a doubly-linked DAG builds itself from a list of objects (of theoretically unknown type)
  and a function for generating each object's "children". It wraps each object in a "node" structure and exposes the
  following:

    - list of all nodes in the DAG (.nodes)
    - lookup dag node from original object (.lookup)
    - set of leaf nodes (.leaves)
    - a method (remove_nodes) that removes nodes and updates the set of leaves appropriately
    - the inverse method (restore_nodes)

  These are useful for computing the order in which to compile what groups of targets.
  """
  def __init__(self, objects, child_fn, logger):
    self._child_fn = child_fn
    self._logger = logger

    self.nodes = [ DoubleDagNode(object) for object in objects ]

    node_ids = [ node.data.id for node in self.nodes ]
    abbreviated_id_map = abbreviate_target_ids(node_ids)
    for node in self.nodes:
      node.short_id = abbreviated_id_map[node.data.id]
      node.data.short_id = abbreviated_id_map[node.data.id]

    self._nodes_by_data_map = {}
    for node in self.nodes:
      self._nodes_by_data_map[node.data] = node

    self._roots = set([])
    self.leaves = set([])

    self._logger.debug("%d nodes:" % len(self.nodes))
    for node in self.nodes:
      self._logger.debug(node.data.id,)
    self._logger.debug('')

    self._init_parent_and_child_relationships()

    self._find_roots_and_leaves()

    self._logger.debug("%d roots:" % len(self._roots))
    for root in self._roots:
      self._logger.debug(root.data.id)
    self._logger.debug('')

    self._logger.debug("%d leaves:" % len(self.leaves))
    for leaf in self.leaves:
      self._logger.debug(leaf.data.id)
    self._logger.debug('')


  def print_tree(self, use_short_ids=True):
    """This method prints out a python dictionary representing this DAG in a format suitable for eval'ing and useful
    for debugging."""
    def short_id(node):
      return node.short_id
    def id(node):
      return node.data.id

    node_fn = short_id if use_short_ids else id
    self._logger.debug("deps = {")
    for node in self.nodes:
      self._logger.debug(
        """  "%s": {"num": %d, "children": [%s]},""" % (
          node_fn(node),
          node.data.num_sources,
          ','.join(['"%s"' % node_fn(child) for child in node.children]))
      )
    self._logger.debug('}')
    self._logger.debug('')

  def lookup(self, data):
    if data in self._nodes_by_data_map:
      return self._nodes_by_data_map[data]
    return None

  def _init_parent_and_child_relationships(self):
    def find_children(original_node, data):
      for child_data in self._child_fn(data):
        if child_data in self._nodes_by_data_map:
          child_node = self._nodes_by_data_map[child_data]
          original_node.children.add(child_node)
          child_node.parents.add(original_node)
        else:
          raise Exception(
            "DAG child_fn shouldn't yield data objects not in tree:\n %s. child of: %s. original data: %s" % (
              str(child_data),
              str(data),
              str(original_node.data)))

    for node in self.nodes:
      find_children(node, node.data)


  def _find_roots_and_leaves(self):
    for node in self.nodes:
      if not node.parents:
        self._roots.add(node)
      if not node.children:
        self.leaves.add(node)


  def remove_nodes(self, nodes):
    """Removes the given nodes, updates self.leaves accordingly, and returns any nodes that have become leaves as a
    result of this removal."""
    new_leaves = set()
    for node in nodes:
      if node not in self.nodes:
        raise Exception("Attempting to remove invalid node: %s" % node.data.id)
      for parent_node in node.parents:
        if parent_node in nodes:
          continue
        parent_node.children.remove(node)
        if not parent_node.children:
          new_leaves.add(parent_node)

    # Do these outside in case 'nodes' is in fact self.leaves, so that we don't change the set we're iterating over.
    self.leaves -= nodes
    self.leaves.update(new_leaves)
    return new_leaves

########NEW FILE########
__FILENAME__ = generator
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import pprint
import pystache

from twitter.common.lang import Compatibility
from twitter.pants.base.mustache import MustacheRenderer


class TemplateData(dict):
  """Encapsulates data for a mustache template as a property-addressable read-only map-like struct.
  """

  def __init__(self, **kwargs):
    dict.__init__(self, MustacheRenderer.expand(kwargs))

  def extend(self, **kwargs):
    """Returns a new TemplateData with this template's data overlayed by the key value pairs
    specified as keyword arguments."""

    props = self.copy()
    props.update(kwargs)
    return TemplateData(**props)

  def __setattr__(self, key, value):
    raise AttributeError("Mutation not allowed - use %s.extend(%s = %s)" % (self, key, value))

  def __getattr__(self, key):
    if key in self:
      return self[key]
    else:
      return object.__getattribute__(self, key)

  def __str__(self):
    return 'TemplateData(%s)' % pprint.pformat(self)


class Generator(object):
  """Generates pants intermediary output files using a configured mustache template."""

  def __init__(self, template_text, **template_data):
    # pystache does a typecheck for unicode in python 2.x but rewrites its sources to deal unicode
    # via str in python 3.x.
    if Compatibility.PY2:
      template_text = unicode(template_text)
    self._template = pystache.parse(template_text)
    self.template_data = template_data

  def write(self, stream):
    """Applies the template to the template data and writes the result to the given file-like
    stream."""

    stream.write(pystache.render(self._template, self.template_data))

########NEW FILE########
__FILENAME__ = hash_utils
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import hashlib


def hash_all(strs, digest=None):
  """Returns a hash of the concatenation of all the strings in strs.

  If a hashlib message digest is not supplied a new sha1 message digest is used.
  """
  digest = digest or hashlib.sha1()
  for s in strs:
    digest.update(s)
  return digest.hexdigest()


def hash_file(path, digest=None):
  """Hashes the contents of the file at the given path and returns the hash digest in hex form.

  If a hashlib message digest is not supplied a new sha1 message digest is used.
  """
  digest = digest or hashlib.sha1()
  with open(path, 'rb') as fd:
    s = fd.read(8192)
    while s:
      digest.update(s)
      s = fd.read(8192)
  return digest.hexdigest()

########NEW FILE########
__FILENAME__ = mustache
import os
import pkgutil
import urlparse

import pystache


class MustacheRenderer(object):
  """Renders text using mustache templates."""

  @staticmethod
  def expand(args):
    # Add foo? for each foo in the map that evaluates to true.
    # Mustache needs this, especially in cases where foo is a list: there is no way to render a
    # block exactly once iff a list is not empty.
    # Note: if the original map contains foo?, it will take precedence over our synthetic foo?.
    def convert_val(x):
      # Pystache can't handle sets, so we convert to maps of key->True.
      if isinstance(x, set):
        return dict([(k, True) for k in x])
      elif isinstance(x, dict):
        return MustacheRenderer.expand(x)
      elif isinstance(x, list):
        return [convert_val(e) for e in x]
      else:
        return x
    items = [(key, convert_val(val)) for (key, val) in args.items()]
    ret = dict([(key + '?', True) for (key, val) in items if val and not key.endswith('?')])
    ret.update(dict(items))
    return ret

  def __init__(self, template_dir=None, package_name=None):
    """Create a renderer that finds templates by name in one of two ways.

    * If template_dir is specified, finds template foo in the file foo.mustache in that dir.
    * Otherwise, if package_name is specified, finds template foo embedded in that
      package under templates/foo.mustache.
    * Otherwise will not find templates by name, so can only be used with an existing
      template string.
    """
    self._template_dir = template_dir
    self._package_name = package_name
    self._pystache_renderer = pystache.Renderer(search_dirs=template_dir)

  def render_name(self, template_name, args):
    # TODO: Precompile and cache the templates?
    if self._template_dir:
      # Let pystache find the template by name.
      return self._pystache_renderer.render_name(template_name, MustacheRenderer.expand(args))
    else:
      # Load the named template embedded in our package.
      template = pkgutil.get_data(self._package_name,
                                  os.path.join('templates', template_name + '.mustache'))
      return self.render(template, args)

  def render(self, template, args):
    return self._pystache_renderer.render(template, MustacheRenderer.expand(args))

  def render_callable(self, inner_template_name, arg_string, outer_args):
    """Handle a mustache callable.

    In a mustache template, when foo is callable, ``{{#foo}}arg_string{{/foo}}`` is replaced
    with the result of calling ``foo(arg_string)``. The callable must interpret ``arg_string``.

    This method provides an implementation of such a callable that does the following:

    #. Parses the arg_string as CGI args.
    #. Adds them to the original args that the enclosing template was rendered with.
    #. Renders some other template against those args.
    #. Returns the resulting text.

    Use by adding
    ``{ 'foo': lambda x: self._renderer.render_callable('foo_template', x, args) }``
    to the args of the outer template, which can then contain ``{{#foo}}arg_string{{/foo}}``.
    """
    # First render the arg_string (mustache doesn't do this for you, and it may itself
    # contain mustache constructs).
    rendered_arg_string = self.render(arg_string, outer_args)
    # Parse the inner args as CGI args.
    inner_args = dict([(k, v[0]) for k, v in urlparse.parse_qs(rendered_arg_string).items()])
    # Order matters: lets the inner args override the outer args.
    args = dict(outer_args.items() + inner_args.items())
    # Render.
    return self.render_name(inner_template_name, args)


########NEW FILE########
__FILENAME__ = parse_context
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import collections
import copy
import os

from functools import partial

from contextlib import contextmanager

from twitter.common.dirutil.fileset import Fileset
from twitter.common.lang import Compatibility

from .build_environment import get_buildroot
from .build_file import BuildFile
from .config import Config


class ParseContext(object):
  """Defines the context of a parseable BUILD file target and provides a mechanism for targets to
  discover their context when invoked via eval.
  """

  class ContextError(Exception):
    """Indicates an action that requires a BUILD file parse context was attempted outside any."""

  _active = collections.deque([])
  _parsed = set()

  _strs_to_exec = [
    "from twitter.pants.base.build_file_context import *",
    "from twitter.common.quantity import Amount, Time",
  ]

  @classmethod
  def add_to_exec_context(cls, str_to_exec):
    """This hook allows for adding symbols to the execution context in which BUILD files are
    parsed. This should only be used for importing symbols that are used fairly ubiquitously in
    BUILD files, and possibly for appending to sys.path to get local python code on the python
    path.

    This will be phased out in favor of a more robust plugin architecture that supports import
    injection and path amendment."""
    cls._strs_to_exec.append(str_to_exec)

  @classmethod
  def locate(cls):
    """Attempts to find the current root directory and buildfile.

    If there is an active parse context (see do_in_context), then it is returned.
    """
    if not cls._active:
      raise cls.ContextError('No parse context active.')
    return next(reversed(cls._active))

  @classmethod
  def path(cls, relpath=None):
    """Returns the active parse context path or `os.getcwd()` if there is no active context.

    If relpath is specified the path returned will be joined to it but in either case the returned
    path will be absolute.
    """
    base = os.getcwd() if not ParseContext._active else cls.locate().current_buildfile.parent_path
    return os.path.abspath(os.path.join(base, relpath) if relpath else base)

  @classmethod
  @contextmanager
  def temp(cls, basedir=None):
    """Activates a temporary parse context in the given basedir relative to the build root or else
    in the build root dir itself if no basedir is specified.
    """
    context = cls(BuildFile(get_buildroot(), basedir or 'BUILD.temp', must_exist=False))
    with cls.activate(context):
      yield

  @classmethod
  @contextmanager
  def activate(cls, ctx):
    """Activates the given ParseContext."""
    if hasattr(ctx, '_on_context_exit'):
      raise cls.ContextError('Context actions registered outside this parse context arg active')

    try:
      cls._active.append(ctx)
      ctx._on_context_exit = []
      yield
    finally:
      for func, args, kwargs in ctx._on_context_exit:
        func(*args, **kwargs)
      del ctx._on_context_exit
      cls._active.pop()

  def __init__(self, buildfile):
    self.buildfile = buildfile
    self._active_buildfile = buildfile
    self._parsed = False

  @classmethod
  def default_globals(cls, config=None):
    """
    Has twitter.pants.*, but not file-specfic things like __file__
    If you want to add new imports to be available to all BUILD files, add a section to the config
    similar to:

      [parse]
      headers: ['from test import get_jar',]

    You may also need to add new roots to the sys.path. see _run in pants_exe.py
    """
    to_exec = list(cls._strs_to_exec)
    if config:
      # TODO: This can be replaced once extensions are enabled with
      # https://github.com/pantsbuild/pants/issues/5
      to_exec.extend(config.getlist('parse', 'headers', default=[]))

    pants_context = {}
    for str_to_exec in to_exec:
      ast = compile(str_to_exec, '<string>', 'exec')
      Compatibility.exec_function(ast, pants_context)

    return pants_context

  def parse(self, **globalargs):
    """The entry point to parsing of a BUILD file.

    from twitter.pants.targets.sources import SourceRoot

    See locate().
    """
    if self.buildfile not in ParseContext._parsed:
      buildfile_family = tuple(self.buildfile.family())

      pants_context = self.default_globals(Config.load())

      with ParseContext.activate(self):
        for buildfile in buildfile_family:
          self._active_buildfile = buildfile
          # We may have traversed a sibling already, guard against re-parsing it.
          if buildfile not in ParseContext._parsed:
            ParseContext._parsed.add(buildfile)

            buildfile_dir = os.path.dirname(buildfile.full_path)

            # TODO(John Sirois): XXX imports are done here to prevent a cycles
            from twitter.pants.targets.jvm_binary import Bundle
            from twitter.pants.targets.sources import SourceRoot

            class RelativeBundle(Bundle):
              def __init__(self, mapper=None, relative_to=None):
                super(RelativeBundle, self).__init__(
                    base=buildfile_dir,
                    mapper=mapper,
                    relative_to=relative_to)

            # TODO(John Sirois): This is not build-dictionary friendly - rework SourceRoot to allow
            # allow for doc of both register (as source_root) and source_root.here(*types).
            class RelativeSourceRoot(object):
              @staticmethod
              def here(*allowed_target_types):
                """Registers the cwd as a source root for the given target types."""
                SourceRoot.register(buildfile_dir, *allowed_target_types)

              def __init__(self, basedir, *allowed_target_types):
                SourceRoot.register(os.path.join(buildfile_dir, basedir), *allowed_target_types)

            eval_globals = copy.copy(pants_context)
            eval_globals.update({
              'ROOT_DIR': buildfile.root_dir,
              '__file__': buildfile.full_path,
              'globs': partial(Fileset.globs, root=buildfile_dir),
              'rglobs': partial(Fileset.rglobs, root=buildfile_dir),
              'zglobs': partial(Fileset.zglobs, root=buildfile_dir),
              'source_root': RelativeSourceRoot,
              'bundle': RelativeBundle
            })
            eval_globals.update(globalargs)
            Compatibility.exec_function(buildfile.code(), eval_globals)

  def on_context_exit(self, func, *args, **kwargs):
    """ Registers a command to invoke just before this parse context is exited.

    It is an error to attempt to register an on_context_exit action outside an active parse
    context.
    """
    if not hasattr(self, '_on_context_exit'):
      raise self.ContextError('Can only register context exit actions when a parse context '
                              'is active')

    if not callable(func):
      raise TypeError('func must be a callable object')

    self._on_context_exit.append((func, args, kwargs))

  def do_in_context(self, work):
    """Executes the callable work in this parse context."""
    if not callable(work):
      raise TypeError('work must be a callable object')

    with ParseContext.activate(self):
      return work()

  def __repr__(self):
    return '%s(%s)' % (type(self).__name__, self.buildfile)

  @property
  def current_buildfile(self):
    """ This property return the current build file being parsed from all BUILD files co-located
    with this BUILD file within the family.
    """
    return self._active_buildfile

########NEW FILE########
__FILENAME__ = rcfile
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.lang import Compatibility
from twitter.common.log import logger
from twitter.pants.base.config import Config


log = logger(name='rcfile')


class RcFile(object):
  """Handles rcfile-style configuration files.

  Precedence is given to rcfiles that come last in the given sequence of paths.
  The effect is as if each rcfile in paths overlays the next in a walk from left to right.
  """

  # TODO(John Sirois): localize handling of this flag value back into pants_exe.py once the new old
  # split is healed.
  _DISABLE_PANTS_RC_OPTION = '--no-pantsrc'

  @staticmethod
  def install_disable_rc_option(parser):
    parser.add_option(RcFile._DISABLE_PANTS_RC_OPTION, action = 'store_true', dest = 'nopantsrc',
                      default = False, help = 'Specifies that pantsrc files should be ignored.')

  def __init__(self, paths, default_prepend=True, process_default=False):
    """
    :param paths: The rcfiles to apply default subcommand options from.
    :param default_prepend: Whether to prepend (the default) or append if default options
      are specified with the ``options`` key.
    :param process_default: True to process options in the [DEFAULT] section and apply
      regardless of goal.
    """

    self.default_prepend = default_prepend
    self.process_default = process_default

    if not paths:
      raise ValueError('One or more rcfile paths must be specified')

    if isinstance(paths, Compatibility.string):
      paths = [paths]
    self.paths = [os.path.expanduser(path) for path in paths]

  def apply_defaults(self, commands, args):
    """Augment arguments with defaults found for the given commands.

    The returned arguments will be a new copy of the given args with possibly extra augmented
    arguments.

    Default options are applied from the following keys under a section with the name of the
    sub-command the default options apply to:

    * `options` - These options are either prepended or appended to the command line args as
      specified in the constructor with default_prepend.
    * `prepend-options` - These options are prepended to the command line args.
    * `append-options` - These options are appended to the command line args.
    """

    args = args[:]

    if RcFile._DISABLE_PANTS_RC_OPTION in args:
      return args

    config = Config.create_parser()
    read_from = config.read(self.paths)
    if not read_from:
      log.debug('no rcfile found')
      return args

    log.debug('using rcfiles: %s to modify args' % ','.join(read_from))

    def get_rcopts(command, key):
      return config.get(command, key).split() if config.has_option(command, key) else []

    commands = list(commands)
    if self.process_default:
      commands.insert(0, Config.DEFAULT_SECTION)

    for cmd in commands:
      opts = get_rcopts(cmd, 'options')
      args = (opts + args) if self.default_prepend else (args + opts)
      args = get_rcopts(cmd, 'prepend-options') + args + get_rcopts(cmd, 'append-options')
    return args

########NEW FILE########
__FILENAME__ = revision
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import re

from itertools import izip_longest


class Revision(object):
  """Represents a software revision that is comparable to another revision describing the same
  software.
  """
  class BadRevision(Exception):
    """Indicates a problem parsing a revision."""

  @classmethod
  def _parse_atom(cls, atom):
    try:
      return int(atom)
    except ValueError:
      return atom

  @classmethod
  def semver(cls, rev):
    """Attempts to parse a Revision from a semantic version.

    See http://semver.org/ for the full specification.
    """
    def parse_extra(delimiter, value):
      if not value:
        return None, None
      else:
        components = value.split(delimiter, 1)
        return components[0], None if len(components) == 1 else components[1]

    def parse_patch(patch):
      patch, pre_release = parse_extra('-', patch)
      if pre_release:
        pre_release, build = parse_extra('+', pre_release)
      else:
        patch, build = parse_extra('+', patch)
      return patch, pre_release, build

    def parse_components(value):
      if not value:
        yield None
      else:
        for atom in value.split('.'):
          yield cls._parse_atom(atom)

    try:
      major, minor, patch = rev.split('.', 2)
      patch, pre_release, build = parse_patch(patch)
      components = [int(major), int(minor), int(patch)]
      components.extend(parse_components(pre_release))
      components.extend(parse_components(build))
      return cls(*components)
    except ValueError:
      raise cls.BadRevision("Failed to parse '%s' as a semantic version number" % rev)

  @classmethod
  def lenient(cls, rev):
    """A lenient revision parser that tries to split the version into logical components with
    heuristics inspired by PHP's version_compare.
    """
    rev = re.sub(r'(\d)([a-zA-Z])', r'\1.\2', rev)
    rev = re.sub(r'([a-zA-Z])(\d)', r'\1.\2', rev)
    return cls(*map(cls._parse_atom, re.split(r'[.+_\-]', rev)))

  def __init__(self, *components):
    self._components = components

  @property
  def components(self):
    """Returns a list of this revision's components from most major to most minor."""
    return list(self._components)

  def __cmp__(self, other):
    for ours, theirs in izip_longest(self._components, other._components, fillvalue=0):
      difference = cmp(ours, theirs)
      if difference != 0:
        return difference
    return 0

  def __repr__(self):
    return '%s(%s)' % (self.__class__.__name__, ', '.join(map(repr, self._components)))

########NEW FILE########
__FILENAME__ = run_info
import getpass
import os
import re
import socket
import time

from twitter.common.dirutil import safe_mkdir_for

from .build_environment import get_scm, get_buildroot


class RunInfo(object):
  """A little plaintext file containing very basic info about a pants run.

  Can only be appended to, never edited.
  """

  @classmethod
  def dir(cls, config):
    """Returns the configured base directory run info files are stored under."""
    # TODO(John Sirois): This is centralized, but in an awkward location.  Isolate RunInfo reading
    # and writing in 1 package or class that could naturally know this location and synthesize
    # info_file names.
    return config.getdefault('info_dir',
                             default=os.path.join(config.getdefault('pants_workdir'), 'runs'))

  def __init__(self, info_file):
    self._info_file = info_file
    safe_mkdir_for(self._info_file)
    self._info = {}
    if os.path.exists(self._info_file):
      with open(self._info_file, 'r') as infile:
        info = infile.read()
      for m in re.finditer("""^([^:]+):(.*)$""", info, re.MULTILINE):
        self._info[m.group(1).strip()] = m.group(2).strip()

  def path(self):
    return self._info_file

  def get_info(self, key):
    return self._info.get(key, None)

  def __getitem__(self, key):
    ret = self.get_info(key)
    if ret is None:
      raise KeyError(key)
    return ret

  def get_as_dict(self):
    return self._info.copy()

  def add_info(self, key, val):
    """Adds the given info and returns a dict composed of just this added info."""
    return self.add_infos((key, val))

  def add_infos(self, *keyvals):
    """Adds the given info and returns a dict composed of just this added info."""
    infos = dict(keyvals)
    with open(self._info_file, 'a') as outfile:
      for key, val in infos.items():
        key = key.strip()
        val = str(val).strip()
        if ':' in key:
          raise Exception, 'info key must not contain a colon'
        outfile.write('%s: %s\n' % (key, val))
        self._info[key] = val
    return infos

  def add_basic_info(self, run_id, timestamp):
    """Adds basic build info and returns a dict composed of just this added info."""
    datetime = time.strftime('%A %b %d, %Y %H:%M:%S', time.localtime(timestamp))
    user = getpass.getuser()
    machine = socket.gethostname()
    path = get_buildroot()
    return self.add_infos(('id', run_id), ('timestamp', timestamp), ('datetime', datetime),
                          ('user', user), ('machine', machine), ('path', path))

  def add_scm_info(self):
    """Adds SCM-related info and returns a dict composed of just this added info."""
    scm = get_scm()
    if scm:
      revision = scm.commit_id
      tag = scm.tag_name or 'none'
      branch = scm.branch_name or revision
    else:
      revision, tag, branch = 'none', 'none', 'none'
    return self.add_infos(('revision', revision), ('tag', tag), ('branch', branch))

########NEW FILE########
__FILENAME__ = target
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import collections
import os
import sys

from twitter.common.collections import OrderedSet, maybe_list
from twitter.common.lang import Compatibility

from .address import Address
from .build_manual import manual
from .hash_utils import hash_all
from .parse_context import ParseContext


class TargetDefinitionException(Exception):
  """Thrown on errors in target definitions."""

  def __init__(self, target, msg):
    address = getattr(target, 'address', None)
    if address is None:
      try:
        location = ParseContext.locate().current_buildfile
      except ParseContext.ContextError:
        location = 'unknown location'
      address = 'unknown target of type %s in %s' % (target.__class__.__name__, location)
    super(Exception, self).__init__('Error with %s: %s' % (address, msg))


class AbstractTarget(object):

  @property
  def is_concrete(self):
    """Returns true if a target resolves to itself."""
    targets = list(self.resolve())
    return len(targets) == 1 and targets[0] == self

  @property
  def has_resources(self):
    """Returns True if the target has an associated set of Resources."""
    return hasattr(self, 'resources') and self.resources

  @property
  def is_exported(self):
    """Returns True if the target provides an artifact exportable from the repo."""
    # TODO(John Sirois): fixup predicate dipping down into details here.
    return self.has_label('exportable') and self.provides

  @property
  def is_internal(self):
    """Returns True if the target is internal to the repo (ie: it might have dependencies)."""
    return self.has_label('internal')

  @property
  def is_jar(self):
    """Returns True if the target is a jar."""
    return False

  @property
  def is_java_agent(self):
    """Returns `True` if the target is a java agent."""
    return self.has_label('java_agent')

  @property
  def is_jvm_app(self):
    """Returns True if the target produces a java application with bundled auxiliary files."""
    return False

  @property
  def is_thrift(self):
    """Returns True if the target has thrift IDL sources."""
    return False

  @property
  def is_jvm(self):
    """Returns True if the target produces jvm bytecode."""
    return self.has_label('jvm')

  @property
  def is_codegen(self):
    """Returns True if the target is a codegen target."""
    return self.has_label('codegen')

  @property
  def is_synthetic(self):
    """Returns True if the target is a synthetic target injected by the runtime."""
    return self.has_label('synthetic')

  @property
  def is_jar_library(self):
    """Returns True if the target is an external jar library."""
    return self.has_label('jars')

  @property
  def is_java(self):
    """Returns True if the target has or generates java sources."""
    return self.has_label('java')

  @property
  def is_apt(self):
    """Returns True if the target exports an annotation processor."""
    return self.has_label('apt')

  @property
  def is_python(self):
    """Returns True if the target has python sources."""
    return self.has_label('python')

  @property
  def is_scala(self):
    """Returns True if the target has scala sources."""
    return self.has_label('scala')

  @property
  def is_scalac_plugin(self):
    """Returns True if the target builds a scalac plugin."""
    return self.has_label('scalac_plugin')

  @property
  def is_test(self):
    """Returns True if the target is comprised of tests."""
    return self.has_label('tests')

  def resolve(self):
    """Returns an iterator over the target(s) this target represents."""
    yield self


@manual.builddict()
class Target(AbstractTarget):
  """The baseclass for all pants targets.

  Handles registration of a target amongst all parsed targets as well as location of the target
  parse context.
  """

  _targets_by_address = None
  _addresses_by_buildfile = None

  @classmethod
  def identify(cls, targets):
    """Generates an id for a set of targets."""
    return cls.combine_ids(target.id for target in targets)

  @classmethod
  def maybe_readable_identify(cls, targets):
    """Generates an id for a set of targets.

    If the set is a single target, just use that target's id."""
    return cls.maybe_readable_combine_ids([target.id for target in targets])

  @staticmethod
  def combine_ids(ids):
    """Generates a combined id for a set of ids."""
    return hash_all(sorted(ids))  # We sort so that the id isn't sensitive to order.

  @classmethod
  def maybe_readable_combine_ids(cls, ids):
    """Generates combined id for a set of ids, but if the set is a single id, just use that."""
    ids = list(ids)  # We can't len a generator.
    return ids[0] if len(ids) == 1 else cls.combine_ids(ids)

  @classmethod
  def get_all_addresses(cls, buildfile):
    """Returns all of the target addresses in the specified buildfile if already parsed; otherwise,
    parses the buildfile to find all the addresses it contains and then returns them.
    """
    def lookup():
      if buildfile in cls._addresses_by_buildfile:
        return cls._addresses_by_buildfile[buildfile]
      else:
        return OrderedSet()

    addresses = lookup()
    if addresses:
      return addresses
    else:
      ParseContext(buildfile).parse()
      return lookup()

  @classmethod
  def _clear_all_addresses(cls):
    cls._targets_by_address = {}
    cls._addresses_by_buildfile = collections.defaultdict(OrderedSet)

  @classmethod
  def get(cls, address):
    """Returns the specified module target if already parsed; otherwise, parses the buildfile in the
    context of its parent directory and returns the parsed target.
    """
    def lookup():
      return cls._targets_by_address.get(address, None)

    target = lookup()
    if target:
      return target
    else:
      ParseContext(address.buildfile).parse()
      return lookup()

  @classmethod
  def resolve_all(cls, targets, *expected_types):
    """Yield the resolved concrete targets checking each is a subclass of one of the expected types
    if specified.
    """
    if targets:
      for target in maybe_list(targets, expected_type=Target):
        concrete_targets = [t for t in target.resolve() if t.is_concrete]
        for resolved in concrete_targets:
          if expected_types and not isinstance(resolved, expected_types):
            raise TypeError('%s requires types: %s and found %s' % (cls, expected_types, resolved))
          yield resolved

  def __init__(self, name, reinit_check=True, exclusives=None):
    """
    :param string name: The target name.
    """
    # See "get_all_exclusives" below for an explanation of the exclusives parameter.
    # This check prevents double-initialization in multiple-inheritance situations.
    # TODO(John Sirois): fix target inheritance - use super() to linearize or use alternatives to
    # multiple inheritance.
    if not reinit_check or not hasattr(self, '_initialized'):
      if not isinstance(name, Compatibility.string):
        self.address = '%s:%s' % (ParseContext.locate().current_buildfile, str(name))
        raise TargetDefinitionException(self, "Invalid target name: %s" % name)
      self.name = name
      self.description = None

      self.address = self._locate()

      # TODO(John Sirois): Transition all references to self.identifier to eliminate id builtin
      # ambiguity
      self.id = self._create_id()

      self._register()

      self.labels = set()

      self._initialized = True

      self.declared_exclusives = collections.defaultdict(set)
      if exclusives is not None:
        for k in exclusives:
          self.declared_exclusives[k].add(exclusives[k])
      self.exclusives = None

      # For synthetic codegen targets this will be the original target from which
      # the target was synthesized.
      self._derived_from = self

  @property
  def derived_from(self):
    """Returns the target this target was derived from.

    If this target was not derived from another, returns itself.
    """
    return self._derived_from

  @derived_from.setter
  def derived_from(self, value):
    """Sets the target this target was derived from.

    Various tasks may create targets not written down in any BUILD file.  Often these targets are
    derived from targets written down in BUILD files though in which case the derivation chain
    should be maintained.
    """
    if value and not isinstance(value, AbstractTarget):
      raise ValueError('Expected derived_from to be a Target, given %s of type %s'
                       % (value, type(value)))
    self._derived_from = value

  def get_declared_exclusives(self):
    return self.declared_exclusives

  def add_to_exclusives(self, exclusives):
    if exclusives is not None:
      for key in exclusives:
        self.exclusives[key] |= exclusives[key]

  def get_all_exclusives(self):
    """ Get a map of all exclusives declarations in the transitive dependency graph.

    For a detailed description of the purpose and use of exclusives tags,
    see the documentation of the CheckExclusives task.

    """
    if self.exclusives is None:
      self._propagate_exclusives()
    return self.exclusives

  def _propagate_exclusives(self):
    if self.exclusives is None:
      self.exclusives = collections.defaultdict(set)
      self.add_to_exclusives(self.declared_exclusives)
      # This may perform more work than necessary.
      # We want to just traverse the immediate dependencies of this target,
      # but for a general target, we can't do that. _propagate_exclusives is overridden
      # in subclasses when possible to avoid the extra work.
      self.walk(lambda t: self._propagate_exclusives_work(t))

  def _propagate_exclusives_work(self, target):
    # Note: this will cause a stack overflow if there is a cycle in
    # the dependency graph, so exclusives checking should occur after
    # cycle detection.
    if hasattr(target, "declared_exclusives"):
      self.add_to_exclusives(target.declared_exclusives)
    return None

  def _post_construct(self, func, *args, **kwargs):
    """Registers a command to invoke after this target's BUILD file is parsed."""
    ParseContext.locate().on_context_exit(func, *args, **kwargs)

  def _create_id(self):
    """Generates a unique identifier for the BUILD target.

    The generated id is safe for use as a path name on unix systems.
    """
    buildfile_relpath = os.path.dirname(self.address.buildfile.relpath)
    if buildfile_relpath in ('.', ''):
      return self.name
    else:
      return "%s.%s" % (buildfile_relpath.replace(os.sep, '.'), self.name)

  def _locate(self):
    parse_context = ParseContext.locate()
    return Address(parse_context.current_buildfile, self.name)

  def _register(self):
    existing = self._targets_by_address.get(self.address)
    if existing and existing is not self:
      if existing.address.buildfile != self.address.buildfile:
        raise TargetDefinitionException(self, "already defined in a sibling BUILD "
                                              "file: %s" % existing.address.buildfile.relpath)
      else:
        raise TargetDefinitionException(self, "duplicate to %s" % existing)

    self._targets_by_address[self.address] = self
    self._addresses_by_buildfile[self.address.buildfile].add(self.address)

  @property
  def identifier(self):
    """A unique identifier for the BUILD target.

    The generated id is safe for use as a path name on unix systems.
    """
    return self.id

  def walk(self, work, predicate=None):
    """Walk of this target's dependency graph visiting each node exactly once.

    If a predicate is supplied it will be used to test each target before handing the target to
    work and descending. Work can return targets in which case these will be added to the walk
    candidate set if not already walked.

    :param work: Callable that takes a :py:class:`twitter.pants.base.target.Target`
      as its single argument.
    :param predicate: Callable that takes a :py:class:`twitter.pants.base.target.Target`
      as its single argument and returns True if the target should passed to ``work``.
    """
    if not callable(work):
      raise ValueError('work must be callable but was %s' % work)
    if predicate and not callable(predicate):
      raise ValueError('predicate must be callable but was %s' % predicate)
    self._walk(set(), work, predicate)

  def _walk(self, walked, work, predicate=None):
    for target in self.resolve():
      if target not in walked:
        walked.add(target)
        if not predicate or predicate(target):
          additional_targets = work(target)
          if hasattr(target, '_walk'):
            target._walk(walked, work, predicate)
          if additional_targets:
            for additional_target in additional_targets:
              if hasattr(additional_target, '_walk'):
                additional_target._walk(walked, work, predicate)

  @manual.builddict()
  def with_description(self, description):
    """Set a human-readable description of this target."""
    self.description = description
    return self

  def add_labels(self, *label):
    self.labels.update(label)

  def remove_label(self, label):
    self.labels.remove(label)

  def has_label(self, label):
    return label in self.labels

  def __eq__(self, other):
    return isinstance(other, Target) and self.address == other.address

  def __hash__(self):
    return hash(self.address)

  def __ne__(self, other):
    return not self.__eq__(other)

  def __repr__(self):
    return "%s(%s)" % (type(self).__name__, self.address)

  @staticmethod
  def has_jvm_targets(targets):
    """Returns true if the given sequence of targets contains at least one jvm target as determined
    by is_jvm(...)
    """

    return len(list(Target.extract_jvm_targets(targets))) > 0

  @staticmethod
  def extract_jvm_targets(targets):
    """Returns an iterator over the jvm targets the given sequence of targets resolve to.  The
    given targets can be a mix of types and only valid jvm targets (as determined by is_jvm(...)
    will be returned by the iterator.
    """

    for target in targets:
      if target is None:
        print('Warning! Null target!', file=sys.stderr)
        continue
      for real_target in target.resolve():
        if real_target.is_jvm:
          yield real_target

  def has_sources(self, extension=None):
    """Returns True if the target has sources.

    If an extension is supplied the target is further checked for at least 1 source with the given
    extension.
    """
    return (self.has_label('sources') and
            (not extension or
             (hasattr(self, 'sources') and
              any(source.endswith(extension) for source in self.sources))))


Target._clear_all_addresses()

########NEW FILE########
__FILENAME__ = timer
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import time

from collections import namedtuple
from contextlib import contextmanager

from twitter.common.lang import Compatibility


Timing = namedtuple('Timing', ['label', 'times', 'overlapping'])


class Timer(object):
  def __init__(self):
    self._timings = []

  @contextmanager
  def timing(self, label):
    """Convenient timing context.

    Use like this:

    with timer.timing(label):
      ... the work that will be timed ...
    """
    start = self.now()
    yield None
    elapsed = self.now() - start
    self.log(label, [elapsed])

  def now(self):
    return time.monotonic() if Compatibility.PY3 else time.time()

  def log(self, label, times, overlapping=False):
    """Code that has to measure its own timings directly can log them here.

    If labels are of the form prefix:suffix, then the sum of all times of consecutively-logged
    timings with the same prefix will also be logged.

    Set overlapping to True if you're logging a timing that overlaps with other, already-logged
    timings.
    """
    self._timings.append(Timing(label, times, overlapping))

  def print_timings(self):
    grand_total_time = 0

    last_prefix = None
    total_time_for_prefix = 0
    num_timings_with_prefix = 0

    def maybe_print_timings_for_prefix():
      if num_timings_with_prefix > 1:
        print('[%(prefix)s] total: %(total).3fs' % {
          'prefix': last_prefix,
          'total': total_time_for_prefix
        })

    for timing in self._timings:
      total_time = sum(timing.times)
      if not timing.overlapping:
        grand_total_time += total_time

      pos = timing.label.find(':')
      if pos != -1:
        prefix = timing.label[0:pos]
        if prefix == last_prefix and not timing.overlapping:
          total_time_for_prefix += total_time
          num_timings_with_prefix += 1
        else:
          maybe_print_timings_for_prefix()
          total_time_for_prefix = total_time
          num_timings_with_prefix = 1
        last_prefix = prefix

      if len(timing.times) > 1:
        print('[%(label)s(%(numsteps)d)] %(timings)s -> %(total).3fs' % {
          'label': timing.label,
          'numsteps': len(timing.times),
          'timings': ','.join('%.3fs' % time for time in timing.times),
          'total': total_time
        })
      else:
        print('[%(label)s] %(total).3fs' % {
          'label': timing.label,
          'total': total_time
        })
    maybe_print_timings_for_prefix()
    print('total: %.3fs' % grand_total_time)

########NEW FILE########
__FILENAME__ = worker_pool
from multiprocessing.pool import ThreadPool
import threading
from twitter.pants.reporting.report import Report


class Work(object):
  """Represents multiple concurrent calls to the same callable."""
  def __init__(self, func, args_tuples, workunit_name=None):
    # A callable.
    self.func = func

    # A list of tuples of args. func will be called once per tuple, concurrently.
    # The length of this list is the cardinality of the work.
    self.args_tuples = args_tuples

    # If specified, each invocation will be executed in a workunit of this name.
    self.workunit_name = workunit_name


class WorkerPool(object):
  """A pool of workers.

  Workers are threads, and so are subject to GIL constraints. Submitting CPU-bound work
  may not be effective. Use this class primarily for IO-bound work.
  """

  def __init__(self, parent_workunit, run_tracker, num_workers):
    self._run_tracker = run_tracker
    # All workers accrue work to the same root.
    self._pool = ThreadPool(processes=num_workers,
                            initializer=self._run_tracker.register_thread,
                            initargs=(parent_workunit, ))
    # We mustn't shutdown when there are pending workchains, as they may need to submit work
    # in the future, and the pool doesn't know about this yet.
    self._pending_workchains = 0
    self._pending_workchains_cond = threading.Condition()  # Protects self._pending_workchains.

    self._shutdown_hooks = []

  def add_shutdown_hook(self, hook):
    self._shutdown_hooks.append(hook)

  def submit_async_work(self, work,  workunit_parent=None, on_success=None, on_failure=None):
    """Submit work to be executed in the background.

    - work: The work to execute.
    - workunit_parent: If specified, work is accounted for under this workunit.
    - on_success: If specified, a callable taking a single argument, which will be a list
                  of return values of each invocation, in order. Called only if all work succeeded.
    - on_failure: If specified, a callable taking a single argument, which is an exception
                  thrown in the work.

    Don't do work in on_success: not only will it block the result handling thread, but
    that thread is not a worker and doesn't have a logging context etc. Use it just to
    submit further work to the pool.
    """
    if work is None or len(work.args_tuples) == 0:  # map_async hangs on 0-length iterables.
      if on_success:
        on_success([])
    else:
      def do_work(*args):
        self._do_work(work.func, *args, workunit_name=work.workunit_name,
                      workunit_parent=workunit_parent, on_failure=on_failure)
      self._pool.map_async(do_work, work.args_tuples, chunksize=1, callback=on_success)

  def submit_async_work_chain(self, work_chain, workunit_parent, done_hook=None):
    """Submit work to be executed in the background.

    - work_chain: An iterable of Work instances. Will be invoked serially. Each instance may
                  have a different cardinality. There is no output-input chaining: the argument
                  tuples must already be present in each work instance.  If any work throws an
                  exception no subsequent work in the chain will be attempted.
    - workunit_parent: Work is accounted for under this workunit.
    - done_hook: If not None, invoked with no args after all work is done, or on error.
    """
    def done():
      if done_hook:
        done_hook()
      with self._pending_workchains_cond:
        self._pending_workchains -= 1
        self._pending_workchains_cond.notify()

    def error(e):
      done()
      self._run_tracker.log(Report.ERROR, '%s' % e)

    # We filter out Nones defensively. There shouldn't be any, but if a bug causes one,
    # Pants might hang indefinitely without this filtering.
    work_iter = iter(filter(None, work_chain))
    def submit_next():
      try:
        self.submit_async_work(work_iter.next(), workunit_parent=workunit_parent,
                               on_success=lambda x: submit_next(), on_failure=error)
      except StopIteration:
        done()  # The success case.

    with self._pending_workchains_cond:
      self._pending_workchains += 1
    try:
      submit_next()
    except Exception as e:  # Handles errors in the submission code.
      done()
      self._run_tracker.log(Report.ERROR, '%s' % e)
      raise

  def submit_work_and_wait(self, work, workunit_parent=None):
    """Submit work to be executed on this pool, but wait for it to complete.

    - work: The work to execute.
    - workunit_parent: If specified, work is accounted for under this workunit.

    Returns a list of return values of each invocation, in order.  Throws if any invocation does.
    """
    if work is None or len(work.args_tuples) == 0:  # map hangs on 0-length iterables.
      return []
    else:
      def do_work(*args):
        return self._do_work(work.func, *args, workunit_name=work.workunit_name,
                             workunit_parent=workunit_parent)
      # We need to specify a timeout explicitly, because otherwise python ignores SIGINT when waiting
      # on a condition variable, so we won't be able to ctrl-c out.
      return self._pool.map_async(do_work, work.args_tuples, chunksize=1).get(timeout=1000000000)

  def _do_work(self, func, args_tuple, workunit_name, workunit_parent, on_failure=None):
    try:
      if workunit_name:
        with self._run_tracker.new_workunit_under_parent(name=workunit_name, parent=workunit_parent):
          return func(*args_tuple)
      else:
        return func(*args_tuple)
    except Exception as e:
      if on_failure:
        # Note that here the work's workunit is closed. So, e.g., it's OK to use on_failure()
        # to close an ancestor workunit.
        on_failure(e)
      raise

  def shutdown(self):
    with self._pending_workchains_cond:
      while self._pending_workchains > 0:
        self._pending_workchains_cond.wait()
      self._pool.close()
      self._pool.join()
      for hook in self._shutdown_hooks:
        hook()

  def abort(self):
    self._pool.terminate()

########NEW FILE########
__FILENAME__ = workunit
import os
import re
import time
import uuid

from twitter.common.dirutil import safe_mkdir_for
from twitter.common.rwbuf.read_write_buffer import FileBackedRWBuf  # XXX pull back into pants


class WorkUnit(object):
  """A hierarchical unit of work, for the purpose of timing and reporting.

  A WorkUnit can be subdivided into further WorkUnits. The WorkUnit concept is deliberately
  decoupled from the phase/task hierarchy. This allows some flexibility in having, say,
  sub-units inside a task. E.g., there might be one WorkUnit representing an entire pants run,
  and that can be subdivided into WorkUnits for each phase. Each of those can be subdivided into
  WorkUnits for each task, and a task can subdivide that into further work units, if finer-grained
  timing and reporting is needed.
  """

  # The outcome of a workunit.
  # It can only be set to a new value <= the old one.
  ABORTED = 0
  FAILURE = 1
  WARNING = 2
  SUCCESS = 3
  UNKNOWN = 4

  @staticmethod
  def choose_for_outcome(outcome, aborted_val, failure_val, warning_val, success_val, unknown_val):
    """Returns one of the 5 arguments, depending on the outcome."""
    if outcome not in range(0, 5):
      raise Exception('Invalid outcome: %s' % outcome)
    return (aborted_val, failure_val, warning_val, success_val, unknown_val)[outcome]

  @staticmethod
  def outcome_string(outcome):
    """Returns a human-readable string describing the outcome."""
    return WorkUnit.choose_for_outcome(outcome, 'ABORTED', 'FAILURE', 'WARNING', 'SUCCESS', 'UNKNOWN')

  # Labels describing a workunit.  Reporting code can use this to decide how to display
  # information about this workunit.
  #
  # Note that a workunit can have multiple labels where this makes sense, e.g., TOOL, COMPILER and NAILGUN.
  SETUP = 0      # Parsing build files etc.
  PHASE = 1      # Executing a phase.
  GOAL = 2       # Executing a goal.
  GROUP = 3      # Executing a group.

  BOOTSTRAP = 4  # Invocation of code to fetch a tool.
  TOOL = 5       # Single invocations of a tool.
  MULTITOOL = 6  # Multiple consecutive invocations of the same tool.
  COMPILER = 7   # Invocation of a compiler.

  TEST = 8       # Running a test.
  JVM = 9        # Running a tool via the JVM.
  NAILGUN = 10   # Running a tool via nailgun.
  RUN = 11       # Running a binary.
  REPL = 12      # Running a repl.

  def __init__(self, run_tracker, parent, name, labels=None, cmd=''):
    """
    - run_tracker: The RunTracker that tracks this WorkUnit.
    - parent: The containing workunit, if any. E.g., 'compile' might contain 'java', 'scala' etc.,
              'scala' might contain 'compile', 'split' etc.
    - name: A short name for this work. E.g., 'resolve', 'compile', 'scala', 'zinc'.
    - labels: An optional iterable of labels. The reporters can use this to decide how to
              display information about this work.
    - cmd: An optional longer string representing this work.
           E.g., the cmd line of a compiler invocation.
    """
    self._outcome = WorkUnit.UNKNOWN

    self.run_tracker = run_tracker
    self.parent = parent
    self.children = []

    self.name = name
    self.labels = set(labels or ())
    self.cmd = cmd
    self.id = uuid.uuid4()

    # In seconds since the epoch. Doubles, to account for fractional seconds.
    self.start_time = 0
    self.end_time = 0

    # A workunit may have multiple outputs, which we identify by a name.
    # E.g., a tool invocation may have 'stdout', 'stderr', 'debug_log' etc.
    self._outputs = {}  # name -> output buffer.

    # Do this last, as the parent's _self_time() might get called before we're
    # done initializing ourselves.
    # TODO: Ensure that a parent can't be ended before all its children are.
    if self.parent:
      self.parent.children.append(self)

  def has_label(self, label):
    return label in self.labels

  def start(self):
    """Mark the time at which this workunit started."""
    self.start_time = time.time()

  def end(self):
    """Mark the time at which this workunit ended."""
    self.end_time = time.time()
    for output in self._outputs.values():
      output.close()
    is_tool = self.has_label(WorkUnit.TOOL)
    path = self.path()
    self.run_tracker.cumulative_timings.add_timing(path, self.duration(), is_tool)
    self.run_tracker.self_timings.add_timing(path, self._self_time(), is_tool)

  def outcome(self):
    """Returns the outcome of this workunit."""
    return self._outcome

  def set_outcome(self, outcome):
    """Set the outcome of this work unit.

    We can set the outcome on a work unit directly, but that outcome will also be affected by
    those of its subunits. The right thing happens: The outcome of a work unit is the
    worst outcome of any of its subunits and any outcome set on it directly."""
    if outcome < self._outcome:
      self._outcome = outcome
      self.choose(0, 0, 0, 0, 0)  # Dummy call, to validate outcome.
      if self.parent: self.parent.set_outcome(self._outcome)

  _valid_name_re = re.compile(r'\w+')

  def output(self, name):
    """Returns the output buffer for the specified output name (e.g., 'stdout')."""
    m = WorkUnit._valid_name_re.match(name)
    if not m or m.group(0) != name:
      raise Exception('Invalid output name: %s' % name)
    if name not in self._outputs:
      path = os.path.join(self.run_tracker.info_dir, 'tool_outputs', '%s.%s' % (self.id, name))
      safe_mkdir_for(path)
      self._outputs[name] = FileBackedRWBuf(path)
    return self._outputs[name]

  def outputs(self):
    """Returns the map of output name -> output buffer."""
    return self._outputs

  def choose(self, aborted_val, failure_val, warning_val, success_val, unknown_val):
    """Returns one of the 5 arguments, depending on our outcome."""
    return WorkUnit.choose_for_outcome(self._outcome,
                            aborted_val, failure_val, warning_val, success_val, unknown_val)

  def duration(self):
    """Returns the time (in fractional seconds) spent in this workunit and its children."""
    return (self.end_time or time.time()) - self.start_time

  def start_time_string(self):
    """A convenient string representation of start_time."""
    return time.strftime('%H:%M:%S', time.localtime(self.start_time))

  def start_delta_string(self):
    """A convenient string representation of how long after the run started we started."""
    delta = int(self.start_time) - int(self.root().start_time)
    return '%02d:%02d' % (delta / 60, delta % 60)

  def root(self):
    ret = self
    while ret.parent is not None:
      ret = ret.parent
    return ret

  def ancestors(self):
    """Returns a list consisting of this workunit and those enclosing it, up to the root."""
    ret = []
    workunit = self
    while workunit is not None:
      ret.append(workunit)
      workunit = workunit.parent
    return ret

  def path(self):
    """Returns a path string for this workunit, E.g., 'all:compile:jvm:scalac'."""
    return ':'.join(reversed([w.name for w in self.ancestors()]))

  def unaccounted_time(self):
    """Returns non-leaf time spent in this workunit.

    This assumes that all major work should be done in leaves.
    TODO: Is this assumption valid?
    """
    return 0 if len(self.children) == 0 else self._self_time()

  def to_dict(self):
    """Useful for providing arguments to templates."""
    ret = {}
    for key in ['name', 'cmd', 'id', 'start_time', 'end_time',
                'outcome', 'start_time_string', 'start_delta_string']:
      val = getattr(self, key)
      ret[key] = val() if hasattr(val, '__call__') else val
    ret['parent'] = self.parent.to_dict() if self.parent else None
    return ret

  def _self_time(self):
    """Returns the time spent in this workunit outside of any children."""
    return self.duration() - sum([child.duration() for child in self.children])


########NEW FILE########
__FILENAME__ = pants_exe
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import optparse
import os
import sys
import traceback

from twitter.common.dirutil import Lock

from twitter.pants.base.build_environment import get_buildroot, get_version
from twitter.pants.base.address import Address
from twitter.pants.base.config import Config
from twitter.pants.base.rcfile import RcFile
from twitter.pants.commands.command import Command
from twitter.pants.commands.register import register_commands
from twitter.pants.goal.initialize_reporting import initial_reporting
from twitter.pants.goal.run_tracker import RunTracker
from twitter.pants.reporting.report import Report
from twitter.pants.tasks.nailgun_task import NailgunTask


_HELP_ALIASES = set([
  '-h',
  '--help',
  'help',
])

_BUILD_COMMAND = 'build'
_LOG_EXIT_OPTION = '--log-exit'
_VERSION_OPTION = '--version'

def _do_exit(result=0, msg=None):
  if msg:
    print(msg, file=sys.stderr)
  if _LOG_EXIT_OPTION in sys.argv and result == 0:
    print("\nSUCCESS\n")
  sys.exit(result)


def _exit_and_fail(msg=None):
  _do_exit(result=1, msg=msg)


def _find_all_commands():
  for cmd in Command.all_commands():
    cls = Command.get_command(cmd)
    yield '%s\t%s' % (cmd, cls.__doc__)


def _help(version, root_dir):
  print('Pants %s @ PANTS_BUILD_ROOT: %s' % (version, root_dir))
  print()
  print('Available subcommands:\n\t%s' % '\n\t'.join(_find_all_commands()))
  print()
  print("""Default subcommand flags can be stored in ~/.pantsrc using the 'options' key of a
section named for the subcommand in ini style format, ie:
  [build]
  options: --log-exit""")
  _exit_and_fail()


def _add_default_options(command, args):
  expanded_options = RcFile(paths=['/etc/pantsrc', '~/.pants.rc']).apply_defaults([command], args)
  if expanded_options != args:
    print("(using ~/.pantsrc expansion: pants %s %s)" % (command, ' '.join(expanded_options)),
          file=sys.stderr)
  return expanded_options


def _synthesize_command(root_dir, args):
  register_commands()
  command = args[0]

  if command in Command.all_commands():
    subcommand_args = args[1:] if len(args) > 1 else []
    return command, _add_default_options(command, subcommand_args)

  if command.startswith('-'):
    _exit_and_fail('Invalid command: %s' % command)

  # assume 'build' if a command was omitted.
  try:
    Address.parse(root_dir, command)
    return _BUILD_COMMAND, _add_default_options(_BUILD_COMMAND, args)
  except:
    _exit_and_fail('Failed to execute pants build: %s' % traceback.format_exc())


def _parse_command(root_dir, args):
  command, args = _synthesize_command(root_dir, args)
  return Command.get_command(command), args


try:
  import psutil

  def _process_info(pid):
    process = psutil.Process(pid)
    return '%d (%s)' % (pid, ' '.join(process.cmdline))
except ImportError:
  def _process_info(pid):
    return '%d' % pid


def _run():
  """
  To add additional paths to sys.path, add a block to the config similar to the following:
  [main]
  roots: ['src/python/twitter/pants_internal/test/',]
  """
  version = get_version()
  if len(sys.argv) == 2 and sys.argv[1] == _VERSION_OPTION:
    _do_exit(version)

  root_dir = get_buildroot()
  if not os.path.exists(root_dir):
    _exit_and_fail('PANTS_BUILD_ROOT does not point to a valid path: %s' % root_dir)

  if len(sys.argv) < 2 or (len(sys.argv) == 2 and sys.argv[1] in _HELP_ALIASES):
    _help(version, root_dir)

  command_class, command_args = _parse_command(root_dir, sys.argv[1:])

  parser = optparse.OptionParser(version=version)
  RcFile.install_disable_rc_option(parser)
  parser.add_option(_LOG_EXIT_OPTION,
                    action='store_true',
                    default=False,
                    dest='log_exit',
                    help = 'Log an exit message on success or failure.')

  config = Config.load()

  # TODO: This can be replaced once extensions are enabled with
  # https://github.com/pantsbuild/pants/issues/5
  roots = config.getlist('parse', 'roots', default=[])
  sys.path.extend(map(lambda root: os.path.join(root_dir, root), roots))

  # XXX(wickman) This should be in the command goal, not un pants_exe.py!
  run_tracker = RunTracker.from_config(config)
  report = initial_reporting(config, run_tracker)
  run_tracker.start(report)

  url = run_tracker.run_info.get_info('report_url')
  if url:
    run_tracker.log(Report.INFO, 'See a report at: %s' % url)
  else:
    run_tracker.log(Report.INFO, '(To run a reporting server: ./pants server)')

  command = command_class(run_tracker, root_dir, parser, command_args)
  try:
    if command.serialized():
      def onwait(pid):
        print('Waiting on pants process %s to complete' % _process_info(pid), file=sys.stderr)
        return True
      runfile = os.path.join(root_dir, '.pants.run')
      lock = Lock.acquire(runfile, onwait=onwait)
    else:
      lock = Lock.unlocked()
    try:
      result = command.run(lock)
      _do_exit(result)
    except KeyboardInterrupt:
      command.cleanup()
      raise
    finally:
      lock.release()
  finally:
    run_tracker.end()
    # Must kill nailguns only after run_tracker.end() is called, because there may still
    # be pending background work that needs a nailgun.
    if (hasattr(command.options, 'cleanup_nailguns') and command.options.cleanup_nailguns) \
        or config.get('nailgun', 'autokill', default=False):
      NailgunTask.killall(None)

def main():
  try:
    _run()
  except KeyboardInterrupt:
    _exit_and_fail('Interrupted by user.')


if __name__ == '__main__':
  main()

########NEW FILE########
__FILENAME__ = binary_util
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import posixpath
import subprocess

from contextlib import closing, contextmanager

from twitter.common import log
from twitter.common.contextutil import temporary_file
from twitter.common.dirutil import chmod_plus_x, safe_delete, safe_open
from twitter.common.lang import Compatibility

if Compatibility.PY3:
  import urllib.request as urllib_request
  import urllib.error as urllib_error
else:
  import urllib2 as urllib_request
  import urllib2 as urllib_error

from .base.config import Config
from .tasks.task_error import TaskError


_ID_BY_OS = {
  'linux': lambda release, machine: ('linux', machine),
  'darwin': lambda release, machine: ('darwin', release.split('.')[0]),
}


_PATH_BY_ID = {
  ('linux', 'x86_64'):  ['linux', 'x86_64'],
  ('linux', 'amd64'):   ['linux', 'x86_64'],
  ('linux', 'i386'):    ['linux', 'i386'],
  ('darwin', '9'):      ['mac', '10.5'],
  ('darwin', '10'):     ['mac', '10.6'],
  ('darwin', '11'):     ['mac', '10.7'],
  ('darwin', '12'):     ['mac', '10.8'],
  ('darwin', '13'):     ['mac', '10.9'],
}


def select_binary(base_path, version, name, config=None):
  """Selects a binary matching the current os and architecture.

  Raises TaskError if no binary of the given version and name could be found.
  """
  # TODO(John Sirois): finish doc of the path structure expexcted under base_path
  config = config or Config.load()
  bootstrap_dir = config.getdefault('pants_bootstrapdir')
  baseurl = config.getdefault('pants_support_baseurl')
  timeout_secs = config.getdefault('pants_support_fetch_timeout_secs', type=int, default=30)

  sysname, _, release, _, machine = os.uname()
  os_id = _ID_BY_OS[sysname.lower()]
  if os_id:
    middle_path = _PATH_BY_ID[os_id(release, machine)]
    if middle_path:
      binary_path = os.path.join(base_path, *(middle_path + [version, name]))
      bootstrapped_binary_path = os.path.join(bootstrap_dir, binary_path)
      if not os.path.exists(bootstrapped_binary_path):
        url = posixpath.join(baseurl, binary_path)
        log.info('Fetching %s binary from: %s' % (name, url))
        downloadpath = bootstrapped_binary_path + '~'
        try:
          with closing(urllib_request.urlopen(url, timeout=timeout_secs)) as binary:
            with safe_open(downloadpath, 'wb') as bootstrapped_binary:
              bootstrapped_binary.write(binary.read())

          os.rename(downloadpath, bootstrapped_binary_path)
          chmod_plus_x(bootstrapped_binary_path)
        except (IOError, urllib_error.HTTPError, urllib_error.URLError) as e:
          raise TaskError('Failed to fetch binary from %s: %s' % (url, e))
        finally:
          safe_delete(downloadpath)
      log.debug('Selected %s binary bootstrapped to: %s' % (name, bootstrapped_binary_path))
      return bootstrapped_binary_path
  raise TaskError('No %s binary found for: %s' % (name, (sysname, release, machine)))


@contextmanager
def safe_args(args,
              max_args=None,
              config=None,
              argfile=None,
              delimiter='\n',
              quoter=None,
              delete=True):
  """
    Yields args if there are less than a limit otherwise writes args to an argfile and yields an
    argument list with one argument formed from the path of the argfile.

    :args The args to work with.
    :max_args The maximum number of args to let though without writing an argfile.  If not specified
              then the maximum will be loaded from config.
    :config Used to lookup the configured maximum number of args that can be passed to a subprocess;
            defaults to the default config and looks for key 'max_subprocess_args' in the DEFAULTS.
    :argfile The file to write args to when there are too many; defaults to a temporary file.
    :delimiter The delimiter to insert between args written to the argfile, defaults to '\n'
    :quoter A function that can take the argfile path and return a single argument value;
            defaults to:
            <code>lambda f: '@' + f<code>
    :delete If True deletes any arg files created upon exit from this context; defaults to True.
  """
  max_args = max_args or (config or Config.load()).getdefault('max_subprocess_args', int, 10)
  if len(args) > max_args:
    def create_argfile(fp):
      fp.write(delimiter.join(args))
      fp.close()
      return [quoter(fp.name) if quoter else '@%s' % fp.name]

    if argfile:
      try:
        with safe_open(argfile, 'w') as fp:
          yield create_argfile(fp)
      finally:
        if delete and os.path.exists(argfile):
          os.unlink(argfile)
    else:
      with temporary_file(cleanup=delete) as fp:
        yield create_argfile(fp)
  else:
    yield args


def _mac_open(files):
  subprocess.call(['open'] + list(files))


def _linux_open(files):
  cmd = "xdg-open"
  if not _cmd_exists(cmd):
    raise TaskError("The program '%s' isn't in your PATH. Please install and re-run this "
                    "goal." % cmd)
  for f in list(files):
    subprocess.call([cmd, f])


# From: http://stackoverflow.com/questions/377017/test-if-executable-exists-in-python
def _cmd_exists(cmd):
  return subprocess.call(["/usr/bin/which", cmd], shell=False, stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE) == 0

_OPENER_BY_OS = {
  'darwin': _mac_open,
  'linux': _linux_open
}


def ui_open(*files):
  """Attempts to open the given files using the preferred native viewer or editor."""
  if files:
    osname = os.uname()[0].lower()
    if not osname in _OPENER_BY_OS:
      print('Sorry, open currently not supported for ' + osname)
    else:
      _OPENER_BY_OS[osname](files)

########NEW FILE########
__FILENAME__ = artifact
import os
import shutil
import errno
import tarfile
from twitter.common.contextutil import open_tar
from twitter.common.dirutil import safe_mkdir_for, safe_mkdir


class ArtifactError(Exception):
  pass


class Artifact(object):
  """Represents a set of files in an artifact."""
  def __init__(self, artifact_root):
    # All files must be under this root.
    self._artifact_root = artifact_root

    # The files known to be in this artifact, relative to artifact_root.
    self._relpaths = set()

  def get_paths(self):
    for relpath in self._relpaths:
      yield os.path.join(self._artifact_root, relpath)

  def override_paths(self, paths):  # Use with care.
    self._relpaths = set([os.path.relpath(path, self._artifact_root) for path in paths])

  def collect(self, paths):
    """Collect the paths (which must be under artifact root) into this artifact."""
    raise NotImplementedError()

  def extract(self):
    """Extract the files in this artifact to their locations under artifact root."""
    raise NotImplementedError()


class DirectoryArtifact(Artifact):
  """An artifact stored as loose files under a directory."""
  def __init__(self, artifact_root, directory):
    Artifact.__init__(self, artifact_root)
    self._directory = directory

  def collect(self, paths):
    for path in paths or ():
      relpath = os.path.relpath(path, self._artifact_root)
      dst = os.path.join(self._directory, relpath)
      safe_mkdir(os.path.dirname(dst))
      if os.path.isdir(path):
        shutil.copytree(path, dst)
      else:
        shutil.copy(path, dst)
      self._relpaths.add(relpath)

  def extract(self):
    for dir_name, _, filenames in os.walk(self._directory):
      for filename in filenames:
        filename = os.path.join(dir_name, filename)
        relpath = os.path.relpath(filename, self._directory)
        dst = os.path.join(self._artifact_root, relpath)
        safe_mkdir_for(dst)
        shutil.copy(filename, dst)
        self._relpaths.add(relpath)


class TarballArtifact(Artifact):
  """An artifact stored in a tarball."""
  def __init__(self, artifact_root, tarfile, compress):
    Artifact.__init__(self, artifact_root)
    self._tarfile = tarfile
    self._compress = compress

  def collect(self, paths):
    # In our tests, gzip is slightly less compressive than bzip2 on .class files,
    # but decompression times are much faster.
    mode = 'w:gz' if self._compress else 'w'
    with open_tar(self._tarfile, mode, dereference=True, errorlevel=2) as tarout:
      for path in paths or ():
        # Adds dirs recursively.
        relpath = os.path.relpath(path, self._artifact_root)
        tarout.add(path, relpath)
        self._relpaths.add(relpath)

  def extract(self):
    try:
      with open_tar(self._tarfile, 'r', errorlevel=2) as tarin:
        # Note: We create all needed paths proactively, even though extractall() can do this for us.
        # This is because we may be called concurrently on multiple artifacts that share directories,
        # and there will be a race condition inside extractall(): task T1 A) sees that a directory
        # doesn't exist and B) tries to create it. But in the gap between A) and B) task T2 creates
        # the same directory, so T1 throws "File exists" in B).
        # This actually happened, and was very hard to debug.
        # Creating the paths here up front allows us to squelch that "File exists" error.
        paths = []
        dirs = set()
        for tarinfo in tarin.getmembers():
          paths.append(tarinfo.name)
          if tarinfo.isdir():
            dirs.add(tarinfo.name)
          else:
            dirs.add(os.path.dirname(tarinfo.name))
        for d in dirs:
          try:
            os.makedirs(os.path.join(self._artifact_root, d))
          except OSError as e:
            if e.errno != errno.EEXIST:
              raise
        tarin.extractall(self._artifact_root)
        self._relpaths.update(paths)
    except tarfile.ReadError as e:
      raise ArtifactError(e.message)

########NEW FILE########
__FILENAME__ = artifact_cache
import os

# Note throughout the distinction between the artifact_root (which is where the artifacts are
# originally built and where the cache restores them to) and the cache root path/URL (which is
# where the artifacts are cached).


class ArtifactCache(object):
  """A map from cache key to a set of build artifacts.

  The cache key must uniquely identify the inputs (sources, compiler flags etc.) needed to
  build the artifacts. Cache keys are typically obtained from a CacheKeyGenerator.

  Subclasses implement the methods below to provide this functionality.
  """

  class CacheError(Exception):
    """Indicates a problem writing to or reading from the cache."""
    pass

  def __init__(self, log, artifact_root):
    """Create an ArtifactCache.

    All artifacts must be under artifact_root.
    """
    self.log = log
    self.artifact_root = artifact_root

  def insert(self, cache_key, paths):
    """Cache the output of a build.

    If there is an existing set of artifacts for this key they are deleted.

    TODO: Check that they're equal? They might not have to be if there are multiple equivalent
          outputs.

    cache_key: A CacheKey object.
    paths: List of absolute paths to generated dirs/files. These must be under the artifact_root.
    """
    missing_files = filter(lambda f: not os.path.exists(f), paths)
    try:
      if missing_files:
        raise ArtifactCache.CacheError('Tried to cache nonexistent files: %s' % missing_files)
      self.try_insert(cache_key, paths)
    except Exception as e:
      self.log.error('Error while writing to artifact cache: %s. ' % e)

  def try_insert(self, cache_key, paths):
    """Attempt to cache the output of a build, without error-handling.

    cache_key: A CacheKey object.
    paths: List of absolute paths to generated dirs/files. These must be under the artifact_root.
    """
    pass

  def has(self, cache_key):
    pass

  def use_cached_files(self, cache_key):
    """Use the files cached for the given key.

    Returns an appropriate Artifact instance if files were found and used, None otherwise.
    Callers will typically only care about the truthiness of the return value. They usually
    don't need to tinker with the returned instance.

    cache_key: A CacheKey object.
    """
    pass

  def delete(self, cache_key):
    """Delete the artifacts for the specified key.

    Deleting non-existent artifacts is a no-op.
    """
    pass

  def prune(self, age_hours):
    """Clean up cache files older than age_hours, if possible."""
    pass



########NEW FILE########
__FILENAME__ = combined_artifact_cache
from twitter.pants.cache.artifact_cache import ArtifactCache


class CombinedArtifactCache(ArtifactCache):
  """An artifact cache that delegates to a list of other caches."""
  def __init__(self, artifact_caches, backfill=True):
    """We delegate to artifact_caches, a list of ArtifactCache instances, in order.

    If backfill is true then we populate earlier caches that were missing an artifact,
    if that artifact was found in a later cache. This is useful for priming a local cache
    from a remote one.
    """
    if not artifact_caches:
      raise ValueError('Must provide at least one underlying artifact cache')
    log = artifact_caches[0].log
    artifact_root = artifact_caches[0].artifact_root
    if any(x.artifact_root != artifact_root for x in artifact_caches):
      raise ValueError('Combined artifact caches must all have the same artifact root.')
    ArtifactCache.__init__(self, log, artifact_root)
    self._artifact_caches = artifact_caches
    self._backfill = backfill

  def insert(self, cache_key, paths):
    for cache in self._artifact_caches:  # Insert into all.
      cache.insert(cache_key, paths)

  def has(self, cache_key):
    return any(cache.has(cache_key) for cache in self._artifact_caches)

  def use_cached_files(self, cache_key):
    to_backfill = []
    for cache in self._artifact_caches:
      artifact = cache.use_cached_files(cache_key)
      if not artifact:
        if self._backfill:
          to_backfill.append(cache)
      else:
        paths = list(artifact.get_paths())
        for cache in to_backfill:
          cache.insert(cache_key, paths)
        return artifact
    return None

  def delete(self, cache_key):
    for cache in self._artifact_caches:  # Delete from all.
      cache.delete(cache_key)

  def prune(self, age_hours):
    for cache in self._artifact_caches:
      cache.prune(age_hours)

########NEW FILE########
__FILENAME__ = local_artifact_cache
import os
import shutil
import uuid

from twitter.common.dirutil import safe_mkdir, safe_mkdir_for, safe_delete
from twitter.pants.cache.artifact import TarballArtifact, ArtifactError
from twitter.pants.cache.artifact_cache import ArtifactCache


class LocalArtifactCache(ArtifactCache):
  """An artifact cache that stores the artifacts in local files."""
  def __init__(self, log, artifact_root, cache_root, compress=True, copy_fn=None):
    """
    cache_root: The locally cached files are stored under this directory.
    copy_fn: An optional function with the signature copy_fn(absolute_src_path, relative_dst_path) that
        will copy cached files into the desired destination. If unspecified, a simple file copy is used.
    """
    ArtifactCache.__init__(self, log, artifact_root)
    self._cache_root = os.path.expanduser(cache_root)
    self._compress = compress

    def copy(src, rel_dst):
      dst = os.path.join(self.artifact_root, rel_dst)
      safe_mkdir_for(dst)
      shutil.copy(src, dst)

    self._copy_fn = copy_fn or copy
    safe_mkdir(self._cache_root)

  def try_insert(self, cache_key, paths):
    tarfile = self._cache_file_for_key(cache_key)
    safe_mkdir_for(tarfile)
    # Write to a temporary name (on the same filesystem), and move it atomically, so if we
    # crash in the middle we don't leave an incomplete or missing artifact.
    tarfile_tmp = tarfile + '.' + str(uuid.uuid4()) + '.tmp'
    if os.path.exists(tarfile_tmp):
      os.unlink(tarfile_tmp)

    artifact = TarballArtifact(self.artifact_root, tarfile_tmp, self._compress)
    artifact.collect(paths)
    # Note: Race condition here if multiple pants runs (in different workspaces)
    # try to write the same thing at the same time. However since rename is atomic,
    # this should not result in corruption. It may however result in a missing artifact
    # If we crash between the unlink and the rename. But that's OK.
    if os.path.exists(tarfile):
      os.unlink(tarfile)
    os.rename(tarfile_tmp, tarfile)

  def has(self, cache_key):
    return os.path.isfile(self._cache_file_for_key(cache_key))

  def use_cached_files(self, cache_key):
    try:
      tarfile = self._cache_file_for_key(cache_key)
      if os.path.exists(tarfile):
        artifact = TarballArtifact(self.artifact_root, tarfile, self._compress)
        artifact.extract()
        return artifact
      else:
        return None
    except Exception as e:
      self.log.warn('Error while reading from local artifact cache: %s' % e)
      return None

  def delete(self, cache_key):
    safe_delete(self._cache_file_for_key(cache_key))

  def prune(self, age_hours):
    pass

  def _cache_file_for_key(self, cache_key):
    # Note: it's important to use the id as well as the hash, because two different targets
    # may have the same hash if both have no sources, but we may still want to differentiate them.
    return os.path.join(self._cache_root, cache_key.id, cache_key.hash) + \
           '.tar.gz' if self._compress else '.tar'

########NEW FILE########
__FILENAME__ = pinger
import httplib
from multiprocessing.pool import ThreadPool
import socket
from twitter.common.contextutil import Timer


_global_pinger_memo = {}  # netloc -> rt time in secs.

class Pinger(object):
  # Signifies that a netloc is unreachable.
  UNREACHABLE = 999999

  def __init__(self, timeout, tries):
    """Try pinging the given number of times, each with the given timeout."""
    self._timeout = timeout
    self._tries = tries

  def ping(self, netloc):
    """Time a single roundtrip to the netloc.

    Note that we don't use actual ICMP pings, because cmd-line ping is
    inflexible and platform-dependent, so shelling out to it is annoying,
    and the ICMP python lib can only be called by the superuser.
    """
    if netloc in _global_pinger_memo:
      return _global_pinger_memo[netloc]

    host, colon, portstr = netloc.partition(':')
    port = int(portstr) if portstr else None
    rt_secs = Pinger.UNREACHABLE
    for _ in xrange(self._tries):
      try:
        with Timer() as timer:
          conn = httplib.HTTPConnection(host, port, timeout=self._timeout)
          conn.request('HEAD', '/')   # Doesn't actually matter if this exists.
          conn.getresponse()
        new_rt_secs = timer.elapsed
      except Exception:
        new_rt_secs = Pinger.UNREACHABLE
      rt_secs = min(rt_secs, new_rt_secs)
    _global_pinger_memo[netloc] = rt_secs
    return rt_secs

  def pings(self, netlocs):
    pool = ThreadPool(processes=len(netlocs))
    rt_secs = pool.map(self.ping, netlocs, chunksize=1)
    pool.close()
    pool.join()
    return zip(netlocs, rt_secs)

########NEW FILE########
__FILENAME__ = read_write_artifact_cache
from twitter.pants.cache.artifact_cache import ArtifactCache


class ReadWriteArtifactCache(ArtifactCache):
  """An artifact cache that delegates to one cache for reading and another for writing.

  The name is slightly misleading: all caches are read-write. But I couldn't think
  of a better one.
  """
  def __init__(self, read_artifact_cache, write_artifact_cache):
    """Either cache can be None, in which case we don't read from/write to it."""
    artifact_roots = []
    logs = []
    def get_root_and_log(cache):
      if cache is not None:
        artifact_roots.append(cache.artifact_root)
        logs.append(cache.log)
    get_root_and_log(read_artifact_cache)
    get_root_and_log(write_artifact_cache)
    if len(artifact_roots) == 0:
      # Parent will never be accessed, so this is OK. In fact, it's a good way to ensure it.
      artifact_root = None
      log = None
    else:
      artifact_root = artifact_roots[0]
      log = logs[0]
      if len(artifact_roots) > 1 and artifact_roots[1] != artifact_root:
        raise ValueError('Read and write artifact caches must have the same artifact root.')
    ArtifactCache.__init__(self, log, artifact_root)
    self._read_artifact_cache = read_artifact_cache
    self._write_artifact_cache = write_artifact_cache

  def insert(self, cache_key, paths):
    if self._write_artifact_cache:
      self._write_artifact_cache.insert(cache_key, paths)

  def has(self, cache_key):
    if self._read_artifact_cache:
      return self._read_artifact_cache.has(cache_key)
    else:
      return False

  def use_cached_files(self, cache_key):
    if self._read_artifact_cache:
      return self._read_artifact_cache.use_cached_files(cache_key)
    else:
      return None

  def delete(self, cache_key):
    if self._write_artifact_cache:
      self._write_artifact_cache.delete(cache_key)

  def prune(self, age_hours):
    if self._write_artifact_cache:
      self._write_artifact_cache.prune(age_hours)


########NEW FILE########
__FILENAME__ = restful_artifact_cache
import httplib
import urlparse
from twitter.common.contextutil import temporary_file_path, temporary_file
from twitter.common.quantity import Amount, Data
from twitter.pants.cache.artifact import TarballArtifact
from twitter.pants.cache.artifact_cache import ArtifactCache


class RESTfulArtifactCache(ArtifactCache):
  """An artifact cache that stores the artifacts on a RESTful service."""

  READ_SIZE = int(Amount(4, Data.MB).as_(Data.BYTES))

  def __init__(self, log, artifact_root, url_base, compress=True):
    """
    url_base: The prefix for urls on some RESTful service. We must be able to PUT and GET to any
              path under this base.
    compress: Whether to compress the artifacts before storing them.
    """
    ArtifactCache.__init__(self, log, artifact_root)
    parsed_url = urlparse.urlparse(url_base)
    if parsed_url.scheme == 'http':
      self._ssl = False
    elif parsed_url.scheme == 'https':
      self._ssl = True
    else:
      raise ValueError('RESTfulArtifactCache only supports HTTP and HTTPS')
    self._timeout_secs = 4.0
    self._netloc = parsed_url.netloc
    self._path_prefix = parsed_url.path.rstrip('/')
    self.compress = compress

  def try_insert(self, cache_key, paths):
    with temporary_file_path() as tarfile:
      artifact = TarballArtifact(self.artifact_root, tarfile, self.compress)
      artifact.collect(paths)

      with open(tarfile, 'rb') as infile:
        remote_path = self._remote_path_for_key(cache_key)
        if not self._request('PUT', remote_path, body=infile):
          raise self.CacheError('Failed to PUT to %s. Error: 404' % self._url_string(remote_path))

  def has(self, cache_key):
    return self._request('HEAD', self._remote_path_for_key(cache_key)) is not None

  def use_cached_files(self, cache_key):
    # This implementation fetches the appropriate tarball and extracts it.
    remote_path = self._remote_path_for_key(cache_key)
    try:
      # Send an HTTP request for the tarball.
      response = self._request('GET', remote_path)
      if response is None:
        return None

      done = False
      with temporary_file() as outfile:
        total_bytes = 0
        # Read the data in a loop.
        while not done:
          data = response.read(self.READ_SIZE)
          outfile.write(data)
          if len(data) < self.READ_SIZE:
            done = True
          total_bytes += len(data)
        outfile.close()
        self.log.debug('Read %d bytes from artifact cache at %s' %
                       (total_bytes,self._url_string(remote_path)))

        # Extract the tarfile.
        artifact = TarballArtifact(self.artifact_root, outfile.name, self.compress)
        artifact.extract()
        return artifact
    except Exception as e:
      self.log.warn('Error while reading from remote artifact cache: %s' % e)
      return None

  def delete(self, cache_key):
    remote_path = self._remote_path_for_key(cache_key)
    self._request('DELETE', remote_path)

  def prune(self, age_hours):
    # Doesn't make sense for a client to prune a remote server.
    # Better to run tmpwatch on the server.
    pass

  def _remote_path_for_key(self, cache_key):
    # Note: it's important to use the id as well as the hash, because two different targets
    # may have the same hash if both have no sources, but we may still want to differentiate them.
    return '%s/%s/%s%s' % (self._path_prefix, cache_key.id, cache_key.hash,
                               '.tar.gz' if self.compress else '.tar')

  def _connect(self):
    if self._ssl:
      return httplib.HTTPSConnection(self._netloc, timeout=self._timeout_secs)
    else:
      return httplib.HTTPConnection(self._netloc, timeout=self._timeout_secs)

  # Returns a response if we get a 200, None if we get a 404 and raises an exception otherwise.
  def _request(self, method, path, body=None):
    self.log.debug('Sending %s request to %s' % (method, self._url_string(path)))
    # TODO(benjy): Keep connection open and reuse?
    conn = self._connect()
    conn.request(method, path, body=body)
    response = conn.getresponse()
    # Allow all 2XX responses. E.g., nginx returns 201 on PUT. HEAD may return 204.
    if int(response.status / 100) == 2:
      return response
    elif response.status == 404:
      return None
    else:
      raise self.CacheError('Failed to %s %s. Error: %d %s' % (method, self._url_string(path),
                                                               response.status, response.reason))

  def _url_string(self, path):
    return '%s://%s%s' % (('https' if self._ssl else 'http'), self._netloc, path)

########NEW FILE########
__FILENAME__ = build
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import sys
import traceback

from twitter.common.collections import OrderedSet

from twitter.pants.base.address import Address
from twitter.pants.base.config import Config
from twitter.pants.base.target import Target
from twitter.pants.commands.command import Command
from twitter.pants.python.interpreter_cache import PythonInterpreterCache
from twitter.pants.python.python_builder import PythonBuilder


class Build(Command):
  """Builds a specified target."""

  __command__ = 'build'

  def setup_parser(self, parser, args):
    parser.set_usage("\n"
                     "  %prog build (options) [spec] (build args)\n"
                     "  %prog build (options) [spec]... -- (build args)")
    parser.add_option("-t", "--timeout", dest="conn_timeout", type="int",
                      default=Config.load().getdefault('connection_timeout'),
                      help="Number of seconds to wait for http connections.")
    parser.add_option('-i', '--interpreter', dest='interpreter', default=None,
                      help='The interpreter requirement for this chroot.')
    parser.add_option('-v', '--verbose', dest='verbose', default=False, action='store_true',
                      help='Show verbose output.')
    parser.disable_interspersed_args()
    parser.epilog = ('Builds the specified Python target(s). Use ./pants goal for JVM and other '
                     'targets.')

  def __init__(self, run_tracker, root_dir, parser, argv):
    Command.__init__(self, run_tracker, root_dir, parser, argv)

    if not self.args:
      self.error("A spec argument is required")

    self.config = Config.load()
    self.interpreter_cache = PythonInterpreterCache(self.config, logger=self.debug)
    self.interpreter_cache.setup()
    interpreters = self.interpreter_cache.select_interpreter(
        list(self.interpreter_cache.matches([self.options.interpreter]
            if self.options.interpreter else [''])))
    if len(interpreters) != 1:
      self.error('Unable to detect suitable interpreter.')
    else:
      self.debug('Selected %s' % interpreters[0])
    self.interpreter = interpreters[0]

    try:
      specs_end = self.args.index('--')
      if len(self.args) > specs_end:
        self.build_args = self.args[specs_end+1:len(self.args)+1]
      else:
        self.build_args = []
    except ValueError:
      specs_end = 1
      self.build_args = self.args[1:] if len(self.args) > 1 else []

    self.targets = OrderedSet()
    for spec in self.args[0:specs_end]:
      try:
        address = Address.parse(root_dir, spec)
      except:
        self.error("Problem parsing spec %s: %s" % (spec, traceback.format_exc()))

      try:
        target = Target.get(address)
      except:
        self.error("Problem parsing BUILD target %s: %s" % (address, traceback.format_exc()))

      if not target:
        self.error("Target %s does not exist" % address)
      self.targets.update(tgt for tgt in target.resolve() if tgt.is_concrete)

  def debug(self, message):
    if self.options.verbose:
      print(message, file=sys.stderr)

  def execute(self):
    print("Build operating on targets: %s" % self.targets)

    python_targets = OrderedSet()
    for target in self.targets:
      if target.is_python:
        python_targets.add(target)
      else:
        self.error("Cannot build target %s" % target)

    if python_targets:
      status = self._python_build(python_targets)
    else:
      status = -1

    return status

  def _python_build(self, targets):
    try:
      executor = PythonBuilder(self.run_tracker, self.root_dir)
      return executor.build(
        targets,
        self.build_args,
        interpreter=self.interpreter,
        conn_timeout=self.options.conn_timeout)
    except:
      self.error("Problem executing PythonBuilder for targets %s: %s" % (targets,
                                                                         traceback.format_exc()))

########NEW FILE########
__FILENAME__ = command
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

from twitter.common.collections import OrderedSet
from twitter.pants.base.build_file import BuildFile
from twitter.pants.base.target import Target


class Command(object):
  """Baseclass for all pants subcommands."""

  @staticmethod
  def get_command(name):
    return Command._commands.get(name, None)

  @staticmethod
  def all_commands():
    return Command._commands.keys()

  _commands = {}

  @classmethod
  def _register(cls):
    """Register a command class."""
    command_name = cls.__dict__.get('__command__', None)
    if command_name:
      Command._commands[command_name] = cls

  @staticmethod
  def scan_addresses(root_dir, base_path=None):
    """Parses all targets available in BUILD files under base_path and
    returns their addresses.  If no base_path is specified, root_dir is
    assumed to be the base_path"""

    addresses = OrderedSet()
    for buildfile in BuildFile.scan_buildfiles(root_dir, base_path):
      addresses.update(Target.get_all_addresses(buildfile))
    return addresses

  @classmethod
  def serialized(cls):
    return False

  def __init__(self, run_tracker, root_dir, parser, args):
    """run_tracker: The (already opened) RunTracker to track this run with
    root_dir: The root directory of the pants workspace
    parser: an OptionParser
    args: the subcommand arguments to parse"""
    self.run_tracker = run_tracker
    self.root_dir = root_dir

    # Override the OptionParser's error with more useful output
    def error(message=None, show_help=True):
      if message:
        print(message + '\n')
      if show_help:
        parser.print_help()
      parser.exit(status=1)
    parser.error = error
    self.error = error

    self.setup_parser(parser, args)
    self.options, self.args = parser.parse_args(args)
    self.parser = parser

  def setup_parser(self, parser, args):
    """Subclasses should override and confiure the OptionParser to reflect
    the subcommand option and argument requirements.  Upon successful
    construction, subcommands will be able to access self.options and
    self.args."""

    pass

  def error(self, message=None, show_help=True):
    """Reports the error message, optionally followed by pants help, and then exits."""

  def run(self, lock):
    """Subcommands that are serialized() should override if they need the ability to interact with
    the global command lock.
    The value returned should be an int, 0 indicating success and any other value indicating
    failure."""
    return self.execute()

  def execute(self):
    """Subcommands that do not require serialization should override to perform the command action.
    The value returned should be an int, 0 indicating success and any other value indicating
    failure."""
    raise NotImplementedError('Either run(lock) or execute() must be over-ridden.')

  def cleanup(self):
    """Called on SIGINT (e.g., when the user hits ctrl-c).
    Subcommands may override to perform cleanup before exit."""
    pass

########NEW FILE########
__FILENAME__ = goal
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import inspect
import multiprocessing
import os
import re
import sys
import signal
import socket
import time
import traceback

from contextlib import contextmanager
from optparse import Option, OptionParser

from twitter.common import log
from twitter.common.collections import OrderedSet
from twitter.common.dirutil import safe_rmtree, safe_mkdir
from twitter.common.lang import Compatibility
from twitter.common.log.options import LogOptions

from twitter.pants import binary_util
from twitter.pants.base.address import Address
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.build_file import BuildFile
from twitter.pants.base.config import Config
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.rcfile import RcFile
from twitter.pants.base.run_info import RunInfo
from twitter.pants.base.target import Target, TargetDefinitionException
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.commands.command import Command
from twitter.pants.engine.engine import Engine
from twitter.pants.engine.group_engine import GroupEngine
from twitter.pants.goal import Context, GoalError, Phase
from twitter.pants.goal import Goal as goal, Group as group
from twitter.pants.goal.initialize_reporting import update_reporting
from twitter.pants.reporting.reporting_server import ReportingServer, ReportingServerManager
from twitter.pants.tasks import Task, TaskError
from twitter.pants.tasks.console_task import ConsoleTask
from twitter.pants.tasks.list_goals import ListGoals
from twitter.pants.tasks.targets_help import TargetsHelp

try:
  import colors
except ImportError:
  turn_off_colored_logging = True
else:
  turn_off_colored_logging = False

StringIO = Compatibility.StringIO


def _list_goals(context, message):
  """Show all installed goals."""
  context.log.error(message)
  # Execute as if the user had run "./pants goals".
  return Goal.execute(context, 'goals')


goal(name='goals', action=ListGoals).install().with_description('List all documented goals.')


goal(name='targets', action=TargetsHelp).install().with_description('List all target types.')


class Help(Task):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    default = None
    if len(args) > 1 and (not args[1].startswith('-')):
      default = args[1]
      del args[1]
    option_group.add_option(mkflag("goal"), dest="help_goal", default=default)

  def execute(self, targets):
    goal = self.context.options.help_goal
    if goal is None:
      return self.list_goals('You must supply a goal name to provide help for.')
    phase = Phase(goal)
    if not phase.goals():
      return self.list_goals('Goal %s is unknown.' % goal)

    parser = OptionParser()
    parser.set_usage('%s goal %s ([target]...)' % (sys.argv[0], goal))
    parser.epilog = phase.description
    Goal.add_global_options(parser)
    Phase.setup_parser(parser, [], [phase])
    parser.parse_args(['--help'])

  def list_goals(self, message):
    return _list_goals(self.context, message)

goal(name='help', action=Help).install().with_description('Provide help for the specified goal.')


def _set_bool(option, opt_str, value, parser):
  setattr(parser.values, option.dest, not opt_str.startswith("--no"))


class SpecParser(object):
  """Parses goal target specs; either simple target addresses or else sibling (:) or descendant
  (::) selector forms
  """

  def __init__(self, root_dir):
    self._root_dir = root_dir

  def _get_dir(self, spec):
    path = spec.split(':', 1)[0]
    if os.path.isdir(path):
      return path
    else:
      if os.path.isfile(path):
        return os.path.dirname(path)
      else:
        return spec

  def _parse_addresses(self, spec):
    if spec.endswith('::'):
      dir = self._get_dir(spec[:-len('::')])
      for buildfile in BuildFile.scan_buildfiles(self._root_dir, os.path.join(self._root_dir, dir)):
        for address in Target.get_all_addresses(buildfile):
          yield address
    elif spec.endswith(':'):
      dir = self._get_dir(spec[:-len(':')])
      for address in Target.get_all_addresses(BuildFile(self._root_dir, dir)):
        yield address
    else:
      yield Address.parse(self._root_dir, spec)

  def parse(self, spec):
    """Parses the given target spec into one or more targets.

    Returns a generator of target, address pairs in which the target may be None if the address
    points to a non-existent target.
    """
    for address in self._parse_addresses(spec):
      target = Target.get(address)
      yield target, address


class Goal(Command):
  """Lists installed goals or else executes a named goal."""

  __command__ = 'goal'

  GLOBAL_OPTIONS = [
    Option("-t", "--timeout", dest="conn_timeout", type='int',
           default=Config.load().getdefault('connection_timeout'),
           help="Number of seconds to wait for http connections."),
    Option("-x", "--time", action="store_true", dest="time", default=False,
           help="Times goal phases and outputs a report."),
    Option("-e", "--explain", action="store_true", dest="explain", default=False,
           help="Explain the execution of goals."),
    Option("-k", "--kill-nailguns", action="store_true", dest="cleanup_nailguns", default=False,
           help="Kill nailguns before exiting"),
    Option("-d", "--logdir", dest="logdir",
           help="[%default] Forks logs to files under this directory."),
    Option("-l", "--level", dest="log_level", type="choice", choices=['debug', 'info', 'warn'],
           help="[info] Sets the logging level to one of 'debug', 'info' or 'warn'."
                "if set."),
    Option("-q", "--quiet", action="store_true", dest="quiet", default=False,
           help="Squelches all console output apart from errors."),
    Option("--no-colors", dest="no_color", action="store_true", default=turn_off_colored_logging,
           help="Do not colorize log messages."),
    Option("-n", "--dry-run", action="store_true", dest="dry_run", default=False,
      help="Print the commands that would be run, without actually running them."),

    Option("--read-from-artifact-cache", "--no-read-from-artifact-cache", action="callback",
      callback=_set_bool, dest="read_from_artifact_cache", default=True,
      help="Whether to read artifacts from cache instead of building them, if configured to do so."),
    Option("--write-to-artifact-cache", "--no-write-to-artifact-cache", action="callback",
      callback=_set_bool, dest="write_to_artifact_cache", default=True,
      help="Whether to write artifacts to cache if configured to do so."),

    # NONE OF THE ARTIFACT CACHE FLAGS BELOW DO ANYTHING ANY MORE.
    # TODO: Remove them once all uses of them are killed.
    Option("--verify-artifact-cache", "--no-verify-artifact-cache", action="callback",
      callback=_set_bool, dest="verify_artifact_cache", default=False,
      help="Whether to verify that cached artifacts are identical after rebuilding them."),

    Option("--local-artifact-cache-readonly", "--no-local-artifact-cache-readonly", action="callback",
           callback=_set_bool, dest="local_artifact_cache_readonly", default=False,
           help="If set, we don't write to local artifact caches, even when writes are enabled."),
    # Note that remote writes are disabled by default, so you have control over who's populating
    # the shared cache.
    Option("--remote-artifact-cache-readonly", "--no-remote-artifact-cache-readonly", action="callback",
           callback=_set_bool, dest="remote_artifact_cache_readonly", default=True,
           help="If set, we don't write to remote artifact caches, even when writes are enabled."),

    Option("--all", dest="target_directory", action="append",
           help="DEPRECATED: Use [dir]: with no flag in a normal target position on the command "
                "line. (Adds all targets found in the given directory's BUILD file. Can be "
                "specified more than once.)"),
    Option("--all-recursive", dest="recursive_directory", action="append",
           help="DEPRECATED: Use [dir]:: with no flag in a normal target position on the command "
                "line. (Adds all targets found recursively under the given directory. Can be "
                "specified more than once to add more than one root target directory to scan.)"),
  ]

  output = None

  @staticmethod
  def add_global_options(parser):
    for option in Goal.GLOBAL_OPTIONS:
      parser.add_option(option)

  @staticmethod
  def parse_args(args):
    goals = OrderedSet()
    specs = OrderedSet()
    help = False
    explicit_multi = False

    def is_spec(spec):
      return os.sep in spec or ':' in spec

    for i, arg in enumerate(args):
      help = help or 'help' == arg
      if not arg.startswith('-'):
        specs.add(arg) if is_spec(arg) else goals.add(arg)
      elif '--' == arg:
        if specs:
          raise GoalError('Cannot intermix targets with goals when using --. Targets should '
                          'appear on the right')
        explicit_multi = True
        del args[i]
        break

    if explicit_multi:
      spec_offset = len(goals) + 1 if help else len(goals)
      specs.update(arg for arg in args[spec_offset:] if not arg.startswith('-'))

    return goals, specs

  @classmethod
  def execute(cls, context, *names):
    parser = OptionParser()
    cls.add_global_options(parser)
    phases = [Phase(name) for name in names]
    Phase.setup_parser(parser, [], phases)
    options, _ = parser.parse_args([])
    context = Context(context.config, options, context.run_tracker, context.target_roots,
                      requested_goals=list(names))
    return cls._execute(context, phases, print_timing=False)

  @staticmethod
  def _execute(context, phases, print_timing):
    engine = GroupEngine(print_timing=print_timing)
    return engine.execute(context, phases)

  # TODO(John Sirois): revisit wholesale locking when we move py support into pants new
  @classmethod
  def serialized(cls):
    # Goal serialization is now handled in goal execution during group processing.
    # The goal command doesn't need to hold the serialization lock; individual goals will
    # acquire the lock if they need to be serialized.
    return False

  def __init__(self, run_tracker, root_dir, parser, args):
    self.targets = []
    Command.__init__(self, run_tracker, root_dir, parser, args)

  @contextmanager
  def check_errors(self, banner):
    errors = {}
    def error(key, include_traceback=False):
      exc_type, exc_value, _ = sys.exc_info()
      msg = StringIO()
      if include_traceback:
        frame = inspect.trace()[-2]
        filename = frame[1]
        lineno = frame[2]
        funcname = frame[3]
        code = ''.join(frame[4]) if frame[4] else None
        traceback.print_list([(filename, lineno, funcname, code)], file=msg)
      if exc_type:
        msg.write(''.join(traceback.format_exception_only(exc_type, exc_value)))
      errors[key] = msg.getvalue()
      sys.exc_clear()

    yield error

    if errors:
      msg = StringIO()
      msg.write(banner)
      invalid_keys = [key for key, exc in errors.items() if not exc]
      if invalid_keys:
        msg.write('\n  %s' % '\n  '.join(invalid_keys))
      for key, exc in errors.items():
        if exc:
          msg.write('\n  %s =>\n    %s' % (key, '\n      '.join(exc.splitlines())))
      # The help message for goal is extremely verbose, and will obscure the
      # actual error message, so we don't show it in this case.
      self.error(msg.getvalue(), show_help=False)

  def setup_parser(self, parser, args):
    self.config = Config.load()
    Goal.add_global_options(parser)

    # We support attempting zero or more goals.  Multiple goals must be delimited from further
    # options and non goal args with a '--'.  The key permutations we need to support:
    # ./pants goal => goals
    # ./pants goal goals => goals
    # ./pants goal compile src/java/... => compile
    # ./pants goal compile -x src/java/... => compile
    # ./pants goal compile src/java/... -x => compile
    # ./pants goal compile run -- src/java/... => compile, run
    # ./pants goal compile run -- src/java/... -x => compile, run
    # ./pants goal compile run -- -x src/java/... => compile, run

    if not args:
      args.append('goals')

    if len(args) == 1 and args[0] in set(['-h', '--help', 'help']):
      def format_usage(usages):
        left_colwidth = 0
        for left, right in usages:
          left_colwidth = max(left_colwidth, len(left))
        lines = []
        for left, right in usages:
          lines.append('  %s%s%s' % (left, ' ' * (left_colwidth - len(left) + 1), right))
        return '\n'.join(lines)

      usages = [
        ("%prog goal goals ([spec]...)", Phase('goals').description),
        ("%prog goal help [goal] ([spec]...)", Phase('help').description),
        ("%prog goal [goal] [spec]...", "Attempt goal against one or more targets."),
        ("%prog goal [goal] ([goal]...) -- [spec]...", "Attempts all the specified goals."),
      ]
      parser.set_usage("\n%s" % format_usage(usages))
      parser.epilog = ("Either lists all installed goals, provides extra help for a goal or else "
                       "attempts to achieve the specified goal for the listed targets." """
                       Note that target specs accept two special forms:
                         [dir]:  to include all targets in the specified directory
                         [dir]:: to include all targets found in all BUILD files recursively under
                                 the directory""")

      parser.print_help()
      sys.exit(0)
    else:
      goals, specs = Goal.parse_args(args)
      self.requested_goals = goals

      with self.run_tracker.new_workunit(name='setup', labels=[WorkUnit.SETUP]):
        # Bootstrap goals by loading any configured bootstrap BUILD files
        with self.check_errors('The following bootstrap_buildfiles cannot be loaded:') as error:
          with self.run_tracker.new_workunit(name='bootstrap', labels=[WorkUnit.SETUP]):
            for path in self.config.getlist('goals', 'bootstrap_buildfiles', default = []):
              try:
                buildfile = BuildFile(get_buildroot(), os.path.relpath(path, get_buildroot()))
                ParseContext(buildfile).parse()
              except (TypeError, ImportError, TaskError, GoalError):
                error(path, include_traceback=True)
              except (IOError, SyntaxError):
                error(path)
        # Now that we've parsed the bootstrap BUILD files, and know about the SCM system.
        self.run_tracker.run_info.add_scm_info()

        # Bootstrap user goals by loading any BUILD files implied by targets.
        spec_parser = SpecParser(self.root_dir)
        with self.check_errors('The following targets could not be loaded:') as error:
          with self.run_tracker.new_workunit(name='parse', labels=[WorkUnit.SETUP]):
            for spec in specs:
              try:
                for target, address in spec_parser.parse(spec):
                  if target:
                    self.targets.append(target)
                    # Force early BUILD file loading if this target is an alias that expands
                    # to others.
                    unused = list(target.resolve())
                  else:
                    siblings = Target.get_all_addresses(address.buildfile)
                    prompt = 'did you mean' if len(siblings) == 1 else 'maybe you meant one of these'
                    error('%s => %s?:\n    %s' % (address, prompt,
                                                  '\n    '.join(str(a) for a in siblings)))
              except (TypeError, ImportError, TaskError, GoalError):
                error(spec, include_traceback=True)
              except (IOError, SyntaxError, TargetDefinitionException):
                error(spec)

      self.phases = [Phase(goal) for goal in goals]

      rcfiles = self.config.getdefault('rcfiles', type=list,
                                       default=['/etc/pantsrc', '~/.pants.rc'])
      if rcfiles:
        rcfile = RcFile(rcfiles, default_prepend=False, process_default=True)

        # Break down the goals specified on the command line to the full set that will be run so we
        # can apply default flags to inner goal nodes.  Also break down goals by Task subclass and
        # register the task class hierarchy fully qualified names so we can apply defaults to
        # baseclasses.

        sections = OrderedSet()
        for phase in Engine.execution_order(self.phases):
          for goal in phase.goals():
            sections.add(goal.name)
            for clazz in goal.task_type.mro():
              if clazz == Task:
                break
              sections.add('%s.%s' % (clazz.__module__, clazz.__name__))

        augmented_args = rcfile.apply_defaults(sections, args)
        if augmented_args != args:
          del args[:]
          args.extend(augmented_args)
          sys.stderr.write("(using pantsrc expansion: pants goal %s)\n" % ' '.join(augmented_args))

      Phase.setup_parser(parser, args, self.phases)

  def run(self, lock):
    # TODO(John Sirois): Consider moving to straight python logging.  The divide between the
    # context/work-unit logging and standard python logging doesn't buy us anything.

    # Enable standard python logging for code with no handle to a context/work-unit.
    if self.options.log_level:
      LogOptions.set_stderr_log_level((self.options.log_level or 'info').upper())
      logdir = self.options.logdir or self.config.get('goals', 'logdir', default=None)
      if logdir:
        safe_mkdir(logdir)
        LogOptions.set_log_dir(logdir)
        log.init('goals')
      else:
        log.init()

    # Update the reporting settings, now that we have flags etc.
    def is_console_task():
      for phase in self.phases:
        for goal in phase.goals():
          if issubclass(goal.task_type, ConsoleTask):
            return True
      return False

    is_explain = self.options.explain
    update_reporting(self.options, is_console_task() or is_explain, self.run_tracker)

    if self.options.dry_run:
      print('****** Dry Run ******')

    context = Context(
      self.config,
      self.options,
      self.run_tracker,
      self.targets,
      requested_goals=self.requested_goals,
      lock=lock)

    if self.options.recursive_directory:
      context.log.warn(
        '--all-recursive is deprecated, use a target spec with the form [dir]:: instead')
      for dir in self.options.recursive_directory:
        self.add_target_recursive(dir)

    if self.options.target_directory:
      context.log.warn('--all is deprecated, use a target spec with the form [dir]: instead')
      for dir in self.options.target_directory:
        self.add_target_directory(dir)

    unknown = []
    for phase in self.phases:
      if not phase.goals():
        unknown.append(phase)

    if unknown:
      _list_goals(context, 'Unknown goal(s): %s' % ' '.join(phase.name for phase in unknown))
      return 1

    return Goal._execute(context, self.phases, print_timing=self.options.time)

  def cleanup(self):
    # TODO: Make this more selective? Only kill nailguns that affect state? E.g., checkstyle
    # may not need to be killed.
    NailgunTask.killall(log.info)
    sys.exit(1)


# Install all default pants provided goals
from twitter.pants.targets.benchmark import Benchmark
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.java_tests import JavaTests as junit_tests
from twitter.pants.targets.jvm_binary import JvmBinary
from twitter.pants.targets.scala_library import ScalaLibrary
from twitter.pants.targets.scala_tests import ScalaTests
from twitter.pants.targets.scalac_plugin import ScalacPlugin
from twitter.pants.tasks.antlr_gen import AntlrGen
from twitter.pants.tasks.benchmark_run import BenchmarkRun
from twitter.pants.tasks.binary_create import BinaryCreate
from twitter.pants.tasks.bootstrap_jvm_tools import BootstrapJvmTools
from twitter.pants.tasks.build_lint import BuildLint
from twitter.pants.tasks.builddictionary import BuildBuildDictionary
from twitter.pants.tasks.bundle_create import BundleCreate
from twitter.pants.tasks.check_exclusives import CheckExclusives
from twitter.pants.tasks.check_published_deps import CheckPublishedDeps
from twitter.pants.tasks.checkstyle import Checkstyle
from twitter.pants.tasks.detect_duplicates import DuplicateDetector
from twitter.pants.tasks.filedeps import FileDeps
from twitter.pants.tasks.ivy_resolve import IvyResolve
from twitter.pants.tasks.jar_create import JarCreate
from twitter.pants.tasks.javadoc_gen import JavadocGen
from twitter.pants.tasks.junit_run import JUnitRun
from twitter.pants.tasks.jvm_compile.java.java_compile import JavaCompile
from twitter.pants.tasks.jvm_compile.scala.scala_compile import ScalaCompile
from twitter.pants.tasks.jvm_run import JvmRun
from twitter.pants.tasks.listtargets import ListTargets
from twitter.pants.tasks.markdown_to_html import MarkdownToHtml
from twitter.pants.tasks.nailgun_task import NailgunTask
from twitter.pants.tasks.pathdeps import PathDeps
from twitter.pants.tasks.prepare_resources import PrepareResources
from twitter.pants.tasks.protobuf_gen import ProtobufGen
from twitter.pants.tasks.jar_publish import JarPublish
from twitter.pants.tasks.scala_repl import ScalaRepl
from twitter.pants.tasks.scaladoc_gen import ScaladocGen
from twitter.pants.tasks.scrooge_gen import ScroogeGen
from twitter.pants.tasks.specs_run import SpecsRun
from twitter.pants.tasks.thrift_gen import ThriftGen


def _cautious_rmtree(root):
  real_buildroot = os.path.realpath(os.path.abspath(get_buildroot()))
  real_root = os.path.realpath(os.path.abspath(root))
  if not real_root.startswith(real_buildroot):
    raise TaskError('DANGER: Attempting to delete %s, which is not under the build root!')
  safe_rmtree(real_root)

try:
  import daemon
  def _async_cautious_rmtree(root):
    if os.path.exists(root):
      new_path = root + '.deletable.%f' % time.time()
      os.rename(root, new_path)
      with daemon.DaemonContext():
        _cautious_rmtree(new_path)
except ImportError:
  pass

class Invalidator(ConsoleTask):
  def execute(self, targets):
    build_invalidator_dir = self.context.config.get('tasks', 'build_invalidator')
    _cautious_rmtree(build_invalidator_dir)
goal(
  name='invalidate',
  action=Invalidator,
  dependencies=['ng-killall']
).install().with_description('Invalidate all targets')


class Cleaner(ConsoleTask):
  def execute(self, targets):
    _cautious_rmtree(self.context.config.getdefault('pants_workdir'))
goal(
  name='clean-all',
  action=Cleaner,
  dependencies=['invalidate']
).install().with_description('Cleans all build output')


class AsyncCleaner(ConsoleTask):
  def execute(self, targets):
    _async_cautious_rmtree(self.context.config.getdefault('pants_workdir'))
goal(
  name='clean-all-async',
  action=AsyncCleaner,
  dependencies=['invalidate']
).install().with_description('Cleans all build output in a background process')


class NailgunKillall(ConsoleTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(NailgunKillall, cls).setup_parser(option_group, args, mkflag)
    option_group.add_option(mkflag("everywhere"), dest="ng_killall_everywhere",
                            default=False, action="store_true",
                            help="[%default] Kill all nailguns servers launched by pants for "
                                 "all workspaces on the system.")

  def execute(self, targets):
    NailgunTask.killall(everywhere=self.context.options.ng_killall_everywhere)

goal(
  name='ng-killall',
  action=NailgunKillall
).install().with_description('Kill any running nailgun servers spawned by pants.')


class RunServer(ConsoleTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(RunServer, cls).setup_parser(option_group, args, mkflag)
    option_group.add_option(mkflag("port"), dest="port", action="store", type="int", default=0,
      help="Serve on this port. Leave unset to choose a free port automatically (recommended if "
           "using pants concurrently in multiple workspaces on the same host).")
    option_group.add_option(mkflag("allowed-clients"), dest="allowed_clients",
      default=["127.0.0.1"], action="append",
      help="Only requests from these IPs may access this server. Useful for temporarily showing " \
           "build results to a colleague. The special value ALL means any client may connect. " \
           "Use with caution, as your source code is exposed to all allowed clients!")

  def console_output(self, targets):
    DONE = '__done_reporting'

    port = ReportingServerManager.get_current_server_port()
    if port:
      return ['Server already running at http://localhost:%d' % port]

    def run_server(reporting_queue):
      def report_launch(actual_port):
        reporting_queue.put(
          'Launching server with pid %d at http://localhost:%d' % (os.getpid(), actual_port))

      def done_reporting():
        reporting_queue.put(DONE)

      try:
        # We mustn't block in the child, because the multiprocessing module enforces that the
        # parent either kills or joins to it. Instead we fork a grandchild that inherits the queue
        # but is allowed to block indefinitely on the server loop.
        if not os.fork():
          # Child process.
          info_dir = RunInfo.dir(self.context.config)
          # If these are specified explicitly in the config, use those. Otherwise
          # they will be None, and we'll use the ones baked into this package.
          template_dir = self.context.config.get('reporting', 'reports_template_dir')
          assets_dir = self.context.config.get('reporting', 'reports_assets_dir')
          settings = ReportingServer.Settings(info_dir=info_dir, template_dir=template_dir,
                                              assets_dir=assets_dir, root=get_buildroot(),
                                              allowed_clients=self.context.options.allowed_clients)
          server = ReportingServer(self.context.options.port, settings)
          actual_port = server.server_port()
          ReportingServerManager.save_current_server_port(actual_port)
          report_launch(actual_port)
          done_reporting()
          # Block forever here.
          server.start()
      except socket.error:
        done_reporting()
        raise

    # We do reporting on behalf of the child process (necessary, since reporting may be buffered in a
    # background thread). We use multiprocessing.Process() to spawn the child so we can use that
    # module's inter-process Queue implementation.
    reporting_queue = multiprocessing.Queue()
    proc = multiprocessing.Process(target=run_server, args=[reporting_queue])
    proc.daemon = True
    proc.start()
    s = reporting_queue.get()
    ret = []
    while s != DONE:
      ret.append(s)
      s = reporting_queue.get()
    # The child process is done reporting, and is now in the server loop, so we can proceed.
    server_port = ReportingServerManager.get_current_server_port()
    if server_port:
      binary_util.ui_open('http://localhost:%d/run/latest' % server_port)
    return ret

goal(
  name='server',
  action=RunServer,
  serialize=False,
).install().with_description('Run the pants reporting server.')

class KillServer(ConsoleTask):
  pidfile_re = re.compile(r'port_(\d+)\.pid')
  def console_output(self, targets):
    pidfiles_and_ports = ReportingServerManager.get_current_server_pidfiles_and_ports()
    if not pidfiles_and_ports:
      return ['No server found.']
    # There should only be one pidfile, but in case there are many, we kill them all here.
    for pidfile, port in pidfiles_and_ports:
      with open(pidfile, 'r') as infile:
        pidstr = infile.read()
      try:
        os.unlink(pidfile)
        pid = int(pidstr)
        os.kill(pid, signal.SIGKILL)
        return ['Killed server with pid %d at http://localhost:%d' % (pid, port)]
      except (ValueError, OSError):
        return []

goal(
  name='killserver',
  action=KillServer,
  serialize=False,
).install().with_description('Kill the pants reporting server.')


# TODO(pl): Make the dependency of every other phase on this phase less explicit
goal(
  name='bootstrap-jvm-tools',
  action=BootstrapJvmTools,
).install('bootstrap').with_description('Bootstrap tools needed for building')

# TODO(John Sirois): Resolve eggs
goal(
  name='ivy',
  action=IvyResolve,
  dependencies=['gen', 'check-exclusives', 'bootstrap']
).install('resolve').with_description('Resolves jar dependencies and produces dependency reports.')

goal(name='check-exclusives',
  dependencies=['gen'],
  action=CheckExclusives).install('check-exclusives').with_description(
  'Check exclusives declarations to verify that dependencies are consistent.')

# TODO(John Sirois): gen attempted as the sole Goal should gen for all known gen types but
# recognize flags to narrow the gen set
goal(name='thrift', action=ThriftGen).install('gen').with_description('Generate code.')
goal(name='scrooge',
     dependencies=['bootstrap'],
     action=ScroogeGen).install('gen')
goal(name='protoc', action=ProtobufGen).install('gen')
goal(name='antlr',
     dependencies=['bootstrap'],
     action=AntlrGen).install('gen')

goal(
  name='checkstyle',
  action=Checkstyle,
  dependencies=['gen', 'resolve']
).install().with_description('Run checkstyle against java source code.')

# When chunking a group, we don't need a new chunk for targets with no sources at all
# (which do sometimes exist, e.g., when creating a BUILD file ahead of its code).
def _has_sources(target, extension):
  return target.has_sources(extension) or target.has_label('sources') and not target.sources

# Note: codegen targets shouldn't really be 'is_java' or 'is_scala', but right now they
# are so they don't cause a lot of islands while chunking. The jvm group doesn't act on them
# anyway (it acts on their synthetic counterparts) so it doesn't matter where they get chunked.
# TODO: Make chunking only take into account the targets actually acted on? This would require
# task types to declare formally the targets they act on.
def _is_java(target):
  return (target.is_java or
          (isinstance(target, (JvmBinary, junit_tests, Benchmark))
           and _has_sources(target, '.java'))) and not target.is_apt

def _is_scala(target):
  return (target.is_scala or
          (isinstance(target, (JvmBinary, junit_tests, Benchmark))
           and _has_sources(target, '.scala')))


goal(name='scala',
     action=ScalaCompile,
     group=group('jvm', _is_scala),
     dependencies=['gen', 'resolve', 'check-exclusives', 'bootstrap']).install('compile').with_description(
       'Compile both generated and checked in code.'
     )

class AptCompile(JavaCompile): pass  # So they're distinct in log messages etc.

goal(name='apt',
     action=AptCompile,
     group=group('jvm', lambda t: t.is_apt),
     dependencies=['gen', 'resolve', 'check-exclusives', 'bootstrap']).install('compile')

goal(name='java',
     action=JavaCompile,
     group=group('jvm', _is_java),
     dependencies=['gen', 'resolve', 'check-exclusives', 'bootstrap']).install('compile')


goal(name='prepare', action=PrepareResources).install('resources')


# TODO(John Sirois): pydoc also
goal(name='javadoc',
     action=JavadocGen,
     dependencies=['compile', 'bootstrap']).install('doc').with_description('Create documentation.')
goal(name='scaladoc',
     action=ScaladocGen,
     dependencies=['compile', 'bootstrap']).install('doc')


if MarkdownToHtml.AVAILABLE:
  goal(name='markdown',
       action=MarkdownToHtml
  ).install('markdown').with_description('Generate html from markdown docs.')


class ScaladocJarShim(ScaladocGen):
  def __init__(self, context, output_dir=None, confs=None):
    super(ScaladocJarShim, self).__init__(context,
                                          output_dir=output_dir,
                                          confs=confs,
                                          active=False)


class JavadocJarShim(JavadocGen):
  def __init__(self, context, output_dir=None, confs=None):
    super(JavadocJarShim, self).__init__(context,
                                         output_dir=output_dir,
                                         confs=confs,
                                         active=False)


goal(name='javadoc_publish',
     action=JavadocJarShim).install('publish')
goal(name='scaladoc_publish',
     action=ScaladocJarShim).install('publish')
goal(name='jar',
     action=JarCreate,
     dependencies=['compile', 'resources', 'bootstrap']).install('jar').with_description('Create one or more jars.')
goal(name='check_published_deps',
     action=CheckPublishedDeps
).install('check_published_deps').with_description(
  'Find references to outdated artifacts published from this BUILD tree.')

goal(name='jar_create_publish',
     action=JarCreate,
     dependencies=['compile', 'resources']).install('publish')

goal(name='publish',
     action=JarPublish).install('publish').with_description('Publish one or more artifacts.')

goal(name='junit',
     action=JUnitRun,
     dependencies=['compile', 'resources', 'bootstrap']).install('test').with_description('Test compiled code.')

goal(name='specs',
     action=SpecsRun,
     dependencies=['compile', 'resources', 'bootstrap']).install('test')

goal(name='bench',
     action=BenchmarkRun,
     dependencies=['compile', 'resources', 'bootstrap']).install('bench')

# TODO(John Sirois): Create pex's in binary phase
goal(
  name='binary',
  action=BinaryCreate,
  dependencies=['jar', 'bootstrap']
).install().with_description('Create a jvm binary jar.')
goal(
  name='dup',
  action=DuplicateDetector,
).install('binary')
goal(
  name='bundle',
  action=BundleCreate,
  dependencies=['binary', 'bootstrap']
).install().with_description('Create an application bundle from binary targets.')

# run doesn't need the serialization lock. It's reasonable to run some code
# in a workspace while there's a compile going on unrelated code.
goal(
  name='detect-duplicates',
  action=DuplicateDetector,
  dependencies=['jar']
).install().with_description('Detect duplicate classes and resources on the classpath.')

goal(
  name='jvm-run',
  action=JvmRun,
  dependencies=['compile', 'resources', 'bootstrap'],
  serialize=False,
).install('run').with_description('Run a (currently JVM only) binary target.')

goal(
  name='jvm-run-dirty',
  action=JvmRun,
  serialize=False,
).install('run-dirty').with_description('Run a (currently JVM only) binary target, using ' +
  'only currently existing binaries, skipping compilation')

# repl doesn't need the serialization lock. It's reasonable to have
# a repl running in a workspace while there's a compile going on unrelated code.
goal(
  name='scala-repl',
  action=ScalaRepl,
  dependencies=['compile', 'resources', 'bootstrap'],
  serialize=False,
).install('repl').with_description(
  'Run a (currently Scala only) REPL with the classpath set according to the targets.')

goal(
  name='scala-repl-dirty',
  action=ScalaRepl,
  serialize=False,
).install('repl-dirty').with_description(
  'Run a (currently Scala only) REPL with the classpath set according to the targets, ' +
  'using the currently existing binaries, skipping compilation')

goal(
  name='filedeps',
  action=FileDeps
).install('filedeps').with_description('Print out a list of all files the target depends on')

goal(
  name='pathdeps',
  action=PathDeps
).install('pathdeps').with_description(
  'Print out a list of all paths containing build files the target depends on')

goal(
  name='list',
  action=ListTargets
).install('list').with_description('List available BUILD targets.')

goal(
  name='buildlint',
  action=BuildLint,
  dependencies=['compile'],  # To pick up missing deps.
).install()

goal(
  name='builddict',
  action=BuildBuildDictionary,
).install()

from twitter.pants.tasks.idea_gen import IdeaGen

goal(
  name='idea',
  action=IdeaGen,
  dependencies=['jar', 'bootstrap']
).install().with_description('Create an IntelliJ IDEA project from the given targets.')


from twitter.pants.tasks.eclipse_gen import EclipseGen

goal(
  name='eclipse',
  action=EclipseGen,
  dependencies=['jar', 'bootstrap']
).install().with_description('Create an Eclipse project from the given targets.')


from twitter.pants.tasks.provides import Provides

goal(
  name='provides',
  action=Provides,
  dependencies=['jar', 'bootstrap']
).install().with_description('Emit the list of symbols provided by the given targets.')


from twitter.pants.tasks.python.setup import SetupPythonEnvironment

goal(
  name='python-setup',
  action=SetupPythonEnvironment,
).install('setup').with_description(
"Setup the target's build environment.")

from twitter.pants.tasks.paths import Path, Paths

goal(
  name='path',
  action=Path,
).install().with_description('Find a dependency path from one target to another')

goal(
  name='paths',
  action=Paths,
).install().with_description('Find all dependency paths from one target to another')


from twitter.pants.tasks.dependees import ReverseDepmap

goal(
  name='dependees',
  action=ReverseDepmap
).install().with_description('Print a reverse dependency mapping for the given targets')


from twitter.pants.tasks.depmap import Depmap

goal(
  name='depmap',
  action=Depmap
).install().with_description('Generates either a textual dependency tree or a graphviz'
                             ' digraph dotfile for the dependency set of a target')


from twitter.pants.tasks.dependencies import Dependencies

goal(
  name='dependencies',
  action=Dependencies
).install().with_description('Extract textual infomation about the dependencies of a target')


from twitter.pants.tasks.filemap import Filemap

goal(
  name='filemap',
  action=Filemap
).install().with_description('Outputs a mapping from source file to'
                             ' the target that owns the source file')


from twitter.pants.tasks.minimal_cover import MinimalCover

goal(
  name='minimize',
  action=MinimalCover
).install().with_description('Print the minimal cover of the given targets.')


from twitter.pants.tasks.filter import Filter

goal(
  name='filter',
  action=Filter
).install().with_description('Filter the input targets based on various criteria.')


from twitter.pants.tasks.sorttargets import SortTargets

goal(
  name='sort',
  action=SortTargets
).install().with_description('Topologically sort the input targets.')


from twitter.pants.tasks.roots import ListRoots

goal(
  name='roots',
  action=ListRoots,
).install('roots').with_description("Prints the source roots and associated target types defined in the repo.")

########NEW FILE########
__FILENAME__ = help
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from copy import copy

from twitter.pants.commands.command import Command


class Help(Command):
  """Provides help for available commands or a single specified command."""

  __command__ = 'help'

  def setup_parser(self, parser, args):
    self.parser = copy(parser)

    parser.set_usage("%prog help ([command])")
    parser.epilog = """Lists available commands with no arguments; otherwise prints help for the
                    specifed command."""

  def __init__(self, run_tracker, root_dir, parser, argv):
    Command.__init__(self, run_tracker, root_dir, parser, argv)

    if len(self.args) > 1:
      self.error("The help command accepts at most 1 argument.")
    self.subcommand = self.args[0]

  def execute(self):
    subcommand_class = Command.get_command(self.subcommand)
    if not subcommand_class:
      self.error("'%s' is not a recognized subcommand." % self.subcommand)
    command = subcommand_class(self.run_tracker, self.root_dir, self.parser, ['--help'])
    return command.execute()

########NEW FILE########
__FILENAME__ = py
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import signal
import sys
import tempfile

from twitter.common.python.pex import PEX
from twitter.common.python.pex_builder import PEXBuilder

from twitter.pants.base.address import Address
from twitter.pants.base.config import Config
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target
from twitter.pants.commands.command import Command
from twitter.pants.python.interpreter_cache import PythonInterpreterCache
from twitter.pants.python.python_chroot import PythonChroot
from twitter.pants.targets.python_binary import PythonBinary
from twitter.pants.targets.python_requirement import PythonRequirement


class Py(Command):
  """Python chroot manipulation."""

  __command__ = 'py'

  def setup_parser(self, parser, args):
    parser.set_usage('\n'
                     '  %prog py (options) [spec] args\n')
    parser.disable_interspersed_args()
    parser.add_option('-t', '--timeout', dest='conn_timeout', type='int',
                      default=Config.load().getdefault('connection_timeout'),
                      help='Number of seconds to wait for http connections.')
    parser.add_option('--pex', dest='pex', default=False, action='store_true',
                      help='Dump a .pex of this chroot instead of attempting to execute it.')
    parser.add_option('--ipython', dest='ipython', default=False, action='store_true',
                      help='Run the target environment in an IPython interpreter.')
    parser.add_option('-r', '--req', dest='extra_requirements', default=[], action='append',
                      help='Additional Python requirements to add to this chroot.')
    parser.add_option('-i', '--interpreter', dest='interpreter', default=None,
                      help='The interpreter requirement for this chroot.')
    parser.add_option('-e', '--entry_point', dest='entry_point', default=None,
                      help='The entry point for the generated PEX.')
    parser.add_option('-v', '--verbose', dest='verbose', default=False, action='store_true',
                      help='Show verbose output.')
    parser.epilog = """Interact with the chroot of the specified target."""

  def __init__(self, run_tracker, root_dir, parser, argv):
    Command.__init__(self, run_tracker, root_dir, parser, argv)

    self.target = None
    self.extra_targets = []
    self.config = Config.load()
    self.interpreter_cache = PythonInterpreterCache(self.config, logger=self.debug)
    self.interpreter_cache.setup()
    interpreters = self.interpreter_cache.select_interpreter(
        list(self.interpreter_cache.matches([self.options.interpreter]
            if self.options.interpreter else [''])))
    if len(interpreters) != 1:
      self.error('Unable to detect suitable interpreter.')
    self.interpreter = interpreters[0]

    for req in self.options.extra_requirements:
      with ParseContext.temp():
        self.extra_targets.append(PythonRequirement(req, use_2to3=True))

    # We parse each arg in the context of the cli usage:
    #   ./pants command (options) [spec] (build args)
    #   ./pants command (options) [spec]... -- (build args)
    # Our command token and our options are parsed out so we see args of the form:
    #   [spec] (build args)
    #   [spec]... -- (build args)
    binaries = []
    for k in range(len(self.args)):
      arg = self.args.pop(0)
      if arg == '--':
        break

      def not_a_target(debug_msg):
        self.debug('Not a target, assuming option: %s.' % e)
        # We failed to parse the arg as a target or else it was in valid address format but did not
        # correspond to a real target.  Assume this is the 1st of the build args and terminate
        # processing args for target addresses.
        self.args.insert(0, arg)

      target = None
      try:
        address = Address.parse(root_dir, arg)
        target = Target.get(address)
        if target is None:
          not_a_target(debug_msg='Unrecognized target')
          break
      except Exception as e:
        not_a_target(debug_msg=e)
        break

      for resolved in filter(lambda t: t.is_concrete, target.resolve()):
        if isinstance(resolved, PythonBinary):
          binaries.append(resolved)
        else:
          self.extra_targets.append(resolved)

    if len(binaries) == 0:
      # treat as a chroot
      pass
    elif len(binaries) == 1:
      # We found a binary and are done, the rest of the args get passed to it
      self.target = binaries[0]
    else:
      self.error('Can only process 1 binary target, %s contains %d:\n\t%s' % (
        arg, len(binaries), '\n\t'.join(str(binary.address) for binary in binaries)
      ))

    if self.target is None:
      if not self.extra_targets:
        self.error('No valid target specified!')
      self.target = self.extra_targets.pop(0)

  def debug(self, message):
    if self.options.verbose:
      print(message, file=sys.stderr)

  def execute(self):
    if self.options.pex and self.options.ipython:
      self.error('Cannot specify both --pex and --ipython!')

    if self.options.entry_point and self.options.ipython:
      self.error('Cannot specify both --entry_point and --ipython!')

    if self.options.verbose:
      print('Build operating on target: %s %s' % (self.target,
        'Extra targets: %s' % ' '.join(map(str, self.extra_targets)) if self.extra_targets else ''))

    builder = PEXBuilder(tempfile.mkdtemp(), interpreter=self.interpreter,
        pex_info=self.target.pexinfo if isinstance(self.target, PythonBinary) else None)

    if self.options.entry_point:
      builder.set_entry_point(self.options.entry_point)

    if self.options.ipython:
      if not self.config.has_section('python-ipython'):
        self.error('No python-ipython sections defined in your pants.ini!')

      builder.info.entry_point = self.config.get('python-ipython', 'entry_point')
      if builder.info.entry_point is None:
        self.error('Must specify entry_point for IPython in the python-ipython section '
                   'of your pants.ini!')

      requirements = self.config.getlist('python-ipython', 'requirements', default=[])

      with ParseContext.temp():
        for requirement in requirements:
          self.extra_targets.append(PythonRequirement(requirement))

    executor = PythonChroot(
        self.target,
        self.root_dir,
        builder=builder,
        interpreter=self.interpreter,
        extra_targets=self.extra_targets,
        conn_timeout=self.options.conn_timeout)

    executor.dump()

    if self.options.pex:
      pex_name = os.path.join(self.root_dir, 'dist', '%s.pex' % self.target.name)
      builder.build(pex_name)
      print('Wrote %s' % pex_name)
      return 0
    else:
      builder.freeze()
      pex = PEX(builder.path(), interpreter=self.interpreter)
      po = pex.run(args=list(self.args), blocking=False)
      try:
        return po.wait()
      except KeyboardInterrupt:
        po.send_signal(signal.SIGINT)
        raise

########NEW FILE########
__FILENAME__ = register

from twitter.pants.commands.build import Build
from twitter.pants.commands.goal import Goal
from twitter.pants.commands.help import Help
from twitter.pants.commands.py import Py
from twitter.pants.commands.setup_py import SetupPy

def register_commands():
  for cmd in (Build, Goal, Help, Py, SetupPy):
    cmd._register()

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# pants documentation build configuration file, created by
# sphinx-quickstart on Wed Aug 28 20:58:14 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

try:
  import sphinx_rtd_theme
except ImportError:
  raise RuntimeError(''.join([
    'Failed importing sphinx_rtd_theme. You likely need to:\n\n',
    '    pip install sphinx_rtd_theme\n\n',
    'For more information, see https://github.com/snide/sphinx_rtd_theme\n\n',
  ]))


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('../../..'))

pants_egg_dir = os.path.abspath('../../../../../.pants.d/python/eggs')
if not os.path.exists(pants_egg_dir):
  raise RuntimeError(''.join([
    'Pants egg dependencies do not exist! Please build pants before ',
    'generating the documentation site. ',
    'Missing dir: %s' % pants_egg_dir,
  ]))
for egg in os.listdir(pants_egg_dir):
  sys.path.insert(0, os.path.join(pants_egg_dir, egg))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'pants'
copyright = u'2013, Twitter'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.1'
# The full version, including alpha/beta/rc tags.
release = '0.1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#html_theme = 'pydoctheme'
html_theme = 'sphinx_rtd_theme'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = 'pants-logo.png'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = 'pants-logo.ico'


# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'pantsdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'pants.tex', u'pants Documentation',
   u'Twitter', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'pants', u'pants Documentation',
     [u'Twitter'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'pants', u'pants Documentation',
   u'Twitter', 'pants', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False

autoclass_content = 'both'

########NEW FILE########
__FILENAME__ = gen
#!/usr/bin/env python2.7

import os
import os.path
import shutil

from string import Template


TEMPLATE = Template('\n'.join([
  ':mod:`$name` Module',
  '-----------------------------------------------',
  '',
  '.. automodule:: twitter.pants.$otype.$name',
  '   :members:',
  '', '',
]))

def gen_targets_reference(targets_rst, targets_dir):
  lines = [
    'Targets Reference',
    '=================',
    '',
    'This page documents targets available as part of the pants build system.',
    '', '',
  ]

  for filename in sorted([filename for filename in os.listdir(targets_dir) if filename.endswith('.py')]):
    if filename == '__init__.py':
      continue # Skip because renaming targets causes duplicates.
    root, _ = os.path.splitext(filename)
    lines.append(TEMPLATE.substitute(otype='targets', name=root))

  with open(targets_rst, 'w') as fh:
    fh.write('\n'.join(lines))

def gen_base_reference(rst_filename, dirname):
  lines = [
    'Base Reference',
    '==============',
    '',
    'This page documents base classes of the pants build system.',
    '', '',
  ]

  for filename in sorted([filename for filename in os.listdir(dirname) if filename.endswith('.py')]):
    if filename == '__init__.py':
      continue # Skip because renaming targets causes duplicates.
    root, _ = os.path.splitext(filename)
    lines.append(TEMPLATE.substitute(otype='base', name=root))

  with open(rst_filename, 'w') as fh:
    fh.write('\n'.join(lines))

def copy_builddict(docs_dir):
  for filename in ['build_dictionary.rst', 'goals_reference.rst']:
    filepath = os.path.abspath(os.path.join(docs_dir,
        '../../../../../dist/builddict', filename))
    try:
      shutil.copy(filepath, docs_dir)
    except IOError as e:
      raise IOError("Forgot to `./pants goal builddict` first? \n\n%s" % e)

def main():
  docs_dir = os.path.dirname(os.path.abspath(__file__))
  pants_src_dir = os.path.dirname(docs_dir)
  tasks_dir = os.path.join(pants_src_dir, 'tasks')

  copy_builddict(docs_dir)

  with open(os.path.join(docs_dir, 'tasks.rst'), 'w') as tasks_rst:
    tasks_rst.write('\n'.join([
      'Tasks Reference',
      '===============',
      '',
      'This page documents tasks available as part of the pants build system.',
      '', '',
    ]))
    for filename in sorted([filename for filename in os.listdir(tasks_dir) if filename.endswith('.py')]):
      root, _ = os.path.splitext(filename)
      tasks_rst.write(TEMPLATE.substitute(otype='tasks', name=root))

  targets_rst = os.path.join(docs_dir, 'targets.rst')
  gen_targets_reference(targets_rst, os.path.join(pants_src_dir, 'targets'))

  gen_base_reference(os.path.join(docs_dir, 'base.rst'), os.path.join(pants_src_dir, 'base'))

if __name__ == '__main__':
  main()

########NEW FILE########
__FILENAME__ = pagerank
from __future__ import division

from collections import defaultdict

from twitter.pants import is_concrete  # XXX This target doesn't exist
from twitter.pants.tasks.console_task import ConsoleTask


class PageRank(ConsoleTask):
  """Measure how "depended-upon" the targets are."""

  def console_output(self, targets):
    dependencies_by_target = defaultdict(set)
    dependees_by_target = defaultdict(set)
    pagerank_by_target = defaultdict(lambda: 1.0)

    self._calc_deps(targets, dependencies_by_target, dependees_by_target)
    self._pagerank(dependees_by_target, dependencies_by_target, pagerank_by_target)
    return self._report(pagerank_by_target)

  def _calc_deps(self, targets, dependencies_by_target, dependees_by_target):
    for target in filter(lambda x: hasattr(x, "dependencies"), targets):
      if not dependencies_by_target.has_key(target):
        for dependency in target.dependencies:
          for resolved in dependency.resolve():
            if is_concrete(resolved):
              dependencies_by_target[target].add(resolved)

      for dependency in target.dependencies:
        for resolved in dependency.resolve():
          if is_concrete(resolved):
            dependees_by_target[resolved].add(target)

  def _pagerank(self, dependees_by_target, dependencies_by_target, pagerank_by_target):
    """Calculate PageRank."""
    d = 0.85
    for x in range(0, 100):
      for target, dependees in dependees_by_target.iteritems():
        contributions = map(
          lambda t: pagerank_by_target[t] / len(dependencies_by_target[t]), dependees)
        pagerank_by_target[target] = (1-d) + d * sum(contributions)

  def _report(self, pagerank_by_target):
    """Yield the report lines."""
    for target in sorted(pagerank_by_target, key=pagerank_by_target.get, reverse=True):
      yield '%f - %s' % (pagerank_by_target[target], target)

########NEW FILE########
__FILENAME__ = engine
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import time

from abc import abstractmethod
from contextlib import contextmanager

from twitter.common.collections.ordereddict import OrderedDict
from twitter.common.lang import AbstractClass

from twitter.pants.goal import GoalError, Phase
from twitter.pants.tasks.task_error import TaskError


class Timer(object):
  """Provides timing support for goal execution."""

  @classmethod
  @contextmanager
  def begin(cls, timer=None):
    """Begins a new ``Timer`` and yields it in a with context.

    The timer will be finished if not already by the block yielded to.
    """
    t = Timer(timer)
    try:
      yield t
    finally:
      t.finish()

  def __init__(self, timer=None):
    """Creates a timer that uses time.time for timing intervals by default.

    :param timer:  A callable that returns the current time in fractional seconds.
    """
    self._now = timer or time.time
    if not(callable(self._now)):
      # TODO(John Sirois): `def jake(bob): pass` is also callable - we want a no-args callable -
      # create a better check.
      raise ValueError('Timer must be a callable object.')

    self._timings = OrderedDict()
    self._elapsed = None
    self._start = self._now()

  def finish(self):
    """Finishes this timer if not already finished.

    Calls to ``timed`` after this will raise a ValueError since the timing window is complete.
    """
    if self._elapsed is None:
      self._elapsed = self._now() - self._start

  @property
  def timings(self):
    """Returns the phase timings as an ordered mapping from the ``Phase`` objects executed to
    ordered mappings of the ``Goal`` objects executed in the phase to the list of timings
    corresponding to each execution of the goal.

    Note that the list of timings will be singleton for all goals except those participating in a
    ``Group``.  Grouped goals will have or more timings in the list corresponding to each chunk of
    targets the goal executed against when iterating the group.
    """
    return self._timings

  @property
  def elapsed(self):
    """Returns the total elapsed time in fractional seconds from the creation of this timer until
    it was ``finished``.
    """
    if self._elapsed is None:
      raise ValueError('Timer has not been finished yet.')
    return self._elapsed

  @contextmanager
  def timed(self, goal):
    """Records the time taken to execute the yielded block an records this timing against the given
    goal's total runtime.
    """
    if self._elapsed is not None:
      raise ValueError('This timer is already finished.')

    start = self._now()
    try:
      yield
    finally:
      self._record(goal, self._now() - start)

  def _record(self, goal, elapsed):
    phase = Phase.of(goal)

    phase_timings = self._timings.get(phase)
    if phase_timings is None:
      phase_timings = OrderedDict(())
      self._timings[phase] = phase_timings

    goal_timings = phase_timings.get(goal)
    if goal_timings is None:
      goal_timings = []
      phase_timings[goal] = goal_timings

    goal_timings.append(elapsed)

  def render_timing_report(self):
    """Renders this timer's timings into the classic pants timing report format."""
    report = ('Timing report\n'
              '=============\n')
    for phase, timings in self.timings.items():
      phase_time = None
      for goal, times in timings.items():
        if len(times) > 1:
          report += '[%(phase)s:%(goal)s(%(numsteps)d)] %(timings)s -> %(total).3fs\n' % {
            'phase': phase.name,
            'goal': goal.name,
            'numsteps': len(times),
            'timings': ','.join('%.3fs' % t for t in times),
            'total': sum(times)
          }
        else:
          report += '[%(phase)s:%(goal)s] %(total).3fs\n' % {
            'phase': phase.name,
            'goal': goal.name,
            'total': sum(times)
          }
        if not phase_time:
          phase_time = 0
        phase_time += sum(times)
      if len(timings) > 1:
        report += '[%(phase)s] total: %(total).3fs\n' % {
          'phase': phase.name,
          'total': phase_time
        }
    report += 'total: %.3fs' % self.elapsed
    return report


class Engine(AbstractClass):
  """An engine for running a pants command line."""

  @staticmethod
  def execution_order(phases):
    """Yields all phases needed to attempt the given phases in proper phase execution order."""

    # Its key that we process phase dependencies depth first to maintain initial phase ordering as
    # passed in when phase graphs are dependency disjoint.  A breadth first sort could mix next
    # order executions and violate the implied intent of the passed in phase ordering.

    processed = set()

    def order(_phases):
      for phase in _phases:
        if phase not in processed:
          processed.add(phase)
          for goal in phase.goals():
            for dep in order(goal.dependencies):
              yield dep
          yield phase

    for ordered in order(phases):
      yield ordered

  def __init__(self, print_timing=False):
    """Creates an engine that prints no timings by default.

    :param print_timing: ``True`` to print detailed timings at the end of the run.
    """
    self._print_timing = print_timing

  def execute(self, context, phases):
    """Executes the supplied phases and their dependencies against the given context.

    :param context: The pants run context.
    :param list phases: A list of ``Phase`` objects representing the command line goals explicitly
                        requested.
    :returns int: An exit code of 0 upon success and non-zero otherwise.
    """
    with Timer.begin() as timer:
      try:
        self.attempt(timer, context, phases)
        return 0
      except (TaskError, GoalError) as e:
        message = '%s' % e
        if message:
          print('\nFAILURE: %s\n' % e)
        else:
          print('\nFAILURE\n')
        return e.exit_code if isinstance(e, TaskError) else 1
      finally:
        timer.finish()
        if self._print_timing:
          print(timer.render_timing_report())

  @abstractmethod
  def attempt(self, timer, context, phases):
    """Given the target context and phases specified (command line goals), attempt to achieve all
    goals.

    :param timer: A ``Timer`` that should be used to record goal timings.
    :param context: The pants run context.
    :param list phases: A list of ``Phase`` objects representing the command line goals explicitly
                        requested.
    """

########NEW FILE########
__FILENAME__ = group_engine
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict, namedtuple

from twitter.common.collections import  maybe_list, OrderedDict, OrderedSet

from twitter.pants.base.workunit import WorkUnit
from twitter.pants.goal import Goal
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.check_exclusives import ExclusivesMapping

from .engine import Engine


class GroupMember(namedtuple('GroupMember', ['group', 'name', 'predicate'])):
  """Represents a member of a goal group."""

  @classmethod
  def from_goal(cls, goal):
    """Creates a ``GroupMember`` from goal metadata."""
    if not isinstance(goal, Goal):
      raise ValueError('The given goal must be a ``Goal`` object, given %s' % goal)
    if not goal.group:
      raise ValueError('Can only form a GroupMember from goals with a group defined, goal %s '
                       'has no group' % goal.name)
    return cls(goal.group.name, goal.name, goal.group.predicate)


class GroupIterator(object):
  """Iterates the goals in a group over the chunks they own,"""

  def __init__(self, targets, group_members):
    """Creates an iterator that yields tuples of ``(GroupMember, [chunk Targets])``.

    Chunks will be returned least dependant to most dependant such that a group member processing a
    chunk can be assured that any dependencies of the chunk have been processed already.

    :param list targets: The universe of targets to divide up amongst group members.
    :param list group_members: A list of group members that forms the group to iterate.
    """
    # TODO(John Sirois): These validations should be happening sooner in the goal registration
    # process.
    assert len(map(lambda m: m.group, group_members)) != 1, 'Expected a single group'
    assert len(map(lambda m: m.name, group_members)) == len(group_members), (
      'Expected group members with unique names')

    self._targets = maybe_list(targets, expected_type=InternalTarget, raise_type=ValueError)
    self._group_members = group_members

  def __iter__(self):
    for chunk in self._create_chunks():
      for group_member in self._group_members:
        member_chunk = filter(group_member.predicate, chunk)
        if len(member_chunk) > 0:
          yield group_member, member_chunk

  def _create_chunks(self):
    def discriminator(tgt):
      for group_member in self._group_members:
        if group_member.predicate(tgt):
          return group_member.name
      return None

    # TODO(John Sirois): coalescing should be made available in another spot, InternalTarget is jvm
    # specific, and all we care is that the Targets have dependencies defined
    coalesced = InternalTarget.coalesce_targets(self._targets, discriminator)
    coalesced = list(reversed(coalesced))

    chunks = []
    flavor = None
    chunk_start = 0
    for chunk_num, target in enumerate(coalesced):
      target_flavor = discriminator(target)
      if target_flavor != flavor and chunk_num > chunk_start:
        chunks.append(OrderedSet(coalesced[chunk_start:chunk_num]))
        chunk_start = chunk_num
      flavor = target_flavor
    if chunk_start < len(coalesced):
      chunks.append(OrderedSet(coalesced[chunk_start:]))
    return chunks


class ExclusivesIterator(object):
  """Iterates over groups of compatible targets."""

  @classmethod
  def from_context(cls, context):
    exclusives = context.products.get_data('exclusives_groups')
    return cls(exclusives)

  def __init__(self, exclusives_mapping):
    """Creates an iterator that yields lists of compatible targets.``.

    Chunks will be returned in least exclusive to most exclusive order.

    :param exclusives_mapping: An ``ExclusivesMapping`` that contains the exclusive chunked targets
      to iterate.
    """
    if not isinstance(exclusives_mapping, ExclusivesMapping):
      raise ValueError('An ExclusivesMapping is required, given %s of type %s'
                       % (exclusives_mapping, type(exclusives_mapping)))
    self._exclusives_mapping = exclusives_mapping

  def __iter__(self):
    sorted_excl_group_keys = self._exclusives_mapping.get_ordered_group_keys()
    for excl_group_key in sorted_excl_group_keys:
      yield self._exclusives_mapping.get_targets_for_group_key(excl_group_key)


class GroupEngine(Engine):
  """The classical phase engine that has direct knowledge of groups and the bang algorithm.

  For grouped goals this engine attempts to make as few passes as possible through the target groups
  found.
  """

  class PhaseExecutor(object):
    def __init__(self, context, phase, tasks_by_goal):
      self._context = context
      self._phase = phase
      self._tasks_by_goal = tasks_by_goal

    @property
    def phase(self):
      return self._phase

    def attempt(self, timer, explain):
      """Executes the named phase against the current context tracking goal executions in executed.
      """

      def execute_task(goal, task, targets):
        """Execute and time a single goal that has had all of its dependencies satisfied."""
        with timer.timed(goal):
          # TODO (Senthil Kumaran):
          # Possible refactoring of the Task Execution Logic (AWESOME-1019)
          if explain:
            self._context.log.debug("Skipping execution of %s in explain mode" % goal.name)
          else:
            task.execute(targets)

      goals = self._phase.goals()
      if not goals:
        raise TaskError('No goals installed for phase %s' % self._phase)

      run_queue = []
      goals_by_group = {}
      for goal in goals:
        if goal.group:
          group_name = goal.group.name
          if group_name not in goals_by_group:
            group_goals = [goal]
            run_queue.append((group_name, group_goals))
            goals_by_group[group_name] = group_goals
          else:
            goals_by_group[group_name].append(goal)
        else:
          run_queue.append((None, [goal]))

      with self._context.new_workunit(name=self._phase.name, labels=[WorkUnit.PHASE]):
        # OrderedSet takes care of not repeating chunked task execution mentions
        execution_phases = defaultdict(OrderedSet)

        for group_name, goals in run_queue:
          if not group_name:
            goal = goals[0]
            execution_phases[self._phase].add(goal.name)
            with self._context.new_workunit(name=goal.name, labels=[WorkUnit.GOAL]):
              execute_task(goal, self._tasks_by_goal[goal], self._context.targets())
          else:
            with self._context.new_workunit(name=group_name, labels=[WorkUnit.GROUP]):
              goals_by_group_member = OrderedDict((GroupMember.from_goal(g), g) for g in goals)

              # First, divide the set of all targets to be built into compatible chunks, based
              # on their declared exclusives. Then, for each chunk of compatible exclusives, do
              # further sub-chunking. At the end, we'll have a list of chunks to be built,
              # which will go through the chunks of each exclusives-compatible group separately.

              # TODO(markcc); chunks with incompatible exclusives require separate ivy resolves.
              # Either interleave the ivy task in this group so that it runs once for each batch of
              # chunks with compatible exclusives, or make the compilation tasks do their own ivy
              # resolves for each batch of targets they're asked to compile.

              goal_chunks = []

              # We won't have exclusives calculated if stopping short for example during an explain.
              if explain:
                exclusive_chunks = [self._context.targets()]
              else:
                exclusive_chunks = ExclusivesIterator.from_context(self._context)

              for exclusive_chunk in exclusive_chunks:
                # TODO(Travis Crawford): Targets should be filtered by is_concrete rather than
                # is_internal, however, at this time python targets are not internal targets.
                group_chunks = GroupIterator(filter(lambda t: t.is_internal, exclusive_chunk),
                                             goals_by_group_member.keys())
                goal_chunks.extend(group_chunks)

              self._context.log.debug('::: created chunks(%d)' % len(goal_chunks))
              for i, (group_member, goal_chunk) in enumerate(goal_chunks):
                self._context.log.debug('  chunk(%d) [flavor=%s]:\n\t%s' % (
                    i, group_member.name, '\n\t'.join(sorted(map(str, goal_chunk)))))

              for group_member, goal_chunk in goal_chunks:
                goal = goals_by_group_member[group_member]
                execution_phases[self._phase].add((group_name, goal.name))
                with self._context.new_workunit(name=goal.name, labels=[WorkUnit.GOAL]):
                  execute_task(goal, self._tasks_by_goal[goal], goal_chunk)

        if explain:
          tasks_by_goalname = dict((goal.name, task.__class__.__name__)
                                   for goal, task in self._tasks_by_goal.items())

          def expand_goal(goal):
            if len(goal) == 2:  # goal is (group, goal)
              group_name, goal_name = goal
              task_name = tasks_by_goalname[goal_name]
              return "%s:%s->%s" % (group_name, goal_name, task_name)
            else:
              task_name = tasks_by_goalname[goal]
              return "%s->%s" % (goal, task_name)

          for phase, goals in execution_phases.items():
            goal_to_task = ", ".join(expand_goal(goal) for goal in goals)
            print("%s [%s]" % (phase, goal_to_task))

  @classmethod
  def _prepare(cls, context, phases):
    tasks_by_goal = {}

    # We loop here because a prepared goal may introduce new BUILDs and thereby new Goals/Phases.
    # We need to prepare these in a subsequent loop until the set of phases and goals quiesces.
    prepared_goals = set()
    round_num = 0
    while True:
      phases = list(cls.execution_order(phases))
      if prepared_goals == reduce(lambda goals, p: goals | set(p.goals()), phases, set()):
        break

      round_num += 1
      context.log.debug('Preparing goals in round %d' % round_num)
      # Prepare tasks roots to leaves and allow for downstream tasks requiring products from
      # upstream tasks they depend upon.
      for phase in reversed(phases):
        for goal in reversed(phase.goals()):
          if goal not in prepared_goals:
            context.log.debug('preparing: %s:%s' % (phase.name, goal.name))
            prepared_goals.add(goal)
            task = goal.task_type(context)
            tasks_by_goal[goal] = task

    return map(lambda p: cls.PhaseExecutor(context, p, tasks_by_goal), phases)

  def attempt(self, timer, context, phases):
    phase_executors = self._prepare(context, phases)

    execution_phases = ' -> '.join(map(str, map(lambda e: e.phase.name, phase_executors)))
    context.log.debug('Executing goals in phases %s' % execution_phases)

    explain = getattr(context.options, 'explain', None)
    if explain:
      print("Phase Execution Order:\n\n%s\n" % execution_phases)
      print("Phase [Goal->Task] Order:\n")

    # We take a conservative locking strategy and lock in the widest needed scope.  If we have a
    # linearized set of phases as such (where x -> y means x depends on y and *z means z needs to be
    # serialized):
    #   a -> b -> *c -> d -> *e
    # Then we grab the lock at the beginning of e's execution and don't relinquish until the largest
    # scope serialization requirement from c is past.
    serialized_phase_executors = list(filter(lambda pe: pe.phase.serialize, phase_executors))
    outer_lock_holder = serialized_phase_executors[-1] if serialized_phase_executors else None

    if outer_lock_holder:
      context.acquire_lock()
    try:
      for phase_executor in phase_executors:
        phase_executor.attempt(timer, explain)
        if phase_executor is outer_lock_holder:
          context.release_lock()
    finally:
      # we may fail before we reach the outer lock holder - so make sure to clean up no matter what.
      if outer_lock_holder:
        context.release_lock()

########NEW FILE########
__FILENAME__ = archive
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

"""Support for wholesale archive creation and extraction in a uniform API across archive types."""

import os

from abc import abstractmethod
from zipfile import ZIP_DEFLATED

from twitter.common.collections.ordereddict import OrderedDict
from twitter.common.contextutil import open_tar, open_zip
from twitter.common.lang import AbstractClass


class Archiver(AbstractClass):
  @classmethod
  def extract(cls, path, outdir):
    """Extracts an archive's contents to the specified outdir."""
    raise NotImplementedError()

  @abstractmethod
  def create(self, basedir, outdir, name, prefix=None):
    """Creates an archive of all files found under basedir to a file at outdir of the given name.

    If prefix is specified, it should be prepended to all archive paths.
    """


class TarArchiver(Archiver):
  """An archiver that stores files in a tar file with optional compression."""

  @classmethod
  def extract(cls, path, outdir):
    with open_tar(path, errorlevel=1) as tar:
      tar.extractall(outdir)

  def __init__(self, mode, extension):
    Archiver.__init__(self)
    self.mode = mode
    self.extension = extension

  def create(self, basedir, outdir, name, prefix=None):
    tarpath = os.path.join(outdir, '%s.%s' % (name, self.extension))
    with open_tar(tarpath, self.mode, dereference=True, errorlevel=1) as tar:
      tar.add(basedir, arcname=prefix or '')
    return tarpath


class ZipArchiver(Archiver):
  """An archiver that stores files in a zip file with optional compression."""

  @classmethod
  def extract(cls, path, outdir):
    """OS X's python 2.6.1 has a bug in zipfile that makes it unzip directories as regular files.

    This method should work on for python 2.6-3.x.
    """
    with open_zip(path) as zip:
      for path in zip.namelist():
        # While we're at it, we also perform this safety test.
        if path.startswith('/') or path.startswith('..'):
          raise ValueError('Zip file contains unsafe path: %s' % path)
        # Ignore directories. extract() will create parent dirs as needed.
        if not path.endswith('/'):
          zip.extract(path, outdir)

  def __init__(self, compression):
    Archiver.__init__(self)
    self.compression = compression

  def create(self, basedir, outdir, name, prefix=None):
    zippath = os.path.join(outdir, '%s.zip' % name)
    with open_zip(zippath, 'w', compression=ZIP_DEFLATED) as zip:
      for root, _, files in os.walk(basedir):
        for file in files:
          full_path = os.path.join(root, file)
          relpath = os.path.relpath(full_path, basedir)
          if prefix:
            relpath = os.path.join(prefix, relpath)
          zip.write(full_path, relpath)
    return zippath


TAR = TarArchiver('w:', 'tar')
TGZ = TarArchiver('w:gz', 'tar.gz')
TBZ2 = TarArchiver('w:bz2', 'tar.bz2')
ZIP = ZipArchiver(ZIP_DEFLATED)

_ARCHIVER_BY_TYPE = OrderedDict(tar=TGZ, tgz=TGZ, tbz2=TBZ2, zip=ZIP)

TYPE_NAMES = frozenset(_ARCHIVER_BY_TYPE.keys())

def archiver(typename):
  """Returns Archivers in common configurations.

  The typename must correspond to one of the following:
  'tar'   Returns a tar archiver that applies no compression and emits .tar files.
  'tgz'   Returns a tar archiver that applies gzip compression and emits .tar.gz files.
  'tbz2'  Returns a tar archiver that applies bzip2 compression and emits .tar.bz2 files.
  'zip'   Returns a zip archiver that applies standard compression and emits .zip files.
  """
  archiver = _ARCHIVER_BY_TYPE.get(typename)
  if not archiver:
    raise ValueError('No archiver registered for %r' % typename)
  return archiver

########NEW FILE########
__FILENAME__ = fs
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import hashlib
import os


# This is the max filename length for HFS+, extX and NTFS - the most likely filesystems pants will
# be run under.
# TODO(John Sirois): consider a better isolation layer
_MAX_FILENAME_LENGTH = 255


def safe_filename(name, extension=None, digest=None, max_length=_MAX_FILENAME_LENGTH):
  """Creates filename from name and extension ensuring that the final length is within the
  max_length constraint.

  By default the length is capped to work on most filesystems and the fallback to achieve
  shortening is a sha1 hash of the proposed name.

  Raises ValueError if the proposed name is not a simple filename but a file path.
  Also raises ValueError when the name is simple but cannot be satisfactorily shortened with the
  given digest.

  name:       the proposed filename without extension
  extension:  an optional extension to append to the filename
  digest:     the digest to fall back on for too-long name, extension concatenations - should
              support the hashlib digest api of update(string) and hexdigest
  max_length: the maximum desired file name length
  """
  if os.path.basename(name) != name:
    raise ValueError('Name must be a filename, handed a path: %s' % name)

  ext = extension or ''
  filename = name + ext
  if len(filename) <= max_length:
    return filename
  else:
    digest = digest or hashlib.sha1()
    digest.update(name)
    safe_name = digest.hexdigest() + ext
    if len(safe_name) > max_length:
      raise ValueError('Digest %s failed to produce a filename <= %d '
                       'characters for %s - got %s' % (digest, max_length, filename, safe_name))
    return safe_name

########NEW FILE########
__FILENAME__ = aggregated_timings
import os

from collections import defaultdict

from twitter.common.dirutil import safe_mkdir_for


class AggregatedTimings(object):
  """Aggregates timings over multiple invocations of 'similar' work.

  If filepath is not none, stores the timings in that file. Useful for finding bottlenecks."""
  def __init__(self, path=None):
    # Map path -> timing in seconds (a float)
    self._timings_by_path = defaultdict(float)
    self._tool_labels = set()
    self._path = path
    safe_mkdir_for(self._path)

  def add_timing(self, label, secs, is_tool=False):
    """Aggregate timings by label.

    secs - a double, so fractional seconds are allowed.
    is_tool - whether this label represents a tool invocation.
    """
    self._timings_by_path[label] += secs
    if is_tool:
      self._tool_labels.add(label)
    # Check existence in case we're a clean-all. We don't want to write anything in that case.
    if self._path and os.path.exists(os.path.dirname(self._path)):
      with open(self._path, 'w') as f:
        for x in self.get_all():
          f.write('%(label)s: %(timing)s\n' % x)

  def get_all(self):
    """Returns all the timings, sorted in decreasing order.

    Each value is a dict: { path: <path>, timing: <timing in seconds> }
    """
    return [{ 'label': x[0], 'timing': x[1], 'is_tool': x[0] in self._tool_labels}
            for x in sorted(self._timings_by_path.items(), key=lambda x: x[1], reverse=True)]

########NEW FILE########
__FILENAME__ = artifact_cache_stats
import os

from collections import defaultdict, namedtuple

from twitter.common.dirutil import safe_mkdir


# Lists of target addresses.
CacheStat = namedtuple('CacheStat', ['hit_targets', 'miss_targets'])

class ArtifactCacheStats(object):
  """Tracks the hits and misses in the artifact cache.

  If dir is specified, writes the hits and misses to files in that dir."""
  def __init__(self, dir=None):
    def init_stat():
      return CacheStat([],[])
    self.stats_per_cache = defaultdict(init_stat)
    self._dir = dir
    safe_mkdir(self._dir)

  def add_hit(self, cache_name, tgt):
    self._add_stat(0, cache_name, tgt)

  def add_miss(self, cache_name, tgt):
    self._add_stat(1, cache_name, tgt)

  def get_all(self):
    """Returns the cache stats as a list of dicts."""
    ret = []
    for cache_name, stat in self.stats_per_cache.items():
      ret.append({
        'cache_name': cache_name,
        'num_hits': len(stat.hit_targets),
        'num_misses': len(stat.miss_targets),
        'hits': stat.hit_targets,
        'misses': stat.miss_targets
      })
    return ret

  # hit_or_miss is the appropriate index in CacheStat, i.e., 0 for hit, 1 for miss.
  def _add_stat(self, hit_or_miss, cache_name, tgt):
    self.stats_per_cache[cache_name][hit_or_miss].append(tgt.address.reference())
    if self._dir and os.path.exists(self._dir):  # Check existence in case of a clean-all.
      suffix = 'misses' if hit_or_miss else 'hits'
      with open(os.path.join(self._dir, '%s.%s' % (cache_name, suffix)), 'a') as f:
        f.write(tgt.address.reference())
        f.write('\n')

########NEW FILE########
__FILENAME__ = context
from __future__ import print_function

import os
import sys

from collections import defaultdict
from contextlib import contextmanager

from twitter.common.collections import OrderedSet
from twitter.common.dirutil import Lock
from twitter.common.process import ProcessProviderFactory
from twitter.common.process.process_provider import ProcessProvider

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.goal.products import Products
from twitter.pants.java.distribution.distribution import Distribution
from twitter.pants.reporting.report import Report
from twitter.pants.targets.pants_target import Pants
from twitter.pants.targets.sources import SourceRoot


# Utility definition for grabbing process info for locking.
def _process_info(pid):
  try:
    ps = ProcessProviderFactory.get()
    ps.collect_set([pid])
    handle = ps.get_handle(pid)
    cmdline = handle.cmdline().replace('\0', ' ')
    return '%d (%s)' % (pid, cmdline)
  except ProcessProvider.UnknownPidError:
    return '%d' % pid


class Context(object):
  """Contains the context for a single run of pants.

  Goal implementations can access configuration data from pants.ini and any flags they have exposed
  here as well as information about the targets involved in the run.

  Advanced uses of the context include adding new targets to it for upstream or downstream goals to
  operate on and mapping of products a goal creates to the targets the products are associated with.
  """

  class Log(object):
    """A logger facade that logs into the pants reporting framework."""
    def __init__(self, run_tracker):
      self._run_tracker = run_tracker

    def debug(self, *msg_elements):
      self._run_tracker.log(Report.DEBUG, *msg_elements)

    def info(self, *msg_elements):
      self._run_tracker.log(Report.INFO, *msg_elements)

    def warn(self, *msg_elements):
      self._run_tracker.log(Report.WARN, *msg_elements)

    def error(self, *msg_elements):
      self._run_tracker.log(Report.ERROR, *msg_elements)

    def fatal(self, *msg_elements):
      self._run_tracker.log(Report.FATAL, *msg_elements)

  def __init__(self, config, options, run_tracker, target_roots, requested_goals=None,
               lock=None, log=None, target_base=None):
    self._config = config
    self._options = options
    self.run_tracker = run_tracker
    self._lock = lock or Lock.unlocked()
    self._log = log or Context.Log(run_tracker)
    self._target_base = target_base or Target

    self._state = {}
    self._products = Products()
    self._buildroot = get_buildroot()
    self._java_sysprops = None  # Computed lazily.
    self.requested_goals = requested_goals or []

    self.replace_targets(target_roots)

  @property
  def config(self):
    """Returns a Config object containing the configuration data found in pants.ini."""
    return self._config

  @property
  def options(self):
    """Returns the command line options parsed at startup."""
    return self._options

  @property
  def lock(self):
    """Returns the global pants run lock so a goal can release it if needed."""
    return self._lock

  @property
  def log(self):
    """Returns the preferred logger for goals to use."""
    return self._log

  @property
  def products(self):
    """Returns the Products manager for the current run."""
    return self._products

  @property
  def target_roots(self):
    """Returns the targets specified on the command line.

    This set is strictly a subset of all targets in play for the run as returned by self.targets().
    Note that for a command line invocation that uses wildcard selectors : or ::, the targets
    globbed by the wildcards are considered to be target roots.
    """
    return self._target_roots

  @property
  def java_sysprops(self):
    """The system properties of the JVM we use."""
    # TODO: In the future we can use these to hermeticize the Java enivronment rather than relying
    # on whatever's on the shell's PATH. E.g., you either specify a path to the Java home via a
    # cmd-line flag or .pantsrc, or we infer one from java.home but verify that the java.version
    # is a supported version.
    if self._java_sysprops is None:
      # TODO(John Sirois): Plumb a sane default distribution through 1 point of control
      self._java_sysprops = Distribution.cached().system_properties
    return self._java_sysprops

  @property
  def java_home(self):
    """Find the java home for the JVM we use."""
    # Implementation is a kind-of-insane hack: we run the jvm to get it to emit its
    # system properties. On some platforms there are so many hard and symbolic links into
    # the JRE dirs that it's actually quite hard to establish what path to use as the java home,
    # e.g., for the purpose of rebasing. In practice, this seems to work fine.
    # Note that for our purposes we take the parent of java.home.
    return os.path.realpath(os.path.dirname(self.java_sysprops['java.home']))

  @property
  def ivy_home(self):
    return os.path.realpath(self.config.get('ivy', 'cache_dir'))

  def __str__(self):
    return 'Context(id:%s, state:%s, targets:%s)' % (self.id, self.state, self.targets())

  def submit_foreground_work_and_wait(self, work, workunit_parent=None):
    """Returns the pool to which tasks can submit foreground (blocking) work."""
    return self.run_tracker.foreground_worker_pool().submit_work_and_wait(
      work, workunit_parent=workunit_parent)

  def submit_background_work_chain(self, work_chain, parent_workunit_name=None):
    background_root_workunit = self.run_tracker.get_background_root_workunit()
    if parent_workunit_name:
      # We have to keep this workunit alive until all its child work is done, so
      # we manipulate the context manually instead of using it as a contextmanager.
      # This is slightly funky, but the with-context usage is so pervasive and
      # useful elsewhere that it's worth the funkiness in this one place.
      workunit_parent_ctx = self.run_tracker.new_workunit_under_parent(
        name=parent_workunit_name, labels=[WorkUnit.MULTITOOL], parent=background_root_workunit)
      workunit_parent = workunit_parent_ctx.__enter__()
      done_hook = lambda: workunit_parent_ctx.__exit__(None, None, None)
    else:
      workunit_parent = background_root_workunit  # Run directly under the root.
      done_hook = None
    self.run_tracker.background_worker_pool().submit_async_work_chain(
      work_chain, workunit_parent=workunit_parent, done_hook=done_hook)

  def background_worker_pool(self):
    """Returns the pool to which tasks can submit background work."""
    return self.run_tracker.background_worker_pool()

  @contextmanager
  def new_workunit(self, name, labels=None, cmd=''):
    """Create a new workunit under the calling thread's current workunit."""
    with self.run_tracker.new_workunit(name=name, labels=labels, cmd=cmd) as workunit:
      yield workunit

  def acquire_lock(self):
    """ Acquire the global lock for the root directory associated with this context. When
    a goal requires serialization, it will call this to acquire the lock.
    """
    def onwait(pid):
      print('Waiting on pants process %s to complete' % _process_info(pid), file=sys.stderr)
      return True
    if self._lock.is_unlocked():
      runfile = os.path.join(self._buildroot, '.pants.run')
      self._lock = Lock.acquire(runfile, onwait=onwait)

  def release_lock(self):
    """Release the global lock if it's held.
    Returns True if the lock was held before this call.
    """
    if self._lock.is_unlocked():
      return False
    else:
      self._lock.release()
      self._lock = Lock.unlocked()
      return True

  def is_unlocked(self):
    """Whether the global lock object is actively holding the lock."""
    return self._lock.is_unlocked()

  def replace_targets(self, target_roots):
    """Replaces all targets in the context with the given roots and their transitive
    dependencies.
    """
    self._target_roots = list(target_roots)

    self._targets = OrderedSet()
    for target in self._target_roots:
      self.add_target(target)
    self.id = Target.identify(self._targets)

  def add_target(self, target):
    """Adds a target and its transitive dependencies to the run context.

    The target is not added to the target roots.
    """
    def add_targets(tgt):
      self._targets.update(tgt for tgt in tgt.resolve() if isinstance(tgt, self._target_base))
    target.walk(add_targets)

  def add_new_target(self, target_base, target_type, *args, **kwargs):
    """Creates a new target, adds it to the context and returns it.

    This method ensures the target resolves files against the given target_base, creating the
    directory if needed and registering a source root.
    """
    if 'derived_from' in kwargs:
      derived_from = kwargs.get('derived_from')
      del kwargs['derived_from']
    else:
      derived_from = None
    target = self._create_new_target(target_base, target_type, *args, **kwargs)
    self.add_target(target)
    if derived_from:
      target.derived_from = derived_from
    return target

  def _create_new_target(self, target_base, target_type, *args, **kwargs):
    if not os.path.exists(target_base):
      os.makedirs(target_base)
    SourceRoot.register(target_base, target_type)
    with ParseContext.temp(target_base):
      return target_type(*args, **kwargs)

  def remove_target(self, target):
    """Removes the given Target object from the context completely if present."""
    if target in self.target_roots:
      self.target_roots.remove(target)
    self._targets.discard(target)

  def targets(self, predicate=None):
    """Selects targets in-play in this run from the target roots and their transitive dependencies.

    If specified, the predicate will be used to narrow the scope of targets returned.
    """
    return filter(predicate, self._targets)

  def dependents(self, on_predicate=None, from_predicate=None):
    """Returns  a map from targets that satisfy the from_predicate to targets they depend on that
      satisfy the on_predicate.
    """
    core = set(self.targets(on_predicate))
    dependees = defaultdict(set)
    for target in self.targets(from_predicate):
      if hasattr(target, 'dependencies'):
        for dependency in target.dependencies:
          if dependency in core:
            dependees[target].add(dependency)
    return dependees

  def resolve(self, spec):
    """Returns an iterator over the target(s) the given address points to."""
    with ParseContext.temp():
      return Pants(spec).resolve()

  @contextmanager
  def state(self, key, default=None):
    value = self._state.get(key, default)
    yield value
    self._state[key] = value

  @contextmanager
  def timing(self, label):
    if self.timer:
      with self.timer.timing(label):
        yield
    else:
      yield

########NEW FILE########
__FILENAME__ = error
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

class GoalError(Exception):
  """Raised to indicate a goal has failed."""

########NEW FILE########
__FILENAME__ = group
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================


class Group(object):
  """Delineates a members of a group of targets that age sources for the same product types."""

  def __init__(self, name, predicate):
    """:param string name: A logical name for this group.
    :param predicate: A predicate that returns ``True`` if a given target is a member of this
                      group.
    """
    self.name = name
    self.predicate = predicate
    self.exclusives = None

  def __repr__(self):
    return "Group(%s,%s)" % (self.name, self.predicate.__name__)

########NEW FILE########
__FILENAME__ = initialize_reporting
import os
import sys

from twitter.common.dirutil import safe_mkdir, safe_rmtree
from twitter.common.lang import Compatibility

from twitter.pants.reporting.plaintext_reporter import PlainTextReporter
from twitter.pants.reporting.html_reporter import HtmlReporter
from twitter.pants.reporting.quiet_reporter import QuietReporter
from twitter.pants.reporting.report import ReportingError, Report
from twitter.pants.reporting.reporting_server import ReportingServerManager

StringIO = Compatibility.StringIO


def initial_reporting(config, run_tracker):
  """Sets up the initial reporting configuration.

  Will be changed after we parse cmd-line flags.
  """
  reports_dir = config.get('reporting', 'reports_dir',
                           default=os.path.join(config.getdefault('pants_workdir'), 'reports'))
  link_to_latest = os.path.join(reports_dir, 'latest')
  if os.path.exists(link_to_latest):
    os.unlink(link_to_latest)

  run_id = run_tracker.run_info.get_info('id')
  if run_id is None:
    raise ReportingError('No run_id set')
  run_dir = os.path.join(reports_dir, run_id)
  safe_rmtree(run_dir)

  html_dir = os.path.join(run_dir, 'html')
  safe_mkdir(html_dir)
  os.symlink(run_dir, link_to_latest)

  report = Report()

  # Capture initial console reporting into a buffer. We'll do something with it once
  # we know what the cmd-line flag settings are.
  outfile = StringIO()
  capturing_reporter_settings = PlainTextReporter.Settings(outfile=outfile, log_level=Report.INFO,
                                                           color=False, indent=True, timing=False,
                                                           cache_stats=False)
  capturing_reporter = PlainTextReporter(run_tracker, capturing_reporter_settings)
  report.add_reporter('capturing', capturing_reporter)

  # Set up HTML reporting. We always want that.
  template_dir = config.get('reporting', 'reports_template_dir')
  html_reporter_settings = HtmlReporter.Settings(log_level=Report.INFO,
                                                 html_dir=html_dir,
                                                 template_dir=template_dir)
  html_reporter = HtmlReporter(run_tracker, html_reporter_settings)
  report.add_reporter('html', html_reporter)

  # Add some useful RunInfo.
  run_tracker.run_info.add_info('default_report', html_reporter.report_path())
  port = ReportingServerManager.get_current_server_port()
  if port:
    run_tracker.run_info.add_info('report_url', 'http://localhost:%d/run/%s' % (port, run_id))

  return report

def update_reporting(options, is_console_task, run_tracker):
  """Updates reporting config once we've parsed cmd-line flags."""

  # Get any output silently buffered in the old console reporter, and remove it.
  old_outfile = run_tracker.report.remove_reporter('capturing').settings.outfile
  old_outfile.flush()
  buffered_output = old_outfile.getvalue()
  old_outfile.close()

  log_level = Report.log_level_from_string(options.log_level or 'info')
  color = not options.no_color
  timing = options.time
  cache_stats = options.time  # TODO: Separate flag for this?

  if options.quiet or is_console_task:
    console_reporter = QuietReporter(run_tracker,
                                     QuietReporter.Settings(log_level=log_level, color=color))
  else:
    # Set up the new console reporter.
    settings = PlainTextReporter.Settings(log_level=log_level, outfile=sys.stdout, color=color,
                                          indent=True, timing=timing, cache_stats=cache_stats)
    console_reporter = PlainTextReporter(run_tracker, settings)
    console_reporter.emit(buffered_output)
    console_reporter.flush()
  run_tracker.report.add_reporter('console', console_reporter)

  if options.logdir:
    # Also write plaintext logs to a file. This is completely separate from the html reports.
    safe_mkdir(options.logdir)
    run_id = run_tracker.run_info.get_info('id')
    outfile = open(os.path.join(options.logdir, '%s.log' % run_id), 'w')
    settings = PlainTextReporter.Settings(log_level=log_level, outfile=outfile, color=False,
                                          indent=True, timing=True, cache_stats=True)
    logfile_reporter = PlainTextReporter(run_tracker, settings)
    logfile_reporter.emit(buffered_output)
    logfile_reporter.flush()
    run_tracker.report.add_reporter('logfile', logfile_reporter)

########NEW FILE########
__FILENAME__ = phase
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

from collections import defaultdict

from twitter.pants.base.build_manual import manual

from .error import GoalError


class SingletonPhases(type):
  phases = dict()
  renames = dict()

  def rename(cls, phase, name):
    """
      Renames the given phase and ensures all future requests for the old name are mapped to the
      given phase instance.
    """
    cls.phases.pop(phase.name)
    cls.renames[phase.name] = name
    phase.name = name
    cls.phases[name] = phase

  def __call__(cls, name):
    name = cls.renames.get(name, name)
    if name not in cls.phases:
      cls.phases[name] = super(SingletonPhases, cls).__call__(name)
    return cls.phases[name]

# Python 2.x + 3.x wankery
PhaseBase = SingletonPhases(str('PhaseBase'), (object,), {})


@manual.builddict()
class Phase(PhaseBase):
  _goals_by_phase = defaultdict(list)
  _phase_by_goal = dict()

  @classmethod
  def clear(cls):
    """Remove all phases and goals.

    This method is EXCLUSIVELY for use in tests.
    """
    cls._goals_by_phase.clear()
    cls._phase_by_goal.clear()

  @staticmethod
  def of(goal):
    return Phase._phase_by_goal[goal]

  @staticmethod
  def goals_of_type(goal_class):
    """Returns all installed goals of the specified type."""
    return [goal for goal in Phase._phase_by_goal.keys() if isinstance(goal, goal_class)]

  @staticmethod
  def setup_parser(parser, args, phases):
    def do_setup_parser(phase, setup):
      for goal in phase.goals():
        if goal not in setup:
          setup.add(goal)
          for dep in goal.dependencies:
            do_setup_parser(dep, setup)
          goal.setup_parser(phase, parser, args)

    setup = set()
    for phase in phases:
      do_setup_parser(phase, setup)

  @staticmethod
  def all():
    """Returns all registered goals as a sorted sequence of phase, goals tuples."""
    return sorted(Phase._goals_by_phase.items(), key=lambda pair: pair[0].name)

  def __init__(self, name):
    self.name = name
    self.description = None

  def with_description(self, description):
    self.description = description
    return self

  def install(self, goal, first=False, replace=False, before=None, after=None):
    """
      Installs the given goal in this phase.  The placement of the goal in this phases' execution
      list defaults to the end but its position can be influence by specifying exactly one of the
      following arguments:

      first: Places the goal 1st in the execution list
      replace: Removes all existing goals in this phase and installs this goal
      before: Places the goal before the named goal in the execution list
      after: Places the goal after the named goal in the execution list
    """

    if (first or replace or before or after) and not (first ^ replace ^ bool(before) ^ bool(after)):
      raise GoalError('Can only specify one of first, replace, before or after')

    Phase._phase_by_goal[goal] = self

    g = self.goals()
    if replace:
      del g[:]
    g_names = map(lambda goal: goal.name, g)
    if first:
      g.insert(0, goal)
    elif before in g_names:
      g.insert(g_names.index(before), goal)
    elif after in g_names:
      g.insert(g_names.index(after) + 1, goal)
    else:
      g.append(goal)
    return self

  def rename(self, name):
    """Renames this goal."""
    PhaseBase.rename(self, name)
    return self

  def copy_to(self, name):
    """Copies this phase to the new named phase carrying along goal dependencies and description."""
    copy = Phase(name)
    copy.goals().extend(self.goals())
    copy.description = self.description
    return copy

  def remove(self, name):
    """Removes the named goal from this phase's list of goals to attempt."""
    goals = self.goals()
    for goal in goals:
      if goal.name == name:
        goals.remove(goal)
        return self
    raise GoalError('Goal %s does not exist in this phase, members are: %s' % (name, goals))

  class UnsatisfiedDependencyError(GoalError):
    """Raised when an operation cannot be completed due to an unsatisfied goal dependency."""

  def uninstall(self):
    """
      Removes the named phase and all its attached goals.  Raises Phase.UnsatisfiedDependencyError
      if the removal cannot be completed due to a dependency.
    """
    for phase, goals in Phase._goals_by_phase.items():
      for goal in goals:
        for dependee_phase in goal.dependencies:
          if self is dependee_phase:
            raise Phase.UnsatisfiedDependencyError(
              '%s is depended on by %s:%s' % (self.name, phase.name, goal.name))
    del Phase._goals_by_phase[self]

  def goals(self):
    return Phase._goals_by_phase[self]

  def serialize(self):
    return any([x.serialize for x in self.goals()])

  def __repr__(self):
    return self.name

########NEW FILE########
__FILENAME__ = products
import os

from collections import defaultdict

from twitter.common.collections import OrderedSet


class RootedProducts(object):
  """Products of a build that have a concept of a 'root' directory.

  E.g., classfiles, under a root package directory."""
  def __init__(self, root):
    self._root = root
    self._rel_paths = OrderedSet()

  def add_abs_paths(self, abs_paths):
    for abs_path in abs_paths:
      if not abs_path.startswith(self._root):
        raise Exception('%s is not under %s' % (abs_path, self._root))
      self._rel_paths.add(os.path.relpath(abs_path, self._root))

  def add_rel_paths(self, rel_paths):
    self._rel_paths.update(rel_paths)

  def root(self):
    return self._root

  def rel_paths(self):
    return self._rel_paths

  def abs_paths(self):
    for relpath in self._rel_paths:
      yield os.path.join(self._root, relpath)


class MultipleRootedProducts(object):
  """A product consisting of multiple roots, with associated products."""
  def __init__(self):
    self._rooted_products_by_root = {}

  def add_rel_paths(self, root, rel_paths):
    self._get_products_for_root(root).add_rel_paths(rel_paths)

  def add_abs_paths(self, root, abs_paths):
    self._get_products_for_root(root).add_abs_paths(abs_paths)

  def rel_paths(self):
    for root, products in self._rooted_products_by_root.items():
      yield root, products.rel_paths()

  def abs_paths(self):
    for root, products in self._rooted_products_by_root.items():
      yield root, products.abs_paths()

  def _get_products_for_root(self, root):
    if root in self._rooted_products_by_root:
      ret = self._rooted_products_by_root[root]
    else:
      ret = RootedProducts(root)
      self._rooted_products_by_root[root] = ret
    return ret


class Products(object):
  """An out-of-band 'dropbox' where tasks can place build product information for later tasks to use.

  Historically, the only type of product was a ProductMapping. However this had some issues, as not
  all products fit into the (basedir, [files-under-basedir]) paradigm. Also, ProductMapping docs
  and varnames refer to targets, and implicitly expect the mappings to be keyed by a target, however
  we sometimes also need to map sources to products.

  So in practice we ended up abusing this in several ways:
    1) Using fake basedirs when we didn't have a basedir concept.
    2) Using objects other than strings as 'product paths' when we had a need to.
    3) Using things other than targets as keys.

  Right now this class is in an intermediate stage, as we transition to a more robust Products concept.
  The abuses have been switched to use 'data_products' (see below) which is just a dictionary
  of product type (e.g., 'classes_by_target') to arbitrary payload. That payload can be anything,
  but the MultipleRootedProducts class is useful for products that do happen to fit into the
  (basedir, [files-under-basedir]) paradigm.

  The long-term future of Products is TBD. But we do want to make it easier to reason about
  which tasks produce which products and which tasks consume them. Currently it's quite difficult
  to match up 'requires' calls to the producers of those requirements, especially when the 'typename'
  is in a variable, not a literal.
  """
  class ProductMapping(object):
    """Maps products of a given type by target. Each product is a map from basedir to a list of
    files in that dir.
    """

    def __init__(self, typename):
      self.typename = typename
      self.by_target = defaultdict(lambda: defaultdict(list))

    def empty(self):
      return len(self.by_target) == 0

    def add(self, target, basedir, product_paths=None):
      """
        Adds a mapping of products for the given target, basedir pair.

        If product_paths are specified, these will over-write any existing mapping for this target.

        If product_paths is omitted, the current mutable list of mapped products for this target
        and basedir is returned for appending.
      """
      if product_paths is not None:
        self.by_target[target][basedir].extend(product_paths)
      else:
        return self.by_target[target][basedir]

    def has(self, target):
      """Returns whether we have a mapping for the specified target."""
      return target in self.by_target

    def get(self, target):
      """
        Returns the product mapping for the given target as a tuple of (basedir, products list).
        Can return None if there is no mapping for the given target.
      """
      return self.by_target.get(target)

    def __getitem__(self, target):
      """
        Support for subscripting into this mapping. Returns the product mapping for the given target
        as a map of <basedir> -> <products list>.
        If no mapping exists, returns an empty map whose values default to empty lists. So you
        can use the result without checking for None.
      """
      return self.by_target[target]

    def itermappings(self):
      """
        Returns an iterable over all pairs (target, product) in this mapping.
        Each product is itself a map of <basedir> -> <products list>.
      """
      return self.by_target.iteritems()

    def keys_for(self, basedir, product):
      """Returns the set of keys the given mapped product is registered under."""
      keys = set()
      for key, mappings in self.by_target.items():
        for mapped in mappings.get(basedir, []):
          if product == mapped:
            keys.add(key)
            break
      return keys

    def __repr__(self):
      return 'ProductMapping(%s) {\n  %s\n}' % (self.typename, '\n  '.join(
        '%s => %s\n    %s' % (str(target), basedir, outputs)
                              for target, outputs_by_basedir in self.by_target.items()
                              for basedir, outputs in outputs_by_basedir.items()))

  def __init__(self):
    self.products = {}  # type -> ProductMapping instance.
    self.predicates_for_type = defaultdict(list)

    self.data_products = {}  # type -> arbitrary object.
    self.required_data_products = set()

  def require(self, typename, predicate=None):
    """Registers a requirement that file products of the given type by mapped.

    If a target predicate is supplied, only targets matching the predicate are mapped.
    """
    if predicate:
      self.predicates_for_type[typename].append(predicate)
    return self.products.setdefault(typename, Products.ProductMapping(typename))

  def isrequired(self, typename):
    """Returns a predicate that selects targets required for the given type if mappings are required.

    Otherwise returns None.
    """
    if typename not in self.products:
      return None
    def combine(first, second):
      return lambda target: first(target) or second(target)
    return reduce(combine, self.predicates_for_type[typename], lambda target: False)

  def get(self, typename):
    """Returns a ProductMapping for the given type name."""
    return self.require(typename)

  def require_data(self, typename):
    """ Registers a requirement that data produced by tasks is required.

    typename: the name of a data product that should be generated.
    """
    self.required_data_products.add(typename)

  def is_required_data(self, typename):
    """ Checks if a particular data product is required by any tasks."""
    return typename in self.required_data_products

  def safe_create_data(self, typename, init_func):
    """Ensures that a data item is created if it doesn't already exist."""
    # Basically just an alias for readability.
    self.get_data(typename, init_func)

  def get_data(self, typename, init_func=None):
    """ Returns a data product.

    If the product isn't found, returns None, unless init_func is set, in which case the product's
    value is set to the return value of init_func(), and returned."""
    if typename not in self.data_products:
      if not init_func:
        return None
      self.data_products[typename] = init_func()
    return self.data_products.get(typename)

########NEW FILE########
__FILENAME__ = run_tracker
from contextlib import contextmanager
import httplib
import json
import os
import sys
import threading
import time
import urllib
from urlparse import urlparse

from twitter.pants.base.config import Config
from twitter.pants.base.run_info import RunInfo
from twitter.pants.base.worker_pool import WorkerPool
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.reporting.report import Report

from .aggregated_timings import AggregatedTimings
from .artifact_cache_stats import ArtifactCacheStats


class RunTracker(object):
  """Tracks and times the execution of a pants run.

  Also manages background work.

  Use like this:

  run_tracker.start()
  with run_tracker.new_workunit('compile'):
    with run_tracker.new_workunit('java'):
      ...
    with run_tracker.new_workunit('scala'):
      ...
  run_tracker.close()

  Can track execution against multiple 'roots', e.g., one for the main thread and another for
  background threads.
  """

  # The name of the tracking root for the main thread (and the foreground worker threads).
  DEFAULT_ROOT_NAME = 'main'

  # The name of the tracking root for the background worker threads.
  BACKGROUND_ROOT_NAME = 'background'

  @classmethod
  def from_config(cls, config):
    if not isinstance(config, Config):
      raise ValueError('Expected a Config object, given %s of type %s' % (config, type(config)))
    info_dir = RunInfo.dir(config)
    stats_upload_url = config.getdefault('stats_upload_url', default=None)
    num_foreground_workers = config.getdefault('num_foreground_workers', default=8)
    num_background_workers = config.getdefault('num_background_workers', default=8)
    return cls(info_dir,
               stats_upload_url=stats_upload_url,
               num_foreground_workers=num_foreground_workers,
               num_background_workers=num_background_workers)

  def __init__(self,
               info_dir,
               stats_upload_url=None,
               num_foreground_workers=8,
               num_background_workers=8):
    self.run_timestamp = time.time()  # A double, so we get subsecond precision for ids.
    cmd_line = ' '.join(['./pants'] + sys.argv[1:])

    # run_id is safe for use in paths.
    millis = (self.run_timestamp * 1000) % 1000
    run_id = 'pants_run_%s_%d' % \
             (time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime(self.run_timestamp)), millis)

    self.info_dir = os.path.join(info_dir, run_id)
    self.run_info = RunInfo(os.path.join(self.info_dir, 'info'))
    self.run_info.add_basic_info(run_id, self.run_timestamp)
    self.run_info.add_info('cmd_line', cmd_line)
    self.stats_url = stats_upload_url

    # Create a 'latest' symlink, after we add_infos, so we're guaranteed that the file exists.
    link_to_latest = os.path.join(os.path.dirname(self.info_dir), 'latest')
    if os.path.exists(link_to_latest):
      os.unlink(link_to_latest)
    os.symlink(self.info_dir, link_to_latest)

    # Time spent in a workunit, including its children.
    self.cumulative_timings = AggregatedTimings(os.path.join(self.info_dir, 'cumulative_timings'))

    # Time spent in a workunit, not including its children.
    self.self_timings = AggregatedTimings(os.path.join(self.info_dir, 'self_timings'))

    # Hit/miss stats for the artifact cache.
    self.artifact_cache_stats = \
      ArtifactCacheStats(os.path.join(self.info_dir, 'artifact_cache_stats'))

    # Number of threads for foreground work.
    self._num_foreground_workers = num_foreground_workers

    # Number of threads for background work.
    self._num_background_workers = num_background_workers

    # We report to this Report.
    self.report = None

    # self._threadlocal.current_workunit contains the current workunit for the calling thread.
    # Note that multiple threads may share a name (e.g., all the threads in a pool).
    self._threadlocal = threading.local()

    # For main thread work. Created on start().
    self._main_root_workunit = None

    # For concurrent foreground work.  Created lazily if needed.
    # Associated with the main thread's root workunit.
    self._foreground_worker_pool = None

    # For background work.  Created lazily if needed.
    self._background_worker_pool = None
    self._background_root_workunit = None

    self._aborted = False

  def register_thread(self, parent_workunit):
    """Register the parent workunit for all work in the calling thread.

    Multiple threads may have the same parent (e.g., all the threads in a pool).
    """
    self._threadlocal.current_workunit = parent_workunit

  def is_under_main_root(self, workunit):
    """Is the workunit running under the main thread's root."""
    return workunit.root() == self._main_root_workunit

  def start(self, report):
    """Start tracking this pants run.

    report: an instance of pants.reporting.Report."""
    self.report = report
    self.report.open()

    self._main_root_workunit = WorkUnit(run_tracker=self, parent=None, labels=[],
                                        name=RunTracker.DEFAULT_ROOT_NAME, cmd=None)
    self.register_thread(self._main_root_workunit)
    self._main_root_workunit.start()
    self.report.start_workunit(self._main_root_workunit)

  @contextmanager
  def new_workunit(self, name, labels=None, cmd=''):
    """Creates a (hierarchical) subunit of work for the purpose of timing and reporting.

    - name: A short name for this work. E.g., 'resolve', 'compile', 'scala', 'zinc'.
    - labels: An optional iterable of labels. The reporters can use this to decide how to
              display information about this work.
    - cmd: An optional longer string representing this work.
           E.g., the cmd line of a compiler invocation.

    Use like this:

    with run_tracker.new_workunit(name='compile', labels=[WorkUnit.GOAL]) as workunit:
      <do scoped work here>
      <set the outcome on workunit if necessary>

    Note that the outcome will automatically be set to failure if an exception is raised
    in a workunit, and to success otherwise, so usually you only need to set the
    outcome explicitly if you want to set it to warning.
    """
    parent = self._threadlocal.current_workunit
    with self.new_workunit_under_parent(name, parent=parent, labels=labels, cmd=cmd) as workunit:
      self._threadlocal.current_workunit = workunit
      try:
        yield workunit
      finally:
        self._threadlocal.current_workunit = parent

  @contextmanager
  def new_workunit_under_parent(self, name, parent, labels=None, cmd=''):
    """Creates a (hierarchical) subunit of work for the purpose of timing and reporting.

    - name: A short name for this work. E.g., 'resolve', 'compile', 'scala', 'zinc'.
    - parent: The new workunit is created under this parent.
    - labels: An optional iterable of labels. The reporters can use this to decide how to
              display information about this work.
    - cmd: An optional longer string representing this work.
           E.g., the cmd line of a compiler invocation.

    Task code should not typically call this directly.
    """
    workunit = WorkUnit(run_tracker=self, parent=parent, name=name, labels=labels, cmd=cmd)
    workunit.start()
    try:
      self.report.start_workunit(workunit)
      yield workunit
    except KeyboardInterrupt:
      workunit.set_outcome(WorkUnit.ABORTED)
      self._aborted = True
      raise
    except:
      workunit.set_outcome(WorkUnit.FAILURE)
      raise
    else:
      workunit.set_outcome(WorkUnit.SUCCESS)
    finally:
      self.report.end_workunit(workunit)
      workunit.end()

  def log(self, level, *msg_elements):
    """Log a message against the current workunit."""
    self.report.log(self._threadlocal.current_workunit, level, *msg_elements)

  def upload_stats(self):
    """Send timing results to URL specified in pants.ini"""
    def error(msg):
      # Report aleady closed, so just print error.
      print("WARNING: Failed to upload stats. %s" % msg)

    if self.stats_url:
      params = {
        'run_info': json.dumps(self.run_info.get_as_dict()),
        'cumulative_timings': json.dumps(self.cumulative_timings.get_all()),
        'self_timings': json.dumps(self.self_timings.get_all()),
        'artifact_cache_stats': json.dumps(self.artifact_cache_stats.get_all())
        }

      headers = {"Content-type": "application/x-www-form-urlencoded", "Accept": "text/plain"}
      url = urlparse(self.stats_url)
      try:
        if url.scheme == 'https':
          http_conn = httplib.HTTPSConnection(url.netloc)
        else:
          http_conn = httplib.HTTPConnection(url.netloc)
        http_conn.request('POST', url.path, urllib.urlencode(params), headers)
        resp = http_conn.getresponse()
        if resp.status != 200:
          error("HTTP error code: %d" % resp.status)
      except Exception as e:
        error("Error: %s" % e)

  def end(self):
    """This pants run is over, so stop tracking it.

    Note: If end() has been called once, subsequent calls are no-ops.
    """
    if self._background_worker_pool:
      if self._aborted:
        self.log(Report.INFO, "Aborting background workers.")
        self._background_worker_pool.abort()
      else:
        self.log(Report.INFO, "Waiting for background workers to finish.")
        self._background_worker_pool.shutdown()
      self.report.end_workunit(self._background_root_workunit)
      self._background_root_workunit.end()

    if self._foreground_worker_pool:
      if self._aborted:
        self.log(Report.INFO, "Aborting foreground workers.")
        self._foreground_worker_pool.abort()
      else:
        self.log(Report.INFO, "Waiting for foreground workers to finish.")
        self._foreground_worker_pool.shutdown()

    self.report.end_workunit(self._main_root_workunit)
    self._main_root_workunit.end()

    outcome = self._main_root_workunit.outcome()
    if self._background_root_workunit:
      outcome = min(outcome, self._background_root_workunit.outcome())
    outcome_str = WorkUnit.outcome_string(outcome)
    log_level = WorkUnit.choose_for_outcome(outcome, Report.ERROR, Report.ERROR,
                                            Report.WARN, Report.INFO, Report.INFO)
    self.log(log_level, outcome_str)

    if self.run_info.get_info('outcome') is None:
      try:
        self.run_info.add_info('outcome', outcome_str)
      except IOError:
        pass  # If the goal is clean-all then the run info dir no longer exists...

    self.report.close()
    self.upload_stats()

  def foreground_worker_pool(self):
    if self._foreground_worker_pool is None:  # Initialize lazily.
      self._foreground_worker_pool = WorkerPool(parent_workunit=self._main_root_workunit,
                                                run_tracker=self,
                                                num_workers=self._num_foreground_workers)
    return self._foreground_worker_pool

  def get_background_root_workunit(self):
    if self._background_root_workunit is None:
      self._background_root_workunit = WorkUnit(run_tracker=self, parent=None, labels=[],
                                                name='background', cmd=None)
      self._background_root_workunit.start()
      self.report.start_workunit(self._background_root_workunit)
    return self._background_root_workunit


  def background_worker_pool(self):
    if self._background_worker_pool is None:  # Initialize lazily.
      self._background_worker_pool = WorkerPool(parent_workunit=self.get_background_root_workunit(),
                                                run_tracker=self,
                                                num_workers=self._num_background_workers)
    return self._background_worker_pool

########NEW FILE########
__FILENAME__ = bootstrapper
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import hashlib
import os
import shutil

from twitter.common import log
from twitter.common.contextutil import temporary_file
from twitter.common.dirutil import safe_delete, touch
from twitter.common.quantity import Amount, Time

from twitter.pants.base.config import Config
from twitter.pants.ivy.ivy import Ivy
from twitter.pants.net.http.fetcher import Fetcher


class Bootstrapper(object):
  """Bootstraps a working ivy resolver.

  By default a working resolver will be bootstrapped from maven central and it will use standard
  public jar repositories and a standard ivy local cache directory to execute resolve operations.

  A combination of site configuration options and environment variables can be used to override this
  default setup.

  By default ivy will be bootstrapped from a stable ivy jar version found in maven central, but
  this can be over-ridden with the ``ivy / bootstrap_jar_url`` config option.  Additionally the
  bootstrapping will use a connect/read timeout of 1 second by default, but this can be raised by
  specifying a ``ivy / bootstrap_fetch_timeout_secs`` config value.

  After bootstrapping, ivy will re-resolve itself.  By default it does this via maven central, but
  a custom ivy tool classpath can be specified by using the ``ivy / ivy_profile`` option to point to
  a custom ivy profile ivy.xml.  This can be useful to upgrade ivy to a version released after pants
  or else mix in auxiliary jars that provide ivy plugins.

  Finally, by default the ivysettings.xml embedded in the ivy jar will be used in conjunction with
  the default ivy local cache directory of ~/.ivy2/cache.  To specify custom values for these you
  can either provide ``ivy / ivy_settings`` and ``ivy / cache_dir`` config values or supply these
  values via the ``PANTS_IVY_SETTINGS_XML`` and ``PANTS_IVY_CACHE_DIR`` environment variables
  respectively.  The environment variables will trump config values if present.
  """

  class Error(Exception):
    """Indicates an error bootstrapping an ivy classpath."""

  _DEFAULT_VERSION = '2.3.0'
  _DEFAULT_URL = ('http://repo1.maven.org/maven2/'
                  'org/apache/ivy/ivy/'
                  '%(version)s/ivy-%(version)s.jar' % {'version': _DEFAULT_VERSION})

  _INSTANCE = None

  @classmethod
  def instance(cls):
    """Returns the default global ivy bootstrapper."""
    if cls._INSTANCE is None:
      cls._INSTANCE = cls()
    return cls._INSTANCE

  @classmethod
  def default_ivy(cls, java_executor=None, bootstrap_workunit_factory=None):
    """Returns an Ivy instance using the default global bootstrapper.

    By default runs ivy via a subprocess java executor.

    :param java_executor: the optional java executor to use
    :param bootstrap_workunit_factory: the optional workunit to bootstrap under.
    :returns: an Ivy instance.
    :raises: Bootstrapper.Error if the default ivy instance could not be bootstrapped
    """
    return cls.instance().ivy(java_executor=java_executor,
                              bootstrap_workunit_factory=bootstrap_workunit_factory)

  def __init__(self):
    """Creates an ivy bootstrapper."""
    self._config = Config.load()
    self._bootstrap_jar_url = self._config.get('ivy', 'bootstrap_jar_url',
                                               default=self._DEFAULT_URL)
    self._timeout = Amount(self._config.getint('ivy', 'bootstrap_fetch_timeout_secs', default=1),
                           Time.SECONDS)
    self._version_or_ivyxml = self._config.get('ivy', 'ivy_profile', default=self._DEFAULT_VERSION)
    self._classpath = None

  def ivy(self, java_executor=None, bootstrap_workunit_factory=None):
    """Returns an ivy instance bootstrapped by this bootstrapper.

    :param java_executor: the optional java executor to use
    :param bootstrap_workunit_factory: the optional workunit to bootstrap under.
    :raises: Bootstrapper.Error if ivy could not be bootstrapped
    """
    return Ivy(self._get_classpath(java_executor, bootstrap_workunit_factory),
               java_executor=java_executor,
               ivy_settings=self._ivy_settings,
               ivy_cache_dir=self.ivy_cache_dir)

  def _get_classpath(self, executor, workunit_factory):
    """Returns the bootstrapped ivy classpath as a list of jar paths.

    :raises: Bootstrapper.Error if the classpath could not be bootstrapped
    """
    if not self._classpath:
      self._classpath = self._bootstrap_ivy_classpath(executor, workunit_factory)
    return self._classpath

  @property
  def _ivy_settings(self):
    """Returns the bootstrapped ivysettings.xml path.

    By default the ivy.ivy_settings value found in pants.ini but can be overridden by via the
    PANTS_IVY_SETTINGS_XML environment variable.  If neither is specified defaults to ivy's built
    in default ivysettings.xml of standard public resolvers.
    """
    return os.getenv('PANTS_IVY_SETTINGS_XML') or self._config.get('ivy', 'ivy_settings')

  @property
  def ivy_cache_dir(self):
    """Returns the bootstrapped ivy cache dir.

    By default the ivy.cache_dir value found in pants.ini but can be overridden via the
    PANTS_IVY_CACHE_DIR environment variable.  If neither is specified defaults to ivy's built
    in default cache dir; ie: ~/.ivy2/cache.
    """
    return (os.getenv('PANTS_IVY_CACHE_DIR')
            or self._config.get('ivy', 'cache_dir', default=os.path.expanduser('~/.ivy2/cache')))

  def _bootstrap_ivy_classpath(self, executor, workunit_factory, retry=True):
    # TODO(John Sirois): Extract a ToolCache class to control the path structure:
    # https://jira.twitter.biz/browse/DPB-283
    ivy_bootstrap_dir = \
      os.path.join(self._config.getdefault('pants_bootstrapdir'), 'tools', 'jvm', 'ivy')

    digest = hashlib.sha1()
    if os.path.isfile(self._version_or_ivyxml):
      with open(self._version_or_ivyxml) as fp:
        digest.update(fp.read())
    else:
      digest.update(self._version_or_ivyxml)
    classpath = os.path.join(ivy_bootstrap_dir, '%s.classpath' % digest.hexdigest())

    if not os.path.exists(classpath):
      ivy = self._bootstrap_ivy(os.path.join(ivy_bootstrap_dir, 'bootstrap.jar'))
      args = ['-confs', 'default', '-cachepath', classpath]
      if os.path.isfile(self._version_or_ivyxml):
        args.extend(['-ivy', self._version_or_ivyxml])
      else:
        args.extend(['-dependency', 'org.apache.ivy', 'ivy', self._version_or_ivyxml])

      try:
        ivy.execute(args=args, executor=executor,
                    workunit_factory=workunit_factory, workunit_name='ivy-bootstrap')
      except ivy.Error as e:
        safe_delete(classpath)
        raise self.Error('Failed to bootstrap an ivy classpath! %s' % e)

    with open(classpath) as fp:
      cp = fp.read().strip().split(os.pathsep)
      if not all(map(os.path.exists, cp)):
        safe_delete(classpath)
        if retry:
          return self._bootstrap_ivy_classpath(executor, workunit_factory, retry=False)
        raise self.Error('Ivy bootstrapping failed - invalid classpath: %s' % ':'.join(cp))
      return cp

  def _bootstrap_ivy(self, bootstrap_jar_path):
    if not os.path.exists(bootstrap_jar_path):
      with temporary_file() as bootstrap_jar:
        fetcher = Fetcher()
        checksummer = fetcher.ChecksumListener(digest=hashlib.sha1())
        try:
          log.info('\nDownloading %s' % self._bootstrap_jar_url)
          # TODO: Capture the stdout of the fetcher, instead of letting it output
          # to the console directly.
          fetcher.download(self._bootstrap_jar_url,
                           listener=fetcher.ProgressListener().wrap(checksummer),
                           path_or_fd=bootstrap_jar,
                           timeout=self._timeout)
          log.info('sha1: %s' % checksummer.checksum)
          bootstrap_jar.close()
          touch(bootstrap_jar_path)
          shutil.move(bootstrap_jar.name, bootstrap_jar_path)
        except fetcher.Error as e:
          raise self.Error('Problem fetching the ivy bootstrap jar! %s' % e)

    return Ivy(bootstrap_jar_path,
               ivy_settings=self._ivy_settings,
               ivy_cache_dir=self.ivy_cache_dir)

########NEW FILE########
__FILENAME__ = ivy
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import maybe_list
from twitter.common.lang import Compatibility

from twitter.pants.java.executor import Executor, SubprocessExecutor
from twitter.pants.java import util


class Ivy(object):
  """Encapsulates the ivy cli taking care of the basic invocation letting you just worry about the
  args to pass to the cli itself.
  """

  class Error(Exception):
    """Indicates an error executing an ivy command."""

  def __init__(self, classpath, java_executor=None, ivy_settings=None, ivy_cache_dir=None):
    """Configures an ivy wrapper for the ivy distribution at the given classpath."""

    self._classpath = maybe_list(classpath)

    self._java = java_executor or SubprocessExecutor()
    if not isinstance(self._java, Executor):
      raise ValueError('java_executor must be an Executor instance, given %s of type %s'
                       % (self._java, type(self._java)))

    self._ivy_settings = ivy_settings
    if self._ivy_settings and not isinstance(self._ivy_settings, Compatibility.string):
      raise ValueError('ivy_settings must be a string, given %s of type %s'
                       % (self._ivy_settings, type(self._ivy_settings)))

    self._ivy_cache_dir = ivy_cache_dir
    if self._ivy_cache_dir and not isinstance(self._ivy_cache_dir, Compatibility.string):
      raise ValueError('ivy_cache_dir must be a string, given %s of type %s'
                       % (self._ivy_cache_dir, type(self._ivy_cache_dir)))

  @property
  def ivy_settings(self):
    """Returns the ivysettings.xml path used by this `Ivy` instance."""
    return self._ivy_settings

  @property
  def ivy_cache_dir(self):
    """Returns the ivy cache dir used by this `Ivy` instance."""
    return self._ivy_cache_dir

  def execute(self, jvm_options=None, args=None, executor=None,
              workunit_factory=None, workunit_name=None, workunit_labels=None):
    """Executes the ivy commandline client with the given args.

    Raises Ivy.Error if the command fails for any reason.
    """
    runner = self.runner(jvm_options=jvm_options, args=args, executor=executor)
    try:
      result = util.execute_runner(runner, workunit_factory, workunit_name, workunit_labels)
      if result != 0:
        raise self.Error('Ivy command failed with exit code %d%s'
                         % (result, ': ' + ' '.join(args) if args else ''))
    except self._java.Error as e:
      raise self.Error('Problem executing ivy: %s' % e)

  def runner(self, jvm_options=None, args=None, executor=None):
    """Creates an ivy commandline client runner for the given args."""
    args = args or []
    executor = executor or self._java
    if not isinstance(executor, Executor):
      raise ValueError('The executor argument must be an Executor instance, given %s of type %s'
                       % (executor, type(executor)))

    if self._ivy_cache_dir and '-cache' not in args:
      # TODO(John Sirois): Currently this is a magic property to support hand-crafted <caches/> in
      # ivysettings.xml.  Ideally we'd support either simple -caches or these hand-crafted cases
      # instead of just hand-crafted.  Clean this up by taking over ivysettings.xml and generating
      # it from BUILD constructs.
      jvm_options = ['-Divy.cache.dir=%s' % self._ivy_cache_dir] + (jvm_options or [])

    if self._ivy_settings and '-settings' not in args:
      args = ['-settings', self._ivy_settings] + args

    return executor.runner(classpath=self._classpath, main='org.apache.ivy.Main',
                           jvm_options=jvm_options, args=args)

########NEW FILE########
__FILENAME__ = distribution
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from contextlib import contextmanager

import os
import pkgutil
import subprocess

from twitter.common import log
from twitter.common.contextutil import temporary_dir
from twitter.common.lang import Compatibility

from twitter.pants.base.revision import Revision


class Distribution(object):
  """Represents a java distribution - either a JRE or a JDK installed on the local system.

  In particular provides access to the distribution's binaries; ie: java while ensuring basic
  constraints are met.  For example a minimum version can be specified if you know need to compile
  source code or run bytecode that exercise features only available in that version forward.
  """

  class Error(Exception):
    """Indicates an invalid java distribution."""

  _CACHE = {}

  @classmethod
  def cached(cls, minimum_version=None, jdk=False):
    def scan_constraint_match():
      for dist in cls._CACHE.values():
        if minimum_version and dist.version < minimum_version:
          continue
        if jdk and not dist.jdk:
          continue
        return dist

    key = (minimum_version, jdk)
    dist = cls._CACHE.get(key)
    if not dist:
      dist = scan_constraint_match()
      if not dist:
        dist = cls.locate(minimum_version=minimum_version, jdk=jdk)
      cls._CACHE[key] = dist
    return dist

  @classmethod
  def locate(cls, minimum_version=None, jdk=False):
    """Finds a java distribution that meets any given constraints and returns it.

    First looks in JDK_HOME and JAVA_HOME if defined falling back to a search on the PATH.
    Raises Distribution.Error if no suitable java distribution could be found.
    """
    def home_bin_path(home_env_var):
      home = os.environ.get(home_env_var)
      return os.path.join(home, 'bin') if home else None

    def search_path():
      yield home_bin_path('JDK_HOME')
      yield home_bin_path('JAVA_HOME')
      path = os.environ.get('PATH')
      if path:
        for p in path.strip().split(os.pathsep):
          yield p

    for path in filter(None, search_path()):
      try:
        dist = cls(path, minimum_version=minimum_version, jdk=jdk)
        dist.validate()
        log.debug('Located %s for constraints: minimum_version'
                  ' %s, jdk %s' % (dist, minimum_version, jdk))
        return dist
      except (ValueError, cls.Error):
        pass

    raise cls.Error('Failed to locate a %s distribution with minimum_version %s'
                    % ('JDK' if jdk else 'JRE', minimum_version))

  @staticmethod
  def _parse_java_version(version):
    # Java version strings have been well defined since release 1.3.1 as defined here:
    #  http://www.oracle.com/technetwork/java/javase/versioning-naming-139433.html
    # These version strings comply with semver except that the traditional pre-release semver
    # slot (the 4th) can be delimited by an _ in the case of update releases of the jdk.
    # We accomodate that difference here.
    return Revision.semver(version.replace('_', '-'))

  @staticmethod
  def _is_executable(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)

  def __init__(self, bin_path='/usr/bin', minimum_version=None, jdk=False):
    """Creates a distribution wrapping the given bin_path.

    :param string bin_path: the path to the java distributions bin dir
    :param minimum_version: a modified semantic version string or else a Revision object
    :param bool jdk: ``True`` to require the distribution be a JDK vs a JRE
    """

    if not os.path.isdir(bin_path):
      raise ValueError('The specified distribution path is invalid: %s' % bin_path)
    self._bin_path = bin_path

    if isinstance(minimum_version, Compatibility.string):
      minimum_version = self._parse_java_version(minimum_version)
    if minimum_version and not isinstance(minimum_version, Revision):
      raise ValueError('minimum_version must be a string or a Revision object,'
                       ' given: %s' % minimum_version)
    self._minimum_version = minimum_version

    self._jdk = jdk

    self._is_jdk = False
    self._system_properties = None
    self._version = None
    self._validated_binaries = {}

  @property
  def jdk(self):
    self.validate()
    return self._is_jdk

  @property
  def system_properties(self):
    """Returns a dict containing the system properties of this java distribution."""
    return dict(self._get_system_properties(self.java))

  @property
  def version(self):
    """Returns the distribution version.

    Raises Distribution.Error if this distribution is not valid according to the configured
    constraints.
    """
    return self._get_version(self.java)

  @property
  def home(self):
    """Returns the distribution JAVA_HOME."""
    return self._get_system_properties(self.java)['java.home']

  @property
  def java(self):
    """Returns the path to this distribution's java command.

    If this distribution has no valid java command raises Distribution.Error.
    """
    return self.binary('java')

  def binary(self, name):
    """Returns the path to the command of the given name for this distribution.

    For example: ::

        >>> d = Distribution()
        >>> jar = d.binary('jar')
        >>> jar
        '/usr/bin/jar'
        >>>

    If this distribution has no valid command of the given name raises Distribution.Error.
    """
    if not isinstance(name, Compatibility.string):
      raise ValueError('name must be a binary name, given %s of type %s' % (name, type(name)))
    self.validate()
    return self._validated_executable(name)

  def validate(self):
    """Validates this distribution against its configured constraints.

    Raises Distribution.Error if this distribution is not valid according to the configured
    constraints.
    """
    if self._validated_binaries:
      return

    with self._valid_executable('java') as java:
      if self._minimum_version:
        version = self._get_version(java)
        if version < self._minimum_version:
          raise self.Error('The java distribution at %s is too old; expecting at least %s and'
                           ' got %s' % (java, self._minimum_version, version))

    try:
      self._validated_executable('javac')  # Calling purely for the check and cache side effects
      self._is_jdk = True
    except self.Error:
      if self._jdk:
        raise

  def _get_version(self, java):
    if not self._version:
      self._version = self._parse_java_version(self._get_system_properties(java)['java.version'])
    return self._version

  def _get_system_properties(self, java):
    if not self._system_properties:
      with temporary_dir() as classpath:
        with open(os.path.join(classpath, 'SystemProperties.class'), 'w+') as fp:
          fp.write(pkgutil.get_data(__name__, 'SystemProperties.class'))
        cmd = [java, '-cp', classpath, 'SystemProperties']
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = process.communicate()
        if process.returncode != 0:
          raise self.Error('Failed to determine java system properties for %s with %s - exit code'
                           ' %d: %s' % (java, ' '.join(cmd), process.returncode, stderr))

      props = {}
      for line in stdout.split(os.linesep):
        key, _, val = line.partition('=')
        props[key] = val
      self._system_properties = props

    return self._system_properties

  def _validate_executable(self, name):
    exe = os.path.join(self._bin_path, name)
    if not self._is_executable(exe):
      raise self.Error('Failed to locate the %s executable, %s does not appear to be a'
                       ' valid %s distribution' % (name, self, 'JDK' if self._jdk else 'JRE'))
    return exe

  def _validated_executable(self, name):
    exe = self._validated_binaries.get(name)
    if not exe:
      exe = self._validate_executable(name)
      self._validated_binaries[name] = exe
    return exe

  @contextmanager
  def _valid_executable(self, name):
    exe = self._validate_executable(name)
    yield exe
    self._validated_binaries[name] = exe

  def __repr__(self):
    return 'Distribution(%r, minimum_version=%r, jdk=%r)' % (self._bin_path, self._minimum_version,
                                                             self._jdk)

########NEW FILE########
__FILENAME__ = executor
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from abc import abstractmethod, abstractproperty
from contextlib import contextmanager

import os
import subprocess

from twitter.common import log
from twitter.common.collections import maybe_list
from twitter.common.contextutil import environment_as
from twitter.common.lang import AbstractClass, Compatibility

from twitter.pants.java.distribution import Distribution


class Executor(AbstractClass):
  """Executes java programs."""

  @staticmethod
  def _scrub_args(classpath, main, jvm_options, args):
    classpath = maybe_list(classpath)
    if not isinstance(main, Compatibility.string) or not main:
      raise ValueError('A non-empty main classname is required, given: %s' % main)
    jvm_options = maybe_list(jvm_options or ())
    args = maybe_list(args or ())
    return classpath, main, jvm_options, args

  class Error(Exception):
    """Indicates an error launching a java program."""

  class Runner(object):
    """A re-usable executor that can run a configured java command line."""

    @abstractproperty
    def executor(self):
      """Returns the executor this runner uses to run itself."""

    @abstractproperty
    def cmd(self):
      """Returns a string representation of the command that will be run."""

    @abstractmethod
    def run(self, stdout=None, stderr=None):
      """Runs the configured java command.

      If there is a problem executing tha java program subclasses should raise Executor.Error.
      Its guaranteed that all arguments are valid as documented in `execute`

      :param stdout: An optional stream to pump stdout to; defaults to `sys.stdout`.
      :param stderr: An optional stream to pump stderr to; defaults to `sys.stderr`.
      """

  def __init__(self, distribution=None):
    """Constructs an Executor that can be used to launch java programs.

    :param distribution: an optional validated java distribution to use when launching java
      programs
    """
    if distribution:
      if not isinstance(distribution, Distribution):
        raise ValueError('A valid distribution is required, given: %s' % distribution)
      distribution.validate()
    else:
      distribution = Distribution.cached()

    self._distribution = distribution

  def runner(self, classpath, main, jvm_options=None, args=None):
    """Returns an `Executor.Runner` for the given java command."""
    return self._runner(*self._scrub_args(classpath, main, jvm_options, args))

  def execute(self, classpath, main, jvm_options=None, args=None, stdout=None, stderr=None):
    """Launches the java program defined by the classpath and main.

    :param list classpath: the classpath for the java program
    :param string main: the fully qualified class name of the java program's entry point
    :param list jvm_options: an optional sequence of options for the underlying jvm
    :param list args: an optional sequence of args to pass to the java program

    Returns the exit code of the java program.
    Raises Executor.Error if there was a problem launching java itself.
    """
    executor = self.runner(classpath=classpath, main=main, jvm_options=jvm_options, args=args)
    return executor.run(stdout=stdout, stderr=stderr)

  @abstractmethod
  def _runner(self, classpath, main, jvm_options, args):
    """Subclasses should return a `Runner` that can execute the given java main."""

  def _create_command(self, classpath, main, jvm_options, args):
    cmd = [self._distribution.java]
    cmd.extend(jvm_options)
    cmd.extend(['-cp', os.pathsep.join(classpath), main])
    cmd.extend(args)
    return cmd


class CommandLineGrabber(Executor):
  """Doesn't actually execute anything, just captures the cmd line."""
  def __init__(self, distribution=None):
    super(CommandLineGrabber, self).__init__(distribution=distribution)
    self._command = None  # Initialized when we run something.

  def _runner(self, classpath, main, jvm_options, args):
    self._command = self._create_command(classpath, main, jvm_options, args)
    class Runner(self.Runner):
      @property
      def executor(_):
        return self

      @property
      def cmd(_):
        return ' '.join(self._command)

      def run(_, stdout=None, stderr=None):
        return 0
    return Runner()

  @property
  def cmd(self):
    return self._command


class SubprocessExecutor(Executor):
  """Executes java programs by launching a jvm in a subprocess."""

  def __init__(self, distribution=None, scrub_classpath=True):
    super(SubprocessExecutor, self).__init__(distribution=distribution)
    self._scrub_classpath = scrub_classpath

  def _runner(self, classpath, main, jvm_options, args):
    command = self._create_command(classpath, main, jvm_options, args)

    class Runner(self.Runner):
      @property
      def executor(_):
        return self

      @property
      def cmd(_):
        return ' '.join(command)

      def run(_, stdout=None, stderr=None):
        return self._spawn(command, stdout=stdout, stderr=stderr).wait()

    return Runner()

  def spawn(self, classpath, main, jvm_options=None, args=None, **subprocess_args):
    """Spawns the java program passing any extra subprocess kwargs on to subprocess.Popen.

    Returns the Popen process object handle to the spawned java program subprocess.
    """
    cmd = self._create_command(*self._scrub_args(classpath, main, jvm_options, args))
    return self._spawn(cmd, **subprocess_args)

  def _spawn(self, cmd, **subprocess_args):
    with self._maybe_scrubbed_classpath():
      log.debug('Executing: %s' % ' '.join(cmd))
      try:
        return subprocess.Popen(cmd, **subprocess_args)
      except OSError as e:
        raise self.Error('Problem executing %s: %s' % (self._distribution.java, e))

  @contextmanager
  def _maybe_scrubbed_classpath(self):
    if self._scrub_classpath:
      classpath = os.getenv('CLASSPATH')
      if classpath:
        log.warn('Scrubbing CLASSPATH=%s' % classpath)
      with environment_as(CLASSPATH=None):
        yield
    else:
      yield

########NEW FILE########
__FILENAME__ = manifest
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from contextlib import closing

from twitter.common.lang import Compatibility
StringIO = Compatibility.StringIO


class Manifest(object):
  """
    Implements the basics of the jar manifest specification.

    See: http://docs.oracle.com/javase/1.5.0/docs/guide/jar/jar.html#Manifest Specification
  """
  @staticmethod
  def _wrap(text):
    with closing(StringIO(text)) as fp:
      yield fp.read(70)
      while True:
        chunk = fp.read(69)
        if not chunk:
          return
        yield ' %s' % chunk

  PATH = 'META-INF/MANIFEST.MF'

  MANIFEST_VERSION = 'Manifest-Version'
  CREATED_BY = 'Created-By'
  MAIN_CLASS = 'Main-Class'
  CLASS_PATH = 'Class-Path'

  def __init__(self, contents=''):
    self._contents = contents.strip()

  def addentry(self, header, value):
    if len(header) > 68:
      raise ValueError('Header name must be 68 characters or less, given %s' % header)
    if self._contents:
      self._contents += '\n'
    self._contents += '\n'.join(self._wrap('%s: %s' % (header, value)))

  def contents(self):
    return self._contents + '\n'

########NEW FILE########
__FILENAME__ = nailgun_client
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import select
import socket
import struct
import sys
import threading

from functools import partial


class NailgunSession(object):
  """Handles a single nailgun command session."""

  class ProtocolError(Exception):
    """Thrown if there is an error in the underlying nailgun protocol."""

  # See: http://www.martiansoftware.com/nailgun/protocol.html
  HEADER_FMT = '>Ic'
  HEADER_LENGTH = 5

  BUFF_SIZE = 8096

  @classmethod
  def _send_chunk(cls, sock, command, payload=''):
    header = struct.pack(cls.HEADER_FMT, len(payload), command)
    sock.sendall(header + payload)

  def __init__(self, sock, ins, out, err):
    self._sock = sock
    self._send_chunk = partial(self._send_chunk, sock)
    self._input_reader = self._InputReader(ins, self._sock, self.BUFF_SIZE) if ins else None
    self._out = out
    self._err = err

  class _InputReader(threading.Thread):
    def __init__(self, ins, sock, buff_size):
      threading.Thread.__init__(self)
      self.daemon = True
      self._ins = ins
      self._sock = sock
      self._buff_size = buff_size
      self._send_chunk = partial(NailgunSession._send_chunk, sock)
      self._stopping = threading.Event()

    def run(self):
      while self._should_run():
        readable, _, errored = select.select([self._ins], [], [self._ins])
        if self._ins in errored:
          self.stop()
        if self._should_run() and self._ins in readable:
          data = os.read(self._ins.fileno(), self._buff_size)
          if self._should_run():
            if data:
              self._send_chunk('0', data)
            else:
              self._send_chunk('.')
              try:
                self._sock.shutdown(socket.SHUT_WR)
              except socket.error:
                # Can happen if response is quick
                pass
              self.stop()

    def stop(self):
      self._stopping.set()

    def _should_run(self):
      return not self._stopping.is_set()

  def execute(self, work_dir, main_class, *args, **environment):
    for arg in args:
      self._send_chunk('A', arg)
    for k, v in environment.items():
      self._send_chunk('E', '%s=%s' % (k, v))
    self._send_chunk('D', work_dir)
    self._send_chunk('C', main_class)

    if self._input_reader:
      self._input_reader.start()
    try:
      return self._read_response()
    finally:
      if self._input_reader:
        self._input_reader.stop()

  def _read_response(self):
    buff = ''
    while True:
      command, payload, buff = self._read_chunk(buff)
      if command == '1':
        self._out.write(payload)
        self._out.flush()
      elif command == '2':
        self._err.write(payload)
        self._err.flush()
      elif command == 'X':
        self._out.flush()
        self._err.flush()
        return int(payload)
      else:
        raise self.ProtocolError('Received unexpected chunk %s -> %s' % (command, payload))

  def _read_chunk(self, buff):
    while len(buff) < self.HEADER_LENGTH:
      buff += self._sock.recv(self.BUFF_SIZE)

    payload_length, command = struct.unpack(self.HEADER_FMT, buff[:self.HEADER_LENGTH])
    buff = buff[self.HEADER_LENGTH:]
    while len(buff) < payload_length:
      buff += self._sock.recv(self.BUFF_SIZE)

    payload = buff[:payload_length]
    rest = buff[payload_length:]
    return command, payload, rest


class NailgunClient(object):
  """A client for the nailgun protocol that allows execution of java binaries within a resident vm.
  """

  class NailgunError(Exception):
    """Indicates an error connecting to or interacting with a nailgun server."""

  DEFAULT_NG_HOST = 'localhost'
  DEFAULT_NG_PORT = 2113

  # For backwards compatibility with nails expecting the ng c client special env vars.
  ENV_DEFAULTS = dict(
    NAILGUN_FILESEPARATOR = os.sep,
    NAILGUN_PATHSEPARATOR = os.pathsep
  )

  def __init__(self,
               host=DEFAULT_NG_HOST,
               port=DEFAULT_NG_PORT,
               ins=sys.stdin,
               out=sys.stdout,
               err=sys.stderr,
               work_dir=None):
    """Creates a nailgun client that can be used to issue zero or more nailgun commands.

    :param string host: the nailgun server to contact (defaults to localhost)
    :param int port: the port the nailgun server is listening on (defaults to the default nailgun
      port: 2113)
    :param file ins: a file to read command standard input from (defaults to stdin) - can be None
      in which case no input is read
    :param file out: a stream to write command standard output to (defaults to stdout)
    :param file err: a stream to write command standard error to (defaults to stderr)
    :param string work_dir: the working directory for all nailgun commands (defaults to PWD)
    """
    self._host = host
    self._port = port
    self._ins = ins
    self._out = out
    self._err = err
    self._work_dir = work_dir or os.path.abspath(os.path.curdir)

    self.execute = self.__call__

  def try_connect(self):
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    return sock if sock.connect_ex((self._host, self._port)) == 0 else None

  def __call__(self, main_class, *args, **environment):
    """Executes the given main_class with any supplied args in the given environment.

    :param string main_class: the fully qualified class name of the main entrypoint
    :param list args: any arguments to pass to the main entrypoint
    :param dict environment: an environment mapping made available to native nails via the nail
      context

    Returns the exit code of the main_class.
    """
    environment = dict(self.ENV_DEFAULTS.items() + environment.items())

    sock = self.try_connect()
    if not sock:
      raise self.NailgunError('Problem connecting to nailgun server'
                              ' %s:%d' % (self._host, self._port))

    session = NailgunSession(sock, self._ins, self._out, self._err)
    try:
      return session.execute(self._work_dir, main_class, *args, **environment)
    except socket.error as e:
      raise self.NailgunError('Problem contacting nailgun server %s:%d:'
                              ' %s' % (self._host, self._port, e))
    except session.ProtocolError as e:
      raise self.NailgunError('Problem executing the nailgun protocol with nailgun server %s:%s:'
                              ' %s' % (self._host, self._port, e))
    finally:
      sock.close()

  def __repr__(self):
    return 'NailgunClient(host=%r, port=%r, work_dir=%r)' % (self._host, self._port, self._work_dir)

########NEW FILE########
__FILENAME__ = nailgun_executor
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import namedtuple

import hashlib
import os
import re
import sys
import time

# TODO: Once we integrate standard logging into our reporting framework, we  can consider making
#  some of the log.debug() below into log.info(). Right now it just looks wrong on the console.
from twitter.common import log
from twitter.common.collections import maybe_list
from twitter.common.dirutil import safe_open
from twitter.common.lang import Compatibility
from twitter.pants.base.build_environment import get_buildroot

from .executor import Executor, SubprocessExecutor
from .nailgun_client import NailgunClient


class NailgunExecutor(Executor):
  """Executes java programs by launching them in nailgun server.

  If a nailgun is not available for a given set of jvm args and classpath, one is launched and
  re-used for the given jvm args and classpath on subsequent runs.
  """

  class Endpoint(namedtuple('Endpoint', ['fingerprint', 'pid', 'port'])):
    """The coordinates for a nailgun server controlled by NailgunExecutor."""

    @classmethod
    def parse(cls, endpoint):
      """Parses an endpoint from a string of the form fingerprint:pid:port"""
      components = endpoint.split(':')
      if len(components) != 3:
        raise ValueError('Invalid endpoint spec %s' % endpoint)
      fingerprint, pid, port = components
      return cls(fingerprint, int(pid), int(port))

  # Used to identify we own a given java nailgun server
  _PANTS_NG_ARG_PREFIX = '-Dpants.buildroot'
  _PANTS_NG_ARG = '%s=%s' % (_PANTS_NG_ARG_PREFIX, get_buildroot())

  _PANTS_FINGERPRINT_ARG_PREFIX = '-Dpants.nailgun.fingerprint='

  @staticmethod
  def _check_pid(pid):
    try:
      os.kill(pid, 0)
      return True
    except OSError:
      return False

  @staticmethod
  def create_owner_arg(workdir):
    # Currently the owner is identified via the full path to the workdir.
    return '-Dpants.nailgun.owner=%s' % workdir

  @classmethod
  def _create_fingerprint_arg(cls, fingerprint):
    return cls._PANTS_FINGERPRINT_ARG_PREFIX + fingerprint

  @classmethod
  def parse_fingerprint_arg(cls, args):
    for arg in args:
      components = arg.split(cls._PANTS_FINGERPRINT_ARG_PREFIX)
      if len(components) == 2 and components[0] == '':
        return components[1]
    return None

  @staticmethod
  def _fingerprint(jvm_args, classpath):
    digest = hashlib.sha1()
    digest.update(''.join(sorted(jvm_args)))
    digest.update(''.join(sorted(classpath)))  # TODO(John Sirois): hash classpath contents?
    return digest.hexdigest()

  @staticmethod
  def _log_kill(pid, port=None, logger=None):
    logger = logger or log.info
    logger('killing ng server @ pid:%d%s' % (pid, ' port:%d' % port if port else ''))

  def __init__(self, workdir, nailgun_classpath, distribution=None, ins=None):
    super(NailgunExecutor, self).__init__(distribution=distribution)

    self._nailgun_classpath = maybe_list(nailgun_classpath)

    if not isinstance(workdir, Compatibility.string):
      raise ValueError('Workdir must be a path string, given %s' % workdir)

    self._workdir = workdir

    self._ng_out = os.path.join(workdir, 'stdout')
    self._ng_err = os.path.join(workdir, 'stderr')

    self._ins = ins

  def _runner(self, classpath, main, jvm_options, args):
    command = self._create_command(classpath, main, jvm_options, args)

    class Runner(self.Runner):
      @property
      def executor(this):
        return self

      @property
      def cmd(this):
        return ' '.join(command)

      def run(this, stdout=sys.stdout, stderr=sys.stderr):
        nailgun = self._get_nailgun_client(jvm_options, classpath, stdout, stderr)
        try:
          log.debug('Executing via %s: %s' % (nailgun, this.cmd))
          return nailgun(main, *args)
        except nailgun.NailgunError as e:
          self.kill()
          raise self.Error('Problem launching via %s command %s %s: %s'
                           % (nailgun, main, ' '.join(args), e))

    return Runner()

  def kill(self):
    """Kills the nailgun server owned by this executor if its currently running."""

    endpoint = self._get_nailgun_endpoint()
    if endpoint:
      self._log_kill(endpoint.pid, endpoint.port)
      try:
        os.kill(endpoint.pid, 9)
      except OSError:
        pass

  def _get_nailgun_endpoint(self):
    if self._find:
      endpoint = self._find(self._workdir)
      if endpoint:
        log.debug('Found ng server with fingerprint %s @ pid:%d port:%d' % endpoint)
      return endpoint
    else:
      return None

  def _get_nailgun_client(self, jvm_args, classpath, stdout, stderr):
    classpath = self._nailgun_classpath + classpath
    new_fingerprint = self._fingerprint(jvm_args, classpath)

    endpoint = self._get_nailgun_endpoint()
    running = endpoint and self._check_pid(endpoint.pid)
    updated = endpoint and endpoint.fingerprint != new_fingerprint
    if running and not updated:
      return self._create_ngclient(endpoint.port, stdout, stderr)
    else:
      if running and updated:
        log.debug('Killing ng server with fingerprint %s @ pid:%d port:%d' % endpoint)
        self.kill()
      return self._spawn_nailgun_server(new_fingerprint, jvm_args, classpath, stdout, stderr)

  # 'NGServer started on 127.0.0.1, port 53785.'
  _PARSE_NG_PORT = re.compile('.*\s+port\s+(\d+)\.$')

  def _parse_nailgun_port(self, line):
    match = self._PARSE_NG_PORT.match(line)
    if not match:
      raise NailgunClient.NailgunError('Failed to determine spawned ng port from response'
                                       ' line: %s' % line)
    return int(match.group(1))

  def _await_nailgun_server(self, stdout, stderr):
    nailgun_timeout_seconds = 5
    max_socket_connect_attempts = 10
    nailgun = None
    port_parse_start = time.time()
    with safe_open(self._ng_out, 'r') as ng_out:
      while not nailgun:
        started = ng_out.readline()
        if started:
          port = self._parse_nailgun_port(started)
          nailgun = self._create_ngclient(port, stdout, stderr)
          log.debug('Detected ng server up on port %d' % port)
        elif time.time() - port_parse_start > nailgun_timeout_seconds:
          raise NailgunClient.NailgunError('Failed to read ng output after'
                                           ' %s seconds' % nailgun_timeout_seconds)

    attempt = 0
    while nailgun:
      sock = nailgun.try_connect()
      if sock:
        sock.close()
        endpoint = self._get_nailgun_endpoint()
        if endpoint:
          log.debug('Connected to ng server with fingerprint %s pid: %d @ port: %d' % endpoint)
        else:
          raise NailgunClient.NailgunError('Failed to connect to ng server.')
        return nailgun
      elif attempt > max_socket_connect_attempts:
        raise nailgun.NailgunError('Failed to connect to ng output after %d connect attempts'
                                   % max_socket_connect_attempts)
      attempt += 1
      log.debug('Failed to connect on attempt %d' % attempt)
      time.sleep(0.1)

  def _create_ngclient(self, port, stdout, stderr):
    return NailgunClient(port=port, ins=self._ins, out=stdout, err=stderr, work_dir=get_buildroot())

  def _spawn_nailgun_server(self, fingerprint, jvm_args, classpath, stdout, stderr):
    log.debug('No ng server found with fingerprint %s, spawning...' % fingerprint)

    with safe_open(self._ng_out, 'w'):
      pass  # truncate

    pid = os.fork()
    if pid != 0:
      # In the parent tine - block on ng being up for connections
      return self._await_nailgun_server(stdout, stderr)

    os.setsid()
    in_fd = open('/dev/null', 'r')
    out_fd = safe_open(self._ng_out, 'w')
    err_fd = safe_open(self._ng_err, 'w')

    java = SubprocessExecutor(self._distribution)

    jvm_args = jvm_args + [self._PANTS_NG_ARG,
                           self.create_owner_arg(self._workdir),
                           self._create_fingerprint_arg(fingerprint)]

    process = java.spawn(classpath=classpath,
                         main='com.martiansoftware.nailgun.NGServer',
                         jvm_options=jvm_args,
                         args=[':0'],
                         stdin=in_fd,
                         stdout=out_fd,
                         stderr=err_fd,
                         close_fds=True,
                         cwd=get_buildroot())

    log.debug('Spawned ng server with fingerprint %s @ %d' % (fingerprint, process.pid))
    # Prevents finally blocks and atexit handlers from being executed, unlike sys.exit(). We
    # don't want to execute finally blocks because we might, e.g., clean up tempfiles that the
    # parent still needs.
    os._exit(0)

  def __str__(self):
    return 'NailgunExecutor(%s, server=%s)' % (self._distribution, self._get_nailgun_endpoint())


# TODO(jsirois): Make psutil and other deps available in dev mode, so we don't need such tricks.
try:
  import psutil

  def _find_ngs(everywhere=False):
    def cmdline_matches(cmdline):
      if everywhere:
        return any(filter(lambda arg: arg.startswith(NailgunExecutor._PANTS_NG_ARG_PREFIX), cmdline))
      else:
        return NailgunExecutor._PANTS_NG_ARG in cmdline

    for proc in psutil.process_iter():
      try:
        if 'java' == proc.name and cmdline_matches(proc.cmdline):
          yield proc
      except (psutil.AccessDenied, psutil.NoSuchProcess):
        pass

  def killall(logger=None, everywhere=False):
    success = True
    for proc in _find_ngs(everywhere=everywhere):
      try:
        NailgunExecutor._log_kill(proc.pid, logger=logger)
        proc.kill()
      except (psutil.AccessDenied, psutil.NoSuchProcess):
        success = False
    return success

  NailgunExecutor.killall = staticmethod(killall)

  def _find_ng_listen_port(proc):
    for connection in proc.get_connections(kind='tcp'):
      if connection.status == 'LISTEN':
        host, port = connection.laddr
        return port
    return None

  def _find(workdir):
    owner_arg = NailgunExecutor.create_owner_arg(workdir)
    for proc in _find_ngs(everywhere=False):
      try:
        if owner_arg in proc.cmdline:
          fingerprint = NailgunExecutor.parse_fingerprint_arg(proc.cmdline)
          port = _find_ng_listen_port(proc)
          if fingerprint and port:
            return NailgunExecutor.Endpoint(fingerprint, proc.pid, port)
      except (psutil.AccessDenied, psutil.NoSuchProcess):
        pass
    return None

  NailgunExecutor._find = staticmethod(_find)
except ImportError:
  NailgunExecutor.killall = None
  NailgunExecutor._find = None

########NEW FILE########
__FILENAME__ = util
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.workunit import WorkUnit

from .executor import Executor, SubprocessExecutor
from .nailgun_executor import NailgunExecutor


def execute_java(classpath, main, jvm_options=None, args=None, executor=None,
                 workunit_factory=None, workunit_name=None, workunit_labels=None):
  """Executes the java program defined by the classpath and main.

  If `workunit_factory` is supplied, does so in the context of a workunit.

  :param list classpath: the classpath for the java program
  :param string main: the fully qualified class name of the java program's entry point
  :param list jvm_options: an optional sequence of options for the underlying jvm
  :param list args: an optional sequence of args to pass to the java program
  :param executor: an optional java executor to use to launch the program; defaults to a subprocess
    spawn of the default java distribution
  :param workunit_factory: an optional callable that can produce a workunit context
  :param string workunit_name: an optional name for the work unit; defaults to the main
  :param list workunit_labels: an optional sequence of labels for the work unit

  Returns the exit code of the java program.
  Raises `twitter.pants.java.Executor.Error` if there was a problem launching java itself.
  """
  executor = executor or SubprocessExecutor()
  if not isinstance(executor, Executor):
    raise ValueError('The executor argument must be a java Executor instance, give %s of type %s'
                     % (executor, type(executor)))

  runner = executor.runner(classpath, main, args=args, jvm_options=jvm_options)
  workunit_name = workunit_name or main
  return execute_runner(runner,
                        workunit_factory=workunit_factory,
                        workunit_name=workunit_name,
                        workunit_labels=workunit_labels)


def execute_runner(runner, workunit_factory=None, workunit_name=None, workunit_labels=None):
  """Executes the given java runner.

  If `workunit_factory` is supplied, does so in the context of a workunit.

  :param runner: the java runner to run
  :param workunit_factory: an optional callable that can produce a workunit context
  :param string workunit_name: an optional name for the work unit; defaults to the main
  :param list workunit_labels: an optional sequence of labels for the work unit

  Returns the exit code of the java runner.
  Raises `twitter.pants.java.Executor.Error` if there was a problem launching java itself.
  """
  if not isinstance(runner, Executor.Runner):
    raise ValueError('The runner argument must be a java Executor.Runner instance, '
                     'given %s of type %s' % (runner, type(runner)))

  if workunit_factory is None:
    return runner.run()
  else:
    workunit_labels = [
        WorkUnit.TOOL,
        WorkUnit.NAILGUN if isinstance(runner.executor, NailgunExecutor) else WorkUnit.JVM
    ] + (workunit_labels or [])

    with workunit_factory(name=workunit_name, labels=workunit_labels, cmd=runner.cmd) as workunit:
      ret = runner.run(stdout=workunit.output('stdout'), stderr=workunit.output('stderr'))
      workunit.set_outcome(WorkUnit.FAILURE if ret else WorkUnit.SUCCESS)
      return ret

########NEW FILE########
__FILENAME__ = fetcher
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from contextlib import closing, contextmanager

import hashlib
import os
import requests
import sys
import tempfile
import time

from twitter.common.dirutil import safe_open
from twitter.common.lang import Compatibility
from twitter.common.quantity import Amount, Data, Time


# TODO(John Sirois): Consider lifting this to twitter.common.http and consolidating with, for
# example, twitter.common.python.http.Http
class Fetcher(object):
  """A streaming URL fetcher that supports listeners."""

  class Error(Exception):
    """Indicates an error fetching an URL."""

  class TransientError(Error):
    """Indicates a fetch error for an operation that may reasonably be retried.

    For example a connection error or fetch timeout are both considered transient.
    """

  class PermanentError(Error):
    """Indicates a fetch error that is likely permanent.

    Retrying operations that raise these errors is unlikely to succeed.  For example, an HTTP 404
    response code is considered a permanent error.
    """
    def __init__(self, value=None, response_code=None):
      super(Fetcher.PermanentError, self).__init__(value)
      if response_code and not isinstance(response_code, Compatibility.integer):
        raise ValueError('response_code must be an integer, got %s' % response_code)
      self._response_code = response_code

    @property
    def response_code(self):
      """The HTTP response code of the failed request.

      May be None it the request failed before receiving a server response.
      """
      return self._response_code

  _TRANSIENT_EXCEPTION_TYPES = (requests.ConnectionError, requests.Timeout)

  class Listener(object):
    """A listener callback interface for HTTP GET requests made by a Fetcher."""

    def status(self, code, content_length=None):
      """Called when the response headers are received before data starts streaming.

      :param int code: the HTTP response code
      :param int content_length: the response Content-Length if known, otherwise None
      """

    def recv_chunk(self, data):
      """Called as each chunk of data is received from the streaming response.

      :param data: a byte string containing the next chunk of response data
      """

    def finished(self):
      """Called when the response has been fully read."""

    def wrap(self, listener=None):
      """Returns a Listener that wraps both the given listener and this listener, calling each in
      turn for each callback method.
      """
      if not listener:
        return self

      class Wrapper(Fetcher.Listener):
        def status(wrapper, code, content_length=None):
          listener.status(code, content_length=content_length)
          self.status(code, content_length=content_length)

        def recv_chunk(wrapper, data):
          listener.recv_chunk(data)
          self.recv_chunk(data)

        def finished(wrapper):
          listener.finished()
          self.finished()

      return Wrapper()

  class DownloadListener(Listener):
    """A Listener that writes all received data to a file like object."""

    def __init__(self, fh):
      """Creates a DownloadListener that writes to the given open file handle.

      The file handle is not closed.

      :param fh: a file handle open for writing
      """
      if not fh or not hasattr(fh, 'write'):
        raise ValueError('fh must be an open file handle, given %s' % fh)
      self._fh = fh

    def recv_chunk(self, data):
      self._fh.write(data)

  class ChecksumListener(Listener):
    """A Listener that checksums the data received."""

    def __init__(self, digest=None):
      """Creates a ChecksumListener with the given hashlib digest or else an MD5 digest if none is
      supplied.

      :param digest: the digest to use to checksum the received data, MDS by default
      """
      self.digest = digest or hashlib.md5()
      self._checksum = None

    def recv_chunk(self, data):
      self.digest.update(data)

    def finished(self):
      self._checksum = self.digest.hexdigest()

    @property
    def checksum(self):
      """Returns the hex digest of the received data.

      Its not valid to access this property before the listener is finished.

      :rtype: string
      :raises: ValueError if accessed before this listener is finished
      """
      if self._checksum is None:
        raise ValueError('The checksum cannot be accessed before this listener is finished.')
      return self._checksum

  class ProgressListener(Listener):
    """A Listener that logs progress to stdout."""

    def __init__(self, width=None, chunk_size=None):
      """Creates a ProgressListener that logs progress for known size items with a progress bar of
      the given width in characters and otherwise logs a progress indicator every chunk_size.

      :param int width: the width of the progress bar for known size downloads, 50 by default
      :param chunk_size: a Data Amount indicating the size of data chunks to note progress for,
        10 KB by default
      """
      self._width = width or 50
      if not isinstance(self._width, Compatibility.integer):
        raise ValueError('The width must be an integer, given %s' % self._width)

      self._chunk_size = chunk_size or Amount(10, Data.KB)
      if not isinstance(self._chunk_size, Amount) or not isinstance(self._chunk_size.unit(), Data):
        raise ValueError('The chunk_size must be a Data Amount, given %s' % self._chunk_size)

      self._start = time.time()

    def _convert(self, amount, to_unit):
      return Amount(int(amount.as_(to_unit)), to_unit)

    def status(self, code, content_length=None):
      self.size = content_length

      if content_length:
        download_kb = int(Amount(content_length, Data.BYTES).as_(Data.KB))
        self.download_size = Amount(download_kb, Data.KB)
        self.chunk = content_length / self._width
      else:
        self.chunk = self._chunk_size.as_(Data.BYTES)

      self.chunks = 0
      self.read = 0

    def recv_chunk(self, data):
      self.read += len(data)
      chunk_count = self.read // self.chunk
      if chunk_count > self.chunks:
        self.chunks = chunk_count
        if self.size:
          sys.stdout.write('\r')
          sys.stdout.write('%3d%% ' % ((self.read * 1.0 / self.size) * 100))
        sys.stdout.write('.' * self.chunks)
        if self.size:
          size_width = len(str(self.download_size))
          downloaded = self._convert(Amount(self.read, Data.BYTES), to_unit=Data.KB)
          sys.stdout.write('%s %s' % (' ' * (self._width - self.chunks),
                                      str(downloaded).rjust(size_width)))
        sys.stdout.flush()

    def finished(self):
      if self.chunks > 0:
        sys.stdout.write(' %.3fs\n' % (time.time() - self._start))
        sys.stdout.flush()

  def __init__(self, requests_api=None):
    """Creates a Fetcher that uses the given requests api object.

    By default uses the requests module, but can be any object conforming to the requests api like
    a requests Session object.
    """
    self._requests = requests_api or requests

  def fetch(self, url, listener, chunk_size=None, timeout=None):
    """Fetches data from the given URL notifying listener of all lifecycle events.

    :param string url: the url to GET data from
    :param listener: the listener to notify of all download lifecycle events
    :param chunk_size: the chunk size to use for buffering data, 10 KB by default
    :param timeout: the maximum time to wait for data to be available, 1 second by default
    :raises: Fetcher.Error if there was a problem fetching all data from the given url
    """
    chunk_size = chunk_size or Amount(10, Data.KB)
    if not isinstance(chunk_size, Amount) or not isinstance(chunk_size.unit(), Data):
      raise ValueError('chunk_size must be a Data Amount, given %s' % chunk_size)

    timeout = timeout or Amount(1, Time.SECONDS)
    if not isinstance(timeout, Amount) or not isinstance(timeout.unit(), Time):
      raise ValueError('chunk_size must be a Time Amount, given %s' % timeout)

    if not isinstance(listener, self.Listener):
      raise ValueError('listener must be a Listener instance, given %s' % listener)

    try:
      with closing(self._requests.get(url, stream=True, timeout=timeout.as_(Time.SECONDS))) as resp:
        if resp.status_code != requests.codes.ok:
          listener.status(resp.status_code)
          raise self.PermanentError('GET request to %s failed with status code %d'
                                    % (url, resp.status_code),
                                    response_code=resp.status_code)

        size = resp.headers.get('content-length')
        listener.status(resp.status_code, content_length=int(size) if size else None)

        read_bytes = 0
        for data in resp.iter_content(chunk_size=int(chunk_size.as_(Data.BYTES))):
          listener.recv_chunk(data)
          read_bytes += len(data)
        if size and read_bytes != int(size):
          raise self.Error('Expected %s bytes, read %d' % (size, read_bytes))
        listener.finished()
    except requests.exceptions.RequestException as e:
      exception_factory = (self.TransientError if isinstance(e, self._TRANSIENT_EXCEPTION_TYPES)
                           else self.PermanentError)
      raise exception_factory('Problem GETing data from %s: %s' % (url, e))

  def download(self, url, listener=None, path_or_fd=None, chunk_size=None, timeout=None):
    """Downloads data from the given URL.

    By default data is downloaded to a temporary file.

    :param string url: the url to GET data from
    :param listener: an optional listener to notify of all download lifecycle events
    :param path_or_fd: an optional file path or open file descriptor to write data to
    :param chunk_size: the chunk size to use for buffering data
    :param timeout: the maximum time to wait for data to be available
    :returns: the path to the file data was downloaded to.
    :raises: Fetcher.Error if there was a problem downloading all data from the given url.
    """
    @contextmanager
    def download_fp(_path_or_fd):
      if _path_or_fd and not isinstance(_path_or_fd, Compatibility.string):
        yield _path_or_fd, _path_or_fd.name
      else:
        if not _path_or_fd:
          fd, _path_or_fd = tempfile.mkstemp()
          os.close(fd)
        with safe_open(_path_or_fd, 'w') as fp:
          yield fp, _path_or_fd

    with download_fp(path_or_fd) as (fp, path):
      listener = self.DownloadListener(fp).wrap(listener)
      self.fetch(url, listener, chunk_size=chunk_size, timeout=timeout)
      return path

########NEW FILE########
__FILENAME__ = doc_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

__author__ = 'Mark McBride'

import os
import pkgutil
import shutil

try:
  import markdown
  HAS_MARKDOWN = True
except ImportError:
  HAS_MARKDOWN = False


from twitter.pants import is_doc  # XXX This no longer exists
from twitter.pants.base.generator import Generator

_TEMPLATE_BASEDIR = 'templates'

class DocBuilder(object):
  def __init__(self, root_dir):
    self.root_dir = root_dir

  def build(self, targets, _):
    template_path = os.path.join(_TEMPLATE_BASEDIR, 'doc.mustache')
    template = pkgutil.get_data(__name__, template_path)
    for target in targets:
      assert is_doc(target), 'DocBuilder can only build DocTargets, given %s' % str(target)
      base_dir = os.path.dirname(target.address.buildfile.full_path)
      target_base = target.target_base
      print('building doc for %s' % str(target))
      output_dir = os.path.normpath(os.path.join(self.root_dir, target.id))
      if not os.path.exists(output_dir):
        os.makedirs(output_dir)
      for filename in target.sources:
        if filename.endswith('md'):
          if not HAS_MARKDOWN:
            print('Missing markdown, cannot process %s' % filename, file=sys.stderr)
          else:
            print('processing %s' % filename)
            html_filename = os.path.splitext(filename)[0] + '.html'
            output_filename = os.path.join(output_dir, os.path.basename(html_filename))
            print('writing file to %s' % output_filename)
            with open(output_filename, 'w') as output:
              with open(os.path.join(target_base, filename), 'r') as md:
                contents = md.read()
                md_html = markdown.markdown(contents)
                generator = Generator(template, root_dir = self.root_dir, text = md_html)
              generator.write(output)
      for filename in target.resources:
        full_filepath = os.path.join(target_base, filename)
        target_file = os.path.join(output_dir, os.path.relpath(full_filepath, base_dir))
        print('copying %s to %s' % (filename, target_file))
        if not os.path.exists(os.path.dirname(target_file)):
          os.makedirs(os.path.dirname(target_file))
        shutil.copy(full_filepath, target_file)
    return 0

########NEW FILE########
__FILENAME__ = xargs
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import subprocess
import errno


class Xargs(object):
  """A subprocess execution wrapper in the spirit of the xargs command line tool.

  Specifically allows encapsulated commands to be passed very large argument lists by chunking up
  the argument lists into a minimal set and then invoking the encapsulated command against each
  chunk in turn.
  """

  @classmethod
  def subprocess(cls, cmd, **kwargs):
    """Creates an xargs engine that uses subprocess.call to execute the given cmd array with extra
    arg chunks.
    """
    def call(args):
      return subprocess.call(cmd + args, **kwargs)
    return cls(call)

  def __init__(self, cmd):
    """Creates an xargs engine that calls cmd with argument chunks.

    :param cmd: A function that can execute a command line in the form of a list of strings
      passed as its sole argument.
    """
    self._cmd = cmd

  def _split_args(self, args):
    half = len(args) // 2
    return args[:half], args[half:]

  def execute(self, args):
    """Executes the configured cmd passing args in one or more rounds xargs style.

    :param list args: Extra arguments to pass to cmd.
    """
    all_args = list(args)
    try:
      return self._cmd(all_args)
    except OSError as e:
      if errno.E2BIG == e.errno:
        args1, args2 = self._split_args(all_args)
        result = self.execute(args1)
        if result != 0:
          return result
        return self.execute(args2)
      else:
        raise e

########NEW FILE########
__FILENAME__ = antlr_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import sys

from twitter.common.dirutil import safe_mkdir

from twitter.pants.ivy.bootstrapper import Bootstrapper
from twitter.pants.ivy.ivy import Ivy

from .code_generator import CodeGenerator


class PythonAntlrBuilder(CodeGenerator):
  """
    Antlr builder.
  """
  def run_antlrs(self, output_dir):
    args = [
      '-dependency', 'org.antlr', 'antlr', self.target.antlr_version,
      '-types', 'jar',
      '-main', 'org.antlr.Tool',
      '--', '-fo', output_dir
    ]
    for source in self.target.sources:
      abs_path = os.path.abspath(os.path.join(self.root, self.target.target_base, source))
      args.append(abs_path)

    try:
      ivy = Bootstrapper.default_ivy()
      ivy.execute(args=args)  # TODO: Needs a workunit, when we have a context here.
      return True
    except (Bootstrapper.Error, Ivy.Error) as e:
      print('ANTLR generation failed! %s' % e, file=sys.stderr)
      return False

  def generate(self):
    # Create the package structure.
    path = self.sdist_root

    package = ''
    for module_name in self.target.module.split('.'):
      path = os.path.join(path, module_name)
      if package == '':
        package = module_name
      else:
        package = package + '.' + module_name
      safe_mkdir(path)
      with open(os.path.join(path, '__init__.py'), 'w') as f:
        if package != self.target.module:  # Only write this in the non-leaf modules.
          f.write("__import__('pkg_resources').declare_namespace(__name__)")
          self.created_namespace_packages.add(package)
      self.created_packages.add(package)

    # autogenerate the python files that we bundle up
    self.run_antlrs(path)

########NEW FILE########
__FILENAME__ = binary_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import tempfile
import time

from twitter.common.python.interpreter import PythonInterpreter
from twitter.common.python.pex_builder import PEXBuilder

from twitter.pants.base.config import Config
from twitter.pants.targets.python_binary import PythonBinary

from .python_chroot import PythonChroot


class PythonBinaryBuilder(object):
  class NotABinaryTargetException(Exception):
    pass

  def __init__(self, target, root_dir, run_tracker, interpreter=None, conn_timeout=None):
    self.target = target
    self.interpreter = interpreter or PythonInterpreter.get()
    if not isinstance(target, PythonBinary):
      raise PythonBinaryBuilder.NotABinaryTargetException(
          "Target %s is not a PythonBinary!" % target)

    config = Config.load()
    self.distdir = config.getdefault('pants_distdir')
    distpath = tempfile.mktemp(dir=self.distdir, prefix=target.name)

    run_info = run_tracker.run_info
    build_properties = {}
    build_properties.update(run_info.add_basic_info(run_id=None, timestamp=time.time()))
    build_properties.update(run_info.add_scm_info())

    pexinfo = target.pexinfo.copy()
    pexinfo.build_properties = build_properties
    builder = PEXBuilder(distpath, pex_info=pexinfo, interpreter=self.interpreter)

    self.chroot = PythonChroot(
        target,
        root_dir,
        builder=builder,
        interpreter=self.interpreter,
        conn_timeout=conn_timeout)

  def run(self):
    print('Building PythonBinary %s:' % self.target)
    env = self.chroot.dump()
    filename = os.path.join(self.distdir, '%s.pex' % self.target.name)
    env.build(filename)
    print('Wrote %s' % filename)
    return 0

########NEW FILE########
__FILENAME__ = code_generator
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import shutil
import tempfile
import textwrap

from twitter.common.dirutil.chroot import RelativeChroot
from twitter.pants.python.sdist_builder import SdistBuilder


class CodeGenerator(object):
  class Error(Exception): pass
  class CodeGenerationException(Error): pass

  def __init__(self, target, root_dir, config, target_suffix=None):
    self.target = target
    self.config = config
    self.suffix = target_suffix or ''
    self.root = root_dir
    distdir = self.config.getdefault('pants_distdir')
    self.chroot = RelativeChroot(root_dir, distdir, target.name)
    codegen_root = tempfile.mkdtemp(dir=self.chroot.path(), prefix='codegen.')
    self.codegen_root = os.path.relpath(codegen_root, self.chroot.path())
    self.created_packages = set()
    self.created_namespace_packages = set()

  def __del__(self):
    self.cleanup()

  def cleanup(self):
    shutil.rmtree(self.chroot.path())

  @staticmethod
  def path_to_module(path):
    return path.replace(os.path.sep, '.')

  def package_name(self):
    return '%s%s' % (self.target.id, self.suffix)

  def requirement_string(self):
    return '%s==0.0.0' % self.package_name()

  @property
  def package_dir(self):
    """Return the code generation root."""
    return "."

  @property
  def install_requires(self):
    return []

  def generate(self):
    """Generate code for this target, updating the sets .created_packages and
       .created_namespace_packages."""
    raise NotImplementedError

  def dump_setup_py(self):
    boilerplate = textwrap.dedent("""
      from setuptools import setup

      setup(name        = "%(package_name)s",
            version     = "0.0.0",
            description = "autogenerated code for %(target_name)s",
            install_requires = %(install_requires)r,
            package_dir = { "": %(package_dir)r },
            packages    = %(packages)s,
            namespace_packages = %(namespace_packages)s)
    """)
    boilerplate = boilerplate % {
      'package_name': self.package_name(),
      'package_dir': self.package_dir,
      'target_name': self.target.name,
      'install_requires': self.install_requires,
      'packages': repr(self.created_packages),
      'namespace_packages': repr(list(self.created_namespace_packages))
    }
    self.chroot.write(boilerplate.encode('utf8'), os.path.join(self.codegen_root, 'setup.py'))
    self.chroot.write("include *.py".encode('utf8'), os.path.join(self.codegen_root, 'MANIFEST.in'))

  @property
  def sdist_root(self):
    return os.path.join(self.chroot.path(), self.codegen_root)

  @property
  def package_root(self):
    return os.path.join(self.sdist_root, self.package_dir)

  def build(self, interpreter=None):
    self.generate()
    self.dump_setup_py()
    return SdistBuilder.build(self.sdist_root, self.target, interpreter=interpreter)

########NEW FILE########
__FILENAME__ = interpreter_cache
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import shutil

from twitter.common.dirutil import safe_mkdir
from twitter.common.python.package import EggPackage, SourcePackage
from twitter.common.python.installer import EggInstaller
from twitter.common.python.interpreter import (
    PythonCapability,
    PythonIdentity,
    PythonInterpreter,
)
from twitter.common.python.obtainer import Obtainer

from .python_setup import PythonSetup
from .resolver import crawler_from_config, fetchers_from_config

from pkg_resources import Requirement


# TODO(wickman) Create a safer version of this and add to twitter.common.dirutil
def safe_link(src, dst):
  try:
    os.unlink(dst)
  except OSError:
    pass
  os.symlink(src, dst)


def resolve_interpreter(config, interpreter, requirement, logger=print):
  """Given a :class:`PythonInterpreter` and :class:`Config`, and a requirement,
     return an interpreter with the capability of resolving that requirement or
     None if it's not possible to install a suitable requirement."""
  interpreter_cache = PythonInterpreterCache.cache_dir(config)
  interpreter_dir = os.path.join(interpreter_cache, str(interpreter.identity))
  if interpreter.satisfies(PythonCapability([requirement])):
    return interpreter
  def installer_provider(sdist):
    return EggInstaller(sdist, strict=requirement.key != 'setuptools', interpreter=interpreter)
  egg = resolve_and_link(
      config,
      requirement,
      os.path.join(interpreter_dir, requirement.key),
      installer_provider,
      logger=logger)
  if egg:
    return interpreter.with_extra(egg.name, egg.raw_version, egg.url)
  else:
    logger('Failed to resolve requirement %s for %s' % (requirement, interpreter))


def resolve_and_link(config, requirement, target_link, installer_provider, logger=print):
  if os.path.exists(target_link) and os.path.exists(os.path.realpath(target_link)):
    egg = EggPackage(os.path.realpath(target_link))
    if egg.satisfies(requirement):
      return egg
  fetchers = fetchers_from_config(config)
  crawler = crawler_from_config(config)
  obtainer = Obtainer(crawler, fetchers, [])
  obtainer_iterator = obtainer.iter(requirement)
  links = [link for link in obtainer_iterator if isinstance(link, SourcePackage)]
  for link in links:
    logger('    fetching %s' % link.url)
    sdist = link.fetch()
    logger('    installing %s' % sdist)
    installer = installer_provider(sdist)
    dist_location = installer.bdist()
    target_location = os.path.join(os.path.dirname(target_link), os.path.basename(dist_location))
    shutil.move(dist_location, target_location)
    safe_link(target_location, target_link)
    logger('    installed %s' % target_location)
    return EggPackage(target_location)


# This is a setuptools <1 and >1 compatible version of Requirement.parse.
# For setuptools <1, if you did Requirement.parse('setuptools'), it would
# return 'distribute' which of course is not desirable for us.  So they
# added a replacement=False keyword arg.  Sadly, they removed this keyword
# arg in setuptools >= 1 so we have to simply failover using TypeError as a
# catch for 'Invalid Keyword Argument'.
def failsafe_parse(requirement):
  try:
    return Requirement.parse(requirement, replacement=False)
  except TypeError:
    return Requirement.parse(requirement)


def resolve(config, interpreter, logger=print):
  """Resolve and cache an interpreter with a setuptools and wheel capability."""

  setuptools_requirement = failsafe_parse(
      'setuptools==%s' % config.get('python-setup', 'setuptools_version', default='2.2'))
  wheel_requirement = failsafe_parse(
      'wheel==%s' % config.get('python-setup', 'wheel_version', default='0.22.0'))

  interpreter = resolve_interpreter(config, interpreter, setuptools_requirement, logger=logger)
  if interpreter:
    return resolve_interpreter(config, interpreter, wheel_requirement, logger=logger)


class PythonInterpreterCache(object):
  @staticmethod
  def cache_dir(config):
    return PythonSetup(config).scratch_dir('interpreter_cache', default_name='interpreters')

  @classmethod
  def select_interpreter(cls, compatibilities, allow_multiple=False):
    if allow_multiple:
      return compatibilities
    return [min(compatibilities)] if compatibilities else []

  def __init__(self, config, logger=None):
    self._path = self.cache_dir(config)
    self._config = config
    safe_mkdir(self._path)
    self._interpreters = set()
    self._logger = logger or (lambda msg: True)

  @property
  def interpreters(self):
    return self._interpreters

  def interpreter_from_path(self, path):
    interpreter_dir = os.path.basename(path)
    identity = PythonIdentity.from_path(interpreter_dir)
    try:
      executable = os.readlink(os.path.join(path, 'python'))
    except OSError:
      return None
    interpreter = PythonInterpreter(executable, identity)
    return resolve(self._config, interpreter, logger=self._logger)

  def setup_interpreter(self, interpreter):
    interpreter_dir = os.path.join(self._path, str(interpreter.identity))
    safe_mkdir(interpreter_dir)
    safe_link(interpreter.binary, os.path.join(interpreter_dir, 'python'))
    return resolve(self._config, interpreter, logger=self._logger)

  def setup_cached(self):
    for interpreter_dir in os.listdir(self._path):
      path = os.path.join(self._path, interpreter_dir)
      pi = self.interpreter_from_path(path)
      if pi:
        self._logger('Detected interpreter %s: %s' % (pi.binary, str(pi.identity)))
        self._interpreters.add(pi)

  def setup_paths(self, paths):
    for interpreter in PythonInterpreter.all(paths):
      identity_str = str(interpreter.identity)
      path = os.path.join(self._path, identity_str)
      pi = self.interpreter_from_path(path)
      if pi is None:
        self.setup_interpreter(interpreter)
        pi = self.interpreter_from_path(path)
        if pi is None:
          continue
      self._interpreters.add(pi)

  def matches(self, filters):
    for interpreter in self._interpreters:
      if any(interpreter.identity.matches(filt) for filt in filters):
        yield interpreter

  def setup(self, paths=(), force=False, filters=('',)):
    has_setup = False
    setup_paths = paths or os.getenv('PATH').split(os.pathsep)
    self.setup_cached()
    if force:
      has_setup = True
      self.setup_paths(setup_paths)
    matches = list(self.matches(filters))
    if len(matches) == 0 and not has_setup:
      self.setup_paths(setup_paths)
      matches = list(self.matches(filters))
    if len(matches) == 0:
      self._logger('Found no valid interpreters!')
    return matches

########NEW FILE########
__FILENAME__ = python_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.python.interpreter import PythonInterpreter

from twitter.pants.targets.python_binary import PythonBinary
from twitter.pants.targets.python_tests import PythonTests, PythonTestSuite

from .binary_builder import PythonBinaryBuilder
from .test_builder import PythonTestBuilder


class PythonBuilder(object):
  def __init__(self, run_tracker, root_dir):
    self._root_dir = root_dir
    self._run_tracker = run_tracker

  def build(self, targets, args, interpreter=None, conn_timeout=None):
    test_targets = []
    binary_targets = []
    interpreter = interpreter or PythonInterpreter.get()

    for target in targets:
      assert target.is_python, "PythonBuilder can only build PythonTargets, given %s" % str(target)

    # PythonBuilder supports PythonTests and PythonBinaries
    for target in targets:
      if isinstance(target, PythonTests) or isinstance(target, PythonTestSuite):
        test_targets.append(target)
      elif isinstance(target, PythonBinary):
        binary_targets.append(target)

    rv = PythonTestBuilder(
        test_targets,
        args,
        self._root_dir,
        interpreter=interpreter,
        conn_timeout=conn_timeout).run()
    if rv != 0:
      return rv

    for binary_target in binary_targets:
      rv = PythonBinaryBuilder(
          binary_target,
          self._root_dir,
          self._run_tracker,
          interpreter=interpreter,
          conn_timeout=conn_timeout).run()
      if rv != 0:
        return rv

    return 0

########NEW FILE########
__FILENAME__ = python_chroot
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

from collections import defaultdict
import os
import random
import shutil
import sys
import tempfile

from twitter.common.collections import OrderedSet
from twitter.common.dirutil import safe_mkdir, safe_rmtree
from twitter.common.python.interpreter import PythonInterpreter
from twitter.common.python.pex_builder import PEXBuilder
from twitter.common.python.platforms import Platform
from twitter.pants.base.build_invalidator import BuildInvalidator, CacheKeyGenerator
from twitter.pants.base.config import Config
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.targets.python_antlr_library import PythonAntlrLibrary
from twitter.pants.targets.python_binary import PythonBinary
from twitter.pants.targets.python_library import PythonLibrary
from twitter.pants.targets.python_requirement import PythonRequirement
from twitter.pants.targets.python_tests import PythonTests
from twitter.pants.targets.python_thrift_library import PythonThriftLibrary

from .antlr_builder import PythonAntlrBuilder
from .python_setup import PythonSetup
from .resolver import resolve_multi
from .thrift_builder import PythonThriftBuilder


class PythonChroot(object):
  _VALID_DEPENDENCIES = {
    PythonLibrary: 'libraries',
    PythonRequirement: 'reqs',
    PythonBinary: 'binaries',
    PythonThriftLibrary: 'thrifts',
    PythonAntlrLibrary: 'antlrs',
    PythonTests: 'tests'
  }

  MEMOIZED_THRIFTS = {}

  class InvalidDependencyException(Exception):
    def __init__(self, target):
      Exception.__init__(self, "Not a valid Python dependency! Found: %s" % target)

  def __init__(self,
               target,
               root_dir,
               extra_targets=None,
               builder=None,
               platforms=None,
               interpreter=None,
               conn_timeout=None):
    self._config = Config.load()
    self._target = target
    self._root = root_dir
    self._platforms = platforms
    self._interpreter = interpreter or PythonInterpreter.get()
    self._extra_targets = list(extra_targets) if extra_targets is not None else []
    self._builder = builder or PEXBuilder(tempfile.mkdtemp(), interpreter=self._interpreter)

    # Note: unrelated to the general pants artifact cache.
    self._egg_cache_root = os.path.join(
        PythonSetup(self._config).scratch_dir('artifact_cache', default_name='artifacts'),
        str(self._interpreter.identity))

    self._key_generator = CacheKeyGenerator()
    self._build_invalidator = BuildInvalidator( self._egg_cache_root)


  def __del__(self):
    if os.getenv('PANTS_LEAVE_CHROOT') is None:
      safe_rmtree(self.path())
    else:
      self.debug('Left chroot at %s' % self.path())

  @property
  def builder(self):
    return self._builder

  def debug(self, msg, indent=0):
    if os.getenv('PANTS_VERBOSE') is not None:
      print('%s%s' % (' ' * indent, msg))

  def path(self):
    return self._builder.path()

  def _dump_library(self, library):
    def copy_to_chroot(base, path, add_function):
      src = os.path.join(self._root, base, path)
      add_function(src, path)

    self.debug('  Dumping library: %s' % library)
    for filename in library.sources:
      copy_to_chroot(library.target_base, filename, self._builder.add_source)
    for filename in library.resources:
      copy_to_chroot(library.target_base, filename, self._builder.add_resource)

  def _dump_requirement(self, req, dynamic, repo):
    self.debug('  Dumping requirement: %s%s%s' % (str(req),
      ' (dynamic)' if dynamic else '', ' (repo: %s)' % repo if repo else ''))
    self._builder.add_requirement(req, dynamic, repo)

  def _dump_distribution(self, dist):
    self.debug('  Dumping distribution: .../%s' % os.path.basename(dist.location))
    self._builder.add_distribution(dist)

  def _generate_requirement(self, library, builder_cls):
    library_key = self._key_generator.key_for_target(library)
    builder = builder_cls(library, self._root, self._config, '-' + library_key.hash[:8])

    cache_dir = os.path.join(self._egg_cache_root, library_key.id)
    if self._build_invalidator.needs_update(library_key):
      sdist = builder.build(interpreter=self._interpreter)
      safe_mkdir(cache_dir)
      shutil.copy(sdist, os.path.join(cache_dir, os.path.basename(sdist)))
      self._build_invalidator.update(library_key)

    with ParseContext.temp():
      return PythonRequirement(builder.requirement_string(), repository=cache_dir, use_2to3=True)

  def _generate_thrift_requirement(self, library):
    return self._generate_requirement(library, PythonThriftBuilder)

  def _generate_antlr_requirement(self, library):
    return self._generate_requirement(library, PythonAntlrBuilder)

  def resolve(self, targets):
    children = defaultdict(OrderedSet)
    def add_dep(trg):
      for target_type, target_key in self._VALID_DEPENDENCIES.items():
        if isinstance(trg, target_type):
          children[target_key].add(trg)
          return
      raise self.InvalidDependencyException(trg)
    for target in targets:
      target.walk(add_dep)
    return children

  def dump(self):
    self.debug('Building PythonBinary %s:' % self._target)

    targets = self.resolve([self._target] + self._extra_targets)

    for lib in targets['libraries'] | targets['binaries']:
      self._dump_library(lib)

    generated_reqs = OrderedSet()
    if targets['thrifts']:
      for thr in set(targets['thrifts']):
        if thr not in self.MEMOIZED_THRIFTS:
          self.MEMOIZED_THRIFTS[thr] = self._generate_thrift_requirement(thr)
        generated_reqs.add(self.MEMOIZED_THRIFTS[thr])
      with ParseContext.temp():
        # trick pants into letting us add this python requirement, otherwise we get
        # TargetDefinitionException: Error in target BUILD.temp:thrift: duplicate to
        # PythonRequirement(thrift)
        #
        # TODO(wickman) Instead of just blindly adding a PythonRequirement for thrift, we
        # should first detect if any explicit thrift requirements have been added and use
        # those.  Only if they have not been supplied should we auto-inject it.
        generated_reqs.add(PythonRequirement('thrift', use_2to3=True,
            name='thrift-' + ''.join(random.sample('0123456789abcdef' * 8, 8))))

    for antlr in targets['antlrs']:
      generated_reqs.add(self._generate_antlr_requirement(antlr))

    targets['reqs'] |= generated_reqs
    reqs_to_build = OrderedSet()
    for req in targets['reqs']:
      if not req.should_build(self._interpreter.python, Platform.current()):
        self.debug('Skipping %s based upon version filter' % req)
        continue
      reqs_to_build.add(req)
      self._dump_requirement(req._requirement, False, req._repository)

    platforms = self._platforms
    if isinstance(self._target, PythonBinary):
      platforms = self._target.platforms
    distributions = resolve_multi(
         self._config,
         reqs_to_build,
         interpreter=self._interpreter,
         platforms=platforms)

    locations = set()
    for platform, dist_set in distributions.items():
      for dist in dist_set:
        if dist.location not in locations:
          self._dump_distribution(dist)
        locations.add(dist.location)

    if len(targets['binaries']) > 1:
      print('WARNING: Target has multiple python_binary targets!', file=sys.stderr)

    return self._builder

########NEW FILE########
__FILENAME__ = resolver
from __future__ import print_function

import os
import time

from twitter.common.dirutil import touch
from twitter.common.python.base import requirement_is_exact
from twitter.common.python.fetcher import Fetcher, PyPIFetcher
from twitter.common.python.http import Crawler
from twitter.common.python.obtainer import Obtainer, CachingObtainer
from twitter.common.python.interpreter import PythonInterpreter
from twitter.common.python.package import distribution_compatible
from twitter.common.python.platforms import Platform
from twitter.common.python.resolver import resolve
from twitter.common.python.translator import (
    ChainedTranslator,
    EggTranslator,
    SourceTranslator,
    Translator
)

from .python_setup import PythonSetup

from pkg_resources import (
    Environment,
    WorkingSet,
)


def get_platforms(platform_list):
  def translate(platform):
    return Platform.current() if platform == 'current' else platform
  return tuple(set(map(translate, platform_list)))


def fetchers_from_config(config):
  fetchers = []
  fetchers.extend(Fetcher([url]) for url in config.getlist('python-repos', 'repos', []))
  fetchers.extend(PyPIFetcher(url) for url in config.getlist('python-repos', 'indices', []))
  return fetchers


def crawler_from_config(config, conn_timeout=None):
  download_cache = PythonSetup(config).scratch_dir('download_cache', default_name='downloads')
  return Crawler(cache=download_cache, conn_timeout=conn_timeout)


class PantsObtainer(CachingObtainer):
  def iter(self, requirement):
    if hasattr(requirement, 'repository') and requirement.repository:
      obtainer = Obtainer(
          crawler=self._crawler,
          fetchers=[Fetcher([requirement.repository])],
          translators=self._translator)
      for package in obtainer.iter(requirement):
        yield package
    else:
      for package in super(PantsObtainer, self).iter(requirement):
        yield package


def resolve_multi(config,
                  requirements,
                  interpreter=None,
                  platforms=None,
                  conn_timeout=None,
                  ttl=3600):
  """Multi-platform dependency resolution for PEX files.

     Given a pants configuration and a set of requirements, return a list of distributions
     that must be included in order to satisfy them.  That may involve distributions for
     multiple platforms.

     :param config: Pants :class:`Config` object.
     :param requirements: A list of :class:`PythonRequirement` objects to resolve.
     :param interpreter: :class:`PythonInterpreter` for which requirements should be resolved.
                         If None specified, defaults to current interpreter.
     :param platforms: Optional list of platforms against requirements will be resolved. If
                         None specified, the defaults from `config` will be used.
     :param conn_timeout: Optional connection timeout for any remote fetching.
     :param ttl: Time in seconds before we consider re-resolving an open-ended requirement, e.g.
                 "flask>=0.2" if a matching distribution is available on disk.  Defaults
                 to 3600.
  """
  distributions = dict()
  interpreter = interpreter or PythonInterpreter.get()
  if not isinstance(interpreter, PythonInterpreter):
    raise TypeError('Expected interpreter to be a PythonInterpreter, got %s' % type(interpreter))

  install_cache = PythonSetup(config).scratch_dir('install_cache', default_name='eggs')
  platforms = get_platforms(platforms or config.getlist('python-setup', 'platforms', ['current']))

  for platform in platforms:
    translator = Translator.default(
        install_cache=install_cache,
        interpreter=interpreter,
        platform=platform,
        conn_timeout=conn_timeout)

    obtainer = PantsObtainer(
        install_cache=install_cache,
        crawler=crawler_from_config(config, conn_timeout=conn_timeout),
        fetchers=fetchers_from_config(config) or [PyPIFetcher()],
        translators=translator)

    distributions[platform] = resolve(requirements=requirements,
                                      obtainer=obtainer,
                                      interpreter=interpreter,
                                      platform=platform)

  return distributions

########NEW FILE########
__FILENAME__ = sdist_builder
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import glob
import os
import subprocess
import sys

from twitter.common.contextutil import pushd
from twitter.common.python.installer import Packager


class SdistBuilder(object):
  """A helper class to run setup.py projects."""

  class Error(Exception): pass
  class SetupError(Error): pass

  @classmethod
  def build(cls, setup_root, target, interpreter=None):
    packager = Packager(setup_root, interpreter=interpreter,
        install_dir=os.path.join(setup_root, 'dist'))
    try:
      return packager.sdist()
    except Packager.InstallFailure as e:
      raise cls.SetupError(str(e))

########NEW FILE########
__FILENAME__ = test_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

__author__ = 'Brian Wickman'

try:
  import configparser
except ImportError:
  import ConfigParser as configparser
import errno
import os
import time
import signal
import sys

from twitter.common.collections import OrderedSet
from twitter.common.contextutil import temporary_file
from twitter.common.dirutil import safe_mkdir
from twitter.common.lang import Compatibility
from twitter.common.quantity import Amount, Time
from twitter.common.python.interpreter import PythonInterpreter
from twitter.common.python.pex import PEX
from twitter.common.python.pex_builder import PEXBuilder

from twitter.pants.base.config import Config
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.python.python_chroot import PythonChroot
from twitter.pants.targets.python_requirement import PythonRequirement
from twitter.pants.targets.python_target import PythonTarget
from twitter.pants.targets.python_tests import PythonTests, PythonTestSuite


class PythonTestResult(object):
  @staticmethod
  def timeout():
    return PythonTestResult('TIMEOUT')

  @staticmethod
  def exception():
    return PythonTestResult('EXCEPTION')

  @staticmethod
  def rc(value):
    return PythonTestResult('SUCCESS' if value == 0 else 'FAILURE',
                            rc=value)

  def __init__(self, msg, rc=None):
    self._rc = rc
    self._msg = msg

  def __str__(self):
    return self._msg

  @property
  def success(self):
    return self._rc == 0


DEFAULT_COVERAGE_CONFIG = """
[run]
branch = True
timid = True

[report]
exclude_lines =
    def __repr__
    raise NotImplementedError

ignore_errors = True
"""

def generate_coverage_config(target):
  cp = configparser.ConfigParser()
  cp.readfp(Compatibility.StringIO(DEFAULT_COVERAGE_CONFIG))
  cp.add_section('html')
  target_dir = os.path.join(Config.load().getdefault('pants_distdir'), 'coverage',
      os.path.dirname(target.address.buildfile.relpath), target.name)
  safe_mkdir(target_dir)
  cp.set('html', 'directory', target_dir)
  return cp


class PythonTestBuilder(object):
  class InvalidDependencyException(Exception): pass
  class ChrootBuildingException(Exception): pass

  TESTING_TARGETS = None

  # TODO(wickman) Expose these as configuratable parameters
  TEST_TIMEOUT = Amount(2, Time.MINUTES)
  TEST_POLL_PERIOD = Amount(100, Time.MILLISECONDS)

  def __init__(self, targets, args, root_dir, interpreter=None, conn_timeout=None):
    self.targets = targets
    self.args = args
    self.root_dir = root_dir
    self.interpreter = interpreter or PythonInterpreter.get()
    self.successes = {}
    self._conn_timeout = conn_timeout

  def run(self):
    self.successes = {}
    rv = self._run_tests(self.targets)
    for target in sorted(self.successes):
      print('%-80s.....%10s' % (target, self.successes[target]))
    return 0 if rv.success else 1

  @classmethod
  def generate_test_targets(cls):
    if cls.TESTING_TARGETS is None:
      with ParseContext.temp():
        cls.TESTING_TARGETS = [
          PythonRequirement('pytest'),
          PythonRequirement('pytest-cov'),
          PythonRequirement('coverage==3.6b1'),
          PythonRequirement('unittest2', version_filter=lambda py, pl: py.startswith('2')),
          PythonRequirement('unittest2py3k', version_filter=lambda py, pl: py.startswith('3'))
        ]
    return cls.TESTING_TARGETS

  @staticmethod
  def generate_junit_args(target):
    args = []
    xml_base = os.getenv('JUNIT_XML_BASE')
    if xml_base:
      xml_base = os.path.abspath(os.path.normpath(xml_base))
      xml_path = os.path.join(
        xml_base, os.path.dirname(target.address.buildfile.relpath), target.name + '.xml')
      try:
        os.makedirs(os.path.dirname(xml_path))
      except OSError as e:
        if e.errno != errno.EEXIST:
          raise PythonTestBuilder.ChrootBuildingException(
            "Unable to establish JUnit target: %s!  %s" % (target, e))
      args.append('--junitxml=%s' % xml_path)
    return args

  @staticmethod
  def cov_setup(target, chroot):
    cp = generate_coverage_config(target)
    with temporary_file(cleanup=False) as fp:
      cp.write(fp)
      filename = fp.name
    if target.coverage:
      source = target.coverage
    else:
      # This technically makes the assumption that tests/python/<target> will be testing
      # src/python/<target>.  To change to honest measurements, do target.walk() here instead,
      # however this results in very useless and noisy coverage reports.
      source = set(os.path.dirname(source).replace(os.sep, '.') for source in target.sources)
    args = ['-p', 'pytest_cov',
            '--cov-config', filename,
            '--cov-report', 'html',
            '--cov-report', 'term']
    for module in source:
      args.extend(['--cov', module])
    return filename, args

  @staticmethod
  def wait_on(popen, timeout=TEST_TIMEOUT):
    total_wait = Amount(0, Time.SECONDS)
    while total_wait < timeout:
      rc = popen.poll()
      if rc is not None:
        return PythonTestResult.rc(rc)
      total_wait += PythonTestBuilder.TEST_POLL_PERIOD
      time.sleep(PythonTestBuilder.TEST_POLL_PERIOD.as_(Time.SECONDS))
    popen.kill()
    return PythonTestResult.timeout()

  def _run_python_test(self, target):
    po = None
    rv = PythonTestResult.exception()
    coverage_rc = None
    coverage_enabled = 'PANTS_PY_COVERAGE' in os.environ

    try:
      builder = PEXBuilder(interpreter=self.interpreter)
      builder.info.entry_point = target.entry_point
      builder.info.ignore_errors = target._soft_dependencies
      chroot = PythonChroot(
          target,
          self.root_dir,
          extra_targets=self.generate_test_targets(),
          builder=builder,
          platforms=('current',),
          interpreter=self.interpreter,
          conn_timeout=self._conn_timeout)
      builder = chroot.dump()
      builder.freeze()
      test_args = PythonTestBuilder.generate_junit_args(target)
      test_args.extend(self.args)
      if coverage_enabled:
        coverage_rc, args = self.cov_setup(target, builder.chroot())
        test_args.extend(args)
      sources = [os.path.join(target.target_base, source) for source in target.sources]
      po = PEX(builder.path(), interpreter=self.interpreter).run(
          args=test_args + sources, blocking=False, setsid=True)
      # TODO(wickman)  If coverage is enabled, write an intermediate .html that points to
      # each of the coverage reports generated and webbrowser.open to that page.
      rv = PythonTestBuilder.wait_on(po, timeout=target.timeout)
    except Exception as e:
      import traceback
      print('Failed to run test!', file=sys.stderr)
      traceback.print_exc()
      rv = PythonTestResult.exception()
    finally:
      if coverage_rc:
        os.unlink(coverage_rc)
      if po and po.returncode != 0:
        try:
          os.killpg(po.pid, signal.SIGTERM)
        except OSError as e:
          if e.errno == errno.EPERM:
            print("Unable to kill process group: %d" % po.pid)
          elif e.errno != errno.ESRCH:
            rv = PythonTestResult.exception()
    self.successes[target._create_id()] = rv
    return rv

  def _run_python_test_suite(self, target, fail_hard=True):
    tests = OrderedSet([])
    def _gather_deps(trg):
      if isinstance(trg, PythonTests):
        tests.add(trg)
      elif isinstance(trg, PythonTestSuite):
        for dependency in trg.dependencies:
          for dep in dependency.resolve():
            _gather_deps(dep)
    _gather_deps(target)

    failed = False
    for test in tests:
      rv = self._run_python_test(test)
      if not rv.success:
        failed = True
        if fail_hard:
          return rv
    return PythonTestResult.rc(1 if failed else 0)

  def _run_tests(self, targets):
    fail_hard = 'PANTS_PYTHON_TEST_FAILSOFT' not in os.environ
    if 'PANTS_PY_COVERAGE' in os.environ:
      # Coverage often throws errors despite tests succeeding, so make PANTS_PY_COVERAGE
      # force FAILSOFT.
      fail_hard = False
    failed = False
    for target in targets:
      if isinstance(target, PythonTests):
        rv = self._run_python_test(target)
      elif isinstance(target, PythonTestSuite):
        rv = self._run_python_test_suite(target, fail_hard)
      else:
        raise PythonTestBuilder.InvalidDependencyException(
          "Invalid dependency in python test target: %s" % target)
      if not rv.success:
        failed = True
        if fail_hard:
          return rv
    return PythonTestResult.rc(1 if failed else 0)

########NEW FILE########
__FILENAME__ = thrift_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import functools
import keyword
import os
import re
import shutil
import subprocess
import sys

from twitter.common.dirutil import safe_mkdir

from twitter.pants.python.code_generator import CodeGenerator
from twitter.pants.targets.python_thrift_library import PythonThriftLibrary
from twitter.pants.thrift_util import select_thrift_binary


class PythonThriftBuilder(CodeGenerator):
  """Code Generator a  Python code from thrift IDL files."""
  class UnknownPlatformException(CodeGenerator.Error):
    def __init__(self, platform):
      super(PythonThriftBuilder.UnknownPlatformException, self).__init__(
          "Unknown platform: %s!" % str(platform))

  def __init__(self, target, root_dir, config, target_suffix=None):
    super(PythonThriftBuilder, self).__init__(target, root_dir, config, target_suffix=target_suffix)
    self._workdir = os.path.join(config.getdefault(option='thrift_workdir'), 'py-thrift')

  @property
  def install_requires(self):
    return ['thrift']

  def run_thrifts(self):
    """
    Generate Python thrift code using thrift compiler specified in pants config.

    Thrift fields conflicting with Python keywords are suffixed with a trailing
    underscore (e.g.: from_).
    """

    def is_py_thrift(target):
      return isinstance(target, PythonThriftLibrary)

    all_thrifts = set()

    def collect_sources(target):
      for source in target.sources:
        all_thrifts.add((target.target_base, source))

    self.target.walk(collect_sources, predicate=is_py_thrift)

    copied_sources = set()
    for base, relative_source in all_thrifts:
      abs_source = os.path.join(base, relative_source)
      copied_source = os.path.join(self._workdir, relative_source)

      safe_mkdir(os.path.dirname(copied_source))
      shutil.copyfile(abs_source, copied_source)
      copied_sources.add(self._modify_thrift(copied_source))

    for src in copied_sources:
      if not self._run_thrift(src):
        raise PythonThriftBuilder.CodeGenerationException("Could not generate .py from %s!" % src)

  def _run_thrift(self, source):
    args = [
        select_thrift_binary(self.config),
        '--gen',
        'py:new_style',
        '-o', self.codegen_root,
        '-I', self._workdir,
        os.path.abspath(source)]

    po = subprocess.Popen(args, cwd=self.chroot.path())
    rv = po.wait()
    if rv != 0:
      comm = po.communicate()
      print('thrift generation failed!', file=sys.stderr)
      print('STDOUT', file=sys.stderr)
      print(comm[0], file=sys.stderr)
      print('STDERR', file=sys.stderr)
      print(comm[1], file=sys.stderr)
    return rv == 0

  def _modify_thrift(self, source):
    """
    Replaces the python keywords in the thrift file

    Find all python keywords in each thrift file and appends a trailing underscore.
    For example, 'from' will be converted to 'from_'.
    """
    rewrites = []
    renames = dict((kw, '%s_' % kw) for kw in keyword.kwlist)
    token_regex = re.compile(r'(\W)(%s)(\W)' % '|'.join(renames.keys()), re.MULTILINE)

    def token_replace(match):
      return '%s%s%s' % (match.group(1), renames[match.group(2)], match.group(3))

    def replace_tokens(contents):
      return token_regex.sub(token_replace, contents)

    rewrites.append(replace_tokens)
    with open(source) as contents:
      modified = functools.reduce(lambda txt, rewrite: rewrite(txt), rewrites, contents.read())
      contents.close()
      with open(source, 'w') as thrift:
        thrift.write(modified)
    return source

  @property
  def package_dir(self):
    return "gen-py"

  def generate(self):
    # auto-generate the python files that we bundle up
    self.run_thrifts()

    # Thrift generates code with all parent namespaces with empty __init__.py's. Generally
    # speaking we want to drop anything w/o an __init__.py, and for anything with an __init__.py,
    # we want to explicitly make it a namespace package, hence the hoops here.
    for root, _, files in os.walk(os.path.normpath(self.package_root)):
      reldir = os.path.relpath(root, self.package_root)
      if reldir == '.':  # skip root
        continue
      if '__init__.py' not in files:  # skip non-packages
        continue
      init_py_abspath = os.path.join(root, '__init__.py')
      module_path = self.path_to_module(reldir)
      self.created_packages.add(module_path)
      if os.path.getsize(init_py_abspath) == 0:  # empty __init__, translate to namespace package
        with open(init_py_abspath, 'wb') as f:
          f.write(b"__import__('pkg_resources').declare_namespace(__name__)")
        self.created_namespace_packages.add(module_path)
      else:
        # non-empty __init__, this is a leaf package, usually with ttypes and constants, leave as-is
        pass

    if not self.created_packages:
      raise self.CodeGenerationException('No Thrift structures declared in %s!' % self.target)

########NEW FILE########
__FILENAME__ = html_reporter
import cgi
import os
import re
import uuid

from collections import namedtuple, defaultdict
from pystache.renderer import Renderer

from twitter.common.dirutil import safe_mkdir
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.mustache import MustacheRenderer
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.reporting.linkify import linkify
from twitter.pants.reporting.report import Report
from twitter.pants.reporting.reporter import Reporter
from twitter.pants.reporting.reporting_utils import items_to_report_element


class HtmlReporter(Reporter):
  """HTML reporting to files.

  The files are intended to be served by the ReportingServer,
  not accessed directly from the filesystem.
  """

  # HTML reporting settings.
  #   html_dir: Where the report files go.
  #   template_dir: Where to find mustache templates.
  Settings = namedtuple('Settings', Reporter.Settings._fields + ('html_dir', 'template_dir'))

  def __init__(self, run_tracker, settings):
    Reporter.__init__(self, run_tracker, settings)
     # The main report, and associated tool outputs, go under this dir.
    self._html_dir = settings.html_dir

    # We render HTML from mustache templates.
    self._renderer = MustacheRenderer(settings.template_dir, __name__)

    # We serve files relative to the build root.
    self._buildroot = get_buildroot()
    self._html_path_base = os.path.relpath(self._html_dir, self._buildroot)

    # We write the main report body to this file object.
    self._report_file = None

    # We redirect stdout, stderr etc. of tool invocations to these files.
    self._output_files = defaultdict(dict)  # workunit_id -> {path -> fileobj}.

  def report_path(self):
    """The path to the main report file."""
    return os.path.join(self._html_dir, 'build.html')

  def open(self):
    """Implementation of Reporter callback."""
    safe_mkdir(os.path.dirname(self._html_dir))
    self._report_file = open(self.report_path(), 'w')

  def close(self):
    """Implementation of Reporter callback."""
    self._report_file.close()
    # Make sure everything's closed.
    for files in self._output_files.values():
      for f in files.values():
        f.close()

  def start_workunit(self, workunit):
    """Implementation of Reporter callback."""
    # We use these properties of the workunit to decide how to render information about it.
    is_bootstrap = workunit.has_label(WorkUnit.BOOTSTRAP)
    is_tool = workunit.has_label(WorkUnit.TOOL)
    is_multitool = workunit.has_label(WorkUnit.MULTITOOL)
    is_test = workunit.has_label(WorkUnit.TEST)

    # Get useful properties from the workunit.
    workunit_dict = workunit.to_dict()
    if workunit_dict['cmd']:
      workunit_dict['cmd'] = linkify(self._buildroot, workunit_dict['cmd'].replace('$', '\\\\$'))

    # Create the template arguments.
    args = { 'indent': len(workunit.ancestors()) * 10,
             'html_path_base': self._html_path_base,
             'workunit': workunit_dict,
             'header_text': workunit.name,
             'initially_open': is_test or not (is_bootstrap or is_tool or is_multitool),
             'is_tool': is_tool,
             'is_multitool': is_multitool }
    args.update({ 'collapsible': lambda x: self._renderer.render_callable('collapsible', x, args) })

    # Render the workunit's div.
    s = self._renderer.render_name('workunit_start', args)

    if is_tool:
      # This workunit is a tool invocation, so render the appropriate content.
      # We use the same args, slightly modified.
      del args['initially_open']
      if is_test:
        # Have test framework stdout open by default, but not that of other tools.
        # This is an arbitrary choice, but one that turns out to be useful to users in practice.
        args['stdout_initially_open'] = True
      s += self._renderer.render_name('tool_invocation_start', args)

    # ... and we're done.
    self._emit(s)

  # CSS classes from pants.css that we use to style the header text to reflect the outcome.
  _outcome_css_classes = ['aborted', 'failure', 'warning', 'success', 'unknown']

  def end_workunit(self, workunit):
    """Implementation of Reporter callback."""
    # Create the template arguments.
    duration = workunit.duration()
    timing = '%.3f' % duration
    unaccounted_time = None
    # Background work may be idle a lot, no point in reporting that as unaccounted.
    if self.is_under_main_root(workunit):
      unaccounted_time_secs = workunit.unaccounted_time()
      if unaccounted_time_secs >= 1 and unaccounted_time_secs > 0.05 * duration:
        unaccounted_time = '%.3f' % unaccounted_time_secs
    args = { 'workunit': workunit.to_dict(),
             'status': workunit.choose(*HtmlReporter._outcome_css_classes),
             'timing': timing,
             'unaccounted_time': unaccounted_time,
             'aborted': workunit.outcome() == WorkUnit.ABORTED }

    s = ''
    if workunit.has_label(WorkUnit.TOOL):
      s += self._renderer.render_name('tool_invocation_end', args)
    s += self._renderer.render_name('workunit_end', args)
    self._emit(s)

    # Update the timings.
    def render_timings(timings):
      timings_dict = timings.get_all()
      for item in timings_dict:
        item['timing_string'] = '%.3f' % item['timing']
      args = {
        'timings': timings_dict
      }
      return self._renderer.render_name('aggregated_timings', args)

    self._overwrite('cumulative_timings', render_timings(self.run_tracker.cumulative_timings))
    self._overwrite('self_timings', render_timings(self.run_tracker.self_timings))

    # Update the artifact cache stats.
    def render_cache_stats(artifact_cache_stats):
      def fix_detail_id(e, _id):
        return e if isinstance(e, basestring) else e + (_id, )

      msg_elements = []
      for cache_name, stat in artifact_cache_stats.stats_per_cache.items():
        msg_elements.extend([
          cache_name + ' artifact cache: ',
          # Explicitly set the detail ids, so their displayed/hidden state survives a refresh.
          fix_detail_id(items_to_report_element(stat.hit_targets, 'hit'), 'cache-hit-details'),
          ', ',
          fix_detail_id(items_to_report_element(stat.miss_targets, 'miss'), 'cache-miss-details'),
          '.'
        ])
      if not msg_elements:
        msg_elements = ['No artifact cache use.']
      return self._render_message(*msg_elements)

    self._overwrite('artifact_cache_stats',
                    render_cache_stats(self.run_tracker.artifact_cache_stats))

    for f in self._output_files[workunit.id].values():
      f.close()

  def handle_output(self, workunit, label, s):
    """Implementation of Reporter callback."""
    if os.path.exists(self._html_dir):  # Make sure we're not immediately after a clean-all.
      path = os.path.join(self._html_dir, '%s.%s' % (workunit.id, label))
      output_files = self._output_files[workunit.id]
      if path not in output_files:
        f = open(path, 'w')
        output_files[path] = f
      else:
        f = output_files[path]
      f.write(self._htmlify_text(s))
      # We must flush in the same thread as the write.
      f.flush()

  _log_level_css_map = {
    Report.FATAL: 'fatal',
    Report.ERROR: 'error',
    Report.WARN:  'warn',
    Report.INFO:  'info',
    Report.DEBUG: 'debug'
  }
  def do_handle_log(self, workunit, level, *msg_elements):
    """Implementation of Reporter callback."""
    content = '<span class="%s">%s</span>' % \
              (HtmlReporter._log_level_css_map[level], self._render_message(*msg_elements))

    # Generate some javascript that appends the content to the workunit's div.
    args = {
      'content_id': uuid.uuid4(),  # Identifies this content.
      'workunit_id': workunit.id,  # The workunit this reporting content belongs to.
      'content': content,  # The content to append.
      }
    s = self._renderer.render_name('append_to_workunit', args)

    # Emit that javascript to the main report body.
    self._emit(s)

  def _render_message(self, *msg_elements):
    elements = []
    detail_ids = []
    for element in msg_elements:
      # Each element can be a message or a (message, detail) pair, as received by handle_log().
      #
      # However, as an internal implementation detail, we also allow an element to be a tuple
      # (message, detail, detail_initially_visible[, detail_id])
      #
      # - If the detail exists, clicking on the text will toggle display of the detail and close
      #   all other details in this message.
      # - If detail_initially_visible is True, the detail will be displayed by default.
      #
      # Toggling is managed via detail_ids: when clicking on a detail, it closes all details
      # in this message with detail_ids different than that of the one being clicked on.
      # We allow detail_id to be explicitly specified, so that the open/closed state can be
      # preserved through refreshes. For example, when looking at the artifact cache stats,
      # if "hits" are open and "misses" are closed, we want to remember that even after
      # the cache stats are updated and the message re-rendered.
      if isinstance(element, basestring):
        element = [element]
      defaults = ('', None, None, False)
      # Map assumes None for missing values, so this will pick the default for those.
      (text, detail, detail_id, detail_initially_visible) = \
        map(lambda x, y: x or y, element, defaults)
      element_args = {'text': self._htmlify_text(text) }
      if detail is not None:
        detail_id = detail_id or uuid.uuid4()
        detail_ids.append(detail_id)
        element_args.update({
          'detail': self._htmlify_text(detail),
          'detail_initially_visible': detail_initially_visible,
          'detail-id': detail_id
        })
      elements.append(element_args)
    args = { 'elements': elements,
             'all-detail-ids': detail_ids }
    return self._renderer.render_name('message', args)

  def _emit(self, s):
    """Append content to the main report file."""
    if os.path.exists(self._html_dir):  # Make sure we're not immediately after a clean-all.
      self._report_file.write(s)
      self._report_file.flush()  # We must flush in the same thread as the write.

  def _overwrite(self, filename, s):
    """Overwrite a file with the specified contents."""
    if os.path.exists(self._html_dir):  # Make sure we're not immediately after a clean-all.
      with open(os.path.join(self._html_dir, filename), 'w') as f:
        f.write(s)

  def _htmlify_text(self, s):
    """Make text HTML-friendly."""
    colored = self._handle_ansi_color_codes(cgi.escape(str(s)))
    return linkify(self._buildroot, colored).replace('\n', '</br>')

  _ANSI_COLOR_CODE_RE = re.compile(r'\033\[((\d|;)*)m')
  def _handle_ansi_color_codes(self, s):
    """Replace ansi color sequences with spans of appropriately named css classes."""
    def ansi_code_to_css(code):
      return ' '.join(['ansi-%s' % c for c in code.split(';')])
    return '<span>' +\
           HtmlReporter._ANSI_COLOR_CODE_RE.sub(
             lambda m: '</span><span class="%s">' % ansi_code_to_css(m.group(1)), s) +\
           '</span>'

########NEW FILE########
__FILENAME__ = linkify
import os
import re

from twitter.pants.base.build_file import BuildFile


# A regex to recognize substrings that are probably URLs or file paths. Broken down for readability.
_PREFIX = r'(https?://)?/?' # http://, https:// or / or nothing.
_OPTIONAL_PORT = r'(:\d+)?'
_REL_PATH_COMPONENT = r'(\w|[-.])+'  # One or more alphanumeric, underscore, dash or dot.
_ABS_PATH_COMPONENT = r'/' + _REL_PATH_COMPONENT
_ABS_PATH_COMPONENTS = r'(%s)+' % _ABS_PATH_COMPONENT
_OPTIONAL_TARGET_SUFFIX = r'(:%s)?' % _REL_PATH_COMPONENT  # For /foo/bar:target.

# Note that we require at least two path components.
# We require the last characgter to be alphanumeric or underscore, because some tools print an
# ellipsis after file names (I'm looking at you, zinc). None of our files end in a dot in practice,
# so this is fine.
_PATH = _PREFIX + _REL_PATH_COMPONENT + _OPTIONAL_PORT + _ABS_PATH_COMPONENTS + \
        _OPTIONAL_TARGET_SUFFIX + '\w'
_PATH_RE = re.compile(_PATH)

def linkify(buildroot, s):
  """Augment text by heuristically finding URL and file references and turning them into links/"""
  def to_url(m):
    if m.group(1):
      return m.group(0)  # It's an http(s) url.
    path = m.group(0)
    if path.startswith('/'):
      path = os.path.relpath(path, buildroot)
    else:
      # See if it's a reference to a target in a BUILD file.
      # TODO: Deal with sibling BUILD files?
      parts = path.split(':')
      if len(parts) == 2:
        putative_dir = parts[0]
      else:
        putative_dir = path
      if os.path.isdir(os.path.join(buildroot, putative_dir)):
        path = os.path.join(putative_dir, BuildFile._CANONICAL_NAME)
    if os.path.exists(os.path.join(buildroot, path)):
      # The reporting server serves file content at /browse/<path_from_buildroot>.
      return '/browse/%s' % path
    else:
      return None

  def maybe_add_link(url, text):
    return '<a target="_blank" href="%s">%s</a>' % (url, text) if url else text
  return _PATH_RE.sub(lambda m: maybe_add_link(to_url(m), m.group(0)), s)

########NEW FILE########
__FILENAME__ = plaintext_reporter
from collections import namedtuple

from twitter.pants.base.workunit import WorkUnit
from twitter.pants.reporting.report import Report
from twitter.pants.reporting.reporter import Reporter


try:
  from colors import cyan, green, red, yellow
  _colorfunc_map = {
    Report.FATAL: red,
    Report.ERROR: red,
    Report.WARN: yellow,
    Report.INFO: green,
    Report.DEBUG: cyan
  }
except ImportError:
  _colorfunc_map = {}


class PlainTextReporter(Reporter):
  """Plain-text reporting to stdout.

  We only report progress for things under the default work root. It gets too
  confusing to try and show progress for background work too.
  """

  # Console reporting settings.
  #   outfile: Write to this file-like object.
  #   color: use ANSI colors in output.
  #   indent: Whether to indent the reporting to reflect the nesting of workunits.
  #   timing: Show timing report at the end of the run.
  #   cache_stats: Show artifact cache report at the end of the run.
  Settings = namedtuple('Settings',
                        Reporter.Settings._fields + ('outfile', 'color', 'indent', 'timing', 'cache_stats'))

  def __init__(self, run_tracker, settings):
    Reporter.__init__(self, run_tracker, settings)

  def open(self):
    """Implementation of Reporter callback."""
    pass

  def close(self):
    """Implementation of Reporter callback."""
    if self.settings.timing:
      self.emit('\n')
      self.emit('\nCumulative Timings')
      self.emit('\n==================')
      self.emit('\n')
      self.emit(self._format_aggregated_timings(self.run_tracker.cumulative_timings))
      self.emit('\n')
      self.emit('\nSelf Timings')
      self.emit('\n============')
      self.emit('\n')
      self.emit(self._format_aggregated_timings(self.run_tracker.self_timings))
    if self.settings.cache_stats:
      self.emit('\n')
      self.emit('\nArtifact Cache Stats')
      self.emit('\n====================')
      self.emit('\n')
      self.emit(self._format_artifact_cache_stats(self.run_tracker.artifact_cache_stats))
    self.emit('\n')

  def start_workunit(self, workunit):
    """Implementation of Reporter callback."""
    if not self.is_under_main_root(workunit):
      return

    if workunit.parent and workunit.parent.has_label(WorkUnit.MULTITOOL):
      # For brevity, we represent each consecutive invocation of a multitool with a dot.
      self.emit('.')
    elif not workunit.parent or \
        all([not x.has_label(WorkUnit.MULTITOOL) and not x.has_label(WorkUnit.BOOTSTRAP)
             for x in workunit.parent.ancestors()]):
      # Bootstrapping can be chatty, so don't show anything for its sub-workunits.
      self.emit('\n%s %s %s[%s]' %
                       (workunit.start_time_string(),
                        workunit.start_delta_string(),
                        self._indent(workunit),
                        workunit.name if self.settings.indent else workunit.path()))
      if self._show_output(workunit):
        # So that emitted output starts on a new line (see below).
        self.emit(self._prefix(workunit, '\n'))
    self.flush()

  def end_workunit(self, workunit):
    """Implementation of Reporter callback."""
    if not self.is_under_main_root(workunit):
      return

    if workunit.outcome() != WorkUnit.SUCCESS and not self._show_output(workunit):
      # Emit the suppressed workunit output, if any, to aid in debugging the problem.
      for name, outbuf in workunit.outputs().items():
        self.emit(self._prefix(workunit, '\n==== %s ====\n' % name))
        self.emit(self._prefix(workunit, outbuf.read_from(0)))
        self.flush()

  def do_handle_log(self, workunit, level, *msg_elements):
    """Implementation of Reporter callback."""
    if not self.is_under_main_root(workunit):
      return

    # If the element is a (msg, detail) pair, we ignore the detail. There's no
    # useful way to display it on the console.
    elements = [e if isinstance(e, basestring) else e[0] for e in msg_elements]
    msg = '\n' + ''.join(elements)
    if self.settings.color:
      msg = _colorfunc_map.get(level, lambda x: x)(msg)
    self.emit(self._prefix(workunit, msg))
    self.flush()

  def handle_output(self, workunit, label, s):
    """Implementation of Reporter callback."""
    if not self.is_under_main_root(workunit):
      return

    if self._show_output_indented(workunit):
      self.emit(self._prefix(workunit, s))
    elif self._show_output_unindented(workunit):
      self.emit(s)
    self.flush()

  def emit(self, s):
    self.settings.outfile.write(s)

  def flush(self):
    self.settings.outfile.flush()

  # Emit output from some tools and not others.
  # This is an arbitrary choice, but one that turns out to be useful to users in practice.

  def _show_output(self, workunit):
    return self._show_output_indented(workunit) or self._show_output_unindented(workunit)

  def _show_output_indented(self, workunit):
    return workunit.has_label(WorkUnit.COMPILER) or workunit.has_label(WorkUnit.TEST)

  def _show_output_unindented(self, workunit):
    # Indenting looks weird in these cases.
    return workunit.has_label(WorkUnit.REPL) or workunit.has_label(WorkUnit.RUN)

  def _format_aggregated_timings(self, aggregated_timings):
    return '\n'.join(['%(timing).3f %(label)s' % x for x in aggregated_timings.get_all()])

  def _format_artifact_cache_stats(self, artifact_cache_stats):
    stats = artifact_cache_stats.get_all()
    return 'No artifact cache reads.' if not stats else \
    '\n'.join(['%(cache_name)s - Hits: %(num_hits)d Misses: %(num_misses)d' % x
               for x in stats])

  def _indent(self, workunit):
    return '  ' * (len(workunit.ancestors()) - 1)

  _time_string_filler = ' ' * len('HH:MM:SS mm:ss ')
  def _prefix(self, workunit, s):
    if self.settings.indent:
      return s.replace('\n', '\n' + PlainTextReporter._time_string_filler + self._indent(workunit))
    else:
      return PlainTextReporter._time_string_filler + s


########NEW FILE########
__FILENAME__ = quiet_reporter
import sys

from collections import namedtuple

from twitter.pants.reporting.report import Report
from twitter.pants.reporting.reporter import Reporter


try:
  from colors import red
  _maybe_color = red
except ImportError:
  _maybe_color = lambda x: x


class QuietReporter(Reporter):
  """Squelched plaintext reporting, only prints errors."""
  Settings = namedtuple('Settings', Reporter.Settings._fields + ('color', ))

  def __init__(self, run_tracker, settings):
    Reporter.__init__(self, run_tracker, settings._replace(log_level=Report.ERROR))

  def open(self):
    """Implementation of Reporter callback."""
    pass

  def close(self):
    """Implementation of Reporter callback."""
    pass

  def start_workunit(self, workunit):
    """Implementation of Reporter callback."""
    pass

  def end_workunit(self, workunit):
    """Implementation of Reporter callback."""
    pass

  def do_handle_log(self, workunit, level, *msg_elements):
    """Implementation of Reporter callback."""
    # If the element is a (msg, detail) pair, we ignore the detail. There's no
    # useful way to display it on the console.
    elements = [e if isinstance(e, basestring) else e[0] for e in msg_elements]
    msg = '\n' + ''.join(elements)
    if self.settings.color:
      msg = _maybe_color(msg)
    self._emit(msg)

  def handle_output(self, workunit, label, s):
    """Implementation of Reporter callback."""
    pass

  def _emit(self, s):
    sys.stdout.write(s)

########NEW FILE########
__FILENAME__ = report
import threading

from twitter.common.threading import PeriodicThread


class ReportingError(Exception):
  pass

class Report(object):
  """A report of a pants run."""

  # Log levels.
  FATAL = 0
  ERROR = 1
  WARN = 2
  INFO = 3
  DEBUG = 4

  _log_level_name_map = {
    'FATAL': FATAL, 'ERROR': ERROR, 'WARN': WARN, 'WARNING': WARN, 'INFO': INFO, 'DEBUG': DEBUG
  }

  @staticmethod
  def log_level_from_string(s):
    s = s.upper()
    return Report._log_level_name_map.get(s, Report.INFO)

  def __init__(self):
    # We periodically emit newly gathered output from tool invocations.
    self._emitter_thread = \
      PeriodicThread(target=self.flush, name='output-emitter', period_secs=0.5)
    self._emitter_thread.daemon = True

    # Map from workunit id to workunit.
    self._workunits = {}

    # We report to these reporters.
    self._reporters = {}  # name -> Reporter instance.

    # We synchronize on this, to support parallel execution.
    self._lock = threading.Lock()

  def open(self):
    with self._lock:
      for reporter in self._reporters.values():
        reporter.open()
    self._emitter_thread.start()

  # Note that if you addr/remove reporters after open() has been called you have
  # to ensure that their state is set up correctly. Best only to do this with
  # stateless reporters, such as ConsoleReporter.

  def add_reporter(self, name, reporter):
    with self._lock:
      self._reporters[name] = reporter

  def remove_reporter(self, name):
    with self._lock:
      ret = self._reporters[name]
      del self._reporters[name]
      return ret

  def start_workunit(self, workunit):
    with self._lock:
      self._workunits[workunit.id] = workunit
      for reporter in self._reporters.values():
        reporter.start_workunit(workunit)

  def log(self, workunit, level, *msg_elements):
    """Log a message.

    Each element of msg_elements is either a message string or a (message, detail) pair.
    """
    with self._lock:
      for reporter in self._reporters.values():
        reporter.handle_log(workunit, level, *msg_elements)

  def end_workunit(self, workunit):
    with self._lock:
      self._notify()  # Make sure we flush everything reported until now.
      for reporter in self._reporters.values():
        reporter.end_workunit(workunit)
      if workunit.id in self._workunits:
        del self._workunits[workunit.id]

  def flush(self):
    with self._lock:
      self._notify()

  def close(self):
    self._emitter_thread.stop()
    with self._lock:
      self._notify()  # One final time.
      for reporter in self._reporters.values():
        reporter.close()

  def _notify(self):
    # Notify for output in all workunits. Note that output may be coming in from workunits other
    # than the current one, if work is happening in parallel.
    # Assumes self._lock is held by the caller.
    for workunit in self._workunits.values():
      for label, output in workunit.outputs().items():
        s = output.read()
        if len(s) > 0:
          for reporter in self._reporters.values():
            reporter.handle_output(workunit, label, s)

########NEW FILE########
__FILENAME__ = reporter
from collections import namedtuple
from twitter.pants.goal.run_tracker import RunTracker


class Reporter(object):
  """Formats and emits reports.

  Subclasses implement the callback methods, to provide specific reporting
  functionality, e.g., to console or to browser.
  """

  # Generic reporting settings.
  #   log_level: Display log messages up to this level.
  #   subsettings: subclass-specific settings.
  Settings = namedtuple('Settings', ['log_level'])

  def __init__(self, run_tracker, settings):
    self.run_tracker = run_tracker
    self.settings = settings

  def open(self):
    """Begin the report."""
    pass

  def close(self):
    """End the report."""
    pass

  def start_workunit(self, workunit):
    """A new workunit has started."""
    pass

  def end_workunit(self, workunit):
    """A workunit has finished."""
    pass

  def handle_log(self, workunit, level, *msg_elements):
    """Handle a message logged by pants code.

    level: One of the constants above.

    Each element in msg_elements is either a message or a (message, detail) pair.
    A subclass must show the message, but may choose to show the detail in some
    sensible way (e.g., when the message text is clicked on in a browser).

    This convenience implementation filters by log level and then delegates to do_handle_log.
    """
    if level <= self.settings.log_level:
      self.do_handle_log(workunit, level, *msg_elements)

  def do_handle_log(self, workunit, level, *msg_elements):
    """Handle a message logged by pants code, after it's passed the log level check."""
    pass

  def handle_output(self, workunit, label, s):
    """Handle output captured from an invoked tool (e.g., javac).

    workunit: The innermost WorkUnit in which the tool was invoked.
    label: Classifies the output e.g., 'stdout' for output captured from a tool's stdout or
           'debug' for debug output captured from a tool's logfiles.
    s: The content captured.
    """
    pass

  def is_under_main_root(self, workunit):
    """Is the workunit running under the main thread's root."""
    return self.run_tracker.is_under_main_root(workunit)

########NEW FILE########
__FILENAME__ = reporting_server
import itertools
import json
import mimetypes
import os
import pkgutil
import pystache
import re
import urllib
import urlparse

import BaseHTTPServer

from collections import namedtuple
from datetime import date, datetime

from pystache import Renderer
from twitter.common.dirutil import safe_mkdir
from twitter.pants.base.build_environment import get_buildroot

from twitter.pants.base.mustache import MustacheRenderer
from twitter.pants.goal.run_tracker import RunInfo


# Google Prettyprint plugin files.
PPP_RE=re.compile("""^lang-.*\.js$""")


class PantsHandler(BaseHTTPServer.BaseHTTPRequestHandler):
  """A handler that demultiplexes various pants reporting URLs."""

  def __init__(self, settings, renderer, request, client_address, server):
    self._settings = settings  # An instance of ReportingServer.Settings.
    self._root = self._settings.root
    self._renderer = renderer
    self._client_address = client_address
    # The underlying handlers for specific URL prefixes.
    self._GET_handlers = [
      ('/runs/', self._handle_runs),  # Show list of known pants runs.
      ('/run/', self._handle_run),  # Show a report for a single pants run.
      ('/browse/', self._handle_browse),  # Browse filesystem under build root.
      ('/content/', self._handle_content),  # Show content of file.
      ('/assets/', self._handle_assets),  # Statically serve assets (css, js etc.)
      ('/poll', self._handle_poll),  # Handle poll requests for raw file content.
      ('/latestrunid', self._handle_latest_runid)  # Return id of latest pants run.
    ]
    BaseHTTPServer.BaseHTTPRequestHandler.__init__(self, request, client_address, server)

  def do_GET(self):
    """GET method implementation for BaseHTTPRequestHandler."""
    if not self._client_allowed():
      return

    try:
      (_, _, path, query, _) = urlparse.urlsplit(self.path)
      params = urlparse.parse_qs(query)
      # Give each handler a chance to respond.
      for prefix, handler in self._GET_handlers:
        if self._maybe_handle(prefix, handler, path, params):
          return
      # If no path specified, default to showing the list of all runs.
      if path == '/':
        self._handle_runs('', {})
        return

      self._send_content('Invalid GET request %s' % self.path, 'text/html')
    except (IOError, ValueError):
      pass  # Printing these errors gets annoying, and there's nothing to do about them anyway.
      #sys.stderr.write('Invalid GET request %s' % self.path)

  def _handle_runs(self, relpath, params):
    """Show a listing of all pants runs since the last clean-all."""
    runs_by_day = self._partition_runs_by_day()
    args = self._default_template_args('run_list')
    args['runs_by_day'] = runs_by_day
    self._send_content(self._renderer.render_name('base', args), 'text/html')

  def _handle_run(self, relpath, params):
    """Show the report for a single pants run."""
    args = self._default_template_args('run')
    run_id = relpath
    run_info = self._get_run_info_dict(run_id)
    if run_info is None:
      args['no_such_run'] = relpath
      if run_id == 'latest':
        args['is_latest'] = 'none'
    else:
      report_abspath = run_info['default_report']
      report_relpath = os.path.relpath(report_abspath, self._root)
      report_dir = os.path.dirname(report_relpath)
      self_timings_path = os.path.join(report_dir, 'self_timings')
      cumulative_timings_path = os.path.join(report_dir, 'cumulative_timings')
      artifact_cache_stats_path = os.path.join(report_dir, 'artifact_cache_stats')
      run_info['timestamp_text'] = \
        datetime.fromtimestamp(float(run_info['timestamp'])).strftime('%H:%M:%S on %A, %B %d %Y')
      args.update({'run_info': run_info,
                   'report_path': report_relpath,
                   'self_timings_path': self_timings_path,
                   'cumulative_timings_path': cumulative_timings_path,
                   'artifact_cache_stats_path': artifact_cache_stats_path})
      if run_id == 'latest':
        args['is_latest'] = run_info['id']
      args.update({
        'collapsible': lambda x: self._renderer.render_callable('collapsible', x, args)
      })
    self._send_content(self._renderer.render_name('base', args), 'text/html')

  def _handle_browse(self, relpath, params):
    """Handle requests to browse the filesystem under the build root."""
    abspath = os.path.normpath(os.path.join(self._root, relpath))
    if not abspath.startswith(self._root):
      raise ValueError  # Prevent using .. to get files from anywhere other than root.
    if os.path.isdir(abspath):
      self._serve_dir(abspath, params)
    elif os.path.isfile(abspath):
      self._serve_file(abspath, params)

  def _handle_content(self, relpath, params):
    """Render file content for pretty display."""
    abspath = os.path.normpath(os.path.join(self._root, relpath))
    if os.path.isfile(abspath):
      with open(abspath, 'r') as infile:
        content = infile.read()
    else:
      content = 'No file found at %s' % abspath
    content_type = mimetypes.guess_type(abspath)[0] or 'text/plain'
    if not content_type.startswith('text/') and not content_type == 'application/xml':
      # Binary file. Display it as hex, split into lines.
      n = 120  # Display lines of this max size.
      content = repr(content)[1:-1]  # Will escape non-printables etc, dropping surrounding quotes.
      content = '\n'.join([content[i:i+n] for i in xrange(0, len(content), n)])
      prettify = False
      prettify_extra_langs = []
    else:
      prettify = True
      if self._settings.assets_dir:
        prettify_extra_dir = os.path.join(self._settings.assets_dir, 'js', 'prettify_extra_langs')
        prettify_extra_langs = [ {'name': x} for x in os.listdir(prettify_extra_dir) ]
      else:
        # TODO: Find these from our package, somehow.
        prettify_extra_langs = []
    linenums = True
    args = { 'prettify_extra_langs': prettify_extra_langs, 'content': content,
             'prettify': prettify, 'linenums': linenums }
    self._send_content(self._renderer.render_name('file_content', args), 'text/html')

  def _handle_assets(self, relpath, params):
    """Statically serve assets: js, css etc."""
    if self._settings.assets_dir:
      abspath = os.path.normpath(os.path.join(self._settings.assets_dir, relpath))
      with open(abspath, 'r') as infile:
        content = infile.read()
    else:
      content = pkgutil.get_data(__name__, os.path.join('assets', relpath))
    content_type = mimetypes.guess_type(relpath)[0] or 'text/plain'
    self._send_content(content, content_type)

  def _handle_poll(self, relpath, params):
    """Handle poll requests for raw file contents."""
    request = json.loads(params.get('q')[0])
    ret = {}
    # request is a polling request for multiple files. For each file:
    #  - id is some identifier assigned by the client, used to differentiate the results.
    #  - path is the file to poll.
    #  - pos is the last byte position in that file seen by the client.
    for poll in request:
      _id = poll.get('id', None)
      path = poll.get('path', None)
      pos = poll.get('pos', 0)
      if path:
        abspath = os.path.normpath(os.path.join(self._root, path))
        if os.path.isfile(abspath):
          with open(abspath, 'r') as infile:
            if pos:
              infile.seek(pos)
            content = infile.read()
            ret[_id] = content
    self._send_content(json.dumps(ret), 'application/json')

  def _handle_latest_runid(self, relpath, params):
    """Handle request for the latest run id.

    Used by client-side javascript to detect when there's a new run to display.
    """
    latest_runinfo = self._get_run_info_dict('latest')
    if latest_runinfo is None:
      self._send_content('none', 'text/plain')
    else:
      self._send_content(latest_runinfo['id'], 'text/plain')

  def _partition_runs_by_day(self):
    """Split the runs by day, so we can display them grouped that way."""
    run_infos = self._get_all_run_infos()
    for x in run_infos:
      ts = float(x['timestamp'])
      x['time_of_day_text'] = datetime.fromtimestamp(ts).strftime('%H:%M:%S')

    def date_text(dt):
      delta_days = (date.today() - dt).days
      if delta_days == 0:
        return 'Today'
      elif delta_days == 1:
        return 'Yesterday'
      elif delta_days < 7:
        return dt.strftime('%A')  # Weekday name.
      else:
        d = dt.day % 10
        suffix = 'st' if d == 1 else 'nd' if d == 2 else 'rd' if d == 3 else 'th'
        return dt.strftime('%B %d') + suffix  # E.g., October 30th.

    keyfunc = lambda x: datetime.fromtimestamp(float(x['timestamp']))
    sorted_run_infos = sorted(run_infos, key=keyfunc, reverse=True)
    return [ { 'date_text': date_text(dt), 'run_infos': [x for x in infos] }
             for dt, infos in itertools.groupby(sorted_run_infos, lambda x: keyfunc(x).date()) ]

  def _get_run_info_dict(self, run_id):
    """Get the RunInfo for a run, as a dict."""
    run_info_path = os.path.join(self._settings.info_dir, run_id, 'info')
    if os.path.exists(run_info_path):
      # We copy the RunInfo as a dict, so we can add stuff to it to pass to the template.
      return RunInfo(run_info_path).get_as_dict()
    else:
      return None

  def _get_all_run_infos(self):
    """Find the RunInfos for all runs since the last clean-all."""
    info_dir = self._settings.info_dir
    if not os.path.isdir(info_dir):
      return []
    paths = [os.path.join(info_dir, x) for x in os.listdir(info_dir)]

    # We copy the RunInfo as a dict, so we can add stuff to it to pass to the template.
    # We filter only those that have a timestamp, to avoid a race condition with writing
    # that field.
    return filter(lambda d: 'timestamp' in d, [RunInfo(os.path.join(p, 'info')).get_as_dict()
            for p in paths if os.path.isdir(p) and not os.path.islink(p)])

  def _serve_dir(self, abspath, params):
    """Show a directory listing."""
    relpath = os.path.relpath(abspath, self._root)
    breadcrumbs = self._create_breadcrumbs(relpath)
    entries = [ {'link_path': os.path.join(relpath, e), 'name': e} for e in os.listdir(abspath)]
    args = self._default_template_args('dir')
    args.update({ 'root_parent': os.path.dirname(self._root),
                  'breadcrumbs': breadcrumbs,
                  'entries': entries,
                  'params': params })
    self._send_content(self._renderer.render_name('base', args), 'text/html')

  def _serve_file(self, abspath, params):
    """Show a file.

    The actual content of the file is rendered by _handle_content."""
    relpath = os.path.relpath(abspath, self._root)
    breadcrumbs = self._create_breadcrumbs(relpath)
    link_path = urlparse.urlunparse([None, None, relpath, None, urllib.urlencode(params), None])
    args = self._default_template_args('file')
    args.update({ 'root_parent': os.path.dirname(self._root),
                  'breadcrumbs': breadcrumbs,
                  'link_path': link_path })
    self._send_content(self._renderer.render_name('base', args), 'text/html')

  def _send_content(self, content, content_type, code=200):
    """Send content to client."""
    self.send_response(code)
    self.send_header('Content-Type', content_type)
    self.send_header('Content-Length', str(len(content)))
    self.end_headers()
    self.wfile.write(content)

  def _client_allowed(self):
    """Check if client is allowed to connect to this server."""
    client_ip = self._client_address[0]
    if not client_ip in self._settings.allowed_clients and \
       not 'ALL' in self._settings.allowed_clients:
      self._send_content('Access from host %s forbidden.' % client_ip, 'text/html')
      return False
    return True

  def _maybe_handle(self, prefix, handler, path, params, data=None):
    """Apply the handler if the prefix matches."""
    if path.startswith(prefix):
      relpath = path[len(prefix):]
      if data:
        handler(relpath, params, data)
      else:
        handler(relpath, params)
      return True
    else:
      return False

  def _create_breadcrumbs(self, relpath):
    """Create filesystem browsing breadcrumb navigation.

    That is, make each path segment into a clickable element that takes you to that dir.
    """
    if relpath == '.':
      breadcrumbs = []
    else:
      path_parts = [os.path.basename(self._root)] + relpath.split(os.path.sep)
      path_links = ['/'.join(path_parts[1:i+1]) for i, name in enumerate(path_parts)]
      breadcrumbs = [{'link_path': link_path, 'name': name }
                     for link_path, name in zip(path_links, path_parts)]
    return breadcrumbs

  def _default_template_args(self, content_template):
    """Initialize template args."""
    def include(text, args):
      template_name = pystache.render(text, args)
      return self._renderer.render_name(template_name, args)
    # Our base template calls include on the content_template.
    ret = { 'content_template': content_template }
    ret['include'] = lambda text: include(text, ret)
    return ret

  def log_message(self, fmt, *args):
    """Silence BaseHTTPRequestHandler's logging."""
    pass


class ReportingServer(object):
  # Reporting server settings.
  #   info_dir: path to dir containing RunInfo files.
  #   template_dir: location of mustache template files. If None, the templates
  #                 embedded in our package are used.
  #   assets_dir: location of assets (js, css etc.) If None, the assets
  #               embedded in our package are used.
  #   root: build root.
  #   allowed_clients: list of ips or ['ALL'].
  Settings = namedtuple('Settings',
    ['info_dir', 'template_dir', 'assets_dir', 'root', 'allowed_clients'])

  def __init__(self, port, settings):
    renderer = MustacheRenderer(settings.template_dir, __name__)

    class MyHandler(PantsHandler):
      def __init__(self, request, client_address, server):
        PantsHandler.__init__(self, settings, renderer, request, client_address, server)

    self._httpd = BaseHTTPServer.HTTPServer(('', port), MyHandler)
    self._httpd.timeout = 0.1  # Not the network timeout, but how often handle_request yields.

  def server_port(self):
    return self._httpd.server_port

  def start(self):
    self._httpd.serve_forever()


class ReportingServerManager(object):
  @staticmethod
  def _get_pidfile_dir():
    return os.path.join(get_buildroot(), '.pids', 'daemon')

  @staticmethod
  def save_current_server_port(port):
    """Save the port of the currently-running server, so we can find it across pants runs."""
    # We don't put the pidfile in .pants.d, because we want to find it even after a clean.
    # NOTE: If changing this dir/file name, also change get_current_server_pidfiles_and_ports
    # appropriately.
    # TODO: Generalize the pidfile idiom into some central library.
    pidfile_dir = ReportingServerManager._get_pidfile_dir()
    safe_mkdir(pidfile_dir)
    pidfile = os.path.join(pidfile_dir, 'port_%d.pid' % port)
    with open(pidfile, 'w') as outfile:
      outfile.write(str(os.getpid()))

  @staticmethod
  def get_current_server_port():
    """Returns the port of the currently-running server, or None if no server is detected."""
    pidfiles_and_ports = ReportingServerManager.get_current_server_pidfiles_and_ports()
    # There should only be one pidfile, but in case there are many due to error,
    # pick the first one.
    return pidfiles_and_ports[0][1] if pidfiles_and_ports else None

  @staticmethod
  def get_current_server_pidfiles_and_ports():
    """Returns a list of pairs (pidfile, port) of all found pidfiles."""
    pidfile_dir = ReportingServerManager._get_pidfile_dir()
    # There should only be one pidfile, but there may be errors/race conditions where
    # there are multiple of them.
    pidfile_names = os.listdir(pidfile_dir) if os.path.exists(pidfile_dir) else []
    ret = []
    for pidfile_name in pidfile_names:
      m = re.match(r'port_(\d+)\.pid', pidfile_name)
      if m is not None:
        ret.append((os.path.join(pidfile_dir, pidfile_name), int(m.group(1))))
    return ret

########NEW FILE########
__FILENAME__ = reporting_utils


def items_to_report_element(items, item_type):
  """Converts an iterable of items to a (message, detail) pair.

  - items: a list of items (e.g., Target instances) that can be str()-ed.
  - item_type: a string describing the type of item (e.g., 'target').

  Returns (message, detail) where message is the count of items (e.g., '26 targets')
  and detail is the text representation of the list of items, one per line.

  The return value can be used as an argument to Report.log().

  This is useful when we want to say "N targets" or "K sources"
  and allow the user to see which ones by clicking on that text.
  """
  def pluralize(x):
    if x.endswith('s'):
      return x + 'es'
    else:
      return x + 's'

  items = [str(x) for x in items]
  n = len(items)
  text = '%d %s' % (n, item_type if n == 1 else pluralize(item_type))
  if n == 0:
    return text
  else:
    detail = '\n'.join(items)
    return text, detail

########NEW FILE########
__FILENAME__ = git
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import subprocess

from . import Scm


class Git(Scm):
  """An Scm implementation backed by git."""

  def __init__(self, binary='git', gitdir=None, worktree=None, remote=None, branch=None, log=None):
    """Creates a git scm proxy that assumes the git repository is in the cwd by default.

    binary:    The path to the git binary to use, 'git' by default.
    gitdir:    The path to the repository's git metadata directory (typically '.git').
    workspace: The path to the git repository working tree directory (typically '.').
    remote:    The default remote to use.
    branch:    The default remote branch to use.
    log:       A log object that supports debug, info, and warn methods.
    """
    Scm.__init__(self)

    self._gitcmd = binary
    self._worktree = os.path.realpath(worktree or os.getcwd())
    self._gitdir = os.path.realpath(gitdir) if gitdir else os.path.join(self._worktree, '.git')
    self._remote = remote
    self._branch = branch

    if log:
      self._log = log
    else:
      from twitter.common import log as c_log
      self._log = c_log

  @property
  def commit_id(self):
    sha = self._check_output(['rev-parse', 'HEAD'], raise_type=Scm.LocalException)
    return self._cleanse(sha)

  @property
  def tag_name(self):
    tag = self._check_output(['describe', '--tags', '--always'], raise_type=Scm.LocalException)
    return None if b'cannot' in tag else self._cleanse(tag)

  @property
  def branch_name(self):
    branch = self._check_output(['rev-parse', '--abbrev-ref', 'HEAD'],
                                raise_type=Scm.LocalException)
    branch = self._cleanse(branch)
    return None if branch == 'HEAD' else branch

  def changed_files(self, from_commit=None, include_untracked=False):
    uncommitted_changes = self._check_output(['diff', '--name-only', 'HEAD'],
                                             raise_type=Scm.LocalException)

    files = set(uncommitted_changes.split())
    if from_commit:
      # Grab the diff from the merge-base to HEAD using ... syntax.  This ensures we have just
      # the changes that have occurred on the current branch.
      committed_changes = self._check_output(['diff', '--name-only', '%s...HEAD' % from_commit],
                                             raise_type=Scm.LocalException)
      files.update(committed_changes.split())
    if include_untracked:
      untracked = self._check_output(['ls-files', '--other', '--exclude-standard'],
                                     raise_type=Scm.LocalException)
      files.update(untracked.split())
    return files

  def changelog(self, from_commit=None, files=None):
    args = ['whatchanged', '--stat', '--find-renames', '--find-copies']
    if from_commit:
      args.append('%s..HEAD' % from_commit)
    if files:
      args.append('--')
      args.extend(files)
    return self._check_output(args, raise_type=Scm.LocalException)

  def refresh(self):
    remote, merge = self._get_upstream()
    self._check_call(['pull', '--ff-only', '--tags', remote, merge], raise_type=Scm.RemoteException)

  def tag(self, name, message=None):
    # We use -a here instead of --annotate to maintain maximum git compatibility.
    # --annotate was only introduced in 1.7.8 via:
    #   https://github.com/git/git/commit/c97eff5a95d57a9561b7c7429e7fcc5d0e3a7f5d
    self._check_call(['tag', '-a', '--message=%s' % (message or ''), name],
                     raise_type=Scm.LocalException)
    self._push('refs/tags/%s' % name)

  def commit(self, message):
    self._check_call(['commit', '--all', '--message=%s' % message], raise_type=Scm.LocalException)
    self._push()

  def _push(self, *refs):
    remote, merge = self._get_upstream()
    self._check_call(['push', remote, merge] + list(refs), raise_type=Scm.RemoteException)

  def _get_upstream(self):
    if not self._remote or not self._branch:
      branch = self.branch_name
      if not branch:
        raise Scm.LocalException('Failed to determine local branch')

      def get_local_config(key):
        value = self._check_output(['config', '--local', '--get', key],
                                   raise_type=Scm.LocalException)
        return value.strip()

      self._remote = self._remote or get_local_config('branch.%s.remote' % branch)
      self._branch = self._branch or get_local_config('branch.%s.merge' % branch)
    return self._remote, self._branch

  def _check_call(self, args, failure_msg=None, raise_type=None):
    cmd = self._create_git_cmdline(args)
    self._log_call(cmd)
    result = subprocess.call(cmd)
    self._check_result(cmd, result, failure_msg, raise_type)

  def _check_output(self, args, failure_msg=None, raise_type=None):
    cmd = self._create_git_cmdline(args)
    self._log_call(cmd)

    # We let stderr flow to wherever its currently mapped for this process - generally to the
    # terminal where the user can see the error.
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
    out, _ = process.communicate()

    self._check_result(cmd, process.returncode, failure_msg, raise_type)
    return out

  def _create_git_cmdline(self, args):
    return [self._gitcmd, '--git-dir=%s' % self._gitdir, '--work-tree=%s' % self._worktree] + args

  def _log_call(self, cmd):
    self._log.debug('Executing: %s' % ' '.join(cmd))

  def _check_result(self, cmd, result, failure_msg=None, raise_type=Scm.ScmException):
    if result != 0:
      raise raise_type(failure_msg or '%s failed with exit code %d' % (' '.join(cmd), result))

  def _cleanse(self, output):
    return output.strip().decode('utf-8')

########NEW FILE########
__FILENAME__ = annotation_processor
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from .exportable_jvm_library import ExportableJvmLibrary
from .resources import WithResources


@manual.builddict(tags=['java'])
class AnnotationProcessor(ExportableJvmLibrary, WithResources):
  """Produces a Java library containing one or more annotation processors."""

  def __init__(self,
               name,
               sources,
               provides=None,
               dependencies=None,
               excludes=None,
               resources=None,
               processors=None,
               exclusives=None):

    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param resources: An optional list of file paths (DEPRECATED) or
      ``resources`` targets (which in turn point to file paths). The paths
      indicate text file resources to place in this module's jar.
    :param processors: A list of the fully qualified class names of the
      annotation processors this library exports.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """
    super(AnnotationProcessor, self).__init__(
        name,
        sources,
        provides,
        dependencies,
        excludes,
        exclusives=exclusives)

    self.resources = resources
    self.processors = processors
    self.add_labels('java', 'apt')

########NEW FILE########
__FILENAME__ = anonymous
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import OrderedSet


# TODO(John Sirois): this is a fragile duck-type, rationalize a dependency bucket interface
class AnonymousDeps(object):
  def __init__(self):
    self._dependencies = OrderedSet()

  @property
  def dependencies(self):
    return self._dependencies

  def resolve(self):
    for dependency in self.dependencies:
      for dep in dependency.resolve():
        yield dep

########NEW FILE########
__FILENAME__ = artifact
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================
from twitter.common.collections import maybe_list

from twitter.common.lang import Compatibility

from twitter.pants.base.build_manual import manual

from .pants_target import Pants
from .repository import Repository
from .util import resolve


@manual.builddict(tags=["jvm"])
class Artifact(object):
  """Represents a jvm artifact ala maven or ivy.

  Used in the ``provides`` parameter to *jvm*\_library targets.
  """

  def __init__(self, org, name, repo, description=None):
    """
    :param string org: Organization of this artifact, or groupId in maven parlance.
    :param string name: Name of the artifact, or artifactId in maven parlance.
    :param repo: :class:`twitter.pants.targets.repository.Repository`
      this artifact is published to.
    :param string description: Description of this artifact.
    """
    if not isinstance(org, Compatibility.string):
      raise ValueError("org must be %s but was %s" % (Compatibility.string, org))
    if not isinstance(name, Compatibility.string):
      raise ValueError("name must be %s but was %s" % (Compatibility.string, name))

    if repo is None:
      raise ValueError("repo must be supplied")
    repos = []
    for tgt in maybe_list(resolve(repo), expected_type=(Pants, Repository)):
      repos.extend(tgt.resolve())
    if len(repos) != 1:
      raise ValueError("An artifact must have exactly 1 repo, given: %s" % repos)
    repo = repos[0]

    if description is not None and not isinstance(description, Compatibility.string):
      raise ValueError("description must be None or %s but was %s"
                       % (Compatibility.string, description))

    self.org = org
    self.name = name
    self.rev = None
    self.repo = repo
    self.description = description

  def __eq__(self, other):
    result = other and (
      type(other) == Artifact) and (
      self.org == other.org) and (
      self.name == other.name)
    return result

  def __hash__(self):
    return hash((self.org, self.name))

  def __ne__(self, other):
    return not self.__eq__(other)

  def __repr__(self):
    return "%s-%s -> %s" % (self.org, self.name, self.repo)

########NEW FILE########
__FILENAME__ = benchmark
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from .jvm_target import JvmTarget
from .resources import WithResources


@manual.builddict(tags=["jvm"])
class Benchmark(JvmTarget, WithResources):
  """A caliper benchmark.

  Run it with the ``bench`` goal.
  """

  def __init__(self,
               name,
               sources=None,
               java_sources=None,
               dependencies=None,
               excludes=None,
               resources=None,
               exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param java_sources:
      :class:`twitter.pants.targets.java_library.JavaLibrary` or list of
      JavaLibrary targets this library has a circular dependency on.
      Prefer using dependencies to express non-circular dependencies.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param resources: An optional list of ``resources`` targets containing text
      file resources to place in this module's jar.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """
    super(Benchmark, self).__init__(name, sources, dependencies, excludes, exclusives=exclusives)

    self.java_sources = java_sources
    self.resources = resources

########NEW FILE########
__FILENAME__ = credentials
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.target import Target


class Credentials(Target):
  """Supplies credentials for a maven repository on demand.

  The ``jar-publish`` section of your ``pants.ini`` file can refer to one
  or more of these.
  """

  def __init__(self, name, username=None, password=None,
               exclusives=None):
    """
    :param string name: The name of these credentials.
    :param username: Either a constant username value or else a callable that can fetch one.
    :type username: string or callable
    :param password: Either a constant password value or else a callable that can fetch one.
    :type password: string or callable
    """
    Target.__init__(self, name, exclusives=exclusives)
    self._username = username if callable(username) else lambda: username
    self._password = password if callable(password) else lambda: password

  def username(self, repository):
    """Returns the username in java system property argument form."""
    return self._username(repository)

  def password(self, repository):
    """Returns the password in java system property argument form."""
    return self._password(repository)

########NEW FILE########
__FILENAME__ = doc
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================


from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import Target

from .internal import InternalTarget
from .pants_target import Pants
from .with_sources import TargetWithSources


class Wiki(Target):
  """Target that identifies a wiki where pages can be published."""

  def __init__(self, name, url_builder, exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param url_builder: Function that accepts a page target and an optional wiki config dict.
    :returns: A tuple of (alias, fully qualified url).
    """
    Target.__init__(self, name, exclusives=exclusives)
    self.url_builder = url_builder


class Page(InternalTarget, TargetWithSources):
  """Describes a single documentation page."""

  def __init__(self, name, source, dependencies=None, resources=None, exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param source: Source of the page in markdown format.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param resources: An optional list of Resources objects.
    """
    InternalTarget.__init__(self, name, dependencies, exclusives=exclusives)
    TargetWithSources.__init__(self, name, sources=[source], exclusives=exclusives)

    self.resources = self._resolve_paths(resources) if resources else []
    self._wikis = {}

  @property
  def source(self):
    return self.sources[0]

  @manual.builddict()
  def register_wiki(self, wiki, **kwargs):
    """Adds this page to the given wiki for publishing.  Wiki-specific configuration is passed as
    kwargs.
    """
    if isinstance(wiki, Pants):
      wiki = wiki.get()
    if not isinstance(wiki, Wiki):
      raise ValueError('The 1st argument must be a wiki target, given: %s' % wiki)
    self._wikis[wiki] = kwargs
    return self

  def wiki_config(self, wiki):
    """Gets the wiki specific config for the given wiki if present or else returns None."""
    return self._wikis.get(wiki)

  def wikis(self):
    """Returns all the wikis registered with this page."""
    return self._wikis.keys()

########NEW FILE########
__FILENAME__ = exclude
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual


@manual.builddict(tags=["jvm"])
class Exclude(object):
  """Represents a dependency exclude pattern to filter transitive dependencies against."""

  def __init__(self, org, name=None):
    """
    :param string org: Organization of the artifact to filter,
      known as groupId in Maven parlance.
    :param string name: Name of the artifact to filter in the org, or filter
      everything if unspecified.
    """
    self.org = org
    self.name = name

  def __eq__(self, other):
    return all([other,
                type(other) == Exclude,
                self.org == other.org,
                self.name == other.name])

  def __hash__(self):
    return hash((self.org, self.name))

  def __ne__(self, other):
    return not self.__eq__(other)

  def __repr__(self):
    return "Exclude(org='%s', name=%s)" % (self.org, ('%s' % self.name) if self.name else None)

########NEW FILE########
__FILENAME__ = exportable_jvm_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from .jvm_target import JvmTarget


class ExportableJvmLibrary(JvmTarget):
  """A baseclass for java targets that support being exported to an artifact repository."""

  def __init__(self,
               name,
               sources,
               provides=None,
               dependencies=None,
               excludes=None,
               exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param provides:
      An optional Dependency object indicating the The ivy artifact to export.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param buildflags: Unused, and will be removed in a future release.
    """

    # It's critical that provides is set 1st since _provides() is called elsewhere in the
    # constructor flow.
    self._provides = provides

    super(ExportableJvmLibrary, self).__init__(
        name,
        sources,
        dependencies,
        excludes,
        exclusives=exclusives)

    self.add_labels('exportable')

  @property
  def provides(self):
    return self._provides

########NEW FILE########
__FILENAME__ = external_dependency
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from abc import abstractmethod
from twitter.common.lang import AbstractClass


class ExternalDependency(AbstractClass):
  @abstractmethod
  def cache_key(self):
    """
      Returns the key that can uniquely identify this target in the build cache.
    """

########NEW FILE########
__FILENAME__ = internal
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import collections
import copy

from functools import partial

from twitter.common.collections import maybe_list, OrderedSet

from twitter.pants.base.target import Target, TargetDefinitionException

from .anonymous import AnonymousDeps
from .external_dependency import ExternalDependency
from .jar_dependency import JarDependency
from .util import resolve


class InternalTarget(Target):
  """A baseclass for targets that support an optional dependency set."""

  class CycleException(Exception):
    """Thrown when a circular dependency is detected."""
    def __init__(self, cycle):
      Exception.__init__(self, 'Cycle detected:\n\t%s' % (
          ' ->\n\t'.join(str(target.address) for target in cycle)
      ))

  @classmethod
  def sort_targets(cls, internal_targets):
    """Returns the targets that internal_targets depend on sorted from most dependent to least."""
    roots = OrderedSet()
    inverted_deps = collections.defaultdict(OrderedSet)  # target -> dependent targets
    visited = set()
    path = OrderedSet()

    def invert(target):
      if target in path:
        path_list = list(path)
        cycle_head = path_list.index(target)
        cycle = path_list[cycle_head:] + [target]
        raise cls.CycleException(cycle)
      path.add(target)
      if target not in visited:
        visited.add(target)
        if getattr(target, 'internal_dependencies', None):
          for internal_dependency in target.internal_dependencies:
            if hasattr(internal_dependency, 'internal_dependencies'):
              inverted_deps[internal_dependency].add(target)
              invert(internal_dependency)
        else:
          roots.add(target)
      path.remove(target)

    for internal_target in internal_targets:
      invert(internal_target)

    ordered = []
    visited.clear()

    def topological_sort(target):
      if target not in visited:
        visited.add(target)
        if target in inverted_deps:
          for dep in inverted_deps[target]:
            topological_sort(dep)
        ordered.append(target)

    for root in roots:
      topological_sort(root)

    return ordered

  @classmethod
  def coalesce_targets(cls, internal_targets, discriminator):
    """Returns a list of targets internal_targets depend on sorted from most dependent to least and
    grouped where possible by target type as categorized by the given discriminator.
    """

    sorted_targets = filter(discriminator, cls.sort_targets(internal_targets))

    # can do no better for any of these:
    # []
    # [a]
    # [a,b]
    if len(sorted_targets) <= 2:
      return sorted_targets

    # For these, we'd like to coalesce if possible, like:
    # [a,b,a,c,a,c] -> [a,a,a,b,c,c]
    # adopt a quadratic worst case solution, when we find a type change edge, scan forward for
    # the opposite edge and then try to swap dependency pairs to move the type back left to its
    # grouping.  If the leftwards migration fails due to a dependency constraint, we just stop
    # and move on leaving "type islands".
    current_type = None

    # main scan left to right no backtracking
    for i in range(len(sorted_targets) - 1):
      current_target = sorted_targets[i]
      if current_type != discriminator(current_target):
        scanned_back = False

        # scan ahead for next type match
        for j in range(i + 1, len(sorted_targets)):
          look_ahead_target = sorted_targets[j]
          if current_type == discriminator(look_ahead_target):
            scanned_back = True

            # swap this guy as far back as we can
            for k in range(j, i, -1):
              previous_target = sorted_targets[k - 1]
              mismatching_types = current_type != discriminator(previous_target)
              not_a_dependency = look_ahead_target not in previous_target.internal_dependencies
              if mismatching_types and not_a_dependency:
                sorted_targets[k] = sorted_targets[k - 1]
                sorted_targets[k - 1] = look_ahead_target
              else:
                break  # out of k

            break  # out of j

        if not scanned_back:  # done with coalescing the current type, move on to next
          current_type = discriminator(current_target)

    return sorted_targets

  def sort(self):
    """Returns a list of targets this target depends on sorted from most dependent to least."""
    return self.sort_targets([self])

  def coalesce(self, discriminator):
    """Returns a list of targets this target depends on sorted from most dependent to least and
    grouped where possible by target type as categorized by the given discriminator.
    """
    return self.coalesce_targets([self], discriminator)

  def __init__(self, name, dependencies, exclusives=None):
    """
    :param string name: The name of this module target, addressable via pants via the
      portion of the spec following the colon.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    """
    Target.__init__(self, name, exclusives=exclusives)
    self._injected_deps = []
    self._processed_dependencies = resolve(dependencies)

    self.add_labels('internal')
    self.dependency_addresses = OrderedSet()

    self._dependencies = OrderedSet()
    self._internal_dependencies = OrderedSet()
    self._jar_dependencies = OrderedSet()

    if dependencies:
      maybe_list(self._processed_dependencies,
                 expected_type=(ExternalDependency, AnonymousDeps, Target),
                 raise_type=partial(TargetDefinitionException, self))

  def add_injected_dependency(self, spec):
    self._injected_deps.append(spec)

  def inject_dependencies(self):
    self.update_dependencies(resolve(self._injected_deps))

  @property
  def dependencies(self):
    self._maybe_apply_deps()
    return self._dependencies

  @property
  def internal_dependencies(self):
    self._maybe_apply_deps()
    return self._internal_dependencies

  @property
  def jar_dependencies(self):
    self._maybe_apply_deps()
    return self._jar_dependencies

  def _maybe_apply_deps(self):
    if self._processed_dependencies is not None:
      self.update_dependencies(self._processed_dependencies)
      self._processed_dependencies = None
    if self._injected_deps:
      self.update_dependencies(resolve(self._injected_deps))
      self._injected_deps = []

  def update_dependencies(self, dependencies):
    if dependencies:
      for dependency in dependencies:
        if hasattr(dependency, 'address'):
          self.dependency_addresses.add(dependency.address)
        if not hasattr(dependency, "resolve"):
          raise TargetDefinitionException(self, 'Cannot add %s as a dependency of %s'
                                                % (dependency, self))
        for resolved_dependency in dependency.resolve():
          if resolved_dependency.is_concrete and not self.valid_dependency(resolved_dependency):
            raise TargetDefinitionException(self, 'Cannot add %s as a dependency of %s'
                                                  % (resolved_dependency, self))
          self._dependencies.add(resolved_dependency)
          if isinstance(resolved_dependency, InternalTarget):
            self._internal_dependencies.add(resolved_dependency)
      self._jar_dependencies = OrderedSet(filter(lambda tgt: isinstance(tgt, JarDependency),
                                                 self._dependencies - self._internal_dependencies))

  def valid_dependency(self, dep):
    """Subclasses can over-ride to reject invalid dependencies."""
    return True

  def replace_dependency(self, dependency, replacement):
    self._dependencies.discard(dependency)
    self._internal_dependencies.discard(dependency)
    self._jar_dependencies.discard(dependency)
    self.update_dependencies([replacement])

  def _walk(self, walked, work, predicate=None):
    Target._walk(self, walked, work, predicate)
    for dep in self.dependencies:
      if isinstance(dep, Target) and not dep in walked:
        walked.add(dep)
        if not predicate or predicate(dep):
          additional_targets = work(dep)
          dep._walk(walked, work, predicate)
          if additional_targets:
            for additional_target in additional_targets:
              additional_target._walk(walked, work, predicate)

  def _propagate_exclusives(self):
    # Note: this overrides Target._propagate_exclusives without
    # calling the supermethod. Targets in pants do not necessarily
    # have a dependencies field, or ever have their dependencies
    # available at all pre-resolve. Subtypes of InternalTarget, however,
    # do have well-defined dependency lists in their dependencies field,
    # so we can do a better job propagating their exclusives quickly.
    if self.exclusives is not None:
      return
    self.exclusives = copy.deepcopy(self.declared_exclusives)
    for t in self.dependencies:
      if isinstance(t, Target):
        t._propagate_exclusives()
        self.add_to_exclusives(t.exclusives)
      elif hasattr(t, "declared_exclusives"):
        self.add_to_exclusives(t.declared_exclusives)

########NEW FILE########
__FILENAME__ = jarable
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from abc import abstractproperty

from twitter.common.lang import AbstractClass

from .jar_dependency import JarDependency


class Jarable(AbstractClass):
  """A mixin that identifies a target as one that can provide a jar."""

  @abstractproperty
  def identifier(self):
    """Subclasses should return a stable unique identifier for the jarable target."""

  @property
  def provides(self):
    """Returns an optional :class:`twitter.pants.targets.Artifact` if this target is exportable.

    Subclasses should override to provide an artifact descriptor when one applies, by default None
    is supplied.
    """
    return None

  def get_artifact_info(self):
    """Returns a triple composed of a :class:`twitter.pants.targets.jar_dependency.JarDependency`
    describing the jar for this target, this target's artifact identifier and a bool indicating if
    this target is exportable.
    """
    exported = bool(self.provides)

    org = self.provides.org if exported else 'internal'
    module = self.provides.name if exported else self.identifier

    id_ = "%s-%s" % (self.provides.org, self.provides.name) if exported else self.identifier

    # TODO(John Sirois): This should return something less than a JarDependency encapsulating just
    # the org and name.  Perhaps a JarFamily?
    return JarDependency(org=org, name=module, rev=None), id_, exported

########NEW FILE########
__FILENAME__ = jar_dependency
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict

from twitter.common.collections import OrderedSet

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import AbstractTarget

from .exclude import Exclude
from .external_dependency import ExternalDependency


class Artifact(object):
  """
  Specification for an Ivy Artifact for this jar dependency.

  See: http://ant.apache.org/ivy/history/latest-milestone/ivyfile/artifact.html
  """

  _HASH_KEYS = (
    'name',
    'type_',
    'ext',
    'conf',
    'url',
    'classifier',
  )

  def __init__(self, name, type_=None, ext=None, conf=None, url=None, classifier=None):
    """
    :param name: The name of the published artifact. This name must not include revision.
    :param type_: The type of the published artifact. It's usually its extension, but not
      necessarily. For instance, ivy files are of type 'ivy' but have 'xml' extension.
    :param ext: The extension of the published artifact.
    :param conf: The public configuration in which this artifact is published. The '*' wildcard can
      be used to designate all public configurations.
    :param url: The url at which this artifact can be found if it isn't located at the standard
      location in the repository
    :param classifier: The maven classifier of this artifact.
    """
    self.name = name
    self.type_ = type_ or 'jar'
    self.ext = ext
    self.conf = conf
    self.url = url
    self.classifier = classifier

  def cache_key(self):
    return ''.join(str(getattr(self, key)) for key in self._HASH_KEYS)

  def __repr__(self):
    return ('Artifact(%r, type_=%r, ext=%r, conf=%r, url=%r, classifier=%r)'
            % (self.name, self.type_, self.ext, self.conf, self.url, self.classifier))



@manual.builddict(tags=["jvm"])
class JarDependency(ExternalDependency, AbstractTarget):
  """A pre-built Maven repository dependency."""

  _JAR_HASH_KEYS = (
    'org',
    'name',
    'rev',
    'force',
    'excludes',
    'transitive',
    'mutable',
  )

  def __init__(self, org, name, rev=None, force=False, ext=None, url=None, apidocs=None,
               type_=None, classifier=None, mutable=None, exclusives=None):
    """
    :param string org: The Maven ``groupId`` of this dependency.
    :param string name: The Maven ``artifactId`` of this dependency.
    :param string rev: The Maven ``version`` of this dependency.
      If unspecified the latest available version is used.
    :param boolean force: Force this specific artifact revision even if other transitive
      dependencies specify a different revision. This requires specifying the ``rev`` parameter.
    :param string ext: Extension of the artifact if different from the artifact type.
      This is sometimes needed for artifacts packaged with Maven bundle type but stored as jars.
    :param string url: URL of this artifact, if different from the Maven repo standard location
      (specifying this parameter is unusual).
    :param string apidocs: URL of existing javadocs, which if specified, pants-generated javadocs
      will properly hyperlink {\ @link}s.
    :param string type_: Artifact packaging type.
    :param string classifier: Classifier specifying the artifact variant to use.
      Use ``with_artifact`` to include multiple artifacts with different classifiers.
    :param boolean mutable: Inhibit caching of this mutable artifact. A common use is for
      Maven -SNAPSHOT style artifacts in an active development/integration cycle.
    """
    self.org = org
    self.name = name
    self.rev = rev
    self.force = force
    self.excludes = []
    self.transitive = True
    self.apidocs = apidocs
    self.mutable = mutable
    self._classifier = classifier

    self.artifacts = []
    if ext or url or type_ or classifier:
      self.with_artifact(name=name, type_=type_, ext=ext, url=url, classifier=classifier)

    self.id = "%s-%s-%s" % (self.org, self.name, self.rev)
    self._configurations = ['default']
    self.declared_exclusives = defaultdict(set)
    if exclusives is not None:
      for k in exclusives:
        self.declared_exclusives[k] |= exclusives[k]

    # Support legacy method names
    # TODO(John Sirois): introduce a deprecation cycle for these and then kill
    self.withSources = self.with_sources
    self.withDocs = self.with_docs

    self.declared_exclusives = defaultdict(set)
    if exclusives is not None:
      for k in exclusives:
        self.declared_exclusives[k] |= exclusives[k]

  @property
  def is_jar(self):
    return True

  @property
  def configurations(self):
    confs = OrderedSet(self._configurations)
    confs.update(artifact.conf for artifact in self.artifacts if artifact.conf)
    return list(confs)

  @property
  def classifier(self):
    """Returns the maven classifier for this jar dependency.

    If the classifier is ambiguous; ie: there was no classifier set in the constructor and the jar
    dependency has multiple attached artifacts, a :class:`ValueError` is raised.
    """
    if self._classifier or len(self.artifacts) == 0:
      return self._classifier
    elif len(self.artifacts) == 1:
      return self.artifacts[0].classifier
    else:
      raise ValueError('Cannot determine classifier. No explicit classifier is set and this jar '
                       'has more than 1 artifact: %s\n\t%s'
                       % (self, '\n\t'.join(map(str, self.artifacts))))

  @manual.builddict()
  def exclude(self, org, name=None):
    """Adds a transitive dependency of this jar to the exclude list."""

    self.excludes.append(Exclude(org, name))
    return self

  @manual.builddict()
  def intransitive(self):
    """Declares this Dependency intransitive, indicating only the jar for the dependency itself
    should be downloaded and placed on the classpath"""

    self.transitive = False
    return self

  @manual.builddict()
  def with_sources(self):
    """This requests the artifact have its source jar fetched.
    (This implies there *is* a source jar to fetch.) Used in contexts
    that can use source jars (as of 2013, just eclipse and idea goals)."""
    self._configurations.append('sources')
    return self

  def with_docs(self):
    """This requests the artifact have its javadoc jar fetched.
    (This implies there *is* a javadoc jar to fetch.) Used in contexts
    that can use source jars (as of 2014, just eclipse and idea goals)."""
    self._configurations.append('javadoc')
    return self

  # TODO: This is necessary duck-typing because in some places JarDependency is treated like
  # a Target, even though it doesn't extend Target. Probably best to fix that.
  def has_label(self, label):
    return False

  def with_artifact(self, name=None, type_=None, ext=None, url=None, configuration=None,
                    classifier=None):
    """Sets an alternative artifact to fetch or adds additional artifacts if called multiple times.
    """
    artifact = Artifact(name or self.name, type_=type_, ext=ext, url=url, conf=configuration,
                        classifier=classifier)
    self.artifacts.append(artifact)
    return self

  def __eq__(self, other):
    result = (isinstance(other, JarDependency)
              and self.org == other.org
              and self.name == other.name
              and self.rev == other.rev)
    return result

  def __hash__(self):
    return hash((self.org, self.name, self.rev))

  def __ne__(self, other):
    return not self.__eq__(other)

  def __repr__(self):
    return self.id

  def cache_key(self):
    key = ''.join(str(getattr(self, key)) for key in self._JAR_HASH_KEYS)
    key += ''.join(sorted(self._configurations))
    key += ''.join(a.cache_key() for a in sorted(self.artifacts, key=lambda a: a.name + a.type_))
    return key

  def resolve(self):
    yield self

  def walk(self, work, predicate=None):
    if not predicate or predicate(self):
      work(self)

  def _as_jar_dependencies(self):
    yield self

########NEW FILE########
__FILENAME__ = jar_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from functools import partial

from twitter.common.collections import maybe_list, OrderedSet

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import Target, TargetDefinitionException

from . import util
from .anonymous import AnonymousDeps
from .exclude import Exclude
from .external_dependency import ExternalDependency
from .exportable_jvm_library import ExportableJvmLibrary
from .pants_target import Pants
from .jar_dependency import JarDependency


@manual.builddict(tags=["anylang"])
class JarLibrary(Target):
  """A set of dependencies that may be depended upon,
  as if depending upon the set of dependencies directly.
  """

  def __init__(self, name, dependencies, overrides=None, exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :param overrides: List of strings, each of which will be recursively resolved to
      any targets that provide artifacts. Those artifacts will override corresponding
      direct/transitive dependencies in the dependencies list.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """
    super(JarLibrary, self).__init__(name, exclusives=exclusives)

    self._pre_override_dependencies = OrderedSet(
        maybe_list(util.resolve(dependencies),
                   expected_type=(ExternalDependency, AnonymousDeps, Target),
                   raise_type=partial(TargetDefinitionException, self)))
    self._dependencies = None
    self._dependency_addresses = None
    self.override_targets = set(map(Pants, overrides or []))
    self.add_labels('jars')

  @property
  def dependencies(self):
    if self._dependencies is None:
      # compute overridden dependencies
      self._dependencies = self._resolve_overrides()
    return self._dependencies

  @property
  def dependency_addresses(self):
    if self._dependency_addresses is None:
      self._dependency_addresses = set()
      for dependency in self.dependencies:
        if hasattr(dependency, 'address'):
          self._dependency_addresses.add(dependency.address)
        # If the dependency is one that supports exclusives, the JarLibrary's
        # exclusives should be added to it.
        if hasattr(dependency, 'declared_exclusives'):
          for k in self.declared_exclusives:
            dependency.declared_exclusives[k] |= self.declared_exclusives[k]
    return self._dependency_addresses

  def resolve(self):
    yield self
    for dependency in self.dependencies:
      for resolved_dependency in dependency.resolve():
        yield resolved_dependency

  def _resolve_overrides(self):
    """
    Resolves override targets, and then excludes and re-includes each of them
    to create and return a new dependency set.
    """
    if not self.override_targets:
      return self._pre_override_dependencies

    result = OrderedSet()

    # resolve overrides and fetch all of their "artifact-providing" dependencies
    excludes = set()
    for override_target in self.override_targets:
      # add pre_override deps of the target as exclusions
      for resolved in override_target.resolve():
        excludes.update(self._excludes(resolved))
      # prepend the target as a new target
      result.add(override_target)

    # add excludes for each artifact
    for direct_dep in self._pre_override_dependencies:
      # add relevant excludes to jar dependencies
      for jar_dep in self._jar_dependencies(direct_dep):
        for exclude in excludes:
          jar_dep.exclude(exclude.org, exclude.name)
      result.add(direct_dep)

    return result

  def _excludes(self, dep):
    """
    A generator for Exclude objects that will recursively exclude all artifacts
    provided by the given dep.
    """
    if isinstance(dep, JarDependency):
      yield Exclude(dep.org, dep.name)
    elif isinstance(dep, ExportableJvmLibrary):
      if not dep.provides:
        raise TargetDefinitionException(self,
            'Targets passed to `overrides` must represent published artifacts. %s does not.' % dep)
      yield Exclude(dep.provides.org, dep.provides.name)
    elif isinstance(dep, JarLibrary):
      for d in dep._pre_override_dependencies:
        for exclude in self._excludes(d):
          yield exclude

  def _jar_dependencies(self, dep):
    """
    A generator for JarDependencies transitively included by the given dep.
    """
    if isinstance(dep, JarDependency):
      yield dep
    elif isinstance(dep, JarLibrary):
      for direct_dep in dep._pre_override_dependencies:
        for dep in self._jar_dependencies(direct_dep):
          yield dep
    elif isinstance(dep, Pants):
      for d in self._jar_dependencies(dep.get()):
        yield d

########NEW FILE########
__FILENAME__ = java_agent
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =====================================

from twitter.common.lang import Compatibility

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import TargetDefinitionException

from .java_library import JavaLibrary


@manual.builddict(tags=['jvm'])
class JavaAgent(JavaLibrary):
  """Defines a java agent entrypoint."""

  def __init__(self,
               name,
               sources=None,
               dependencies=None,
               excludes=None,
               resources=None,
               exclusives=None,
               premain=None,
               agent_class=None,
               can_redefine=False,
               can_retransform=False,
               can_set_native_method_prefix=False):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param resources: An optional list of file paths (DEPRECATED) or
      ``resources`` targets (which in turn point to file paths). The paths
      indicate text file resources to place in this module's jar.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    :param string premain: When an agent is specified at JVM launch time this attribute specifies
      the agent class. Exactly one of ``premain`` or ``agent_class`` must be specified.
    :param string agent_class: If an implementation supports a mechanism to start agents sometime
      after the VM has started then this attribute specifies the agent class. Exactly one of
      ``premain`` or ``agent_class`` must be specified.
    :param bool can_redefine: `True` if the ability to redefine classes is needed by this agent;
      `False` by default.
    :param bool can_retransform: `True` if the ability to retransform classes is needed by this
      agent; `False` by default.
    :param bool can_set_native_method_prefix: `True` if the ability to set he native method prefix
      is needed by this agent; `False` by default.
    """

    super(JavaAgent, self).__init__(
        name,
        sources,
        provides=None,
        dependencies=dependencies,
        excludes=excludes,
        resources=resources,
        exclusives=exclusives)

    if not premain or agent_class:
      raise TargetDefinitionException(self, "Must have at least one of 'premain' or 'agent_class' "
                                            "defined.")
    if premain and not isinstance(premain, Compatibility.string):
      raise TargetDefinitionException(self, 'The premain must be a fully qualified class name, '
                                            'given %s of type %s' % (premain, type(premain)))

    if agent_class and not isinstance(agent_class, Compatibility.string):
      raise TargetDefinitionException(self,
                                      'The agent_class must be a fully qualified class name, given '
                                      '%s of type %s' % (agent_class, type(agent_class)))

    self._premain = premain
    self._agent_class = agent_class
    self._can_redefine = can_redefine
    self._can_retransform = can_retransform
    self._can_set_native_method_prefix = can_set_native_method_prefix

    self.add_labels('java_agent')

  @property
  def premain(self):
    """The launch time agent fully qualified class name.

    Either ``agent_class`` or ``premain`` will be defined and the other will be `None`.
    """
    return self._premain

  @property
  def agent_class(self):
    """The post-launch-time agent fully qualified class name.

    Either ``agent_class`` or ``premain`` will be defined and the other will be `None`.
    """
    return self._agent_class

  @property
  def can_redefine(self):
    """Returns `True` if the ability to redefine classes is needed by this agent."""
    return self._can_redefine

  @property
  def can_retransform(self):
    """Returns `True` if the ability to retransform classes is needed by this agent."""
    return self._can_retransform

  @property
  def can_set_native_method_prefix(self):
    """Returns `True` if the ability to set he native method prefix is needed by this agent."""
    return self._can_set_native_method_prefix

########NEW FILE########
__FILENAME__ = java_antlr_library
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Brian Larson'

from twitter.pants.base.build_manual import manual

from .exportable_jvm_library import ExportableJvmLibrary


@manual.builddict(tags=["jvm"])
class JavaAntlrLibrary(ExportableJvmLibrary):
  """Generates a stub Java library from Antlr grammar files."""

  def __init__(self,
               name,
               sources,
               provides=None,
               dependencies=None,
               excludes=None,
               compiler='antlr3'):

    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param compiler: The name of the compiler used to compile the ANTLR files.
        Currently only supports 'antlr3' and 'antlr4'
    """

    ExportableJvmLibrary.__init__(self,
                                  name,
                                  sources,
                                  provides,
                                  dependencies,
                                  excludes)
    self.add_labels('codegen')

    if compiler not in ('antlr3', 'antlr4'):
        raise ValueError("Illegal value for 'compiler': {}".format(compiler))
    self.compiler = compiler

########NEW FILE########
__FILENAME__ = java_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =====================================

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import TargetDefinitionException

from .exportable_jvm_library import ExportableJvmLibrary
from .resources import WithResources


@manual.builddict(tags=['java'])
class JavaLibrary(ExportableJvmLibrary, WithResources):
  """A collection of Java code.

  Normally has conceptually-related sources; invoking the ``compile`` goal
  on this target compiles Java and generates classes. Invoking the ``jar``
  goal on this target creates a ``.jar``; but that's an unusual thing to do.
  Instead, a ``jvm_binary`` might depend on this library; that binary is a
  more sensible thing to bundle.
  """

  def __init__(self,
               name,
               sources=None,
               provides=None,
               dependencies=None,
               excludes=None,
               resources=None,
               exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param resources: An optional list of file paths (DEPRECATED) or
      ``resources`` targets (which in turn point to file paths). The paths
      indicate text file resources to place in this module's jar.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """
    super(JavaLibrary, self).__init__(
        name,
        sources,
        provides,
        dependencies,
        excludes,
        exclusives=exclusives)

    if (sources is None) and (resources is None):
      raise TargetDefinitionException(self, 'Must specify sources and/or resources.')

    self.resources = resources
    self.add_labels('java')

########NEW FILE########
__FILENAME__ = java_protobuf_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from .exportable_jvm_library import ExportableJvmLibrary


@manual.builddict(tags=["java"])
class JavaProtobufLibrary(ExportableJvmLibrary):
  """Generates a stub Java library from protobuf IDL files."""

  def __init__(self,
               name,
               sources,
               provides=None,
               dependencies=None,
               excludes=None,
               buildflags=None,
               exclusives=None):

    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param buildflags: Unused, and will be removed in a future release.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """

    ExportableJvmLibrary.__init__(self,
                                  name,
                                  sources,
                                  provides,
                                  dependencies,
                                  excludes,
                                  exclusives=exclusives)

    self.add_labels('codegen')

########NEW FILE########
__FILENAME__ = java_tests
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from .jvm_target import JvmTarget
from .resources import WithResources


@manual.builddict(tags=['jvm'])
class JavaTests(JvmTarget, WithResources):
  """Tests JVM sources with JUnit."""

  def __init__(self,
               name,
               sources=None,
               dependencies=None,
               excludes=None,
               resources=None,
               exclusives=None):
    """
   :param string name: The name of this target, which combined with this
     build file defines the target :class:`twitter.pants.base.address.Address`.
   :param sources: A list of filenames representing the source code
     this library is compiled from.
   :type sources: list of strings
   :param Artifact provides:
     The :class:`twitter.pants.targets.artifact.Artifact`
     to publish that represents this target outside the repo.
   :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
     this target depends on.
   :type dependencies: list of targets
   :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
     to filter this target's transitive dependencies against.
   :param resources: An optional list of ``resources`` targets containing text
     file resources to place in this module's jar.
   :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
   """
    super(JavaTests, self).__init__(name, sources, dependencies, excludes, exclusives=exclusives)

    self.resources = resources

    # TODO(John Sirois): These could be scala, clojure, etc.  'jvm' and 'tests' are the only truly
    # applicable labels - fixup the 'java' misnomer.
    self.add_labels('java', 'tests')

########NEW FILE########
__FILENAME__ = java_thrift_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import Iterable
from functools import partial

from twitter.common.collections import maybe_list

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import TargetDefinitionException

from .jar_dependency import JarDependency
from .jvm_target import JvmTarget
from .pants_target import Pants


@manual.builddict(tags=['java'])
class JavaThriftLibrary(JvmTarget):
  """Generates a stub Java or Scala library from thrift IDL files."""


  _COMPILERS = frozenset(['thrift', 'scrooge', 'scrooge-legacy'])
  _COMPILER_DEFAULT = 'thrift'

  _LANGUAGES = frozenset(['java', 'scala'])
  _LANGUAGE_DEFAULT = 'java'

  _RPC_STYLES = frozenset(['sync', 'finagle', 'ostrich'])
  _RPC_STYLE_DEFAULT = 'sync'

  def __init__(self,
               name,
               sources,
               provides=None,
               dependencies=None,
               excludes=None,
               compiler=_COMPILER_DEFAULT,
               language=_LANGUAGE_DEFAULT,
               rpc_style=_RPC_STYLE_DEFAULT,
               namespace_map=None,
               exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param compiler: An optional compiler used to compile the thrift files.
    :param language: The language used to generate the output files.
      One of 'java' or 'scala' with a default of 'java'.
    :param rpc_style: An optional rpc style to generate service stubs with.
      One of 'sync', 'finagle' or 'ostrich' with a default of 'sync'.
    :param namespace_map: A dictionary of namespaces to remap (old: new)
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """

    # It's critical that provides is set 1st since _provides() is called elsewhere in the
    # constructor flow.
    self._provides = provides

    super(JavaThriftLibrary, self).__init__(
        name,
        sources,
        dependencies,
        excludes,
        exclusives=exclusives)

    self.add_labels('codegen')

    if dependencies:
      if not isinstance(dependencies, Iterable):
        raise TargetDefinitionException(self,
                                        'dependencies must be Iterable but was: %s' % dependencies)
      maybe_list(dependencies, expected_type=(JarDependency, JavaThriftLibrary, Pants),
                 raise_type=partial(TargetDefinitionException, self))

    def check_value_for_arg(arg, value, values):
      if value not in values:
        raise TargetDefinitionException(self, "%s may only be set to %s ('%s' not valid)" %
                                        (arg, ', or '.join(map(repr, values)), value))
      return value

    # TODO(John Sirois): The defaults should be grabbed from the workspace config.

    # some gen BUILD files explicitly set this to None
    compiler = compiler or self._COMPILER_DEFAULT
    self.compiler = check_value_for_arg('compiler', compiler, self._COMPILERS)

    language = language or self._LANGUAGE_DEFAULT
    self.language = check_value_for_arg('language', language, self._LANGUAGES)

    rpc_style = rpc_style or self._RPC_STYLE_DEFAULT
    self.rpc_style = check_value_for_arg('rpc_style', rpc_style, self._RPC_STYLES)

    self.namespace_map = namespace_map

  @property
  def is_thrift(self):
    return True

  @property
  def provides(self):
    return self._provides

########NEW FILE########
__FILENAME__ = jvm_binary
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from functools import partial

from twitter.common.collections import maybe_list
from twitter.common.dirutil import Fileset
from twitter.common.lang import Compatibility

from twitter.pants.base.build_manual import manual
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import TargetDefinitionException

from . import util
from .internal import InternalTarget
from .jar_library import JarLibrary
from .jvm_target import JvmTarget
from .pants_target import Pants
from .resources import WithResources


@manual.builddict(tags=["jvm"])
class JvmBinary(JvmTarget, WithResources):
  """Produces a JVM binary optionally identifying a launcher main class.

  Below are a summary of how key goals affect targets of this type:

  * ``bundle`` - Creates a self-contained directory with the binary and all
    its dependencies, optionally archived, suitable for deployment.
  * ``binary`` - Create an executable jar of the binary. On the JVM
    this means the jar has a manifest specifying the main class.
  * ``run`` - Executes the main class of this binary locally.
  """
  def __init__(self, name,
               main=None,
               basename=None,
               source=None,
               resources=None,
               dependencies=None,
               excludes=None,
               deploy_excludes=None,
               configurations=None,
               exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param string main: The name of the ``main`` class, e.g.,
      ``'com.twitter.common.examples.pingpong.Main'``. This class may be
      present as the source of this target or depended-upon library.
    :param string basename: Base name for the generated ``.jar`` file, e.g.,
      ``'pingpong'``. (By default, uses ``name`` param)
    :param string source: Name of one ``.java`` or ``.scala`` file (a good
      place for a ``main``).
    :param resources: List of ``resource``\s to include in bundle.
    :param dependencies: List of targets (probably ``java_library`` and
     ``scala_library`` targets) to "link" in.
    :param excludes: List of ``exclude``\s to filter this target's transitive
     dependencies against.
    :param deploy_excludes: List of ``excludes`` to apply at deploy time.
      If you, for example, deploy a java servlet that has one version of
      ``servlet.jar`` onto a Tomcat environment that provides another version,
      they might conflict. ``deploy_excludes`` gives you a way to build your
      code but exclude the conflicting ``jar`` when deploying.
    :param configurations: Ivy configurations to resolve for this target.
      This parameter is not intended for general use.
    :type configurations: tuple of strings
    """
    super(JvmBinary, self).__init__(name=name,
                                    sources=[source] if source else None,
                                    dependencies=dependencies,
                                    excludes=excludes,
                                    configurations=configurations,
                                    exclusives=exclusives)

    if main and not isinstance(main, Compatibility.string):
      raise TargetDefinitionException(self, 'main must be a fully qualified classname')

    if source and not isinstance(source, Compatibility.string):
      raise TargetDefinitionException(self, 'source must be a single relative file path')

    self.main = main
    self.basename = basename or name
    self.resources = resources
    self.deploy_excludes = deploy_excludes or []


class RelativeToMapper(object):
  """A mapper that maps files specified relative to a base directory."""

  def __init__(self, base):
    """The base directory files should be mapped from."""

    self.base = base

  def __call__(self, file):
    return os.path.relpath(file, self.base)

  def __repr__(self):
    return 'IdentityMapper(%s)' % self.base


@manual.builddict(tags=["jvm"])
class Bundle(object):
  """A set of files to include in an application bundle.

  Looking for Java-style resources accessible via the ``Class.getResource`` API?
  Those are :ref:`bdict_resources`\ .

  Files added to the bundle will be included when bundling an application target.
  By default relative paths are preserved. For example, to include ``config``
  and ``scripts`` directories: ::

    bundles=[
      bundle().add(rglobs('config/*', 'scripts/*')),
    ]

  To include files relative to some path component use the ``relative_to`` parameter.
  The following places the contents of ``common/config`` in a  ``config`` directory
  in the bundle. ::

    bundles=[
      bundle(relative_to='common').add(globs('common/config/*'))
    ]
  """

  def __init__(self, base=None, mapper=None, relative_to=None):
    """
    :param base: Base path of the "source" file paths. By default, path of the
      BUILD file. Useful for assets that don't live in the source code repo.
    :param mapper: Function that takes a path string and returns a path string. Takes a path in
      the source tree, returns a path to use in the resulting bundle. By default, an identity
      mapper.
    :param string relative_to: Set up a simple mapping from source path to bundle path.
      E.g., ``relative_to='common'`` removes that prefix from all files in the application bundle.
    """
    if mapper and relative_to:
      raise ValueError("Must specify exactly one of 'mapper' or 'relative_to'")

    self._base = base or ParseContext.path()

    if relative_to:
      base = os.path.join(self._base, relative_to)
      if not os.path.isdir(base):
        raise ValueError('Could not find a directory to bundle relative to at %s' % base)
      self.mapper = RelativeToMapper(base)
    else:
      self.mapper = mapper or RelativeToMapper(self._base)

    self.filemap = {}

  @manual.builddict()
  def add(self, *filesets):
    """Add files to the bundle, where ``filesets`` is a filename, ``globs``, or ``rglobs``.
    Note this is a variable length param and may be specified any number of times.
    """
    for fileset in filesets:
      paths = fileset() if isinstance(fileset, Fileset) \
                        else fileset if hasattr(fileset, '__iter__') \
                        else [fileset]
      for path in paths:
        abspath = path
        if not os.path.isabs(abspath):
          abspath = os.path.join(self._base, path)
        if not os.path.exists(abspath):
          raise ValueError('Given path: %s with absolute path: %s which does not exist'
                           % (path, abspath))
        self.filemap[abspath] = self.mapper(abspath)
    return self

  def resolve(self):
    yield self

  def __repr__(self):
    return 'Bundle(%s, %s)' % (self.mapper, self.filemap)


@manual.builddict(tags=["jvm"])
class JvmApp(InternalTarget):
  """A JVM-based application consisting of a binary plus "extra files".

  Invoking the ``bundle`` goal on one of these targets creates a
  self-contained artifact suitable for deployment on some other machine.
  The artifact contains the executable jar, its dependencies, and
  extra files like config files, startup scripts, etc.
  """

  def __init__(self, name, binary, bundles, basename=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param binary: The :class:`twitter.pants.targets.jvm_binary.JvmBinary`,
      or a :class:`twitter.pants.targets.pants_target.Pants` pointer to one.
    :param bundles: One or more :class:`twitter.pants.targets.jvm_binary.Bundle`'s
      describing "extra files" that should be included with this app
      (e.g.: config files, startup scripts).
    :param string basename: Name of this application, if different from the
      ``name``. Pants uses this in the ``bundle`` goal to name the distribution
      artifact. In most cases this parameter is not necessary.
    """
    super(JvmApp, self).__init__(name, dependencies=[])

    self._binaries = maybe_list(
        util.resolve(binary),
        expected_type=(Pants, JarLibrary, JvmBinary),
        raise_type=partial(TargetDefinitionException, self))

    self._bundles = maybe_list(bundles, expected_type=Bundle,
                               raise_type=partial(TargetDefinitionException, self))

    if name == basename:
      raise TargetDefinitionException(self, 'basename must not equal name.')
    self.basename = basename or name

    self._resolved_binary = None
    self._resolved_bundles = []

  def is_jvm_app(self):
    return True

  @property
  def binary(self):
    self._maybe_resolve_binary()
    return self._resolved_binary

  def _maybe_resolve_binary(self):
    if self._binaries is not None:
      binaries_list = []
      for binary in self._binaries:
        binaries_list.extend(filter(lambda t: t.is_concrete, binary.resolve()))

      if len(binaries_list) != 1 or not isinstance(binaries_list[0], JvmBinary):
        raise TargetDefinitionException(self,
                                        'must supply exactly 1 JvmBinary, got %s' % binaries_list)
      self._resolved_binary = binaries_list[0]
      self.update_dependencies([self._resolved_binary])
      self._binaries = None

  @property
  def bundles(self):
    self._maybe_resolve_bundles()
    return self._resolved_bundles

  def _maybe_resolve_bundles(self):
    if self._bundles is not None:
      def is_resolvable(item):
        return hasattr(item, 'resolve')

      def is_bundle(bundle):
        return isinstance(bundle, Bundle)

      def resolve(item):
        return list(item.resolve()) if is_resolvable(item) else [None]

      if is_resolvable(self._bundles):
        self._bundles = resolve(self._bundles)

      try:
        for item in iter(self._bundles):
          for bundle in resolve(item):
            if not is_bundle(bundle):
              raise TypeError()
            self._resolved_bundles.append(bundle)
      except TypeError:
        raise TargetDefinitionException(self, 'bundles must be one or more Bundle objects, '
                                              'got %s' % self._bundles)
      self._bundles = None

  @property
  def dependencies(self):
    self._maybe_resolve_binary()
    return super(JvmApp, self).dependencies

  def resolve(self):
    # TODO(John Sirois): Clean this up when BUILD parse refactoring is tackled.
    unused_resolved_binary = self.binary
    unused_resolved_bundles = self.bundles

    for resolved in super(JvmApp, self).resolve():
      yield resolved

########NEW FILE########
__FILENAME__ = jvm_target
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.collections import maybe_list

from .exclude import Exclude
from .internal import InternalTarget
from .jarable import Jarable
from .with_sources import TargetWithSources


class JvmTarget(InternalTarget, TargetWithSources, Jarable):
  """A base class for all java module targets that provides path and dependency translation."""

  def __init__(self,
               name,
               sources,
               dependencies,
               excludes=None,
               configurations=None,
               exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: One or more :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param configurations: One or more ivy configurations to resolve for this target.
      This parameter is not intended for general use.
    :type configurations: tuple of strings
    """
    InternalTarget.__init__(self, name, dependencies, exclusives=exclusives)
    TargetWithSources.__init__(self, name, sources)

    self.add_labels('jvm')
    for source in self.sources:
      rel_path = os.path.join(self.target_base, source)
      TargetWithSources.register_source(rel_path, self)
    self.excludes = maybe_list(excludes or [], Exclude)
    self.configurations = maybe_list(configurations or [])

  def _provides(self):
    return None

########NEW FILE########
__FILENAME__ = pants_target
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.address import Address
from twitter.pants.base.build_manual import manual
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target, TargetDefinitionException


@manual.builddict(tags=["anylang"])
class Pants(Target):
  """A pointer to a pants target.

  Useful, for example, in a target's dependencies list. One target can depend
  on several others; Each pants() target refers to one of those.
  """

  _DEFINITION_ERROR_MSG = ("An invalid pants pointer has been specified. "
                           "Please identify this reference and correct the issue: ")

  def __init__(self, spec, exclusives=None):
    """
    :param string spec: target address. E.g., `src/java/com/twitter/common/util/BUILD\:util`
    """
    # it's critical the spec is parsed 1st, the results are needed elsewhere in constructor flow
    parse_context = ParseContext.locate()

    def parse_address():
      if spec.startswith(':'):
        # the :[target] could be in a sibling BUILD - so parse using the canonical address
        pathish = "%s:%s" % (parse_context.buildfile.canonical_relpath, spec[1:])
        return Address.parse(parse_context.buildfile.root_dir, pathish, False)
      else:
        return Address.parse(parse_context.buildfile.root_dir, spec, False)

    try:
      self.address = parse_address()
    except IOError as e:
      self.address = parse_context.buildfile.relpath
      raise TargetDefinitionException(self, '%s%s' % (self._DEFINITION_ERROR_MSG, e))

    # We must disable the re-init check, because our funky __getattr__ breaks it.
    # We're not involved in any multiple inheritance, so it's OK to disable it here.
    super(Pants, self).__init__(self.address.target_name, reinit_check=False, exclusives=exclusives)

  def _register(self):
    # A pants target is a pointer, do not register it as an actual target (see resolve).
    pass

  def _locate(self):
    return self.address

  def resolve(self):
    # De-reference this pants pointer to an actual parsed target.
    resolved = Target.get(self.address)
    if not resolved:
      raise TargetDefinitionException(self, '%s%s' % (self._DEFINITION_ERROR_MSG, self.address))
    for dep in resolved.resolve():
      yield dep

  def get(self):
    """De-reference this pants pointer to a single target.

    If the pointer aliases more than one target a LookupError is raised.
    """
    resolved = [t for t in self.resolve() if t.is_concrete]
    if len(resolved) > 1:
      raise LookupError('%s points to more than one target: %s' % (self, resolved))
    return resolved.pop()

  def __getattr__(self, name):
    try:
      return Target.__getattribute__(self, name)
    except AttributeError as e:
      try:
        return getattr(self.get(), name)
      except (AttributeError, LookupError):
        raise e

########NEW FILE########
__FILENAME__ = python_antlr_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import OrderedSet
from twitter.pants.base.build_manual import manual
from twitter.pants.targets.python_target import PythonTarget
from twitter.pants.targets.pants_target import Pants


@manual.builddict(tags=["python"])
class PythonAntlrLibrary(PythonTarget):
  """Generates a stub Python library from Antlr grammar files."""

  def __init__(self,
               name,
               module,
               antlr_version='3.1.3',
               sources=None,
               resources=None,
               dependencies=None,
               exclusives=None):
    """
    :param name: Name of library
    :param module: everything beneath module is relative to this module name, None if root namespace
    :param antlr_version:
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param resources: non-Python resources, e.g. templates, keys, other data (it is
        recommended that your application uses the pkgutil package to access these
        resources in a .zip-module friendly way.)
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param dict exclusives: An optional dict of exclusives tags. See CheckExclusives for details.
    """

    def get_all_deps():
      all_deps = OrderedSet()
      all_deps.update(Pants('3rdparty/python:antlr-%s' % antlr_version).resolve())
      if dependencies:
        all_deps.update(dependencies)
      return all_deps

    super(PythonAntlrLibrary, self).__init__(name, sources, resources, get_all_deps(),
                                             exclusives=exclusives)

    self.module = module
    self.antlr_version = antlr_version

########NEW FILE########
__FILENAME__ = python_artifact
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual


@manual.builddict(tags=["python"])
class PythonArtifact(object):
  """Represents a Python setup.py-based project."""
  class MissingArgument(Exception): pass
  class UnsupportedArgument(Exception): pass

  UNSUPPORTED_ARGS = frozenset([
    'data_files',
    'package_dir',
    'package_data',
    'packages',
  ])

  def __init__(self, **kwargs):
    """Passes params to `setuptools.setup <https://pythonhosted.org/setuptools/setuptools.html>`_."""
    self._kw = kwargs
    self._binaries = {}

    def has(name):
      value = self._kw.get(name)
      if value is None:
        raise self.MissingArgument('PythonArtifact requires %s to be specified!' % name)
      return value

    def misses(name):
      if name in self._kw:
        raise self.UnsupportedArgument('PythonArtifact prohibits %s from being specified' % name)

    self._version = has('version')
    self._name = has('name')
    for arg in self.UNSUPPORTED_ARGS:
      misses(arg)

  @property
  def name(self):
    return self._name

  @property
  def version(self):
    return self._version

  @property
  def key(self):
    return '%s==%s' % (self._name, self._version)

  @property
  def setup_py_keywords(self):
    return self._kw

  @property
  def binaries(self):
    return self._binaries

  @manual.builddict()
  def with_binaries(self, *args, **kw):
    """Add binaries tagged to this artifact.

    For example: ::

      provides = setup_py(
        name = 'my_library',
        zip_safe = True
      ).with_binaries(
        my_command = pants(':my_library_bin')
      )

    This adds a console_script entry_point for the python_binary target
    pointed at by :my_library_bin.  Currently only supports
    python_binaries that specify entry_point explicitly instead of source.

    Also can take a dictionary, e.g.
    with_binaries({'my-command': pants(...)})
    """
    for arg in args:
      if isinstance(arg, dict):
        self._binaries.update(arg)
    self._binaries.update(kw)
    return self

########NEW FILE########
__FILENAME__ = python_binary
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.collections import maybe_list
from twitter.common.lang import Compatibility
from twitter.common.python.pex_info import PexInfo

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import Target, TargetDefinitionException

from .python_target import PythonTarget


@manual.builddict(tags=['python'])
class PythonBinary(PythonTarget):
  """Produces a Python binary.

  Python binaries are pex files, self-contained executable shell
  scripts that contain a complete Python environment capable of
  running the target. For more information about pex files see
  https://github.com/twitter/commons/blob/master/src/python/twitter/pants/python/README.md"""

  # TODO(wickman) Consider splitting pex options out into a separate PexInfo builder that can be
  # attached to the binary target.  Ideally the PythonBinary target is agnostic about pex mechanics
  def __init__(self,
               name,
               source=None,
               dependencies=None,
               entry_point=None,
               inherit_path=False,        # pex option
               zip_safe=True,             # pex option
               always_write_cache=False,  # pex option
               repositories=None,         # pex option
               indices=None,              # pex option
               ignore_errors=False,       # pex option
               allow_pypi=False,          # pex option
               platforms=(),
               compatibility=None,
               exclusives=None):
    """
    :param name: target name
    :param source: the python source file that becomes this binary's __main__.
      If None specified, drops into an interpreter by default.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param entry_point: the default entry point for this binary.  if None, drops into the entry
      point that is defined by source
    :param inherit_path: inherit the sys.path of the environment that this binary runs in
    :param zip_safe: whether or not this binary is safe to run in compacted (zip-file) form
    :param always_write_cache: whether or not the .deps cache of this PEX file should always
      be written to disk.
    :param repositories: a list of repositories to query for dependencies.
    :param indices: a list of indices to use for packages.
    :param platforms: extra platforms to target when building this binary.
    :param compatibility: either a string or list of strings that represents
      interpreter compatibility for this target, using the Requirement-style format,
      e.g. ``'CPython>=3', or just ['>=2.7','<3']`` for requirements agnostic to interpreter class.
    :param dict exclusives: An optional dict of exclusives tags. See CheckExclusives for details.
    """

    # TODO(John Sirois): Fixup TargetDefinitionException - it has awkward Target base-class
    # initialization requirements right now requiring this Target.__init__.
    Target.__init__(self, name, exclusives=exclusives)

    if source is None and entry_point is None:
      raise TargetDefinitionException(self,
          'A python binary target must specify either source or entry_point.')

    PythonTarget.__init__(self,
        name,
        [] if source is None else [source],
        compatibility=compatibility,
        dependencies=dependencies,
        exclusives=exclusives,
    )

    if not isinstance(platforms, (list, tuple)) and not isinstance(platforms, Compatibility.string):
      raise TargetDefinitionException(self, 'platforms must be a list, tuple or string.')

    self._entry_point = entry_point
    self._inherit_path = bool(inherit_path)
    self._zip_safe = bool(zip_safe)
    self._always_write_cache = bool(always_write_cache)
    self._repositories = maybe_list(repositories or [])
    self._indices = maybe_list(indices or [])
    self._ignore_errors = bool(ignore_errors)
    self._platforms = tuple(maybe_list(platforms or []))

    if source and entry_point:
      entry_point_module = entry_point.split(':', 1)[0]
      source_entry_point = self._translate_to_entry_point(self.sources[0])
      if entry_point_module != source_entry_point:
        raise TargetDefinitionException(self,
            'Specified both source and entry_point but they do not agree: %s vs %s' % (
            source_entry_point, entry_point_module))

  @property
  def platforms(self):
    return self._platforms

  # TODO(wickman) These should likely be attributes on PythonLibrary targets
  # and not PythonBinary targets, or at the very worst, both.
  @property
  def repositories(self):
    return self._repositories

  @property
  def indices(self):
    return self._indices

  def _translate_to_entry_point(self, source):
    source_base, _ = os.path.splitext(source)
    return source_base.replace(os.path.sep, '.')

  @property
  def entry_point(self):
    if self._entry_point:
      return self._entry_point
    elif self.sources:
      assert len(self.sources) == 1
      return self._translate_to_entry_point(self.sources[0])
    else:
      return None

  @property
  def pexinfo(self):
    info = PexInfo.default()
    for repo in self._repositories:
      info.add_repository(repo)
    for index in self._indices:
      info.add_index(index)
    info.zip_safe = self._zip_safe
    info.always_write_cache = self._always_write_cache
    info.inherit_path = self._inherit_path
    info.entry_point = self.entry_point
    info.ignore_errors = self._ignore_errors
    return info

########NEW FILE########
__FILENAME__ = python_egg
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# XXX(wickman) This should probably die?

import os

from glob import glob as fsglob
from zipimport import zipimporter

from twitter.pants.base.build_manual import manual
from twitter.pants.base.parse_context import ParseContext

from .python_requirement import PythonRequirement

from pkg_resources import Distribution, EggMetadata, PathMetadata


@manual.builddict(tags=["python"])
def PythonEgg(glob, name=None):
  """Refers to pre-built Python eggs in the file system. (To instead fetch
  eggs in a ``pip``/``easy_install`` way, use ``python_requirement``)

  E.g., ``egg(name='foo', glob='foo-0.1-py2.6.egg')`` would pick up the
  file ``foo-0.1-py2.6.egg`` from the ``BUILD`` file's directory; targets
  could depend on it by name ``foo``.

  :param string glob: File glob pattern.
  :param string name: Target name; by default uses the egg's project name.
  """
  # TODO(John Sirois): Rationalize with globs handling in ParseContext
  eggs = fsglob(ParseContext.path(glob))

  requirements = set()
  for egg in eggs:
    if os.path.isdir(egg):
      metadata = PathMetadata(egg, os.path.join(egg, 'EGG-INFO'))
    else:
      metadata = EggMetadata(zipimporter(egg))
    dist = Distribution.from_filename(egg, metadata=metadata)
    requirements.add(dist.as_requirement())

  if len(requirements) > 1:
    raise ValueError('Got multiple egg versions! => %s' % requirements)

  return PythonRequirement(str(requirements.pop()), name=name)

########NEW FILE########
__FILENAME__ = python_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual
from twitter.pants.targets.python_target import PythonTarget


@manual.builddict(tags=["python"])
class PythonLibrary(PythonTarget):
  """Produces a Python library."""

  def __init__(self,
               name,
               sources=(),
               resources=(),
               dependencies=(),
               provides=None,
               compatibility=None,
               exclusives=None):
    """
    :param name: Name of library
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param resources: non-Python resources, e.g. templates, keys, other data (it is
      recommended that your application uses the pkgutil package to access these
      resources in a .zip-module friendly way.)
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param provides:
      The :ref:`setup_py <bdict_setup_py>` (implemented by
      :class:`twitter.pants.targets.artifact.PythonArtifact`)
      to publish that represents this target outside the repo.
    :param dict exclusives: An optional dict of exclusives tags. See CheckExclusives for details.
    """
    PythonTarget.__init__(self,
        name,
        sources=sources,
        resources=resources,
        dependencies=dependencies,
        provides=provides,
        compatibility=compatibility,
        exclusives=exclusives,
    )

########NEW FILE########
__FILENAME__ = python_requirement
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import Target

from .external_dependency import ExternalDependency

from pkg_resources import Requirement


@manual.builddict(tags=["python"])
class PythonRequirement(Target, ExternalDependency):
  """Pants wrapper around pkg_resources.Requirement"""

  def __init__(self, requirement, name=None, repository=None, version_filter=None, use_2to3=False,
               compatibility=None, exclusives=None):
    # TODO(wickman) Allow PythonRequirements to be specified using pip-style vcs or url identifiers,
    # e.g. git+https or just http://...
    self._requirement = Requirement.parse(requirement)
    self._repository = repository
    self._name = name or self._requirement.project_name
    self._use_2to3 = use_2to3
    self._version_filter = version_filter or (lambda py, pl: True)
    # TODO(wickman) Unify this with PythonTarget .compatibility
    self.compatibility = compatibility or ['']
    Target.__init__(self, self._name, exclusives=exclusives)

  def should_build(self, python, platform):
    return self._version_filter(python, platform)

  @property
  def use_2to3(self):
    return self._use_2to3

  @property
  def repository(self):
    return self._repository

  # duck-typing Requirement interface for Resolver, since Requirement cannot be
  # subclassed (curses!)
  @property
  def key(self):
    return self._requirement.key

  @property
  def extras(self):
    return self._requirement.extras

  @property
  def specs(self):
    return self._requirement.specs

  @property
  def project_name(self):
    return self._requirement.project_name

  @property
  def requirement(self):
    return self._requirement

  def __contains__(self, item):
    return item in self._requirement

  def cache_key(self):
    return str(self._requirement)

  def __repr__(self):
    return 'PythonRequirement(%s)' % self._requirement

########NEW FILE########
__FILENAME__ = python_target
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict

from twitter.common.collections import OrderedSet
from twitter.common.python.interpreter import PythonIdentity

from twitter.pants.base.target import Target, TargetDefinitionException

from .with_dependencies import TargetWithDependencies
from .with_sources import TargetWithSources

from twitter.pants.targets.python_artifact import PythonArtifact

class PythonTarget(TargetWithDependencies, TargetWithSources):
  """Base class for all Python targets."""

  def __init__(self,
               name,
               sources,
               resources=None,
               dependencies=None,
               provides=None,
               compatibility=None,
               exclusives=None):
    TargetWithSources.__init__(self, name, sources=sources, exclusives=exclusives)
    TargetWithDependencies.__init__(self, name, dependencies=dependencies, exclusives=exclusives)

    self.add_labels('python')
    self.resources = self._resolve_paths(resources) if resources else OrderedSet()

    if provides and not isinstance(provides, PythonArtifact):
      raise TargetDefinitionException(self,
        "Target must provide a valid pants setup_py object. Received a '%s' object instead." %
          provides.__class__.__name__)
    self.provides = provides

    self.compatibility = compatibility or ['']
    for req in self.compatibility:
      try:
        PythonIdentity.parse_requirement(req)
      except ValueError as e:
        raise TargetDefinitionException(str(e))

  def _walk(self, walked, work, predicate=None):
    super(PythonTarget, self)._walk(walked, work, predicate)
    if self.provides and self.provides.binaries:
      for binary in self.provides.binaries.values():
        binary._walk(walked, work, predicate)

  def _propagate_exclusives(self):
    self.exclusives = defaultdict(set)
    for k in self.declared_exclusives:
      self.exclusives[k] = self.declared_exclusives[k]
    for t in self.dependencies:
      if isinstance(t, Target):
        t._propagate_exclusives()
        self.add_to_exclusives(t.exclusives)
      elif hasattr(t, "declared_exclusives"):
        self.add_to_exclusives(t.declared_exclusives)

########NEW FILE########
__FILENAME__ = python_tests
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import maybe_list
from twitter.common.quantity import Amount, Time
from twitter.pants.base.build_manual import manual
from twitter.pants.targets.python_target import PythonTarget


@manual.builddict(tags=["python"])
class PythonTests(PythonTarget):
  """Tests a Python library."""

  def __init__(self,
               name,
               sources,
               resources=None,
               dependencies=None,
               timeout=Amount(2, Time.MINUTES),
               coverage=None,
               soft_dependencies=False,
               entry_point='pytest',
               exclusives=None):
    """
    :param name: See PythonLibrary target
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param resources: See PythonLibrary target
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param timeout: Amount of time before this test should be considered timed-out.
    :param coverage: the module(s) whose coverage should be generated, e.g.
      'twitter.common.log' or ['twitter.common.log', 'twitter.common.http']
    :param soft_dependencies: Whether or not we should ignore dependency resolution
      errors for this test.
    :param entry_point: The entry point to use to run the tests.
    :param dict exclusives: An optional dict of exclusives tags. See CheckExclusives for details.
    """
    self._timeout = timeout
    self._soft_dependencies = bool(soft_dependencies)
    self._coverage = maybe_list(coverage) if coverage is not None else []
    self._entry_point = entry_point
    super(PythonTests, self).__init__(name, sources, resources, dependencies, exclusives=exclusives)
    self.add_labels('python', 'tests')

  @property
  def timeout(self):
    return self._timeout

  @property
  def coverage(self):
    return self._coverage

  @property
  def entry_point(self):
    return self._entry_point


class PythonTestSuite(PythonTarget):
  """Tests one or more python test targets."""

  def __init__(self, name, dependencies=None):
    super(PythonTestSuite, self).__init__(name, (), (), dependencies)

########NEW FILE########
__FILENAME__ = python_thrift_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from .python_target import PythonTarget


@manual.builddict(tags=["python"])
class PythonThriftLibrary(PythonTarget):
  """Generates a stub Python library from thrift IDL files."""

  def __init__(self, name,
               sources=None,
               resources=None,
               dependencies=None,
               provides=None,
               exclusives=None):
    """
    :param name: Name of library
    :param sources: thrift source files (If more than one tries to use the same
      namespace, beware https://issues.apache.org/jira/browse/THRIFT-515)
    :param resources: non-Python resources, e.g. templates, keys, other data (it is
      recommended that your application uses the pkgutil package to access these
      resources in a .zip-module friendly way.)
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param dict exclusives: An optional dict of exclusives tags. See CheckExclusives for details.
    """
    super(PythonThriftLibrary, self).__init__(name, sources, resources, dependencies, provides,
                                              exclusives=exclusives)

########NEW FILE########
__FILENAME__ = repository
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import Target


@manual.builddict(tags=["jvm"])
class Repository(Target):
  """An artifact repository, such as a maven repo."""

  def __init__(self, name, url, push_db, exclusives=None):
    """
    :param string name: Name of the repository.
    :param string url: Optional URL of the repository.
    :param string push_db: Path of the push history file.
    """

    super(Repository, self).__init__(name, exclusives=exclusives)

    self.name = name
    self.url = url
    self.push_db = push_db

  def __eq__(self, other):
    result = other and (
      type(other) == Repository) and (
      self.name == other.name)
    return result

  def __hash__(self):
    return hash(self.name)

  def __ne__(self, other):
    return not self.__eq__(other)

  def __repr__(self):
    return "%s -> %s (%s)" % (self.name, self.url, self.push_db)

########NEW FILE########
__FILENAME__ = resources
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from . import util
from .internal import InternalTarget
from .with_sources import TargetWithSources


@manual.builddict(tags=['jvm'])
class Resources(InternalTarget, TargetWithSources):
  """A set of files accessible as resources from the JVM classpath.

  Looking for loose files in your application bundle? Those are :ref:`bdict_bundle`\ s.

  Resources are Java-style resources accessible via the ``Class.getResource``
  and friends API. In the ``jar`` goal, the resource files are placed in the resulting `.jar`.
  """

  def __init__(self, name, sources, exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the resources
      this library provides.
    """
    # TODO(John Sirois): XXX Review why this is an InternalTarget
    InternalTarget.__init__(self, name, dependencies=None, exclusives=exclusives)
    TargetWithSources.__init__(self, name, sources=sources, exclusives=exclusives)

  def has_sources(self, extension=None):
    """``Resources`` never own sources of any particular native type, like for example
    ``JavaLibrary``.
    """
    # TODO(John Sirois): track down the reason for this hack and kill or explain better.
    return extension is None


class WithResources(InternalTarget):
  """A mixin for internal targets that have resources."""

  def __init__(self, *args, **kwargs):
    super(WithResources, self).__init__(*args, **kwargs)
    self._resources = []
    self._raw_resources = None

  @property
  def resources(self):
    if self._raw_resources is not None:
      self._resources = list(self.resolve_all(self._raw_resources, Resources))
      self.update_dependencies(self._resources)
      self._raw_resources = None
    return self._resources

  @resources.setter
  def resources(self, resources):
    self._resources = []
    self._raw_resources = util.resolve(resources)

  def resolve(self):
    # TODO(John Sirois): Clean this up when BUILD parse refactoring is tackled.
    unused_resolved_resources = self.resources

    for resolved in super(WithResources, self).resolve():
      yield resolved

########NEW FILE########
__FILENAME__ = scalac_plugin
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from .scala_library import ScalaLibrary


@manual.builddict(tags=['scala'])
class ScalacPlugin(ScalaLibrary):
  """Defines a target that produces a scalac_plugin."""

  def __init__(self,
               name,
               classname,
               plugin=None,
               sources=None,
               java_sources=None,
               provides=None,
               dependencies=None,
               excludes=None,
               resources=None,
               exclusives=None):

    """
    :param name: The name of this module target, addressable via pants via the portion of the
      spec following the colon - required.
    :param classname: The fully qualified plugin class name - required.
    :param plugin: The name of the plugin which defaults to name if not supplied.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param java_sources:
      :class:`twitter.pants.targets.java_library.JavaLibrary` or list of
      JavaLibrary targets this library has a circular dependency on.
      Prefer using dependencies to express non-circular dependencies.
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against
    :param resources: An optional list of paths (DEPRECATED) or ``resources``
      targets containing resources that belong on this library's classpath.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """

    super(ScalacPlugin, self).__init__(
        name,
        sources,
        java_sources,
        provides,
        dependencies,
        excludes,
        resources,
        exclusives=exclusives)

    self.plugin = plugin or name
    self.classname = classname
    self.add_labels('scalac_plugin')

########NEW FILE########
__FILENAME__ = scala_library
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import maybe_list

from twitter.pants.base.build_manual import manual
from twitter.pants.base.target import Target, TargetDefinitionException

from . import util
from .exportable_jvm_library import ExportableJvmLibrary
from .java_library import JavaLibrary
from .resources import WithResources


@manual.builddict(tags=['scala'])
class ScalaLibrary(ExportableJvmLibrary, WithResources):
  """A collection of Scala code.

  Normally has conceptually-related sources; invoking the ``compile`` goal
  on this target compiles scala and generates classes. Invoking the ``bundle``
  goal on this target creates a ``.jar``; but that's an unusual thing to do.
  Instead, a ``jvm_binary`` might depend on this library; that binary is a
  more sensible thing to bundle.
  """

  def __init__(self,
               name,
               sources=None,
               java_sources=None,
               provides=None,
               dependencies=None,
               excludes=None,
               resources=None,
               exclusives=None):
    """
    :param string name: The name of this target, which combined with this
      build file defines the target :class:`twitter.pants.base.address.Address`.
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param java_sources:
      :class:`twitter.pants.targets.java_library.JavaLibrary` or list of
      JavaLibrary targets this library has a circular dependency on.
      Prefer using dependencies to express non-circular dependencies.
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param resources: An optional list of paths (DEPRECATED) or ``resources``
      targets containing resources that belong on this library's classpath.
    :param exclusives: An optional list of exclusives tags.
    """
    super(ScalaLibrary, self).__init__(
        name,
        sources,
        provides,
        dependencies,
        excludes,
        exclusives=exclusives)

    if (sources is None) and (resources is None):
      raise TargetDefinitionException(self, 'Must specify sources and/or resources.')

    self.resources = resources

    self._java_sources = []
    self._raw_java_sources = util.resolve(java_sources)

    self.add_labels('scala')

  @property
  def java_sources(self):
    if self._raw_java_sources is not None:
      self._java_sources = list(Target.resolve_all(maybe_list(self._raw_java_sources, Target),
                                                   JavaLibrary))

      self._raw_java_sources = None

      # TODO(John Sirois): reconsider doing this auto-linking.
      # We have circular java/scala dep, add an inbound dependency edge from java to scala in this
      # case to force scala compilation to precede java - since scalac supports generating java
      # stubs for these cycles and javac does not this is both necessary and always correct.
      for java_target in self._java_sources:
        java_target.update_dependencies([self])
    return self._java_sources

  def resolve(self):
    # TODO(John Sirois): Clean this up when BUILD parse refactoring is tackled.
    unused_resolved_java_sources = self.java_sources

    for resolved in super(ScalaLibrary, self).resolve():
      yield resolved


########NEW FILE########
__FILENAME__ = scala_tests
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_manual import manual

from .jvm_target import JvmTarget
from .resources import WithResources


@manual.builddict(tags=['scala'])
class ScalaTests(JvmTarget, WithResources):
  """Tests a Scala library."""

  def __init__(self,
               name,
               sources=None,
               java_sources=None,
               dependencies=None,
               excludes=None,
               resources=None,
               exclusives=None):

    """
    :param name: The name of this module target, addressable via pants via the portion of the spec
      following the colon
    :param sources: A list of filenames representing the source code
      this library is compiled from.
    :type sources: list of strings
    :param java_sources:
      :class:`twitter.pants.targets.java_library.JavaLibrary` or list of
      JavaLibrary targets this library has a circular dependency on.
      Prefer using dependencies to express non-circular dependencies.
    :param Artifact provides:
      The :class:`twitter.pants.targets.artifact.Artifact`
      to publish that represents this target outside the repo.
    :param dependencies: List of :class:`twitter.pants.base.target.Target` instances
      this target depends on.
    :type dependencies: list of targets
    :param excludes: List of :class:`twitter.pants.targets.exclude.Exclude` instances
      to filter this target's transitive dependencies against.
    :param resources: An optional list of Resources that should be in this target's classpath.
    :param exclusives: An optional map of exclusives tags. See CheckExclusives for details.
    """
    super(ScalaTests, self).__init__(name, sources, dependencies, excludes, exclusives=exclusives)

    # TODO(John Sirois): Merge handling with ScalaLibrary.java_sources - which is different and
    # likely more correct.
    self.java_sources = java_sources

    self.resources = resources
    self.add_labels('scala', 'tests')

########NEW FILE########
__FILENAME__ = sources
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.collections import OrderedSet

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.build_manual import manual
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import TargetDefinitionException


@manual.builddict()
class SourceRoot(object):
  """Allows registration of a source root for a set of targets.

  A source root is the base path sources for a particular language are found relative to.
  Generally compilers or interpreters for the source will expect sources relative to a base path
  and a source root allows calculation of the correct relative paths.

  E.g., a Java compiler probably expects to find ``.java`` files for
  ``package com.twitter.common.net`` in ``*something*/com/twitter/common/net``.
  The ``source_root`` command specifies that *something*.

  It is illegal to have nested source roots.
  """
  _ROOTS_BY_TYPE = {}
  _TYPES_BY_ROOT = {}
  _SEARCHED = set()

  @classmethod
  def reset(cls):
    """Reset all source roots to empty. Only intended for testing."""
    cls._ROOTS_BY_TYPE = {}
    cls._TYPES_BY_ROOT = {}
    cls._SEARCHED = set()

  @classmethod
  def find(cls, target):
    """Finds the source root for the given target.

    If none is registered, returns the parent directory of the target's BUILD file.
    """
    target_path = os.path.relpath(target.address.buildfile.parent_path, get_buildroot())

    def _find():
      for root_dir, types in cls._TYPES_BY_ROOT.items():
        if target_path.startswith(root_dir):  # The only candidate root for this target.
          # Validate the target type, if restrictions were specified.
          if types and not isinstance(target, tuple(types)):
            # TODO: Find a way to use the BUILD file aliases in the error message, instead
            # of target.__class__.__name__. E.g., java_tests instead of JavaTests.
            raise TargetDefinitionException(target,
                'Target type %s not allowed under %s' % (target.__class__.__name__, root_dir))
          return root_dir
      return None

    # Try already registered roots
    root = _find()
    if root:
      return root

    # Fall back to searching the ancestor path for a root.
    # TODO(John Sirois): We currently allow for organic growth of maven multi-module layout style
    # projects (for example) and do not require a global up-front registration of all source roots
    # and instead do lazy resolution here.  This allows for parse cycles that lead to surprising
    # runtime errors.  Re-consider allowing lazy source roots at all.
    for buildfile in reversed(target.address.buildfile.ancestors()):
      if buildfile not in cls._SEARCHED:
        ParseContext(buildfile).parse()
        cls._SEARCHED.add(buildfile)
        root = _find()
        if root:
          return root

    # Finally, resolve files relative to the BUILD file parent dir as the target base
    return target_path

  @classmethod
  def types(cls, root):
    """Returns the set of target types rooted at root."""
    return cls._TYPES_BY_ROOT[root]

  @classmethod
  def roots(cls, target_type):
    """Returns the set of roots for given target type."""
    return cls._ROOTS_BY_TYPE[target_type]

  @classmethod
  def all_roots(cls):
    """Returns a mapping from source roots to the associated target types."""
    return dict(cls._TYPES_BY_ROOT)

  @classmethod
  def register(cls, basedir, *allowed_target_types):
    """Registers the given basedir (relative to the buildroot) as a source root.

    :param string basedir: The base directory to resolve sources relative to.
    :param list allowed_target_types: Optional list of target types. If specified, we enforce that
      only targets of those types appear under this source root.
    """
    cls._register(basedir, *allowed_target_types)

  @classmethod
  def _register(cls, source_root_dir, *allowed_target_types):
    """Registers a source root.

    :source_root_dir The source root directory against which we resolve source paths,
                     relative to the build root.
    :allowed_target_types Optional list of target types. If specified, we enforce that
                          only targets of those types appear under this source root.
    """
    # Verify that source_root_dir doesn't reach outside buildroot.
    buildroot = get_buildroot()
    if source_root_dir.startswith(buildroot):
      abspath = os.path.normpath(source_root_dir)
    else:
      abspath = os.path.normpath(os.path.join(buildroot, source_root_dir))
    if not abspath.startswith(buildroot):
      raise ValueError('Source root %s is not under the build root %s' % (abspath, buildroot))
    source_root_dir = os.path.relpath(abspath, buildroot)

    types = cls._TYPES_BY_ROOT.get(source_root_dir)
    if types is None:
      types = OrderedSet()
      cls._TYPES_BY_ROOT[source_root_dir] = types

    for allowed_target_type in allowed_target_types:
      types.add(allowed_target_type)
      roots = cls._ROOTS_BY_TYPE.get(allowed_target_type)
      if roots is None:
        roots = OrderedSet()
        cls._ROOTS_BY_TYPE[allowed_target_type] = roots
      roots.add(source_root_dir)

########NEW FILE########
__FILENAME__ = util
# ==================================================================================================
# Copyright 2013 Foursquare Labs, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Ryan Williams'

from collections import Iterable

from twitter.common.lang import Compatibility

from .pants_target import Pants


def resolve(arg, clazz=Pants):
  """Wraps strings in Pants() targets, for BUILD file convenience.

    - single string literal gets wrapped in Pants() target
    - single object is left alone
    - list of strings and other miscellaneous objects gets its strings wrapped in Pants() targets
  """
  if isinstance(arg, Compatibility.string):
    return clazz(arg)
  elif isinstance(arg, Iterable):
    # If arg is iterable, recurse on its elements.
    return [resolve(dependency, clazz=clazz) for dependency in arg]
  else:
    # NOTE(ryan): Ideally we'd check isinstance(arg, Target) here, but some things that Targets
    # depend on are not themselves subclasses of Target, notably JarDependencies.
    return arg


########NEW FILE########
__FILENAME__ = with_dependencies
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import OrderedSet
from twitter.pants.base.target import Target

from .util import resolve


class TargetWithDependencies(Target):
  def __init__(self, name, dependencies=None, exclusives=None):
    Target.__init__(self, name, exclusives=exclusives)
    self.dependencies = OrderedSet(resolve(dependencies)) if dependencies else OrderedSet()

  def _walk(self, walked, work, predicate=None):
    Target._walk(self, walked, work, predicate)
    for dependency in self.dependencies:
      for dep in dependency.resolve():
        if isinstance(dep, Target) and not dep in walked:
          walked.add(dep)
          if not predicate or predicate(dep):
            additional_targets = work(dep)
            dep._walk(walked, work, predicate)
            if additional_targets:
              for additional_target in additional_targets:
                if hasattr(additional_target, '_walk'):
                  additional_target._walk(walked, work, predicate)

########NEW FILE########
__FILENAME__ = with_sources
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from collections import defaultdict

from twitter.common.lang import Compatibility
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.target import Target
from twitter.pants.targets.sources import SourceRoot


class TargetWithSources(Target):
  _source_to_targets = defaultdict(set)

  @classmethod
  def register_source(cls, source, target):
    cls._source_to_targets[source].add(target)

  def __init__(self, name, sources=None, exclusives=None):
    Target.__init__(self, name, exclusives=exclusives)

    self.add_labels('sources')
    self.target_base = SourceRoot.find(self)
    self._unresolved_sources = sources or []
    self._resolved_sources = None

  def expand_files(self, recursive=True, include_buildfile=True):
    """Expand files used to build this target to absolute paths.  By default this expansion is done
    recursively and target BUILD files are included.
    """

    files = []

    def _expand(target):
      files.extend([os.path.abspath(os.path.join(target.target_base, s))
          for s in (target.sources or [])])
      if include_buildfile:
        files.append(target.address.buildfile.full_path)
      if recursive:
        for dep in target.dependencies:
          if isinstance(dep, TargetWithSources):
            _expand(dep)
          elif hasattr(dep, 'address'):
            # Don't know what it is, but we'll include the BUILD file to be paranoid
            files.append(dep.address.buildfile.full_path)

    _expand(self)
    return files

  @property
  def sources(self):
    if self._resolved_sources is None:
      self._resolved_sources = self._resolve_paths(self._unresolved_sources or [])
    return self._resolved_sources

  def sources_relative_to_buildroot(self):
    """Returns this target's sources, relative to the buildroot.

    Prefer this over .sources unless you need to know about the target_base.
    """
    for src in self.sources:
      yield os.path.join(self.target_base, src)

  def sources_absolute_paths(self):
    """Returns the absolute paths of this target's sources.

    Prefer this over .sources unless you need to know about the target_base.
    """
    abs_target_base = os.path.join(get_buildroot(), self.target_base)
    for src in self.sources:
      yield os.path.join(abs_target_base, src)

  def set_resolved_sources(self, sources):
    """Set resolved sources directly, skipping the resolution.

    Useful when synthesizing targets.
    """
    self._resolved_sources = sources

  def _resolve_paths(self, paths):
    """Resolves paths."""
    if not paths:
      return []

    def flatten_paths(*items):
      """Flattens one or more items into a list.

      If the item is iterable each of its items is flattened.  If an item is callable, it is called
      and the result is flattened.  Otherwise the atom is appended to the flattened list.  These
      rules are applied recursively such that the returned list will only contain non-iterable,
      non-callable atoms.
      """

      flat = []

      def flatmap(item):
        if isinstance(item, Compatibility.string):
          flat.append(item)
        else:
          try:
            for i in iter(item):
              flatmap(i)
          except TypeError:
            if callable(item):
              flatmap(item())
            else:
              flat.append(item)

      for item in items:
        flatmap(item)

      return flat

    src_relpath = os.path.relpath(self.address.buildfile.parent_path,
                                  os.path.join(get_buildroot(), self.target_base))

    return [os.path.normpath(os.path.join(src_relpath, path)) for path in flatten_paths(paths)]

########NEW FILE########
__FILENAME__ = antlr_gen
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.collections import OrderedSet
from twitter.common.dirutil import safe_mkdir

from twitter.pants.targets.java_antlr_library import JavaAntlrLibrary
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.code_gen import CodeGen
from twitter.pants.tasks.nailgun_task import NailgunTask


class AntlrGen(CodeGen, NailgunTask):

  # Maps the compiler attribute of a target to the config key in pants.ini
  _CONFIG_SECTION_BY_COMPILER = {
    'antlr3': 'antlr-gen',
    'antlr4': 'antlr4-gen',
  }

  def __init__(self, context):
    CodeGen.__init__(self, context)
    NailgunTask.__init__(self, context)

    # TODO(John Sirois): kill if not needed by prepare_gen
    self._classpath_by_compiler = {}

    active_compilers = set(map(lambda t: t.compiler, context.targets(predicate=self.is_gentarget)))
    for compiler, tools in self._all_possible_antlr_bootstrap_tools():
      if compiler in active_compilers:
        self._jvm_tool_bootstrapper.register_jvm_tool(compiler, tools)

  def is_gentarget(self, target):
    return isinstance(target, JavaAntlrLibrary)

  def is_forced(self, lang):
    return True

  def genlangs(self):
    return dict(java=lambda t: t.is_jvm)

  def prepare_gen(self, targets):
    compilers = set(map(lambda t: t.compiler, targets))
    for compiler in compilers:
      classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(compiler)
      self._classpath_by_compiler[compiler] = classpath

  def genlang(self, lang, targets):
    if lang != 'java':
      raise TaskError('Unrecognized antlr gen lang: %s' % lang)

    # TODO: Instead of running the compiler for each target, collect the targets
    # by type and invoke it twice, once for antlr3 and once for antlr4.

    for target in targets:
      java_out = self._java_out(target)
      safe_mkdir(java_out)

      antlr_classpath = self._classpath_by_compiler[target.compiler]
      args = ["-o", java_out]

      if target.compiler == 'antlr3':
        java_main = 'org.antlr.Tool'
      elif target.compiler == 'antlr4':
        args.append("-visitor")  # Generate Parse Tree Vistor As Well
        java_main = 'org.antlr.v4.Tool'
      else:
        raise TaskError("Unknown ANTLR compiler: {}".format(target.compiler))

      sources = self._calculate_sources([target])
      args.extend(sources)
      result = self.runjava(classpath=antlr_classpath, main=java_main,
                            args=args, workunit_name='antlr')
      if result != 0:
        raise TaskError('java %s ... exited non-zero (%i)' % (java_main, result))

  def _calculate_sources(self, targets):
    sources = set()

    def collect_sources(target):
      if self.is_gentarget(target):
        sources.update(target.sources_relative_to_buildroot())
    for target in targets:
      target.walk(collect_sources)
    return sources

  def createtarget(self, lang, gentarget, dependees):
    if lang != 'java':
      raise TaskError('Unrecognized antlr gen lang: %s' % lang)
    return self._create_java_target(gentarget, dependees)

  def _create_java_target(self, target, dependees):
    antlr_files_suffix = ["Lexer.java", "Parser.java"]
    if (target.compiler == 'antlr4'):
      antlr_files_suffix = ["BaseListener.java", "BaseVisitor.java",
                            "Listener.java", "Visitor.java"] + antlr_files_suffix

    generated_sources = []
    for source in target.sources:
      # Antlr enforces that generated sources are relative to the base filename, and that
      # each grammar filename must match the resulting grammar Lexer and Parser classes.
      source_base, source_ext = os.path.splitext(source)
      for suffix in antlr_files_suffix:
        generated_sources.append(source_base + suffix)

    deps = self._resolve_java_deps(target)

    tgt = self.context.add_new_target(os.path.join(self._java_out(target), target.target_base),
                                      JavaLibrary,
                                      name=target.id,
                                      sources=generated_sources,
                                      provides=target.provides,
                                      dependencies=deps,
                                      excludes=target.excludes)
    for dependee in dependees:
      dependee.update_dependencies([tgt])
    return tgt

  def _resolve_java_deps(self, target):
    key = self._CONFIG_SECTION_BY_COMPILER[target.compiler]

    deps = OrderedSet()
    for dep in self.context.config.getlist(key, 'javadeps'):
        deps.update(self.context.resolve(dep))
    return deps

  def _all_possible_antlr_bootstrap_tools(self):
    for compiler, key in self._CONFIG_SECTION_BY_COMPILER.items():
      yield compiler, self.context.config.getlist(key, 'javadeps')

  def _java_out(self, target):
    key = self._CONFIG_SECTION_BY_COMPILER[target.compiler]
    return os.path.join(self.context.config.get(key, 'workdir'), 'gen-java')

########NEW FILE########
__FILENAME__ = args_resource_mapper
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os

from twitter.pants.java.jar import open_jar
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.scala_library import ScalaLibrary

from .task import Task


RESOURCE_RELDIR = 'com/twitter/common/args/apt'
RESOURCE_BASENAME = 'cmdline.arg.info.txt'


class ArgsResourceMapper(Task):
  """Maps resource files generated by com.twitter.common#args-apt into a binary jar."""

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("include_all"), mkflag("include_all", negate=True),
                            dest="args_resource_mapper_include_all", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Include all arg fields resources.")

  def __init__(self, context, select_targets, transitive, main):
    """
    :param context: The pants context.
    :param select_targets: A predicate that selects the targets to create a trimmed cmdline args
      resource file for.
    :param transitive: If True, splits cmdline args resource info for all classes in the
      transitive closure of classes depended on by the selected targets; otherwise, just
      selects cmdline info for the classes owned by the selected targets directly.
    :param main: True if the split cmdline arg resource info is for a main; False otherwise.
    """
    Task.__init__(self, context)

    self.select_targets = select_targets
    self.transitive = transitive

    # The args apt plugin uses a sequential suffix scheme to detect a family of cmdline args
    # resource files available on a classpath.  The 0th slot is normally skipped and reserved to
    # the cmdline arg resource file of a main.
    self.resource_index = 0 if main else 1

    context.products.require('jars', self.select_targets)
    context.products.require_data('classes_by_target')
    default_args_resource_mapper = [
        os.path.join(self.get_workdir(key='java_workdir', workdir='javac'), 'classes')]
    self.classdirs = context.config.getlist('args-resource-mapper', 'classdirs',
                                            default=default_args_resource_mapper)
    self.include_all = context.options.args_resource_mapper_include_all

  def execute(self, targets):
    if self.classdirs:
      jarmap = self.context.products.get('jars')
      for target in filter(self.select_targets, targets):
        mapping = jarmap.get(target)
        if mapping:
          for basedir, jars in mapping.items():
            for jar in jars:
              self._addargsresources(os.path.join(basedir, jar), target)
        else:
          self.context.log.warn('No classes found for target %s' % target)

  def _addargsresources(self, jar, target):
    lines = set()
    for resourcedir in [os.path.join(classdir, RESOURCE_RELDIR) for classdir in self.classdirs]:
      if os.path.exists(resourcedir):
        for path in os.listdir(resourcedir):
          if path.startswith(RESOURCE_BASENAME):
            with open(os.path.join(resourcedir, path)) as resource:
              lines.update(resource.readlines())

    if lines:
      class Args(object):
        def __init__(self, context, transitive, classes_by_target):
          self.context = context
          self.classnames = set()

          def add_classnames(target):
            if target.has_sources('.java'):
              target_products = classes_by_target.get(target)
              if target_products:
                for _, classes in target_products.rel_paths():
                  for cls in classes:
                    self.classnames.add(cls.replace('.class', '').replace('/', '.'))
              else:
                self.context.log.debug('No mapping for %s' % target)

          if transitive:
            target.walk(add_classnames, lambda t: t.is_internal)
          else:
            add_classnames(target)

        def matches(self, line):
          line = line.strip()
          if not line:
            return False
          components = line.split(' ')
          keyname = components[0]
          if keyname in ('positional', 'field'):
            # Line format: [key] class field
            return components[1] in self.classnames
          elif keyname == 'parser':
            # Line format: [key] parsed-class parser-class
            return components[2] in self.classnames
          elif keyname == 'verifier':
            # Line format: [key] verified-class verification-annotation-class verifier-class
            return components[2] in self.classnames and components[3] in self.classnames
          else:
            # Unknown line (comments, ws, unknown configuration types
            return True

      classes_by_target = self.context.products.get_data('classes_by_target')
      self._addargs(lines if self.include_all
                          else filter(Args(self.context,
                                           self.transitive,
                                           classes_by_target).matches, lines),
                    jar,
                    target)

  def _addargs(self, lines, jarfile, target):
    def is_configurationinfo(line):
      line = line.strip()
      return line and not line.startswith('#')

    if any(filter(is_configurationinfo, lines)):
      resource = os.path.join(RESOURCE_RELDIR, '%s.%d' % (RESOURCE_BASENAME, self.resource_index))

      content = '# Created by pants goal args-apt\n'
      content += ''.join(sorted(lines))

      with open_jar(jarfile, 'a') as jar:
        jar.writestr(resource, content)
        self.context.log.debug('Added args-apt resource file %s for %s:'
                               '\n%s' % (resource, target, content))

########NEW FILE########
__FILENAME__ = benchmark_run
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import shutil

from twitter.pants.java.util import execute_java

from .jvm_task import JvmTask

from . import TaskError


class BenchmarkRun(JvmTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("target"), dest="target_class", action="append",
                            help="Name of the benchmark class.")

    option_group.add_option(mkflag("memory"), mkflag("memory", negate=True),
                            dest="memory_profiling", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Enable memory profiling.")

    option_group.add_option(mkflag("debug"), mkflag("debug", negate=True),
                            dest="debug", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Enable caliper debug mode.")

    option_group.add_option(mkflag("caliper-args"), dest="extra_caliper_args", default=[],
                            action="append",
                            help="Allows the user to pass additional command line options to "
                                 "caliper. Can be used multiple times and arguments will be "
                                 "concatenated. Example use: --bench-caliper-args='-Dsize=10,20 "
                                 "-Dcomplex=true,false' --bench-caliper-args=-Dmem=1,2,3")

  def __init__(self, context):
    super(BenchmarkRun, self).__init__(context)

    config = context.config
    self.confs = config.getlist('benchmark-run', 'confs', default=['default'])
    self.jvm_args = config.getlist('benchmark-run', 'jvm_args',
                                   default=['-Xmx1g', '-XX:MaxPermSize=256m'])

    self._benchmark_bootstrap_key = 'benchmark-tool'
    benchmark_bootstrap_tools = config.getlist('benchmark-run', 'bootstrap-tools',
                                               default=[':benchmark-caliper-0.5'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._benchmark_bootstrap_key,
                                                  benchmark_bootstrap_tools)
    self._agent_bootstrap_key = 'benchmark-agent'
    agent_bootstrap_tools = config.getlist('benchmark-run', 'agent_profile',
                                           default=[':benchmark-java-allocation-instrumenter-2.1'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._agent_bootstrap_key, agent_bootstrap_tools)

    # TODO(Steve Gury):
    # Find all the target classes from the Benchmark target itself
    # https://jira.twitter.biz/browse/AWESOME-1938
    self.caliper_args = context.options.target_class

    if context.options.memory_profiling:
      self.caliper_args += ['--measureMemory']

    if context.options.debug:
      self.jvm_args.extend(context.config.getlist('jvm', 'debug_args'))
      self.caliper_args += ['--debug']

    self.caliper_args.extend(context.options.extra_caliper_args)

  def execute(self, targets):
    # For rewriting JDK classes to work, the JAR file has to be listed specifically in
    # the JAR manifest as something that goes in the bootclasspath.
    # The MANIFEST list a jar 'allocation.jar' this is why we have to rename it
    agent_tools_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
        self._agent_bootstrap_key)
    agent_jar = agent_tools_classpath[0]
    allocation_jar = os.path.join(os.path.dirname(agent_jar), "allocation.jar")

    # TODO(Steve Gury): Find a solution to avoid copying the jar every run and being resilient
    # to version upgrade
    shutil.copyfile(agent_jar, allocation_jar)
    os.environ['ALLOCATION_JAR'] = str(allocation_jar)

    benchmark_tools_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
        self._benchmark_bootstrap_key)

    classpath = self.classpath(benchmark_tools_classpath,
                               confs=self.confs,
                               exclusives_classpath=self.get_base_classpath_for_target(targets[0]))

    caliper_main = 'com.google.caliper.Runner'
    exit_code = execute_java(classpath=classpath,
                             main=caliper_main,
                             jvm_options=self.jvm_args,
                             args=self.caliper_args,
                             workunit_factory=self.context.new_workunit,
                             workunit_name='caliper')
    if exit_code != 0:
      raise TaskError('java %s ... exited non-zero (%i)' % (caliper_main, exit_code))

########NEW FILE########
__FILENAME__ = binary_create
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from zipfile import ZIP_STORED, ZIP_DEFLATED
import zipfile

from twitter.common.contextutil import temporary_dir
from twitter.common.dirutil import safe_mkdir

from twitter.pants.base.build_environment import get_buildroot, get_version
from twitter.pants.tasks import TaskError
from twitter.pants.fs.archive import ZIP
from twitter.pants.java.jar import open_jar, Manifest
from twitter.pants.tasks.jvm_binary_task import JvmBinaryTask


class BinaryCreate(JvmBinaryTask):
  """Creates a runnable monolithic binary deploy jar."""

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    JvmBinaryTask.setup_parser(option_group, args, mkflag)
    option_group.add_option(mkflag("compressed"), mkflag("compressed", negate=True),
                            dest="binary_create_compressed", default=True,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Create a compressed binary jar.")

    option_group.add_option(mkflag("zip64"), mkflag("zip64", negate=True),
                            dest="binary_create_zip64", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Create the binary jar with zip64 extensions.")

  def __init__(self, context):
    JvmBinaryTask.__init__(self, context)

    self.outdir = os.path.abspath(
      context.options.jvm_binary_create_outdir or
      context.config.get('binary-create', 'outdir',
                         default=context.config.getdefault('pants_distdir'))
    )
    self.compression = ZIP_DEFLATED if context.options.binary_create_compressed else ZIP_STORED
    self.zip64 = (
      context.options.binary_create_zip64
      or context.config.getbool('binary-create', 'zip64', default=False)
    )
    self.deployjar = context.options.jvm_binary_create_deployjar

    context.products.require('jars', predicate=self.is_binary)
    context.products.require_data('classes_by_target')
    context.products.require_data('resources_by_target')
    if self.deployjar:
      self.require_jar_dependencies()

  def execute(self, targets):
    for binary in filter(self.is_binary, targets):
      self.create_binary(binary)

  def create_binary(self, binary):
    import platform
    safe_mkdir(self.outdir)

    jarmap = self.context.products.get('jars')

    binary_jarname = '%s.jar' % binary.basename
    binaryjarpath = os.path.join(self.outdir, binary_jarname)
    self.context.log.info('creating %s' % os.path.relpath(binaryjarpath, get_buildroot()))

    with open_jar(binaryjarpath, 'w', compression=self.compression, allowZip64=self.zip64) as jar:
      def add_jars(target):
        generated = jarmap.get(target)
        if generated:
          for basedir, jars in generated.items():
            for internaljar in jars:
              self.dump(os.path.join(basedir, internaljar), jar)

      binary.walk(add_jars, lambda t: t.is_internal)

      if self.deployjar:
        for basedir, externaljar in self.list_jar_dependencies(binary):
          self.dump(os.path.join(basedir, externaljar), jar)

      def write_binary_data(product_type):
        data = self.context.products.get_data(product_type).get(binary)
        if data:
          for root, rel_paths in data.rel_paths():
            for rel_path in rel_paths:
              jar.write(os.path.join(root, rel_path), arcname=rel_path)

      write_binary_data('classes_by_target')
      write_binary_data('resources_by_target')

      manifest = Manifest()
      manifest.addentry(Manifest.MANIFEST_VERSION, '1.0')
      manifest.addentry(
        Manifest.CREATED_BY,
        'python %s pants %s (Twitter, Inc.)' % (platform.python_version(), get_version())
      )
      main = binary.main or '*** java -jar not supported, please use -cp and pick a main ***'
      manifest.addentry(Manifest.MAIN_CLASS,  main)
      jar.writestr(Manifest.PATH, manifest.contents())

      jarmap.add(binary, self.outdir, [binary_jarname])

  def dump(self, jarpath, jarfile):
    self.context.log.debug('  dumping %s' % jarpath)

    with temporary_dir() as tmpdir:
      try:
        ZIP.extract(jarpath, tmpdir)
      except zipfile.BadZipfile:
        raise TaskError('Bad JAR file, maybe empty: %s' % jarpath)
      for root, dirs, files in os.walk(tmpdir):
        for f in files:
          path = os.path.join(root, f)
          relpath = os.path.relpath(path, tmpdir)
          if Manifest.PATH != relpath:
            jarfile.write(path, relpath)


########NEW FILE########
__FILENAME__ = bootstrap_jvm_tools
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===================================================================================================

import threading

from twitter.pants.base.workunit import WorkUnit

from . import Task, TaskError


class BootstrapJvmTools(Task):

  def __init__(self, context):
    super(BootstrapJvmTools, self).__init__(context)
    context.products.require_data('jvm_build_tools')

  def execute(self, targets):
    context = self.context
    if context.products.is_required_data('jvm_build_tools_classpath_callbacks'):
      tool_product_map = context.products.get_data('jvm_build_tools') or {}
      callback_product_map = context.products.get_data('jvm_build_tools_classpath_callbacks') or {}
      # We leave a callback in the products map because we want these Ivy calls
      # to be done lazily (they might never actually get executed) and we want
      # to hit Task.invalidated (called in Task.ivy_resolve) on the instance of
      # BootstrapJvmTools rather than the instance of whatever class requires
      # the bootstrap tools.  It would be awkward and possibly incorrect to call
      # self.invalidated twice on a Task that does meaningful invalidation on its
      # targets. -pl
      for key, deplist in tool_product_map.iteritems():
        callback_product_map[key] = self.cached_bootstrap_classpath_callback(key, deplist)
      context.products.safe_create_data('jvm_build_tools_classpath_callbacks',
                                        lambda: callback_product_map)

  def resolve_tool_targets(self, tools):
    if not tools:
      raise TaskError("BootstrapJvmTools.resolve_tool_targets called with no tool"
                      " dependency addresses.  This probably means that you don't"
                      " have an entry in your pants.ini for this tool.")
    for tool in tools:
      try:
        targets = list(self.context.resolve(tool))
        if not targets:
          raise KeyError
      except KeyError:
        self.context.log.error("Failed to resolve target for bootstrap tool: %s. "
                               "You probably need to add this dep to your tools "
                               "BUILD file(s), usually located in the root of the build." %
                               tool)
        raise
      for target in targets:
        yield target

  def cached_bootstrap_classpath_callback(self, key, tools):
    cache = {}
    cache_lock = threading.Lock()

    def bootstrap_classpath(executor=None):
      with cache_lock:
        if 'classpath' not in cache:
          targets = list(self.resolve_tool_targets(tools))
          workunit_name = 'bootstrap-%s' % str(key)
          cache['classpath'] = self.ivy_resolve(targets,
                                                executor=executor,
                                                silent=True,
                                                workunit_name=workunit_name,
                                                workunit_labels=[WorkUnit.BOOTSTRAP])
        return cache['classpath']
    return bootstrap_classpath

########NEW FILE########
__FILENAME__ = builddictionary
# =============================================================================
# Copyright 2013 Twitter, Inc.
# -----------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================

import inspect
import os

from twitter.common.dirutil import Fileset, safe_open
from twitter.pants.base.build_file_helpers import maven_layout
from twitter.pants.base.build_manual import get_builddict_info
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.generator import Generator, TemplateData
from twitter.pants.goal.phase import Phase
from twitter.pants.tasks import Task, TaskError

from pkg_resources import resource_string


def entry(nom, classdoc=None, msg_rst=None, argspec=None, funcdoc=None, methods=None):
  """Create a struct that our template expects to see.

  :param nom: Symbol name, e.g. python_binary
  :param classdoc: plain text appears above argspec
  :param msg_rst: reST. useful in hand-crafted entries
  :param argspec: arg string like (x, y="deflt")
  :param funcdoc: function's __doc__, plain text
  :param methods: list of entries for class' methods
  """

  def indent_docstring_by_1(s):
    """Given a non-empty docstring, return a version indented by a space.
    Given an empty thing, return the thing itself
    """
    # In reST, it's useful to have strings that are similarly-indented.
    # If we have a classdoc indented by 2 next to an __init__ funcdoc indented
    # by 4, reST doesn't format things nicely. Oh, totally-dedenting doesn't
    # format nicely either.

    # Docstring indentation: more gnarly than you'd think:
    # http://www.python.org/dev/peps/pep-0257/#handling-docstring-indentation
    if not s: return s
    # Convert tabs to spaces (following the normal Python rules)
    # and split into a list of lines:
    lines = s.expandtabs().splitlines()
    # Determine minimum indentation (first line doesn't count):
    indent = 999
    for line in lines[1:]:
      stripped = line.lstrip()
      if stripped:
        indent = min(indent, len(line) - len(stripped))
    # Remove indentation (first line is special):
    trimmed = [lines[0].strip()]
    if indent < 999:
      for line in lines[1:]:
        trimmed.append(line[indent:].rstrip())
    # Strip off trailing and leading blank lines:
    while trimmed and not trimmed[-1]:
      trimmed.pop()
    while trimmed and not trimmed[0]:
      trimmed.pop(0)
    # Return a single string:
    return '\n'.join([" " + t for t in trimmed])

  return TemplateData(
    nom=nom.strip(),
    classdoc=indent_docstring_by_1(classdoc),
    msg_rst=indent_docstring_by_1(msg_rst),
    argspec=argspec,
    funcdoc=indent_docstring_by_1(funcdoc),
    methods=methods,
    showmethods=(methods and len(methods) > 0))


def msg_entry(nom, defn):
  """For hard-wired entries a la "See Instead" or other simple stuff"""
  return entry(nom, msg_rst=defn)


def entry_for_one_func(nom, func):
  """Generate a BUILD dictionary entry for a function
  nom: name like 'python_binary'
  func: function object"""
  args, varargs, varkw, defaults = inspect.getargspec(func)
  argspec = inspect.formatargspec(args, varargs, varkw, defaults)
  return entry(nom,
               argspec=argspec,
               funcdoc=func.__doc__)


def entry_for_one_method(nom, method):
  """Generate a BUILD dictionary entry for a method
  nom: name like 'with_description'
  method: method object"""
  # TODO(lhosken) : This is darned similar to entry_for_one_func. Merge 'em?
  #                 (Punted so far since funcdoc indentation made my head hurt)
  assert inspect.ismethod(method)
  args, varargs, varkw, defaults = inspect.getargspec(method)
  # args[:1] instead of args to discard "self" arg
  argspec = inspect.formatargspec(args[1:], varargs, varkw, defaults)
  return entry(nom,
               argspec=argspec,
               funcdoc=(method.__doc__ or "").replace("\n", " "))


def entry_for_one(nom, sym):
  if inspect.isclass(sym):
    return entry_for_one_class(nom, sym)
  if inspect.ismethod(sym) or inspect.isfunction(sym):
    return entry_for_one_func(nom, sym)
  return msg_entry(nom, "TODO! no doc gen for %s %s" % (
        str(type(sym)), str(sym)))


PREDEFS = {  # some hardwired entries
  "Amount": {"defn": msg_entry("Amount", """
                                `Amount from twitter.commons.quantity <https://github.com/twitter/commons/blob/master/src/python/twitter/common/quantity/__init__.py>`_
                                E.g., ``Amount(2, Time.MINUTES)``.""")},
  "__file__": {"defn": msg_entry("__file__", "Path to BUILD file (string).")},
  "globs": {"defn": entry_for_one("globs", Fileset.globs)},
  "jar_library": {"defn": msg_entry("jar_library",
                  """Old name for `dependencies`_""")},
  "java_tests": {"defn": msg_entry("java_tests",
                  """Old name for `junit_tests`_""")},
  "maven_layout": {"defn": entry_for_one("maven_layout", maven_layout)},
  "python_artifact": {"suppress": True}, # unused alias for PythonArtifact
  "rglobs": {"defn": entry_for_one("rglobs", Fileset.rglobs)},
  "ROOT_DIR": {"defn": msg_entry("ROOT_DIR",
                                  "Root directory of source code (string).")},
  "scala_tests": {"defn": msg_entry("scala_tests",
                  """Old name for `scala_specs`_""")},
  "Time": {"defn": msg_entry("Time", """
                             `Amount from twitter.commons.quantity <https://github.com/twitter/commons/blob/master/src/python/twitter/common/quantity/__init__.py>`_
                             E.g., ``Amount(2, Time.MINUTES)``."""), },
}


# Thingies like scala_library
# Returns list of duples [(name, object), (name, object), (name, object),...]
def get_syms():
  r = {}
  vc = ParseContext.default_globals()
  for s in vc:
    if s in PREDEFS: continue
    if s[0].isupper(): continue  # REMIND see both jvm_binary and JvmBinary??
    o = vc[s]
    r[s] = o
  return r


def tocl(d):
  """Generate TOC, in-page links to the IDs we're going to define below"""
  anchors = sorted(d.keys(), key=str.lower)
  return TemplateData(t="All The Things", e=[a for a in anchors])


def tags_tocl(d, tag_list, title):
  """Generate specialized TOC.
  E.g., tags_tocl(d, ["python", "anylang"], "Python")
  tag_list: if an entry's tags contains any of these, use it
  title: pretty title
  """
  filtered_anchors = []
  for anc in sorted(d.keys(), key=str.lower):
    entry = d[anc]
    if not "tags" in entry: continue
    found = [t for t in tag_list if t in entry["tags"]]
    if not found: continue
    filtered_anchors.append(anc)
  return TemplateData(t=title, e=filtered_anchors)


def entry_for_one_class(nom, klas):
  """  Generate a BUILD dictionary entry for a class.
  nom: name like 'python_binary'
  klas: class like twitter.pants.python_binary"""
  try:
    args, varargs, varkw, defaults = inspect.getargspec(klas.__init__)
    argspec = inspect.formatargspec(args[1:], varargs, varkw, defaults)
    funcdoc = klas.__init__.__doc__

    methods = []
    for attrname in dir(klas):
      attr = getattr(klas, attrname)
      attr_bdi = get_builddict_info(attr)
      if not attr_bdi: continue
      if inspect.ismethod(attr):
        methods.append(entry_for_one_method(attrname, attr))
        continue
      raise TaskError('@manual.builddict on non-method %s within class %s '
                      'but I only know what to do with methods' %
                      (attrname, nom))

  except TypeError:  # __init__ might not be a Python function
    argspec = None
    funcdoc = None
    methods = None

  return entry(nom,
               classdoc=klas.__doc__,
               argspec=argspec,
               funcdoc=funcdoc,
               methods=methods)


def assemble(predefs=PREDEFS, symbol_hash=None):
  """Assemble big hash of entries suitable for smushing into a template.

  predefs: Hash of "hard-wired" predefined entries.
  symbol_hash: Python syms from which to generate more entries. Default: get from BUILD context"""
  d = {}
  for k in PREDEFS:
    v = PREDEFS[k]
    if "suppress" in v and v["suppress"]: continue
    d[k] = v
  if symbol_hash is None:
    symbol_hash = get_syms()
  for k in symbol_hash:
    bdi = get_builddict_info(symbol_hash[k])
    if bdi is None: continue
    d[k] = bdi.copy()
    if not "defn" in d[k]:
      d[k]["defn"] = entry_for_one(k, symbol_hash[k])
  return d


class BuildBuildDictionary(Task):
  """Generate documentation for the Sphinx site."""

  def __init__(self, context):
    super(BuildBuildDictionary, self).__init__(context)
    self._templates_dir = os.path.join('templates', 'builddictionary')
    self._outdir = os.path.join(self.context.config.getdefault("pants_distdir"), "builddict")

  def execute(self, targets):
    self._gen_goals_reference()

    d = assemble()
    template = resource_string(__name__, os.path.join(self._templates_dir, 'page.mustache'))
    tocs = [tocl(d),
            tags_tocl(d, ["java", "scala", "jvm", "anylang"], "JVM"),
            tags_tocl(d, ["python", "anylang"], "Python")]
    defns = [d[t]["defn"] for t in sorted(d.keys(), key=str.lower)]
    filename = os.path.join(self._outdir, 'build_dictionary.rst')
    self.context.log.info('Generating %s' % filename)
    with safe_open(filename, 'w') as outfile:
      generator = Generator(template,
                            tocs=tocs,
                            defns=defns)
      generator.write(outfile)

  def _gen_goals_reference(self):
    """Generate the goals reference rst doc."""
    phase_dict = {}
    phase_names = []
    for phase, raw_goals in Phase.all():
      goals = []
      for g in raw_goals:
        # TODO(lahosken) generalize indent_docstring, use here
        doc = (g.task_type.__doc__ or "").replace("\n", " ").strip()
        goals.append(TemplateData(name=g.task_type.__name__, doc=doc))
      phase_dict[phase.name] = TemplateData(phase=phase, goals=goals)
      phase_names.append(phase.name)

    phases = [phase_dict[name] for name in sorted(phase_names, key=str.lower)]

    template = resource_string(__name__,
                               os.path.join(self._templates_dir, 'goals_reference.mustache'))
    filename = os.path.join(self._outdir, 'goals_reference.rst')
    self.context.log.info('Generating %s' % filename)
    with safe_open(filename, 'w') as outfile:
      generator = Generator(template, phases=phases)
      generator.write(outfile)

########NEW FILE########
__FILENAME__ = build_lint
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================
import os

__author__ = 'Benjy Weinberger'

import difflib
import re

from collections import defaultdict

from twitter.pants.tasks import Task


class BuildLint(Task):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    Task.setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag("transitive"), mkflag("transitive", negate=True),
      dest="buildlint_transitive", default=False,
      action="callback", callback=mkflag.set_bool,
      help="[%default] apply lint rules transitively to all dependency buildfiles.")

    option_group.add_option(mkflag("include-intransitive-deps"),
      mkflag("include-intransitive-deps", negate=True),
      dest="buildlint_include_intransitive", default=False,
      action="callback", callback=mkflag.set_bool,
      help="[%default] correct both simple missing dependencies and intransitive missing deps")


    option_group.add_option(mkflag("action"), dest="buildlint_actions", default=[],
      action="append", type="choice", choices=['diff', 'rewrite'],
      help="diff=print out diffs, rewrite=apply changes to BUILD files directly.")

  def __init__(self, context):
    Task.__init__(self, context)
    context.products.require('missing_deps')
    self.transitive = context.options.buildlint_transitive
    self.actions = set(context.options.buildlint_actions)
    self.include_intransitive = context.options.buildlint_include_intransitive
    # Manually apply the default. Can't use flag default, because action is 'append', so
    # diffs would always be printed, even if we only wanted to rewrite.
    if not self.actions:
      self.actions.add('diff')

  def execute(self, targets):
    # Map from buildfile path to map of target name -> missing deps for that target.
    buildfile_paths = defaultdict(lambda: defaultdict(list))
    genmap_trans = self.context.products.get('missing_deps')
    genmap_intrans = self.context.products.get('missing_intransitive_deps')

    def add_buildfile_for_target(target, genmap):
      missing_dep_map = genmap[target]
      missing_deps = missing_dep_map[self.context._buildroot] if missing_dep_map else defaultdict(list)
      buildfile_paths[target.address.buildfile.full_path][target.name] += missing_deps

    if self.transitive:
      for target in targets:
        add_buildfile_for_target(target, genmap_trans)
        if self.include_intransitive:
          add_buildfile_for_target(target, genmap_intrans)
    else:
      for target in self.context.target_roots:
        add_buildfile_for_target(target, genmap_trans)
        if self.include_intransitive:
          add_buildfile_for_target(target, genmap_intrans)

    for buildfile_path, missing_dep_map in buildfile_paths.items():
      self._fix_lint(buildfile_path, missing_dep_map)


  # We use heuristics to find target names and their list of dependencies.
  # Attempts to use the Python AST proved to be extremely complex and not worth the trouble.
  NAMES_RE = re.compile('^\w+\(\s*name\s*=\s*["\']((?:\w|-)+)["\']', flags=re.DOTALL|re.MULTILINE)
  DEPS_RE = re.compile(r'^\s*dependencies\s*=\s*\[([^\]]*)\s*\]', flags=re.DOTALL|re.MULTILINE)
  INLINE_SINGLE_DEP_RE = re.compile(r'^ *dependencies *= *\[[^\n,\]]* *\]')

  def _fix_lint(self, buildfile_path, missing_dep_map):
    if os.path.exists(buildfile_path):
      with open(buildfile_path, 'r') as infile:
        old_buildfile_source = infile.read()
      names = []
      for m in BuildLint.NAMES_RE.finditer(old_buildfile_source):
        names.append(m.group(1))

      # We'll step through this to find the name of the target whose deps we're currently looking at.
      nameiter = iter(names)

      def sort_deps(m):
        try:
          name = nameiter.next()
        except StopIteration:
          name = '-UNKNOWN-'
        deps = m.group(1).split('\n')
        deps = filter(lambda x: x, [x.strip().replace('"', "'") for x in deps])
        missing_deps = ["'%s'," % x for x in missing_dep_map[name]]
        deps.extend(missing_deps)
        if deps:  # Add comma if needed. We must do this before sorting.
          # Allow a single dep on a single line, if that's what the file already had.
          # This is common in 3rdparty/BUILD files.
          if len(deps) == 1 and BuildLint.INLINE_SINGLE_DEP_RE.match(m.group(0)):
            return '  dependencies = [%s]' % deps[0]
          parts = [x.strip() for x in deps[-1].split('#')]
          if not parts[0].rstrip().endswith(','):
            deps[-1] = '%s,%s' % (parts[0], ' #' + parts[1] if len(parts) > 1 else '')

        # The key hack is to make sure local imports (those starting with a colon) come last.
        deps = sorted(deps, key=lambda x: 'zzz' + x if (x.startswith("':") or x.startswith("pants(':")) else x)
        res = '  dependencies = [\n    %s\n  ]' % ('\n    '.join(deps)) if deps else 'dependencies = []'
        return res

      new_buildfile_source = BuildLint.DEPS_RE.sub(sort_deps, old_buildfile_source)
      if new_buildfile_source != old_buildfile_source:
        if 'rewrite' in self.actions:
          with open(buildfile_path, 'w') as outfile:
            outfile.write(new_buildfile_source)
        if 'diff' in self.actions:
          diff = '\n'.join(difflib.unified_diff(old_buildfile_source.split('\n'),
            new_buildfile_source.split('\n'), buildfile_path))
          print(diff)

########NEW FILE########
__FILENAME__ = bundle_create
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from zipfile import ZIP_DEFLATED

from twitter.common.collections import OrderedSet
from twitter.common.contextutil import open_zip
from twitter.common.dirutil import safe_mkdir

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.fs import archive
from twitter.pants.java.jar import Manifest
from twitter.pants.targets.jvm_binary import JvmApp, JvmBinary
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.jvm_binary_task import JvmBinaryTask


class BundleCreate(JvmBinaryTask):

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    JvmBinaryTask.setup_parser(option_group, args, mkflag)

    archive_flag = mkflag("archive")
    option_group.add_option(archive_flag, dest="bundle_create_archive",
                            type="choice", choices=list(archive.TYPE_NAMES),
                            help="[%%default] Create an archive from the bundle. "
                                 "Choose from %s" % sorted(archive.TYPE_NAMES))

    option_group.add_option(mkflag("archive-prefix"), mkflag("archive-prefix", negate=True),
                            dest="bundle_create_prefix", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%%default] Used in conjunction with %s this packs the archive "
                                 "with its basename as the path prefix." % archive_flag)

  def __init__(self, context):
    JvmBinaryTask.__init__(self, context)

    self.outdir = (
      context.options.jvm_binary_create_outdir or
      context.config.get('bundle-create', 'outdir',
                         default=context.config.getdefault('pants_distdir'))
    )

    self.prefix = context.options.bundle_create_prefix

    def fill_archiver_type():
      self.archiver_type = context.options.bundle_create_archive
      # If no option specified, check if anyone is requiring it
      if not self.archiver_type:
        for archive_type in archive.TYPE_NAMES:
          if context.products.isrequired(archive_type):
            self.archiver_type = archive_type

    fill_archiver_type()
    self.deployjar = context.options.jvm_binary_create_deployjar
    if not self.deployjar:
      self.context.products.require('jars', predicate=self.is_binary)
    self.require_jar_dependencies()

  class App(object):
    """A uniform interface to an app."""

    @staticmethod
    def is_app(target):
      return isinstance(target, (JvmApp, JvmBinary))

    def __init__(self, target):
      assert self.is_app(target), "%s is not a valid app target" % target

      self.binary = target if isinstance(target, JvmBinary) else target.binary
      self.bundles = [] if isinstance(target, JvmBinary) else target.bundles
      self.basename = target.basename

  def execute(self, _):
    archiver = archive.archiver(self.archiver_type) if self.archiver_type else None
    for target in self.context.target_roots:
      for app in map(self.App, filter(self.App.is_app, target.resolve())):
        basedir = self.bundle(app)
        if archiver:
          archivemap = self.context.products.get(self.archiver_type)
          archivepath = archiver.create(
            basedir,
            self.outdir,
            app.basename,
            prefix=app.basename if self.prefix else None
          )
          archivemap.add(app, self.outdir, [archivepath])
          self.context.log.info('created %s' % os.path.relpath(archivepath, get_buildroot()))

  def bundle(self, app):
    """Create a self-contained application bundle containing the target
    classes, dependencies and resources.
    """
    assert(isinstance(app, BundleCreate.App))

    bundledir = os.path.join(self.outdir, '%s-bundle' % app.basename)
    self.context.log.info('creating %s' % os.path.relpath(bundledir, get_buildroot()))

    safe_mkdir(bundledir, clean=True)

    classpath = OrderedSet()
    if not self.deployjar:
      libdir = os.path.join(bundledir, 'libs')
      os.mkdir(libdir)

      # Add internal dependencies to the bundle.
      def add_jars(target):
        target_jars = self.context.products.get('jars').get(target)
        if target_jars is not None:
          for basedir, jars in target_jars.items():
            for internaljar in jars:
              os.symlink(os.path.join(basedir, internaljar),
                         os.path.join(libdir, internaljar))
              classpath.add(internaljar)
      app.binary.walk(add_jars, lambda t: t.is_internal)

      # Add external dependencies to the bundle.
      for basedir, externaljar in self.list_jar_dependencies(app.binary):
        path = os.path.join(basedir, externaljar)
        os.symlink(path, os.path.join(libdir, externaljar))
        classpath.add(externaljar)

    for basedir, jars in self.context.products.get('jars').get(app.binary).items():
      if len(jars) != 1:
        raise TaskError('Expected 1 mapped binary for %s but found: %s' % (app.binary, jars))

      binary = jars[0]
      binary_jar = os.path.join(basedir, binary)
      bundle_jar = os.path.join(bundledir, binary)
      if not classpath:
        os.symlink(binary_jar, bundle_jar)
      else:
        with open_zip(binary_jar, 'r') as src:
          with open_zip(bundle_jar, 'w', compression=ZIP_DEFLATED) as dest:
            for item in src.infolist():
              buf = src.read(item.filename)
              if Manifest.PATH == item.filename:
                manifest = Manifest(buf)
                manifest.addentry(Manifest.CLASS_PATH,
                                  ' '.join(os.path.join('libs', jar) for jar in classpath))
                buf = manifest.contents()
              dest.writestr(item, buf)

    for bundle in app.bundles:
      for path, relpath in bundle.filemap.items():
        bundlepath = os.path.join(bundledir, relpath)
        safe_mkdir(os.path.dirname(bundlepath))
        os.symlink(path, bundlepath)

    return bundledir

########NEW FILE########
__FILENAME__ = cache_manager
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

try:
  import cPickle as pickle
except ImportError:
  import pickle

from twitter.pants.base.build_invalidator import (
    BuildInvalidator,
    CacheKeyGenerator,
    NO_SOURCES,
    TARGET_SOURCES)
from twitter.pants.base.target import Target
from twitter.pants.targets.external_dependency import ExternalDependency
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.targets.jar_library import JarLibrary
from twitter.pants.targets.pants_target import Pants
from twitter.pants.targets.with_sources import TargetWithSources


class VersionedTargetSet(object):
  """Represents a list of targets, a corresponding CacheKey, and a flag determining whether the
  list of targets is currently valid.

  When invalidating a single target, this can be used to represent that target as a singleton.
  When checking the artifact cache, this can also be used to represent a list of targets that are
  built together into a single artifact.
  """

  @staticmethod
  def from_versioned_targets(versioned_targets):
    first_target = versioned_targets[0]
    cache_manager = first_target._cache_manager

    # Quick sanity check; all the versioned targets should have the same cache manager.
    # TODO(ryan): the way VersionedTargets store their own links to a single CacheManager instance
    # feels hacky; see if there's a cleaner way for callers to handle awareness of the CacheManager.
    for versioned_target in versioned_targets:
      if versioned_target._cache_manager != cache_manager:
        raise ValueError("Attempting to combine versioned targets %s and %s with different"
                         " CacheManager instances: %s and %s" % (first_target, versioned_target,
                                                                 cache_manager,
                                                                 versioned_target._cache_manager))
    return VersionedTargetSet(cache_manager, versioned_targets)

  def __init__(self, cache_manager, versioned_targets):
    self._cache_manager = cache_manager
    self.versioned_targets = versioned_targets
    self.targets = [vt.target for vt in versioned_targets]
    # The following line is a no-op if cache_key was set in the VersionedTarget __init__ method.
    self.cache_key = CacheKeyGenerator.combine_cache_keys([vt.cache_key
                                                           for vt in versioned_targets])
    self.num_sources = self.cache_key.num_sources
    self.sources = self.cache_key.sources
    self.valid = not cache_manager.needs_update(self.cache_key)

  def update(self):
    self._cache_manager.update(self)

  def force_invalidate(self):
    self._cache_manager.force_invalidate(self)

  def __repr__(self):
    return 'VTS(%s, %s)' % (','.join(target.id for target in self.targets),
                            'valid' if self.valid else 'invalid')


class VersionedTarget(VersionedTargetSet):
  """This class represents a singleton VersionedTargetSet, and has links to VersionedTargets that
  the wrapped target depends on (after having resolved through any "alias" targets.
  """
  def __init__(self, cache_manager, target, cache_key):
    if not isinstance(target, Target):
      raise ValueError("The target %s must be an instance of Target but is not." % target.id)

    self.target = target
    self.cache_key = cache_key
    # Must come after the assignments above, as they are used in the parent's __init__.
    VersionedTargetSet.__init__(self, cache_manager, [self])
    self.id = target.id
    self.dependencies = set()


# The result of calling check() on a CacheManager.
# Each member is a list of VersionedTargetSet objects in topological order.
# Tasks may need to perform no, some or all operations on either of these, depending on how they
# are implemented.
class InvalidationCheck(object):
  @classmethod
  def _partition_versioned_targets(cls, versioned_targets, partition_size_hint):
    """Groups versioned targets so that each group has roughly the same number of sources.

    versioned_targets is a list of VersionedTarget objects  [vt1, vt2, vt3, vt4, vt5, vt6, ...].

    Returns a list of VersionedTargetSet objects, e.g., [VT1, VT2, VT3, ...] representing the
    same underlying targets. E.g., VT1 is the combination of [vt1, vt2, vt3], VT2 is the combination
    of [vt4, vt5] and VT3 is [vt6].

    The new versioned targets are chosen to have roughly partition_size_hint sources.

    This is useful as a compromise between flat mode, where we build all targets in a
    single compiler invocation, and non-flat mode, where we invoke a compiler for each target,
    which may lead to lots of compiler startup overhead. A task can choose instead to build one
    group at a time.
    """
    res = []

    # Hack around the python outer scope problem.
    class VtGroup(object):
      def __init__(self):
        self.vts = []
        self.total_sources = 0

    current_group = VtGroup()

    def add_to_current_group(vt):
      current_group.vts.append(vt)
      current_group.total_sources += vt.num_sources

    def close_current_group():
      if len(current_group.vts) > 0:
        new_vt = VersionedTargetSet.from_versioned_targets(current_group.vts)
        res.append(new_vt)
        current_group.vts = []
        current_group.total_sources = 0

    for vt in versioned_targets:
      add_to_current_group(vt)
      if current_group.total_sources > 1.5 * partition_size_hint and len(current_group.vts) > 1:
        # Too big. Close the current group without this vt and add it to the next one.
        current_group.vts.pop()
        close_current_group()
        add_to_current_group(vt)
      elif current_group.total_sources > partition_size_hint:
        close_current_group()
    close_current_group()  # Close the last group, if any.

    return res

  def __init__(self, all_vts, invalid_vts, partition_size_hint=None):
    # All the targets, valid and invalid.
    self.all_vts = all_vts

    # All the targets, partitioned if so requested.
    self.all_vts_partitioned = self._partition_versioned_targets(
      all_vts, partition_size_hint) if partition_size_hint else all_vts

    # Just the invalid targets.
    self.invalid_vts = invalid_vts

    # Just the invalid targets, partitioned if so requested.
    self.invalid_vts_partitioned = self._partition_versioned_targets(
        invalid_vts, partition_size_hint) if partition_size_hint else invalid_vts


class CacheManager(object):
  """Manages cache checks, updates and invalidation keeping track of basic change
  and invalidation statistics.
  Note that this is distinct from the ArtifactCache concept, and should probably be renamed.
  """
  def __init__(self, cache_key_generator, build_invalidator_dir,
               invalidate_dependents, extra_data, only_externaldeps):
    self._cache_key_generator = cache_key_generator
    self._invalidate_dependents = invalidate_dependents
    self._extra_data = pickle.dumps(extra_data)  # extra_data may be None.
    self._sources = NO_SOURCES if only_externaldeps else TARGET_SOURCES

    self._invalidator = BuildInvalidator(build_invalidator_dir)

  def update(self, vts):
    """Mark a changed or invalidated VersionedTargetSet as successfully processed."""
    for vt in vts.versioned_targets:
      self._invalidator.update(vt.cache_key)
      vt.valid = True
    self._invalidator.update(vts.cache_key)
    vts.valid = True

  def force_invalidate(self, vts):
    """Force invalidation of a VersionedTargetSet."""
    for vt in vts.versioned_targets:
      self._invalidator.force_invalidate(vt.cache_key)
      vt.valid = False
    self._invalidator.force_invalidate(vts.cache_key)
    vts.valid = False

  def check(self, targets, partition_size_hint=None):
    """Checks whether each of the targets has changed and invalidates it if so.

    Returns a list of VersionedTargetSet objects (either valid or invalid). The returned sets
    'cover' the input targets, possibly partitioning them, and are in topological order.
    The caller can inspect these in order and, e.g., rebuild the invalid ones.
    """
    all_vts = self._sort_and_validate_targets(targets)
    invalid_vts = filter(lambda vt: not vt.valid, all_vts)
    return InvalidationCheck(all_vts, invalid_vts, partition_size_hint)

  def _sort_and_validate_targets(self, targets):
    """Validate each target.

    Returns a topologically ordered set of VersionedTargets, each representing one input target.
    """
    # We must check the targets in this order, to ensure correctness if invalidate_dependents=True,
    # since we use earlier cache keys to compute later cache keys in this case.
    ordered_targets = self._order_target_list(targets)

    # This will be a list of VersionedTargets that correspond to @targets.
    versioned_targets = []

    # This will be a mapping from each target to its corresponding VersionedTarget.
    versioned_targets_by_target = {}

    # Map from id to current fingerprint of the target with that id. We update this as we iterate,
    # in topological order, so when handling a target, this will already contain all its deps (in
    # this round).
    id_to_hash = {}

    for target in ordered_targets:
      dependency_keys = set()
      if self._invalidate_dependents and hasattr(target, 'dependencies'):
        # Note that we only need to do this for the immediate deps, because those will already
        # reflect changes in their own deps.
        for dep in target.dependencies:
          # We rely on the fact that any deps have already been processed, either in an earlier
          # round or because they came first in ordered_targets.
          # Note that only external deps (e.g., JarDependency) or targets with sources can
          # affect invalidation. Other targets (JarLibrary, Pants) are just dependency scaffolding.
          if isinstance(dep, ExternalDependency):
            dependency_keys.add(dep.cache_key())
          elif isinstance(dep, TargetWithSources):
            fprint = id_to_hash.get(dep.id, None)
            if fprint is None:
              # It may have been processed in a prior round, and therefore the fprint should
              # have been written out by the invalidator.
              fprint = self._invalidator.existing_hash(dep.id)
              # Note that fprint may still be None here. E.g., a codegen target is in the list
              # of deps, but its fprint is not visible to our self._invalidator (that of the
              # target synthesized from it is visible, so invalidation will still be correct.)
              #
              # Another case where this can happen is a dep of a codegen target on, say,
              # a java target that hasn't been built yet (again, the synthesized target will
              # depend on that same java target, so invalidation will still be correct.)
              # TODO(benjy): Make this simpler and more obviously correct.
            if fprint is not None:
              dependency_keys.add(fprint)
          elif isinstance(dep, JarLibrary) or isinstance(dep, Pants):
            pass
          else:
            raise ValueError('Cannot calculate a cache_key for a dependency: %s' % dep)
      cache_key = self._key_for(target, dependency_keys)
      id_to_hash[target.id] = cache_key.hash

      # Create a VersionedTarget corresponding to @target.
      versioned_target = VersionedTarget(self, target, cache_key)

      # Add the new VersionedTarget to the list of computed VersionedTargets.
      versioned_targets.append(versioned_target)

      # Add to the mapping from Targets to VersionedTargets, for use in hooking up VersionedTarget
      # dependencies below.
      versioned_targets_by_target[target] = versioned_target

    # Having created all applicable VersionedTargets, now we build the VersionedTarget dependency
    # graph, looking through targets that don't correspond to VersionedTargets themselves.
    versioned_target_deps_by_target = {}

    def get_versioned_target_deps_for_target(target):
      # For every dependency of @target, we will store its corresponding VersionedTarget here. For
      # dependencies that don't correspond to a VersionedTarget (e.g. pass-through dependency
      # wrappers), we will resolve their actual dependencies and find VersionedTargets for them.
      versioned_target_deps = set([])
      if hasattr(target, 'dependencies'):
        for dep in target.dependencies:
          for dependency in dep.resolve():
            if dependency in versioned_targets_by_target:
              # If there exists a VersionedTarget corresponding to this Target, store it and
              # continue.
              versioned_target_deps.add(versioned_targets_by_target[dependency])
            elif dependency in versioned_target_deps_by_target:
              # Otherwise, see if we've already resolved this dependency to the VersionedTargets it
              # depends on, and use those.
              versioned_target_deps.update(versioned_target_deps_by_target[dependency])
            else:
              # Otherwise, compute the VersionedTargets that correspond to this dependency's
              # dependencies, cache and use the computed result.
              versioned_target_deps_by_target[dependency] = get_versioned_target_deps_for_target(
                  dependency)
              versioned_target_deps.update(versioned_target_deps_by_target[dependency])

      # Return the VersionedTarget dependencies that this target's VersionedTarget should depend on.
      return versioned_target_deps

    # Initialize all VersionedTargets to point to the VersionedTargets they depend on.
    for versioned_target in versioned_targets:
      versioned_target.dependencies = get_versioned_target_deps_for_target(versioned_target.target)

    return versioned_targets

  def needs_update(self, cache_key):
    return self._invalidator.needs_update(cache_key)

  def _order_target_list(self, targets):
    """Orders the targets topologically, from least to most dependent."""
    targets = set(t for t in targets if isinstance(t, Target))
    return filter(targets.__contains__, reversed(InternalTarget.sort_targets(targets)))

  def _key_for(self, target, dependency_keys):
    def fingerprint_extra(sha):
      sha.update(self._extra_data)
      for key in sorted(dependency_keys):  # Sort to ensure hashing in a consistent order.
        sha.update(key)

    return self._cache_key_generator.key_for_target(
      target,
      sources=self._sources,
      fingerprint_extra=fingerprint_extra
    )


########NEW FILE########
__FILENAME__ = checkstyle
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.dirutil import safe_open

from twitter.pants.process.xargs import Xargs

from .nailgun_task import NailgunTask

from . import TaskError


CHECKSTYLE_MAIN = 'com.puppycrawl.tools.checkstyle.Main'


class Checkstyle(NailgunTask):
  @staticmethod
  def _is_checked(target):
    return target.is_java and not target.is_synthetic

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    NailgunTask.setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag("skip"), mkflag("skip", negate=True),
                            dest="checkstyle_skip", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Skip checkstyle.")

  def __init__(self, context):
    super(Checkstyle, self).__init__(context)

    self._checkstyle_bootstrap_key = 'checkstyle'
    bootstrap_tools = context.config.getlist('checkstyle', 'bootstrap-tools',
                                             default=[':twitter-checkstyle'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._checkstyle_bootstrap_key, bootstrap_tools)

    self._configuration_file = context.config.get('checkstyle', 'configuration')

    self._work_dir = context.config.get('checkstyle', 'workdir')
    self._properties = context.config.getdict('checkstyle', 'properties')
    self._confs = context.config.getlist('checkstyle', 'confs', default=['default'])
    self.context.products.require_data('exclusives_groups')

  def execute(self, targets):
    if not self.context.options.checkstyle_skip:
      with self.invalidated(filter(Checkstyle._is_checked, targets)) as invalidation_check:
        invalid_targets = []
        for vt in invalidation_check.invalid_vts:
          invalid_targets.extend(vt.targets)
        sources = self.calculate_sources(invalid_targets)
        if sources:
          result = self.checkstyle(sources, invalid_targets)
          if result != 0:
            raise TaskError('java %s ... exited non-zero (%i)' % (CHECKSTYLE_MAIN, result))

  def calculate_sources(self, targets):
    sources = set()
    for target in targets:
      sources.update([os.path.join(target.target_base, source)
                      for source in target.sources if source.endswith('.java')])
    return sources

  def checkstyle(self, sources, targets):
    egroups = self.context.products.get_data('exclusives_groups')
    etag = egroups.get_group_key_for_target(targets[0])
    classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._checkstyle_bootstrap_key)
    cp = egroups.get_classpath_for_group(etag)
    classpath.extend(jar for conf, jar in cp if conf in self._confs)

    args = [
      '-c', self._configuration_file,
      '-f', 'plain'
    ]

    if self._properties:
      properties_file = os.path.join(self._work_dir, 'checkstyle.properties')
      with safe_open(properties_file, 'w') as pf:
        for k, v in self._properties.items():
          pf.write('%s=%s\n' % (k, v))
      args.extend(['-p', properties_file])

    # We've hit known cases of checkstyle command lines being too long for the system so we guard
    # with Xargs since checkstyle does not accept, for example, @argfile style arguments.
    def call(xargs):
      return self.runjava(classpath=classpath, main=CHECKSTYLE_MAIN,
                          args=args + xargs, workunit_name='checkstyle')
    checks = Xargs(call)

    return checks.execute(sources)

########NEW FILE########
__FILENAME__ = check_exclusives
from twitter.common.collections import OrderedSet

__author__ = 'Mark Chu-Carroll (markcc@foursquare.com)'


from collections import defaultdict
from copy import copy
from twitter.pants.base.target import Target
from twitter.pants.tasks import Task, TaskError
from twitter.pants.targets.internal import InternalTarget


class CheckExclusives(Task):
  """Computes transitive exclusive maps.

  This computes transitive exclusive tags for a dependency graph rooted
  with a set of build targets specified by a user. If this process produces
  any collisions where a single target contains multiple tag values for a single
  exclusives key, then it generates an error and the compilation will fail.

  The syntax of the exclusives attribute is: ::

     exclusives = {"id": "value", ...}

  For example, suppose that we had two java targets, jliba and jlibb. jliba uses
  slf4j, which includes in its jar package an implementation of log4j. jlibb uses
  log4j directly. But the version of log4j that's packaged inside of slf4j is
  different from the version used by jlibb. ::

     java_library(name='jliba',
       depedencies = ['slf4j-with-log4j-2.4'])
     java_library(name='jlibb',
       dependencies=['log4j-1.9'])
     java_binary(name='javabin', dependencies=[':jliba', ':jlibb'])

  In this case, the binary target 'javabin' depends on both slf4j with its
  packaged log4j version 2.4, and on log4j-1.9.
  Pants doesn't know that the slf4j and log4j jar_dependencies contain
  incompatible versions of the same library, and so it can't detect the error.

  With exclusives, the jar_library target for the joda libraries would declare
  exclusives tags: ::

     jar_library(name='slf4j-with-log4j-2.4', exclusives={'log4j': '2.4'})
     jar_library(name='joda-2.1', exclusives={'log4j': '1.9'})

  With the exclusives declared, pants can recognize that 'javabin' has conflicting
  dependencies, and can generate an appropriate error message.

  Data about exclusives is provided to other tasks via data build products.
  If the build data product 'exclusives_groups' is required, then an
  ExclusivesMapping object will be created.
  """

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    Task.setup_parser(option_group, args, mkflag)
    option_group.add_option(mkflag('error_on_collision'),
                            mkflag('error_on_collision', negate=True),
                            dest='exclusives_error_on_collision', default=True,
                            action='callback', callback=mkflag.set_bool,
                            help=("[%default] Signal an error and abort the build if an " +
                                  "exclusives collision is detected"))

  def __init__(self, context, signal_error=None):
    Task.__init__(self, context)
    self.signal_error = (context.options.exclusives_error_on_collision
                         if signal_error is None else signal_error)

  def _compute_exclusives_conflicts(self, targets):
    """Compute the set of distinct chunks of targets that are required based on exclusives.
    If two targets have different values for a particular exclusives tag,
    then those targets must end up in different chunks.
    This method computes the exclusives values that define each chunk.
    e.g.: if target a has exclusives {"x": "1", "z": "1"}, target b has {"x": "2"},
    target c has {"y", "1"}, and target d has {"y", "2", "z": "1"}, then we need to
    divide into chunks on exclusives tags "x" and "y". We don't need to include
    "z" in the chunk specification, because there are no conflicts on z.

    Parameters:
      targets: a list of the targets being built.
    Return: the set of exclusives tags that should be used for chunking.
    """
    exclusives_map = defaultdict(set)
    for t in targets:
      if t.exclusives is not None:
        for k in t.exclusives:
          exclusives_map[k] |= t.exclusives[k]
    conflicting_keys = defaultdict(set)
    for k in exclusives_map:
      if len(exclusives_map[k]) > 1:
        conflicting_keys[k] = exclusives_map[k]
    return conflicting_keys

  def execute(self, targets):
    # compute transitive exclusives
    for t in targets:
      t._propagate_exclusives()
    # Check for exclusives collision.
    for t in targets:
      excl = t.get_all_exclusives()
      for key in excl:
        if len(excl[key]) > 1:
          msg = 'target %s has more than one exclusives tag for key %s: %s' % \
                (t.address.reference(), key, list(excl[key]))
          if self.signal_error:
            raise TaskError(msg)
          else:
            print('Warning: %s' % msg)

    if self.context.products.is_required_data('exclusives_groups'):
      mapping = ExclusivesMapping(self.context)
      partition_keys = self._compute_exclusives_conflicts(targets)
      for key in partition_keys:
        mapping.add_conflict(key, partition_keys[key])
      mapping._populate_target_maps(targets)
      self.context.products.safe_create_data('exclusives_groups', lambda: mapping)


class ExclusivesMapping(object):
  def __init__(self, context):
    self.context = context
    self.conflicting_exclusives = {}
    self.key_to_targets = defaultdict(set)
    self.target_to_key = {}
    self.ordering = None
    self._group_classpaths = {}  # key -> OrderedSet.

  def add_conflict(self, key, values):
    """Register a conflict on an exclusives key.
    Parameters:
      key the exclusives key with a conflicting_exclusives
      value the different values used for the key in different targets.
    """
    self.conflicting_exclusives[key] = values

  def get_targets_for_group_key(self, key):
    """Gets the set of targets that share exclusives.
    Parameters:
      key: a key, generated by _get_exclusives_key, for the exclusives
          settings shared by a group of targets.
    Return: the set of targets that share the exclusives settings. Returns
       an empty set if no targets have that key.
    """

    return self.key_to_targets[key]

  def get_group_key_for_target(self, target):
    """ Get the exclusives key for a target """
    return self.target_to_key[target]

  def get_group_keys(self):
    """Get the set of keys for all exclusives groups in the current build."""
    if len(self.conflicting_exclusives) == 0:
      return ["<none>"]
    else:
      return self.key_to_targets.keys()

  def get_ordered_group_keys(self):
    """Compute the correct order in which to compile exclusives groups.

    In group, we already do group-based ordering. But that ordering is done separately on
    each exclusives group. If we have a grouping:
      a(exclusives={x: 1, y:2}, dependencies=[ ':b', ':c' ])
      b(exclusives={x:"<none>", y: "<none>"}, dependencies=[])
      c(exclusives={x:<none>, y:2}, dependencies=[':b'])

    If we were to do grouping in the exclusives ordering {x:<none>, y:2}, {x: <none>, y:<none>},
     {x:1, y:2}, then we'd be compiling the group containing c before the group containing b; but
    c depends on b.
    """
    def number_of_emptys(key):
      if key == "<none>":
        return len(self.conflicting_keys)
      return key.count("<none>")

    if self.ordering is not None:
      return self.ordering
    # The correct order is from least exclusives to most exclusives - a target can only depend on
    # other targets with fewer exclusives than itself.
    keys_by_empties = [ [] for l in range(len(self.key_to_targets)) ]
    # Flag to indicate whether there are any groups without any exclusives.
    no_exclusives = False
    for k in self.key_to_targets:
      if k == "<none>":
        no_exclusives = True
      else:
        keys_by_empties[number_of_emptys(k)].append(k)
    result = [ ]
    for i in range(len(keys_by_empties)):
      for j in range(len(keys_by_empties[i])):
        result.append(keys_by_empties[i][j])
    if no_exclusives:
      result.append("<none>")
    result.reverse()
    self.ordering = result
    return self.ordering

  def _get_exclusives_key(self, target):
    # compute an exclusives group key: a list of the exclusives values for the keys
    # in the conflicting keys list.
    target_key = []
    for k in self.conflicting_exclusives:
      excl = target.exclusives if isinstance(target, Target) else target.declared_exclusives
      if len(excl[k]) > 0:
        target_key.append("%s=%s" % (k, list(excl[k])[0]))
      else:
        target_key.append("%s=<none>" % k)

    if target_key == []:
      return "<none>"
    else:
      return ','.join(target_key)

  def _populate_target_maps(self, targets):
    """Populates maps of exclusive keys to targets, and vice versa."""
    all_targets = set()
    workqueue = copy(targets)
    while len(workqueue) > 0:
      t = workqueue.pop()
      if t not in all_targets:
        all_targets.add(t)
        if isinstance(t, InternalTarget):
          workqueue += t.dependencies

    for t in all_targets:
      key = self._get_exclusives_key(t)
      if key == '':
        raise TaskError('Invalid empty group key')
      if key not in self._group_classpaths:
        self._group_classpaths[key] = OrderedSet()
      self.key_to_targets[key].add(t)
      self.target_to_key[t] = key

  def get_classpath_for_group(self, group_key):
    """Get the classpath to use for jvm compilations of a group.

    Each exclusives group requires a distinct classpath. We maintain
    them here as a map from the exclusives key to a classpath. The
    classpath is updated during compilations to add the results of
    compiling a group to the classpaths of other groups that could depend on it.
    """
    if group_key not in self._group_classpaths:
      self._group_classpaths[group_key] = OrderedSet()
    # get the classpath to use for compiling targets within the group specified by group_key.
    return list(reversed(self._group_classpaths[group_key]))

  def _key_to_map(self, key):
    result = {}
    if key == '<none>' or key == '':
      return result
    pairs = key.split(',')
    for p in pairs:
      (k, v) = p.split("=")
      result[k] = v
    return result

  def _is_compatible(self, mod_key, other_key):
    # Check if a set of classpath modifications produced by compiling elements of the group
    # specified by mod_key should be added to the classpath of other_key's group.

    # A key is a list of comma separated name=value keys.
    # keys match, if and only of for all pairs k=v1 from mod, and k=v2 from other,
    # either v1 == v2 or v1 == <none>.
    mod_map = self._key_to_map(mod_key)
    other_map = self._key_to_map(other_key)
    for k in mod_map:
      vm = mod_map[k]
      vo = other_map[k]
      if not (vm == vo or vm == "<none>"):
        return False
    return True

  def update_compatible_classpaths(self, group_key, path_additions):
    """Update the classpath of all groups compatible with group_key, adding path_additions to their
    classpath.
    """
    additions = list(reversed(path_additions))
    for key in self._group_classpaths:
      if group_key is None or self._is_compatible(group_key, key):
        group_classpath = self._group_classpaths[key]
        group_classpath.update(additions)

  def set_base_classpath_for_group(self, group_key, classpath):
    # set the initial classpath of the elements of group_key to classpath.
    self._group_classpaths[group_key] = OrderedSet(reversed(classpath))


########NEW FILE########
__FILENAME__ = check_published_deps
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.build_file import BuildFile
from twitter.pants.base.target import Target
from twitter.pants.targets.jar_dependency import JarDependency
from twitter.pants.tasks.console_task import ConsoleTask
from twitter.pants.tasks.jar_publish import PushDb


class CheckPublishedDeps(ConsoleTask):

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(CheckPublishedDeps, cls).setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag('print-uptodate'), mkflag('print-uptodate', negate=True),
                            dest='check_deps_print_uptodate', default=False,
                            action='callback', callback=mkflag.set_bool,
                            help='[%default] Also print up-to-date dependencies.')

  def __init__(self, context):
    ConsoleTask.__init__(self, context)

    self._print_uptodate = context.options.check_deps_print_uptodate
    self.repos = context.config.getdict('jar-publish', 'repos')
    self._artifacts_to_targets = {}
    all_addresses = (address for buildfile in BuildFile.scan_buildfiles(get_buildroot())
                     for address in Target.get_all_addresses(buildfile))
    for address in all_addresses:
      target = Target.get(address)
      if target.is_exported:
        provided_jar, _, _ = target.get_artifact_info()
        artifact = (provided_jar.org, provided_jar.name)
        if not artifact in self._artifacts_to_targets:
          self._artifacts_to_targets[artifact] = target

  def console_output(self, targets):
    push_dbs = {}

    def get_jar_with_version(target):
      db = target.provides.repo.push_db
      if db not in push_dbs:
        push_dbs[db] = PushDb.load(db)
      return push_dbs[db].as_jar_with_version(target)

    visited = set()
    for target in targets:
      for dependency in target.dependencies:
        for dep in dependency.resolve():
          if isinstance(dep, JarDependency):
            artifact = (dep.org, dep.name)
            if artifact in self._artifacts_to_targets and not artifact in visited:
              visited.add(artifact)
              artifact_target = self._artifacts_to_targets[artifact]
              _, semver, sha, _ = get_jar_with_version(artifact_target)
              if semver.version() != dep.rev:
                yield 'outdated %s#%s %s latest %s' % (dep.org, dep.name, dep.rev, semver.version())
              elif self._print_uptodate:
                yield 'up-to-date %s#%s %s' % (dep.org, dep.name, semver.version())

########NEW FILE########
__FILENAME__ = code_gen
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.tasks import Task


class CodeGen(Task):
  """Encapsulates the common machinery for codegen targets that support multiple output languages.

  This Task will only invoke code generation for changed targets and for the set of languages
  in the active context that require codegen unless forced.
  """

  def is_gentarget(self, target):
    """Subclass must return True if it handles generating for the target."""
    raise NotImplementedError

  def is_forced(self, lang):
    """Subclass may return True to force code generation for the given language."""
    return False

  def genlangs(self):
    """Subclass must use this to identify the targets consuming each language it generates for.

    Return value is a dict mapping supported generation target language names
    to a predicate that can select targets consuming that language.
    """
    raise NotImplementedError

  def prepare_gen(self, targets):
    """
      Subclasses should override if they need to prepare for potential upcoming calls to genlang.

      Note that this does not mean genlang will necessarily be called.
    """
    pass

  def genlang(self, lang, targets):
    """Subclass must override and generate code in :lang for the given targets.

    May return a list of pairs (target, files) where files is a list of files
    to be cached against the target.
    """
    raise NotImplementedError

  def createtarget(self, lang, gentarget, dependees):
    """Subclass must override and create a synthetic target.

     The target must contain the sources generated for the given gentarget.
    """
    raise NotImplementedError

  def getdependencies(self, gentarget):
    # TODO(John Sirois): fix python/jvm dependencies handling to be uniform
    if hasattr(gentarget, 'internal_dependencies'):
      return gentarget.internal_dependencies
    else:
      return gentarget.dependencies

  def updatedependencies(self, target, dependency):
    if hasattr(target, 'update_dependencies'):
      target.update_dependencies([dependency])
    else:
      target.dependencies.add(dependency)

  def execute(self, targets):
    gentargets = [t for t in targets if self.is_gentarget(t)]
    capabilities = self.genlangs() # lang_name => predicate
    gentargets_by_dependee = self.context.dependents(
      on_predicate=self.is_gentarget,
      from_predicate=lambda t: not self.is_gentarget(t)
    )
    dependees_by_gentarget = defaultdict(set)
    for dependee, tgts in gentargets_by_dependee.items():
      for gentarget in tgts:
        dependees_by_gentarget[gentarget].add(dependee)

    def find_gentargets(predicate):
      tgts = set()
      for dependee in gentargets_by_dependee.keys():
        if predicate(dependee):
          for tgt in gentargets_by_dependee.pop(dependee):
            tgt.walk(tgts.add, self.is_gentarget)
      return tgts.intersection(set(gentargets))

    gentargets_bylang = {}
    for lang, predicate in capabilities.items():
      gentargets_bylang[lang] = gentargets if self.is_forced(lang) else find_gentargets(predicate)
    if gentargets_by_dependee:
      self.context.log.warn('Left with unexpected unconsumed gen targets:\n\t%s' % '\n\t'.join(
        '%s -> %s' % (dependee, gentargets)
        for dependee, gentargets in gentargets_by_dependee.items()
      ))

    if gentargets:
      self.prepare_gen(gentargets)
      with self.invalidated(gentargets, invalidate_dependents=True) as invalidation_check:
        for vts in invalidation_check.invalid_vts_partitioned:
          invalid_targets = set(vts.targets)
          for lang, tgts in gentargets_bylang.items():
            invalid_lang_tgts = invalid_targets.intersection(tgts)
            if invalid_lang_tgts:
              self.genlang(lang, invalid_lang_tgts)

      # Link synthetic targets for all in-play gen targets.
      invalid_vts_by_target = dict([(vt.target, vt) for vt in invalidation_check.invalid_vts])
      vts_artifactfiles_pairs = []
      write_to_artifact_cache = self.artifact_cache_writes_enabled() if invalid_vts_by_target else False
      for lang, tgts in gentargets_bylang.items():
        if tgts:
          langtarget_by_gentarget = {}
          for target in tgts:
            syn_target = self.createtarget(
              lang,
              target,
              dependees_by_gentarget.get(target, [])
            )
            syn_target.derived_from = target
            syn_target.add_labels('codegen', 'synthetic')
            if write_to_artifact_cache and target in invalid_vts_by_target:
              generated_sources = list(syn_target.sources_absolute_paths())
              vts_artifactfiles_pairs.append((invalid_vts_by_target[target], generated_sources))
            langtarget_by_gentarget[target] = syn_target
          genmap = self.context.products.get(lang)
          for gentarget, langtarget in langtarget_by_gentarget.items():
            genmap.add(gentarget, get_buildroot(), [langtarget])
            # Transfer dependencies from gentarget to its synthetic counterpart.
            for dep in self.getdependencies(gentarget):
              if self.is_gentarget(dep):  # Translate the dep to its synthetic counterpart.
                self.updatedependencies(langtarget, langtarget_by_gentarget[dep])
              else:  # Depend directly on the dep.
                self.updatedependencies(langtarget, dep)
      if write_to_artifact_cache:
        self.update_artifact_cache(vts_artifactfiles_pairs)

########NEW FILE########
__FILENAME__ = confluence_publish
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import textwrap

import os

from twitter.common.confluence import Confluence, ConfluenceError
from twitter.common.dirutil import safe_open

from twitter.pants import binary_util
from twitter.pants.targets.doc import Page
from twitter.pants.tasks import Task, TaskError

"""Classes to ease publishing Page targets to Confluence wikis."""

class ConfluencePublish(Task):

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    cls.url_option = option_group.add_option(mkflag("url"), dest="confluence_publish_url",
                                             help="The url of the confluence site to post to.")

    option_group.add_option(mkflag("force"), mkflag("force", negate=True),
                            dest = "confluence_publish_force",
                            action="callback", callback=mkflag.set_bool, default=False,
                            help = "[%default] Force publish the page even if its contents is "
                                   "identical to the contents on confluence.")

    option_group.add_option(mkflag("open"), mkflag("open", negate=True),
                            dest="confluence_publish_open", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Attempt to open the published confluence wiki page "
                                 "in a browser.")

    option_group.add_option(mkflag("user"), dest="confluence_user",
                            help="Confluence user name, defaults to unix user.")

  def __init__(self, context):
    Task.__init__(self, context)

    self.url = (
      context.options.confluence_publish_url
      or context.config.get('confluence-publish', 'url')
    )

    if not self.url:
      raise TaskError("Unable to proceed publishing to confluence. Please configure a 'url' under "
                      "the 'confluence-publish' heading in pants.ini or using the %s command line "
                      "option." % self.url_option)

    self.force = context.options.confluence_publish_force
    self.open = context.options.confluence_publish_open
    self.context.products.require('wiki_html')
    self._wiki = None
    self.user = context.options.confluence_user

  def wiki(self):
    raise NotImplementedError('Subclasses must provide the wiki target they are associated with')

  def api(self):
    return 'confluence1'

  def execute(self, targets):
    pages = []
    for target in targets:
      if isinstance(target, Page):
        wikiconfig = target.wiki_config(self.wiki())
        if wikiconfig:
          pages.append((target, wikiconfig))

    urls = list()

    genmap = self.context.products.get('wiki_html')
    for page, wikiconfig in pages:
      html_info = genmap.get((self.wiki(), page))
      if len(html_info) > 1:
        raise TaskError('Unexpected resources for %s: %s' % (page, html_info))
      basedir, htmls = html_info.items()[0]
      if len(htmls) != 1:
        raise TaskError('Unexpected resources for %s: %s' % (page, htmls))
      with safe_open(os.path.join(basedir, htmls[0])) as contents:
        url = self.publish_page(
          page.address,
          wikiconfig['space'],
          wikiconfig['title'],
          contents.read(),
          parent=wikiconfig.get('parent')
        )
        if url:
          urls.append(url)
          self.context.log.info('Published %s to %s' % (page, url))

    if self.open and urls:
      binary_util.ui_open(*urls)

  def publish_page(self, address, space, title, content, parent=None):
    body = textwrap.dedent('''

    <!-- DO NOT EDIT - generated by pants from %s -->

    %s
    ''').strip() % (address, content)

    pageopts = dict(
      versionComment = 'updated by pants!'
    )
    wiki = self.login()
    existing = wiki.getpage(space, title)
    if existing:
      if not self.force and existing['content'].strip() == body.strip():
        self.context.log.warn("Skipping publish of '%s' - no changes" % title)
        return

      pageopts['id'] = existing['id']
      pageopts['version'] = existing['version']

    try:
      page = wiki.create_html_page(space, title, body, parent, **pageopts)
      return page['url']
    except ConfluenceError as e:
      raise TaskError('Failed to update confluence: %s' % e)

  def login(self):
    if not self._wiki:
      try:
        self._wiki = Confluence.login(self.url, self.user, self.api())
      except ConfluenceError as e:
        raise TaskError('Failed to login to confluence: %s' % e)
    return self._wiki

########NEW FILE########
__FILENAME__ = console_task
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import errno
import sys

from contextlib import contextmanager

from . import Task


class ConsoleTask(Task):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("sep"), dest="console_%s_separator" % cls.__name__,
                            default='\\n', help="String to use to separate results.")

  def __init__(self, context, outstream=sys.stdout):
    super(ConsoleTask, self).__init__(context)
    separator_option = "console_%s_separator" % self.__class__.__name__
    self._console_separator = getattr(context.options, separator_option).decode('string-escape')
    self._outstream = outstream

  @contextmanager
  def _guard_sigpipe(self):
    try:
      yield
    except IOError as e:
      # If the pipeline only wants to read so much, that's fine; otherwise, this error is probably
      # legitimate.
      if e.errno != errno.EPIPE:
        raise e

  def execute(self, targets):
    with self._guard_sigpipe():
      try:
        for value in self.console_output(targets):
          self._outstream.write(str(value))
          self._outstream.write(self._console_separator)
      finally:
        self._outstream.flush()

  def console_output(self, targets):
    raise NotImplementedError('console_output must be implemented by subclasses of ConsoleTask')

########NEW FILE########
__FILENAME__ = dependees
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict

from twitter.common.collections import OrderedSet

import twitter.pants.base.build_file_context

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.target import Target
from twitter.pants.base.build_file import BuildFile
from twitter.pants.targets.sources import SourceRoot

from .console_task import ConsoleTask

from . import TaskError


class ReverseDepmap(ConsoleTask):
  """Outputs all targets whose dependencies include at least one of the input targets."""

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(ReverseDepmap, cls).setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag("transitive"), mkflag("transitive", negate=True),
                            dest="reverse_depmap_transitive", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] List transitive dependees.")

    option_group.add_option(mkflag("closed"), mkflag("closed", negate=True),
                            dest="reverse_depmap_closed", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Include the input targets in the output along with "
                                 "the dependees.")

    option_group.add_option(mkflag('type'), dest='dependees_type', action='append', default=[],
                            help="Identifies target types to include. Multiple type inclusions "
                                 "can be specified at once in a comma separated list or else by "
                                 "using multiple instances of this flag.")

  def __init__(self, context):
    ConsoleTask.__init__(self, context)

    self._transitive = context.options.reverse_depmap_transitive
    self._closed = context.options.reverse_depmap_closed
    self._dependees_type = context.options.dependees_type

  def console_output(self, _):
    buildfiles = OrderedSet()
    if self._dependees_type:
      base_paths = OrderedSet()
      for dependees_type in self._dependees_type:
        try:
          # Try to do a fully qualified import 1st for filtering on custom types.
          from_list, module, type_name = dependees_type.rsplit('.', 2)
          __import__('%s.%s' % (from_list, module), fromlist=[from_list])
        except (ImportError, ValueError):
          # Fall back on pants provided target types.
          if hasattr(twitter.pants.base.build_file_context, dependees_type):
            type_name = getattr(twitter.pants.base.build_file_context, dependees_type)
          else:
            raise TaskError('Invalid type name: %s' % dependees_type)
        # Find the SourceRoot for the given input type
        base_paths.update(SourceRoot.roots(type_name))
      if not base_paths:
        raise TaskError('No SourceRoot set for any target type in %s.' % self._dependees_type +
                        '\nPlease define a source root in BUILD file as:' +
                        '\n\tsource_root(\'<src-folder>\', %s)' % ', '.join(self._dependees_type))
      for base_path in base_paths:
        buildfiles.update(BuildFile.scan_buildfiles(get_buildroot(), base_path))
    else:
      buildfiles = BuildFile.scan_buildfiles(get_buildroot())

    dependees_by_target = defaultdict(set)
    for buildfile in buildfiles:
      for address in Target.get_all_addresses(buildfile):
        for target in Target.get(address).resolve():
          # TODO(John Sirois): tighten up the notion of targets written down in a BUILD by a
          # user vs. targets created by pants at runtime.
          target = self.get_concrete_target(target)
          if hasattr(target, 'dependencies'):
            for dependencies in target.dependencies:
              for dependency in dependencies.resolve():
                dependency = self.get_concrete_target(dependency)
                dependees_by_target[dependency].add(target)

    roots = set(self.context.target_roots)
    if self._closed:
      for root in roots:
        yield str(root.address)

    for dependant in self.get_dependants(dependees_by_target, roots):
      yield str(dependant.address)

  def get_dependants(self, dependees_by_target, roots):
    check = set(roots)
    known_dependants = set()
    while True:
      dependants = set(known_dependants)
      for target in check:
        dependants.update(dependees_by_target[target])
      check = dependants - known_dependants
      if not check or not self._transitive:
        return dependants - set(roots)
      known_dependants = dependants

  def get_concrete_target(self, target):
    return target.derived_from if isinstance(target, Target) else target

########NEW FILE########
__FILENAME__ = dependencies
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

from twitter.pants.targets.jar_dependency import JarDependency
from twitter.pants.targets.python_requirement import PythonRequirement
from twitter.pants.tasks import TaskError
from .console_task import ConsoleTask


class Dependencies(ConsoleTask):
  """Generates a textual list (using the target format) for the dependency set of a target."""

  @staticmethod
  def _is_jvm(target):
    return target.is_jvm or target.is_jvm_app

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(Dependencies, cls).setup_parser(option_group, args, mkflag)

    cls.internal_only_flag = mkflag("internal-only")
    cls.external_only_flag = mkflag("external-only")

    option_group.add_option(cls.internal_only_flag,
                            action="store_true",
                            dest="dependencies_is_internal_only",
                            default=False,
                            help='Specifies that only internal dependencies should'
                                 ' be included in the graph output (no external jars).')
    option_group.add_option(cls.external_only_flag,
                            action="store_true",
                            dest="dependencies_is_external_only",
                            default=False,
                            help='Specifies that only external dependencies should'
                                 ' be included in the graph output (only external jars).')

  def __init__(self, context, **kwargs):
    super(Dependencies, self).__init__(context, **kwargs)

    if (self.context.options.dependencies_is_internal_only and
        self.context.options.dependencies_is_external_only):

      error_str = "At most one of %s or %s can be selected." % (self.internal_only_flag,
                                                                self.external_only_flag)
      raise TaskError(error_str)

    self.is_internal_only = self.context.options.dependencies_is_internal_only
    self.is_external_only = self.context.options.dependencies_is_external_only

  def console_output(self, unused_method_argument):
    for target in self.context.target_roots:
      if all(self._is_jvm(t) for t in target.resolve() if t.is_concrete):
        for line in self._dependencies_list(target):
          yield line

      elif target.is_python:
        if self.is_internal_only:
          raise TaskError('Unsupported option for Python target: is_internal_only: %s' %
                          self.is_internal_only)
        if self.is_external_only:
          raise TaskError('Unsupported option for Python target: is_external_only: %s' %
                          self.is_external_only)
        for line in self._python_dependencies_list(target):
          yield line

  def _dep_id(self, dep):
    if isinstance(dep, JarDependency):
      if dep.rev:
        return False, '%s:%s:%s' % (dep.org, dep.name, dep.rev)
      else:
        return True, '%s:%s' % (dep.org, dep.name)
    else:
      return True, str(dep.address)

  def _python_dependencies_list(self, target):
    if isinstance(target, PythonRequirement):
      yield str(target._requirement)
    else:
      yield str(target.address)

    if hasattr(target, 'dependencies'):
      for dep in target.dependencies:
        for d in dep.resolve():
          for dep in self._python_dependencies_list(d):
            yield dep

  def _dependencies_list(self, target):
    def print_deps(visited, dep):
      internal, address = self._dep_id(dep)

      if not dep in visited:
        if internal and (not self.is_external_only or self.is_internal_only):
          yield address

        visited.add(dep)

        if self._is_jvm(dep):
          for internal_dependency in dep.internal_dependencies:
            for line in print_deps(visited, internal_dependency):
              yield line

        if not self.is_internal_only:
          if self._is_jvm(dep):
            for jar_dep in dep.jar_dependencies:
              internal, address  = self._dep_id(jar_dep)
              if not internal:
                if jar_dep not in visited:
                  if self.is_external_only or not self.is_internal_only:
                    yield address
                  visited.add(jar_dep)

    visited = set()
    for t in target.resolve():
      for dep in print_deps(visited, t):
        yield dep

########NEW FILE########
__FILENAME__ = depmap
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

from twitter.pants.tasks.console_task import ConsoleTask
from twitter.pants.tasks import TaskError

from twitter.pants.targets.jar_dependency import JarDependency


class Depmap(ConsoleTask):
  """Generates either a textual dependency tree or a graphviz digraph dot file for the dependency
  set of a target.
  """

  @staticmethod
  def _is_jvm(dep):
    return dep.is_jvm or dep.is_jvm_app

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(Depmap, cls).setup_parser(option_group, args, mkflag)

    cls.internal_only_flag = mkflag("internal-only")
    cls.external_only_flag = mkflag("external-only")
    option_group.add_option(cls.internal_only_flag,
                            action="store_true",
                            dest="depmap_is_internal_only",
                            default=False,
                            help='Specifies that only internal dependencies should'
                                 ' be included in the graph output (no external jars).')
    option_group.add_option(cls.external_only_flag,
                            action="store_true",
                            dest="depmap_is_external_only",
                            default=False,
                            help='Specifies that only external dependencies should'
                                 ' be included in the graph output (only external jars).')
    option_group.add_option(mkflag("minimal"),
                            action="store_true",
                            dest="depmap_is_minimal",
                            default=False,
                            help='For a textual dependency tree, only prints a dependency the 1st'
                                 ' time it is encountered.  For graph output this does nothing.')
    option_group.add_option(mkflag("separator"),
                            dest="depmap_separator",
                            default="-",
                            help='Specifies the separator to use between the org/name/rev'
                                 ' components of a dependency\'s fully qualified name.')
    option_group.add_option(mkflag("graph"),
                            action="store_true",
                            dest="depmap_is_graph",
                            default=False,
                            help='Specifies the internal dependency graph should be'
                                 ' output in the dot digraph format')

  def __init__(self, context):
    ConsoleTask.__init__(self, context)

    if (self.context.options.depmap_is_internal_only
        and self.context.options.depmap_is_external_only):
      cls = self.__class__
      error_str = "At most one of %s or %s can be selected." % (cls.internal_only_flag,
                                                                cls.external_only_flag)
      raise TaskError(error_str)

    self.is_internal_only = self.context.options.depmap_is_internal_only
    self.is_external_only = self.context.options.depmap_is_external_only
    self.is_minimal = self.context.options.depmap_is_minimal
    self.is_graph = self.context.options.depmap_is_graph
    self.separator = self.context.options.depmap_separator

  def console_output(self, targets):
    if len(self.context.target_roots) == 0:
      raise TaskError("One or more target addresses are required.")

    for target in self.context.target_roots:
      if all(self._is_jvm(t) for t in target.resolve() if t.is_concrete):
        if self.is_graph:
          for line in self._output_digraph(target):
            yield line
        else:
          for line in self._output_dependency_tree(target):
            yield line
      elif target.is_python:
        raise TaskError('Unsupported for Python targets')
      else:
        raise TaskError('Unsupported for target %s' % target)

  def _dep_id(self, dependency):
    """Returns a tuple of dependency_id , is_internal_dep."""

    params = dict(sep=self.separator)
    if isinstance(dependency, JarDependency):
      params.update(org=dependency.org, name=dependency.name, rev=dependency.rev)
    else:
      params.update(org='internal', name=dependency.id)

    if params.get('rev'):
      return "%(org)s%(sep)s%(name)s%(sep)s%(rev)s" % params, False
    else:
      return "%(org)s%(sep)s%(name)s" % params, True

  def _output_dependency_tree(self, target):
    def output_dep(dep, indent):
      return "%s%s" % (indent * "  ", dep)

    def output_deps(dep, indent=0, outputted=set()):
      dep_id, _ = self._dep_id(dep)
      if dep_id in outputted:
        return [output_dep("*%s" % dep_id, indent)] if not self.is_minimal else []
      else:
        output = []
        if not self.is_external_only:
          output += [output_dep(dep_id, indent)]
          outputted.add(dep_id)
          indent += 1

        if self._is_jvm(dep):
          for internal_dep in dep.internal_dependencies:
            output += output_deps(internal_dep, indent, outputted)

        if not self.is_internal_only:
          if self._is_jvm(dep):
            for jar_dep in dep.jar_dependencies:
              jar_dep_id, internal = self._dep_id(jar_dep)
              if not internal:
                if jar_dep_id not in outputted or (not self.is_minimal
                                                   and not self.is_external_only):
                  output += [output_dep(jar_dep_id, indent)]
                  outputted.add(jar_dep_id)
        return output

    return [dependency for t in target.resolve() for dependency in output_deps(t)]

  def _output_digraph(self, target):
    color_by_type = {}

    def output_candidate(internal):
      return ((self.is_internal_only and internal)
              or (self.is_external_only and not internal)
              or (not self.is_internal_only and not self.is_external_only))

    def output_dep(dep):
      dep_id, internal = self._dep_id(dep)
      if internal:
        fmt = '  "%(id)s" [style=filled, fillcolor="%(color)d"];'
      else:
        fmt = '  "%(id)s" [style=filled, fillcolor="%(color)d", shape=ellipse];'
      if not color_by_type.has_key(type(dep)):
        color_by_type[type(dep)] = len(color_by_type.keys()) + 1
      return fmt % {'id': dep_id, 'color': color_by_type[type(dep)]}

    def output_deps(outputted, dep, parent=None):
      output = []

      if dep not in outputted:
        outputted.add(dep)
        output.append(output_dep(dep))
        if parent:
          output.append('  "%s" -> "%s";' % (self._dep_id(parent)[0], self._dep_id(dep)[0]))

        for dependency in dep.resolve():
          if self._is_jvm(dependency):
            for internal_dependency in dependency.internal_dependencies:
              output += output_deps(outputted, internal_dependency, dependency)

          for jar in (dependency.jar_dependencies if self._is_jvm(dependency) else [dependency]):
            jar_id, internal = self._dep_id(jar)
            if output_candidate(internal):
              if jar not in outputted:
                output += [output_dep(jar)]
                outputted.add(jar)

              target_id, _ = self._dep_id(target)
              dep_id, _ = self._dep_id(dependency)
              left_id = target_id if self.is_external_only else dep_id
              if (left_id, jar_id) not in outputted:
                styled = internal and not self.is_internal_only
                output += ['  "%s" -> "%s"%s;' % (left_id, jar_id,
                                                  ' [style="dashed"]' if styled else '')]
                outputted.add((left_id, jar_id))
      return output
    header = ['digraph "%s" {' % target.id]
    graph_attr = ['  node [shape=rectangle, colorscheme=set312;];', '  rankdir=LR;']
    return header + graph_attr + output_deps(set(), target) + ['}']

########NEW FILE########
__FILENAME__ = detect_duplicates
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from collections import defaultdict
from contextlib import closing
from zipfile import ZipFile

from twitter.pants.java.jar import Manifest

from .jvm_binary_task import JvmBinaryTask

from . import TaskError


class DuplicateDetector(JvmBinaryTask):
  """ Detect classes and resources with the same qualified name on the classpath. """

  @staticmethod
  def _isdir(name):
    return name[-1] == '/'

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    JvmBinaryTask.setup_parser(option_group, args, mkflag)
    option_group.add_option(mkflag("fail-fast"), mkflag("fail-fast", negate=True),
                            dest="fail_fast", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Fail fast if duplicate classes/resources are found.")

  def __init__(self, context):
    JvmBinaryTask.__init__(self, context)
    self.require_jar_dependencies()
    self.fail_fast = context.options.fail_fast

  def execute(self, targets):
    for binary in filter(self.is_binary, targets):
      self.detect_duplicates_for_target(binary)

  def detect_duplicates_for_target(self, binary_target):
    list_path = []
    for basedir, externaljar in self.list_jar_dependencies(binary_target):
      list_path.append(os.path.join(basedir, externaljar))
    self._is_conflicts(list_path, binary_target)

  def _is_conflicts(self, jar_paths, binary_target):
    artifacts_by_file_name = defaultdict(set)
    for jarpath in jar_paths:
      self.context.log.debug('  scanning %s' % jarpath)
      with closing(ZipFile(jarpath)) as zip:
        for file_name in zip.namelist():
          jar_name = os.path.basename(jarpath)
          if (not self._isdir(file_name)) and Manifest.PATH != file_name:
            artifacts_by_file_name[file_name].add(jar_name)
        zip.close()

    conflicts_by_artifacts = self._get_conflicts_by_artifacts(artifacts_by_file_name)

    if len(conflicts_by_artifacts) > 0:
      self._log_conflicts(conflicts_by_artifacts, binary_target)
      if self.fail_fast:
        raise TaskError('Failing build for target %s.' % binary_target)
      return True
    return False

  def _get_conflicts_by_artifacts(self, artifacts_by_file_name):
    conflicts_by_artifacts = defaultdict(set)
    for (file_name, artifacts) in artifacts_by_file_name.items():
      if (not artifacts) or len(artifacts) < 2: continue
      conflicts_by_artifacts[tuple(sorted(artifacts))].add(file_name)
    return conflicts_by_artifacts

  def _log_conflicts(self, conflicts_by_artifacts, target):
    self.context.log.warn('\n ===== For target %s:' % target)
    for artifacts, duplicate_files in conflicts_by_artifacts.items():
      if len(artifacts) < 2: continue
      self.context.log.warn(
        'Duplicate classes and/or resources detected in artifacts: %s' % str(artifacts))
      for duplicate_file in list(duplicate_files)[:10]:
        self.context.log.warn('     %s' % duplicate_file)


########NEW FILE########
__FILENAME__ = eclipse_gen
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pkgutil

from collections import defaultdict

from twitter.common.collections import OrderedSet
from twitter.common.dirutil import safe_delete, safe_mkdir, safe_open

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.generator import TemplateData, Generator
from twitter.pants.tasks.ide_gen import IdeGen


_TEMPLATE_BASEDIR = os.path.join('templates', 'eclipse')


_VERSIONS = {
  '3.5': '3.7', # 3.5-3.7 are .project/.classpath compatible
  '3.6': '3.7',
  '3.7': '3.7',
}


_SETTINGS = (
  'org.eclipse.core.resources.prefs',
  'org.eclipse.jdt.ui.prefs',
)


class EclipseGen(IdeGen):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    IdeGen.setup_parser(option_group, args, mkflag)

    supported_versions = sorted(list(_VERSIONS.keys()))
    option_group.add_option(mkflag("eclipse-version"), dest="eclipse_gen_version",
                            default='3.6', type="choice", choices=supported_versions,
                            help="[%%default] The Eclipse version the project "
                                   "configuration should be generated for; can be one of: "
                                   "%s" % supported_versions)

  def __init__(self, context):
    IdeGen.__init__(self, context)

    version = _VERSIONS[context.options.eclipse_gen_version]
    self.project_template = os.path.join(_TEMPLATE_BASEDIR, 'project-%s.mustache' % version)
    self.classpath_template = os.path.join(_TEMPLATE_BASEDIR, 'classpath-%s.mustache' % version)
    self.apt_template = os.path.join(_TEMPLATE_BASEDIR, 'factorypath-%s.mustache' % version)
    self.pydev_template = os.path.join(_TEMPLATE_BASEDIR, 'pydevproject-%s.mustache' % version)
    self.debug_template = os.path.join(_TEMPLATE_BASEDIR, 'debug-launcher-%s.mustache' % version)
    self.coreprefs_template = os.path.join(_TEMPLATE_BASEDIR,
                                           'org.eclipse.jdt.core.prefs-%s.mustache' % version)

    self.project_filename = os.path.join(self.cwd, '.project')
    self.classpath_filename = os.path.join(self.cwd, '.classpath')
    self.apt_filename = os.path.join(self.cwd, '.factorypath')
    self.pydev_filename = os.path.join(self.cwd, '.pydevproject')
    self.coreprefs_filename = os.path.join(self.cwd, '.settings', 'org.eclipse.jdt.core.prefs')

  def generate_project(self, project):
    def linked_folder_id(source_set):
      return source_set.source_base.replace(os.path.sep, '.')

    def base_path(source_set):
      return os.path.join(source_set.root_dir, source_set.source_base)

    def create_source_base_template(source_set):
      source_base = base_path(source_set)
      return source_base, TemplateData(
        id=linked_folder_id(source_set),
        path=source_base
      )

    source_bases = dict(map(create_source_base_template, project.sources))
    if project.has_python:
      source_bases.update(map(create_source_base_template, project.py_sources))
      source_bases.update(map(create_source_base_template, project.py_libs))

    def create_source_template(base_id, includes=None, excludes=None):
      return TemplateData(
        base=base_id,
        includes='|'.join(OrderedSet(includes)) if includes else None,
        excludes='|'.join(OrderedSet(excludes)) if excludes else None,
      )

    def create_sourcepath(base_id, sources):
      def normalize_path_pattern(path):
        return '%s/' % path if not path.endswith('/') else path

      includes = [normalize_path_pattern(src_set.path) for src_set in sources if src_set.path]
      excludes = []
      for source_set in sources:
        excludes.extend(normalize_path_pattern(exclude) for exclude in source_set.excludes)

      return create_source_template(base_id, includes, excludes)

    pythonpaths = []
    if project.has_python:
      for source_set in project.py_sources:
        pythonpaths.append(create_source_template(linked_folder_id(source_set)))
      for source_set in project.py_libs:
        lib_path = source_set.path if source_set.path.endswith('.egg') else '%s/' % source_set.path
        pythonpaths.append(create_source_template(linked_folder_id(source_set),
                                                  includes=[lib_path]))

    configured_project = TemplateData(
      name=self.project_name,
      java=TemplateData(
        jdk=self.java_jdk,
        language_level=('1.%d' % self.java_language_level)
      ),
      python=project.has_python,
      scala=project.has_scala and not project.skip_scala,
      source_bases=source_bases.values(),
      pythonpaths=pythonpaths,
      debug_port=project.debug_port,
    )

    outdir = os.path.abspath(os.path.join(self.work_dir, 'bin'))
    safe_mkdir(outdir)

    source_sets = defaultdict(OrderedSet)  # base_id -> source_set
    for source_set in project.sources:
      source_sets[linked_folder_id(source_set)].add(source_set)
    sourcepaths = [create_sourcepath(base_id, sources) for base_id, sources in source_sets.items()]

    libs = []

    def add_jarlibs(classpath_entries):
      for classpath_entry in classpath_entries:
        # TODO(John Sirois): Plumb javadoc jars
        libs.append((classpath_entry.jar, classpath_entry.source_jar))
    add_jarlibs(project.internal_jars)
    add_jarlibs(project.external_jars)

    configured_classpath = TemplateData(
      sourcepaths=sourcepaths,
      has_tests=project.has_tests,
      libs=libs,
      scala=project.has_scala,

      # Eclipse insists the outdir be a relative path unlike other paths
      outdir=os.path.relpath(outdir, get_buildroot()),
    )

    def apply_template(output_path, template_relpath, **template_data):
      with safe_open(output_path, 'w') as output:
        Generator(pkgutil.get_data(__name__, template_relpath), **template_data).write(output)

    apply_template(self.project_filename, self.project_template, project=configured_project)
    apply_template(self.classpath_filename, self.classpath_template, classpath=configured_classpath)
    apply_template(os.path.join(self.work_dir, 'Debug on port %d.launch' % project.debug_port),
                   self.debug_template, project=configured_project)
    apply_template(self.coreprefs_filename, self.coreprefs_template, project=configured_project)

    for resource in _SETTINGS:
      with safe_open(os.path.join(self.cwd, '.settings', resource), 'w') as prefs:
        prefs.write(pkgutil.get_data(__name__, os.path.join('files', 'eclipse', resource)))

    factorypath = TemplateData(
      project_name=self.project_name,

      # The easiest way to make sure eclipse sees all annotation processors is to put all libs on
      # the apt factorypath - this does not seem to hurt eclipse performance in any noticeable way.
      jarpaths=libs
    )
    apply_template(self.apt_filename, self.apt_template, factorypath=factorypath)

    if project.has_python:
      apply_template(self.pydev_filename, self.pydev_template, project=configured_project)
    else:
      safe_delete(self.pydev_filename)

    print('\nGenerated project at %s%s' % (self.work_dir, os.sep))

########NEW FILE########
__FILENAME__ = filedeps
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import itertools
import os

from twitter.pants.targets.jvm_binary import JvmApp
from twitter.pants.targets.with_sources import TargetWithSources

from .console_task import ConsoleTask

__author__ = 'Dave Buchfuhrer'


class FileDeps(ConsoleTask):
  def console_output(self, targets):
    files = set()
    for target in targets:
      if isinstance(target, TargetWithSources):
        files.update(target.expand_files(recursive=False))
      if isinstance(target, JvmApp):
        files.update(itertools.chain(*[bundle.filemap.keys() for bundle in target.bundles]))
    return files

########NEW FILE########
__FILENAME__ = filemap
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.build_file import BuildFile
from twitter.pants.base.target import Target

from .console_task import ConsoleTask


class Filemap(ConsoleTask):
  """Outputs a mapping from source file to the target that owns the source file."""

  def console_output(self, _):
    visited = set()
    for target in self._find_targets():
      if target not in visited:
        visited.add(target)
        if hasattr(target, 'sources') and target.sources is not None:
          for sourcefile in target.sources:
            path = os.path.join(target.target_base, sourcefile)
            yield '%s %s' % (path, target.address)

  def _find_targets(self):
    if len(self.context.target_roots) > 0:
      for target in self.context.target_roots:
        yield target
    else:
      for buildfile in BuildFile.scan_buildfiles(get_buildroot()):
        target_addresses = Target.get_all_addresses(buildfile)
        for target_address in target_addresses:
          yield Target.get(target_address)

########NEW FILE########
__FILENAME__ = filter
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import operator
import re
import sys

import twitter.pants.base.build_file_aliases

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.address import Address
from twitter.pants.base.target import Target

from twitter.pants.tasks import TaskError
from twitter.pants.tasks.console_task import ConsoleTask


_identity = lambda x: x


def _extract_modifier(value):
  if value.startswith('+'):
    return _identity, value[1:]
  elif value.startswith('-'):
    return operator.not_, value[1:]
  else:
    return _identity, value


def _create_filters(list_option, predicate):
  for value in list_option:
    modifier, value = _extract_modifier(value)
    predicates = map(predicate, value.split(','))
    def filter(target):
      return modifier(any(map(lambda predicate: predicate(target), predicates)))
    yield filter


def _get_target(address):
  try:
    address = Address.parse(get_buildroot(), address, is_relative=False)
  except IOError as e:
    raise TaskError('Failed to parse address: %s: %s' % (address, e))
  match = Target.get(address)
  if not match:
    raise TaskError('Invalid target address: %s' % address)
  return match


class Filter(ConsoleTask):
  """Filters targets based on various criteria."""

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(Filter, cls).setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag('type'), dest='filter_type', action='append', default=[],
                            help="Identifies target types to include (optional '+' prefix) or "
                                 "exclude ('-' prefix).  Multiple type inclusions or exclusions "
                                 "can be specified at once in a comma separated list or else by "
                                 "using multiple instances of this flag.")

    option_group.add_option(mkflag('target'), dest='filter_target', action='append', default=[],
                            help="Identifies specific targets to include (optional '+' prefix) or "
                                 "exclude ('-' prefix).  Multiple target inclusions or exclusions "
                                 "can be specified at once in a comma separated list or else by "
                                 "using multiple instances of this flag.")

    option_group.add_option(mkflag('ancestor'), dest='filter_ancestor', action='append', default=[],
                            help="Identifies ancestor targets (containing targets) that make a "
                                 "select child (contained) targets to include "
                                 "(optional '+' prefix) or exclude ('-' prefix).  Multiple "
                                 "ancestor inclusions or exclusions can be specified at once in "
                                 "a comma separated list or else by using multiple instances of "
                                 "this flag.")

    option_group.add_option(mkflag('regex'), dest='filter_regex', action='append', default=[],
                            help="Identifies regexes of target addresses to include "
                                 "(optional '+' prefix) or exclude ('-' prefix).  Multiple target "
                                 "inclusions or exclusions can be specified at once in a comma "
                                 "separated list or else by using multiple instances of this flag.")

  def __init__(self, context, outstream=sys.stdout):
    super(Filter, self).__init__(context, outstream)

    self._filters = []

    def filter_for_address(address):
      match = _get_target(address)
      return lambda target: target == match
    self._filters.extend(_create_filters(context.options.filter_target, filter_for_address))

    def filter_for_type(name):
      try:
        # Try to do a fully qualified import 1st for filtering on custom types.
        from_list, module, type_name = name.rsplit('.', 2)
        module = __import__('%s.%s' % (from_list, module), fromlist=[from_list])
        target_type = getattr(module, type_name)
      except (ImportError, ValueError):
        # Fall back on pants provided target types.
        if not hasattr(twitter.pants.base.build_file_aliases, name):
          raise TaskError('Invalid type name: %s' % name)
        target_type = getattr(twitter.pants.base.build_file_aliases, name)
      if not issubclass(target_type, Target):
        raise TaskError('Not a Target type: %s' % name)
      return lambda target: isinstance(target, target_type)
    self._filters.extend(_create_filters(context.options.filter_type, filter_for_type))

    def filter_for_ancestor(address):
      ancestor = _get_target(address)
      children = set()
      ancestor.walk(children.add)
      return lambda target: target in children
    self._filters.extend(_create_filters(context.options.filter_ancestor, filter_for_ancestor))

    def filter_for_regex(regex):
      parser = re.compile(regex)
      return lambda target: parser.search(str(target.address))
    self._filters.extend(_create_filters(context.options.filter_regex, filter_for_regex))

  def console_output(self, _):
    filtered = set()
    for target in self.context.target_roots:
      if target not in filtered:
        filtered.add(target)
        for filter in self._filters:
          if not filter(target):
            break
        else:
          yield str(target.address)

########NEW FILE########
__FILENAME__ = idea_gen
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pkgutil
import shutil
import tempfile

from xml.dom import minidom

from twitter.common.dirutil import safe_mkdir
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.generator import TemplateData, Generator
from twitter.pants.targets.java_tests import JavaTests
from twitter.pants.targets.scala_tests import ScalaTests
from twitter.pants.targets.sources import SourceRoot
from twitter.pants.tasks.ide_gen import IdeGen, Project, SourceSet


_TEMPLATE_BASEDIR = 'templates/idea'


_VERSIONS = {
  '9': '12', # 9 and 12 are ipr/iml compatible
  '10': '12', # 10 and 12 are ipr/iml compatible
  '11': '12', # 11 and 12 are ipr/iml compatible
  '12': '12'
}


_SCALA_VERSION_DEFAULT = '2.9'
_SCALA_VERSIONS = {
  '2.8':                  'Scala 2.8',
  _SCALA_VERSION_DEFAULT: 'Scala 2.9',
  '2.10':                 'Scala 2.10',
  '2.10-virt':            'Scala 2.10 virtualized'
}


class IdeaGen(IdeGen):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    IdeGen.setup_parser(option_group, args, mkflag)

    supported_versions = sorted(list(_VERSIONS.keys()))
    option_group.add_option(mkflag("idea-version"), dest="idea_gen_version",
                            default='11', type="choice", choices=supported_versions,
                            help="[%%default] The IntelliJ IDEA version the project "
                                   "configuration should be generated for; can be one of: " \
                                   "%s" % supported_versions)

    option_group.add_option(mkflag("merge"), mkflag("merge", negate=True), default=True,
                            action="callback", callback=mkflag.set_bool, dest="idea_gen_merge",
                            help="[%default] Merge any manual customizations in existing "
                                   "Intellij IDEA configuration. If False, manual customizations "
                                   "will be over-written.")

    option_group.add_option(mkflag("open"), mkflag("open", negate=True), default=True,
                            action="callback", callback=mkflag.set_bool, dest="idea_gen_open",
                            help="[%default] Attempts top open the generated project in IDEA.")

    option_group.add_option(mkflag("bash"), mkflag("bash", negate=True), default=False,
                            action="callback", callback=mkflag.set_bool, dest="idea_gen_bash",
                            help="Adds a bash facet to the generated project configuration.")

    option_group.add_option(mkflag("scala-language-level"), default=_SCALA_VERSION_DEFAULT,
                            type="choice", choices=_SCALA_VERSIONS.keys(),
                            dest="idea_scala_language_level",
                            help="[%default] Set the scala language level used for IDEA linting.")
    option_group.add_option(mkflag("scala-maximum-heap-size"),
                            dest="idea_gen_scala_maximum_heap_size",
                            help="[%default] Sets the maximum heap size (in megabytes) for scalac.")
    option_group.add_option(mkflag("fsc"), mkflag("fsc", negate=True), default=False,
                            action="callback", callback=mkflag.set_bool, dest="idea_gen_fsc",
                            help="If the project contains any scala targets this specifies the "
                                   "fsc compiler should be enabled.")

    option_group.add_option(mkflag("java-encoding"), default="UTF-8",
                            dest="idea_gen_java_encoding",
                            help="[%default] Sets the file encoding for java files in this "
                                   "project.")
    option_group.add_option(mkflag("java-maximum-heap-size"),
                            dest="idea_gen_java_maximum_heap_size",
                            help="[%default] Sets the maximum heap size (in megabytes) for javac.")

  def __init__(self, context):
    IdeGen.__init__(self, context)


    self.intellij_output_dir = os.path.join(self.work_dir, 'out')
    self.nomerge = not context.options.idea_gen_merge
    self.open = context.options.idea_gen_open
    self.bash = context.options.idea_gen_bash

    self.scala_language_level = _SCALA_VERSIONS.get(context.options.idea_scala_language_level, None)
    self.scala_maximum_heap_size = (
      context.options.idea_gen_scala_maximum_heap_size
      or context.config.getint('idea', 'scala_maximum_heap_size_mb', default=512)
    )
    self.fsc = context.options.idea_gen_fsc

    self.java_encoding = context.options.idea_gen_java_encoding
    self.java_maximum_heap_size = (
      context.options.idea_gen_java_maximum_heap_size
      or context.config.getint('idea', 'java_maximum_heap_size_mb', default=128)
    )

    idea_version = _VERSIONS[context.options.idea_gen_version]
    self.project_template = os.path.join(_TEMPLATE_BASEDIR, 'project-%s.mustache' % idea_version)
    self.module_template = os.path.join(_TEMPLATE_BASEDIR, 'module-%s.mustache' % idea_version)

    self.project_filename = os.path.join(self.cwd, '%s.ipr' % self.project_name)
    self.module_filename = os.path.join(self.work_dir, '%s.iml' % self.project_name)

  def generate_project(self, project):
    def is_test(source_set):
      # Non test targets that otherwise live in test target roots (say a java_library), must
      # be marked as test for IDEA to correctly link the targets with the test code that uses
      # them. Therefore we check the base instead of the is_test flag.
      return source_set.source_base in SourceSet.TEST_BASES

    def create_content_root(source_set):
      root_relative_path = os.path.join(source_set.source_base, source_set.path) \
                           if source_set.path else source_set.source_base

      sources = TemplateData(
        path=root_relative_path,
        package_prefix=source_set.path.replace('/', '.') if source_set.path else None,
        is_test=is_test(source_set)
      )

      return TemplateData(
        path=root_relative_path,
        sources=[sources],
        exclude_paths=[os.path.join(source_set.source_base, x) for x in source_set.excludes],
      )

    content_roots = [create_content_root(source_set) for source_set in project.sources]
    if project.has_python:
      content_roots.extend(create_content_root(source_set) for source_set in project.py_sources)

    scala = None
    if project.has_scala:
      scala = TemplateData(
        language_level=self.scala_language_level,
        maximum_heap_size=self.scala_maximum_heap_size,
        fsc=self.fsc,
        compiler_classpath=project.scala_compiler_classpath
      )

    configured_module = TemplateData(
      root_dir=get_buildroot(),
      path=self.module_filename,
      content_roots=content_roots,
      bash=self.bash,
      python=project.has_python,
      scala=scala,
      internal_jars=[cp_entry.jar for cp_entry in project.internal_jars],
      internal_source_jars=[cp_entry.source_jar for cp_entry in project.internal_jars
                            if cp_entry.source_jar],
      external_jars=[cp_entry.jar for cp_entry in project.external_jars],
      external_javadoc_jars=[cp_entry.javadoc_jar for cp_entry in project.external_jars
                             if cp_entry.javadoc_jar],
      external_source_jars=[cp_entry.source_jar for cp_entry in project.external_jars
                            if cp_entry.source_jar],
      extra_components=[],
    )

    outdir = os.path.abspath(self.intellij_output_dir)
    if not os.path.exists(outdir):
      os.makedirs(outdir)

    configured_project = TemplateData(
      root_dir=get_buildroot(),
      outdir=outdir,
      modules=[ configured_module ],
      java=TemplateData(
        encoding=self.java_encoding,
        maximum_heap_size=self.java_maximum_heap_size,
        jdk=self.java_jdk,
        language_level = 'JDK_1_%d' % self.java_language_level
      ),
      resource_extensions=list(project.resource_extensions),
      scala=scala,
      checkstyle_suppression_files=','.join(project.checkstyle_suppression_files),
      checkstyle_classpath=';'.join(project.checkstyle_classpath),
      debug_port=project.debug_port,
      extra_components=[],
    )

    existing_project_components = None
    existing_module_components = None
    if not self.nomerge:
      # Grab the existing components, which may include customized ones.
      existing_project_components = self._parse_xml_component_elements(self.project_filename)
      existing_module_components = self._parse_xml_component_elements(self.module_filename)

    # Generate (without merging in any extra components).
    safe_mkdir(os.path.abspath(self.intellij_output_dir))

    ipr = self._generate_to_tempfile(
        Generator(pkgutil.get_data(__name__, self.project_template), project = configured_project))
    iml = self._generate_to_tempfile(
        Generator(pkgutil.get_data(__name__, self.module_template), module = configured_module))

    if not self.nomerge:
      # Get the names of the components we generated, and then delete the
      # generated files.  Clunky, but performance is not an issue, and this
      # is an easy way to get those component names from the templates.
      extra_project_components = self._get_components_to_merge(existing_project_components, ipr)
      extra_module_components =  self._get_components_to_merge(existing_module_components, iml)
      os.remove(ipr)
      os.remove(iml)

      # Generate again, with the extra components.
      ipr = self._generate_to_tempfile(Generator(pkgutil.get_data(__name__, self.project_template),
          project = configured_project.extend(extra_components = extra_project_components)))
      iml = self._generate_to_tempfile(Generator(pkgutil.get_data(__name__, self.module_template),
          module = configured_module.extend(extra_components = extra_module_components)))

    shutil.move(ipr, self.project_filename)
    shutil.move(iml, self.module_filename)

    print('\nGenerated project at %s%s' % (self.work_dir, os.sep))

    return self.project_filename if self.open else None

  def _generate_to_tempfile(self, generator):
    """Applies the specified generator to a temp file and returns the path to that file.
    We generate into a temp file so that we don't lose any manual customizations on error."""
    (output_fd, output_path) = tempfile.mkstemp()
    with os.fdopen(output_fd, 'w') as output:
      generator.write(output)
    return output_path

  def _get_resource_extensions(self, project):
    resource_extensions = set()
    resource_extensions.update(project.resource_extensions)

    # TODO(John Sirois): make test resources 1st class in ant build and punch this through to pants
    # model
    for _, _, files in os.walk(os.path.join(get_buildroot(), 'tests', 'resources')):
      resource_extensions.update(Project.extract_resource_extensions(files))

    return resource_extensions

  def _parse_xml_component_elements(self, path):
    """Returns a list of pairs (component_name, xml_fragment) where xml_fragment is the xml text of
    that <component> in the specified xml file."""
    if not os.path.exists(path):
      return []  # No existing components.
    dom = minidom.parse(path)
    # .ipr and .iml files both consist of <component> elements directly under a root element.
    return [ (x.getAttribute('name'), x.toxml()) for x in dom.getElementsByTagName('component') ]

  def _get_components_to_merge(self, mergable_components, path):
    """Returns a list of the <component> fragments in mergable_components that are not
    superceded by a <component> in the specified xml file.
    mergable_components is a list of (name, xml_fragment) pairs."""

    # As a convenience, we use _parse_xml_component_elements to get the
    # superceding component names, ignoring the generated xml fragments.
    # This is fine, since performance is not an issue.
    generated_component_names = set(
      [ name for (name, _) in self._parse_xml_component_elements(path) ])
    return [ x[1] for x in mergable_components if x[0] not in generated_component_names]


########NEW FILE########
__FILENAME__ = ide_gen
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import shutil

from collections import defaultdict

from twitter.common.collections.orderedset import OrderedSet
from twitter.common.dirutil import safe_mkdir

from twitter.pants import binary_util
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.target import Target
from twitter.pants.goal.phase import Phase
from twitter.pants.targets.jvm_binary import JvmBinary
from twitter.pants.tasks.checkstyle import Checkstyle

from .jvm_binary_task import JvmBinaryTask

from . import TaskError


# We use custom checks for scala and java targets here for 2 reasons:
# 1.) jvm_binary could have either a scala or java source file attached so we can't do a pure
#     target type test
# 2.) the target may be under development in which case it may not have sources yet - its pretty
#     common to write a BUILD and ./pants goal idea the target inside to start development at which
#     point there are no source files yet - and the developer intents to add them using the ide.

def is_scala(target):
  return target.has_sources('.scala') or target.is_scala


def is_java(target):
  return target.has_sources('.java') or target.is_java


class IdeGen(JvmBinaryTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("project-name"), dest="ide_gen_project_name", default="project",
                            help="[%default] Specifies the name to use for the generated project.")

    gen_dir = mkflag("project-dir")
    option_group.add_option(gen_dir, dest="ide_gen_project_dir",
                            help="[%default] Specifies the directory to output the generated "
                                "project files to.")
    option_group.add_option(mkflag("project-cwd"), dest="ide_gen_project_cwd",
                            help="[%%default] Specifies the directory the generated project should "
                                 "use as the cwd for processes it launches.  Note that specifying "
                                 "this trumps %s and not all project related files will be stored "
                                 "there." % gen_dir)

    option_group.add_option(mkflag("intransitive"), default=False,
                            action="store_true", dest='ide_gen_intransitive',
                            help="Limits the sources included in the generated project to just "
                                 "those owned by the targets specified on the command line")

    option_group.add_option(mkflag("python"), mkflag("python", negate=True), default=False,
                            action="callback", callback=mkflag.set_bool, dest='ide_gen_python',
                            help="[%default] Adds python support to the generated project "
                                 "configuration.")

    option_group.add_option(mkflag("java"), mkflag("java", negate=True), default=True,
                            action="callback", callback=mkflag.set_bool, dest='ide_gen_java',
                            help="[%default] Includes java sources in the project; otherwise "
                                 "compiles them and adds them to the project classpath.")
    java_language_level = mkflag("java-language-level")
    # TODO(John Sirois): Advance the default to 7 when 8 is released.
    option_group.add_option(java_language_level, default=6,
                            dest="ide_gen_java_language_level", type="int",
                            help="[%default] Sets the java language and jdk used to compile the "
                                 "project's java sources.")
    option_group.add_option(mkflag("java-jdk-name"), default=None,
                            dest="ide_gen_java_jdk",
                            help="Sets the jdk used to compile the project's java sources. If "
                                 "unset the default jdk name for the "
                                 "%s is used." % java_language_level)

    option_group.add_option(mkflag("scala"), mkflag("scala", negate=True), default=True,
                            action="callback", callback=mkflag.set_bool, dest='ide_gen_scala',
                            help="[%default] Includes scala sources in the project; otherwise "
                                 "compiles them and adds them to the project classpath.")

  def __init__(self, context):
    super(IdeGen, self).__init__(context)

    self.project_name = context.options.ide_gen_project_name
    self.python = context.options.ide_gen_python
    self.skip_java = not context.options.ide_gen_java
    self.skip_scala = not context.options.ide_gen_scala

    self.java_language_level = context.options.ide_gen_java_language_level
    if context.options.ide_gen_java_jdk:
      self.java_jdk = context.options.ide_gen_java_jdk
    else:
      self.java_jdk = '1.%d' % self.java_language_level

    self.work_dir = os.path.abspath(
      context.options.ide_gen_project_dir
      or os.path.join(
        context.config.get('ide', 'workdir'), self.__class__.__name__, self.project_name
      )
    )
    self.cwd = (
      os.path.abspath(context.options.ide_gen_project_cwd) if context.options.ide_gen_project_cwd
      else self.work_dir
    )

    self.intransitive = context.options.ide_gen_intransitive

    self.checkstyle_suppression_files = context.config.getdefault(
      'checkstyle_suppression_files', type=list, default=[]
    )
    self.debug_port = context.config.getint('ide', 'debug_port')

    self.checkstyle_bootstrap_key = 'checkstyle'
    checkstyle = context.config.getlist('checkstyle', 'bootstrap-tools',
                                        default=[':twitter-checkstyle'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self.checkstyle_bootstrap_key, checkstyle)

    self.scalac_bootstrap_key = None
    if not self.skip_scala:
      self.scalac_bootstrap_key = 'scalac'
      scalac = context.config.getlist('scala-compile', 'compile-bootstrap-tools',
                                      default=[':scala-compile-2.9.3'])
      self._jvm_tool_bootstrapper.register_jvm_tool(self.scalac_bootstrap_key, scalac)

    targets, self._project = self.configure_project(
        context.targets(),
        self.checkstyle_suppression_files,
        self.debug_port)

    self.configure_compile_context(targets)

    if self.python:
      self.context.products.require('python')
    if not self.skip_java:
      self.context.products.require('java')
    if not self.skip_scala:
      self.context.products.require('scala')

    self.context.products.require('jars')
    self.context.products.require('source_jars')

  def configure_project(self, targets, checkstyle_suppression_files, debug_port):

    jvm_targets = Target.extract_jvm_targets(targets)
    if self.intransitive:
      jvm_targets = set(self.context.target_roots).intersection(jvm_targets)
    project = Project(self.project_name,
                      self.python,
                      self.skip_java,
                      self.skip_scala,
                      get_buildroot(),
                      checkstyle_suppression_files,
                      debug_port,
                      jvm_targets,
                      not self.intransitive,
                      self.context.new_workunit)

    if self.python:
      python_source_paths = self.context.config.getlist('ide', 'python_source_paths', default=[])
      python_test_paths = self.context.config.getlist('ide', 'python_test_paths', default=[])
      python_lib_paths = self.context.config.getlist('ide', 'python_lib_paths', default=[])
      project.configure_python(python_source_paths, python_test_paths, python_lib_paths)

    extra_source_paths = self.context.config.getlist('ide', 'extra_jvm_source_paths', default=[])
    extra_test_paths = self.context.config.getlist('ide', 'extra_jvm_test_paths', default=[])
    all_targets = project.configure_jvm(extra_source_paths, extra_test_paths)
    return all_targets, project

  def configure_compile_context(self, targets):
    """
      Trims the context's target set to just those targets needed as jars on the IDE classpath.
      All other targets only contribute their external jar dependencies and excludes to the
      classpath definition.
    """
    def is_cp(target):
      return (
        target.is_codegen or

        # Some IDEs need annotation processors pre-compiled, others are smart enough to detect and
        # proceed in 2 compile rounds
        target.is_apt or

        (self.skip_java and is_java(target)) or
        (self.skip_scala and is_scala(target)) or
        (self.intransitive and target not in self.context.target_roots)
      )

    jars = OrderedSet()
    excludes = OrderedSet()
    compiles = OrderedSet()
    def prune(target):
      if target.is_jvm:
        if target.excludes:
          excludes.update(target.excludes)
        jars.update(jar for jar in target.jar_dependencies if jar.rev)
        if is_cp(target):
          target.walk(compiles.add)

    for target in targets:
      target.walk(prune)

    self.context.replace_targets(compiles)

    self.binary = self.context.add_new_target(self.work_dir,
                                              JvmBinary,
                                              name='%s-external-jars' % self.project_name,
                                              dependencies=jars,
                                              excludes=excludes,
                                              configurations=('default', 'sources', 'javadoc'))
    self.require_jar_dependencies(predicate=lambda t: t == self.binary)

    self.context.log.debug('pruned to cp:\n\t%s' % '\n\t'.join(
      str(t) for t in self.context.targets())
    )

  def map_internal_jars(self, targets):
    internal_jar_dir = os.path.join(self.work_dir, 'internal-libs')
    safe_mkdir(internal_jar_dir, clean=True)

    internal_source_jar_dir = os.path.join(self.work_dir, 'internal-libsources')
    safe_mkdir(internal_source_jar_dir, clean=True)

    internal_jars = self.context.products.get('jars')
    internal_source_jars = self.context.products.get('source_jars')
    for target in targets:
      mappings = internal_jars.get(target)
      if mappings:
        for base, jars in mappings.items():
          if len(jars) != 1:
            raise TaskError('Unexpected mapping, multiple jars for %s: %s' % (target, jars))

          jar = jars[0]
          cp_jar = os.path.join(internal_jar_dir, jar)
          shutil.copy(os.path.join(base, jar), cp_jar)

          cp_source_jar = None
          mappings = internal_source_jars.get(target)
          if mappings:
            for base, jars in mappings.items():
              if len(jars) != 1:
                raise TaskError(
                  'Unexpected mapping, multiple source jars for %s: %s' % (target, jars)
                )
              jar = jars[0]
              cp_source_jar = os.path.join(internal_source_jar_dir, jar)
              shutil.copy(os.path.join(base, jar), cp_source_jar)

          self._project.internal_jars.add(ClasspathEntry(cp_jar, source_jar=cp_source_jar))

  def map_external_jars(self):
    external_jar_dir = os.path.join(self.work_dir, 'external-libs')
    safe_mkdir(external_jar_dir, clean=True)

    external_source_jar_dir = os.path.join(self.work_dir, 'external-libsources')
    safe_mkdir(external_source_jar_dir, clean=True)

    external_javadoc_jar_dir = os.path.join(self.work_dir, 'external-libjavadoc')
    safe_mkdir(external_javadoc_jar_dir, clean=True)

    confs = ['default', 'sources', 'javadoc']
    for entry in self.list_jar_dependencies(self.binary, confs=confs):
      jar = entry.get('default')
      if jar:
        cp_jar = os.path.join(external_jar_dir, os.path.basename(jar))
        shutil.copy(jar, cp_jar)

        cp_source_jar = None
        source_jar = entry.get('sources')
        if source_jar:
          cp_source_jar = os.path.join(external_source_jar_dir, os.path.basename(source_jar))
          shutil.copy(source_jar, cp_source_jar)

        cp_javadoc_jar = None
        javadoc_jar = entry.get('javadoc')
        if javadoc_jar:
          cp_javadoc_jar = os.path.join(external_javadoc_jar_dir, os.path.basename(javadoc_jar))
          shutil.copy(javadoc_jar, cp_javadoc_jar)

        self._project.external_jars.add(ClasspathEntry(cp_jar,
                                                       source_jar=cp_source_jar,
                                                       javadoc_jar=cp_javadoc_jar))

  def execute(self, targets):
    """Stages IDE project artifacts to a project directory and generates IDE configuration files."""
    checkstyle_enabled = len(Phase.goals_of_type(Checkstyle)) > 0
    if checkstyle_enabled:
      checkstyle_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
          self.checkstyle_bootstrap_key)
    else:
      checkstyle_classpath = []

    if self.scalac_bootstrap_key:
      scalac_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
          self.scalac_bootstrap_key)
    else:
      scalac_classpath = []

    self._project.set_tool_classpaths(checkstyle_classpath, scalac_classpath)

    self.map_internal_jars(targets)
    self.map_external_jars()

    idefile = self.generate_project(self._project)
    if idefile:
      binary_util.ui_open(idefile)

  def generate_project(self, project):
    raise NotImplementedError('Subclasses must generate a project for an ide')


class ClasspathEntry(object):
  """Represents a classpath entry that may have sources available."""
  def __init__(self, jar, source_jar=None, javadoc_jar=None):
    self.jar = jar
    self.source_jar = source_jar
    self.javadoc_jar = javadoc_jar


class SourceSet(object):
  """Models a set of source files."""

  TEST_BASES = set()

  def __init__(self, root_dir, source_base, path, is_test):
    """
      root_dir: the full path to the root directory of the project containing this source set
      source_base: the relative path from root_dir to the base of this source set
      path: the relative path from the source_base to the base of the sources in this set
      is_test: true iff the sources contained by this set implement test cases
    """

    self.root_dir = root_dir
    self.source_base = source_base
    self.path = path
    self.is_test = is_test
    self._excludes = []
    if is_test:
      SourceSet.TEST_BASES.add(self.source_base)

  @property
  def excludes(self):
    """Paths relative to self.path that are excluded from this source set."""

    return self._excludes


class Project(object):
  """Models a generic IDE project that is comprised of a set of BUILD targets."""

  @staticmethod
  def extract_resource_extensions(resources):
    """Returns the set of unique extensions (including the .) from the given resource files."""

    if resources:
      for resource in resources:
        _, ext = os.path.splitext(resource)
        yield ext

  def __init__(self, name, has_python, skip_java, skip_scala, root_dir,
               checkstyle_suppression_files, debug_port, targets, transitive, workunit_factory):
    """Creates a new, unconfigured, Project based at root_dir and comprised of the sources visible
    to the given targets."""

    self.name = name
    self.root_dir = root_dir
    self.targets = OrderedSet(targets)
    self.transitive = transitive
    self.workunit_factory = workunit_factory

    self.sources = []
    self.py_sources = []
    self.py_libs = []
    self.resource_extensions = set()

    self.has_python = has_python
    self.skip_java = skip_java
    self.skip_scala = skip_scala
    self.has_scala = False
    self.has_tests = False

    self.checkstyle_suppression_files = checkstyle_suppression_files # Absolute paths.
    self.debug_port = debug_port

    self.internal_jars = OrderedSet()
    self.external_jars = OrderedSet()

  def configure_python(self, source_roots, test_roots, lib_roots):
    self.py_sources.extend(SourceSet(get_buildroot(), root, None, False) for root in source_roots)
    self.py_sources.extend(SourceSet(get_buildroot(), root, None, True) for root in test_roots)
    for root in lib_roots:
      for path in os.listdir(os.path.join(get_buildroot(), root)):
        if os.path.isdir(os.path.join(get_buildroot(), root, path)) or path.endswith('.egg'):
          self.py_libs.append(SourceSet(get_buildroot(), root, path, False))

  def configure_jvm(self, extra_source_paths, extra_test_paths):
    """
      Configures this project's source sets returning the full set of targets the project is
      comprised of.  The full set can be larger than the initial set of targets when any of the
      initial targets only has partial ownership of its source set's directories.
    """

    # TODO(John Sirois): much waste lies here, revisit structuring for more readable and efficient
    # construction of source sets and excludes ... and add a test!

    analyzed = OrderedSet()
    targeted = set()

    def source_target(target):
      return ((self.transitive or target in self.targets) and
              target.has_sources() and
              (not target.is_codegen and
               not (self.skip_java and is_java(target)) and
               not (self.skip_scala and is_scala(target))))

    def configure_source_sets(relative_base, sources, is_test):
      absolute_base = os.path.join(self.root_dir, relative_base)
      paths = set([ os.path.dirname(source) for source in sources])
      for path in paths:
        absolute_path = os.path.join(absolute_base, path)
        if absolute_path not in targeted:
          targeted.add(absolute_path)
          self.sources.append(SourceSet(self.root_dir, relative_base, path, is_test))

    def find_source_basedirs(target):
      dirs = set()
      if source_target(target):
        absolute_base = os.path.join(self.root_dir, target.target_base)
        dirs.update([ os.path.join(absolute_base, os.path.dirname(source))
                      for source in target.sources ])
      return dirs

    def configure_target(target):
      if target not in analyzed:
        analyzed.add(target)

        self.has_scala = not self.skip_scala and (self.has_scala or is_scala(target))

        if target.has_resources:
          resources_by_basedir = defaultdict(set)
          for resources in target.resources:
            resources_by_basedir[resources.target_base].update(resources.sources)
          for basedir, resources in resources_by_basedir.items():
            self.resource_extensions.update(Project.extract_resource_extensions(resources))
            configure_source_sets(basedir, resources, is_test=False)

        if target.sources:
          test = target.is_test
          self.has_tests = self.has_tests or test
          configure_source_sets(target.target_base, target.sources, is_test = test)

        # Other BUILD files may specify sources in the same directory as this target.  Those BUILD
        # files might be in parent directories (globs('a/b/*.java')) or even children directories if
        # this target globs children as well.  Gather all these candidate BUILD files to test for
        # sources they own that live in the directories this targets sources live in.
        target_dirset = find_source_basedirs(target)
        candidates = Target.get_all_addresses(target.address.buildfile)
        for ancestor in target.address.buildfile.ancestors():
          candidates.update(Target.get_all_addresses(ancestor))
        for sibling in target.address.buildfile.siblings():
          candidates.update(Target.get_all_addresses(sibling))
        for descendant in target.address.buildfile.descendants():
          candidates.update(Target.get_all_addresses(descendant))

        def is_sibling(target):
          return source_target(target) and target_dirset.intersection(find_source_basedirs(target))

        return filter(is_sibling, [ Target.get(a) for a in candidates if a != target.address ])

    for target in self.targets:
      target.walk(configure_target, predicate = source_target)

    # We need to figure out excludes, in doing so there are 2 cases we should not exclude:
    # 1.) targets depend on A only should lead to an exclude of B
    # A/BUILD
    # A/B/BUILD
    #
    # 2.) targets depend on A and C should not lead to an exclude of B (would wipe out C)
    # A/BUILD
    # A/B
    # A/B/C/BUILD
    #
    # 1 approach: build set of all paths and parent paths containing BUILDs our targets depend on -
    # these are unexcludable

    unexcludable_paths = set()
    for source_set in self.sources:
      parent = os.path.join(self.root_dir, source_set.source_base, source_set.path)
      while True:
        unexcludable_paths.add(parent)
        parent, _ = os.path.split(parent)
        # no need to add the repo root or above, all source paths and extra paths are children
        if parent == self.root_dir:
          break

    for source_set in self.sources:
      paths = set()
      source_base = os.path.join(self.root_dir, source_set.source_base)
      for root, dirs, _ in os.walk(os.path.join(source_base, source_set.path)):
        if dirs:
          paths.update([os.path.join(root, directory) for directory in dirs])
      unused_children = paths - targeted
      if unused_children:
        for child in unused_children:
          if child not in unexcludable_paths:
            source_set.excludes.append(os.path.relpath(child, source_base))

    targets = OrderedSet()
    for target in self.targets:
      target.walk(lambda target: targets.add(target), source_target)
    targets.update(analyzed - targets)

    self.sources.extend(SourceSet(get_buildroot(), p, None, False) for p in extra_source_paths)
    self.sources.extend(SourceSet(get_buildroot(), p, None, True) for p in extra_test_paths)

    return targets

  def set_tool_classpaths(self, checkstyle_classpath, scalac_classpath):
    self.checkstyle_classpath = checkstyle_classpath
    self.scala_compiler_classpath = scalac_classpath

########NEW FILE########
__FILENAME__ = ivy_resolve
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function
from collections import defaultdict

import os
import shutil
import time

from twitter.common.dirutil import safe_mkdir
from twitter.pants import binary_util
from twitter.pants.ivy.bootstrapper import Bootstrapper
from .cache_manager import VersionedTargetSet
from .ivy_utils import IvyUtils
from .nailgun_task import NailgunTask
from . import TaskError


class IvyResolve(NailgunTask):

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    NailgunTask.setup_parser(option_group, args, mkflag)

    flag = mkflag('override')
    option_group.add_option(flag, action='append', dest='ivy_resolve_overrides',
                            help="""Specifies a jar dependency override in the form:
                            [org]#[name]=(revision|url)

                            For example, to specify 2 overrides:
                            %(flag)s=com.foo#bar=0.1.2 \\
                            %(flag)s=com.baz#spam=file:///tmp/spam.jar
                            """ % dict(flag=flag))

    report = mkflag("report")
    option_group.add_option(report, mkflag("report", negate=True), dest = "ivy_resolve_report",
                            action="callback", callback=mkflag.set_bool, default=False,
                            help = "[%default] Generate an ivy resolve html report")

    option_group.add_option(mkflag("open"), mkflag("open", negate=True),
                            dest="ivy_resolve_open", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%%default] Attempt to open the generated ivy resolve report "
                                 "in a browser (implies %s)." % report)

    option_group.add_option(mkflag("outdir"), dest="ivy_resolve_outdir",
                            help="Emit ivy report outputs in to this directory.")

    option_group.add_option(mkflag("args"), dest="ivy_args", action="append", default=[],
                            help = "Pass these extra args to ivy.")

    option_group.add_option(mkflag("mutable-pattern"), dest="ivy_mutable_pattern",
                            help="If specified, all artifact revisions matching this pattern will "
                                 "be treated as mutable unless a matching artifact explicitly "
                                 "marks mutable as False.")

  def __init__(self, context, confs=None):
    super(IvyResolve, self).__init__(context)
    work_dir = context.config.get('ivy-resolve', 'workdir')

    self._ivy_bootstrapper = Bootstrapper.instance()
    self._cachedir = self._ivy_bootstrapper.ivy_cache_dir
    self._confs = confs or context.config.getlist('ivy-resolve', 'confs', default=['default'])
    self._classpath_dir = os.path.join(work_dir, 'mapped')

    self._outdir = context.options.ivy_resolve_outdir or os.path.join(work_dir, 'reports')
    self._open = context.options.ivy_resolve_open
    self._report = self._open or context.options.ivy_resolve_report

    self._ivy_bootstrap_key = 'ivy'
    ivy_bootstrap_tools = context.config.getlist('ivy-resolve', 'bootstrap-tools', ':xalan')
    self._jvm_tool_bootstrapper.register_jvm_tool(self._ivy_bootstrap_key, ivy_bootstrap_tools)

    self._ivy_utils = IvyUtils(config=context.config,
                               options=context.options,
                               log=context.log)
    context.products.require_data('exclusives_groups')

    # Typically this should be a local cache only, since classpaths aren't portable.
    self.setup_artifact_cache_from_config(config_section='ivy-resolve')

  def invalidate_for(self):
    return self.context.options.ivy_resolve_overrides

  def execute(self, targets):
    """Resolves the specified confs for the configured targets and returns an iterator over
    tuples of (conf, jar path).
    """
    groups = self.context.products.get_data('exclusives_groups')
    executor = self.create_java_executor()

    # Below, need to take the code that actually execs ivy, and invoke it once for each
    # group. Then after running ivy, we need to take the resulting classpath, and load it into
    # the build products.

    # The set of groups we need to consider is complicated:
    # - If there are no conflicting exclusives (ie, there's only one entry in the map),
    #   then we just do the one.
    # - If there are conflicts, then there will be at least three entries in the groups map:
    #   - the group with no exclusives (X)
    #   - the two groups that are in conflict (A and B).
    # In the latter case, we need to do the resolve twice: Once for A+X, and once for B+X,
    # because things in A and B can depend on things in X; and so they can indirectly depend
    # on the dependencies of X.
    # (I think this well be covered by the computed transitive dependencies of
    # A and B. But before pushing this change, review this comment, and make sure that this is
    # working correctly.)
    for group_key in groups.get_group_keys():
      # Narrow the groups target set to just the set of targets that we're supposed to build.
      # Normally, this shouldn't be different from the contents of the group.
      group_targets = groups.get_targets_for_group_key(group_key) & set(targets)

      # NOTE(pl): The symlinked ivy.xml (for IDEs, particularly IntelliJ) in the presence of
      # multiple exclusives groups will end up as the last exclusives group run.  I'd like to
      # deprecate this eventually, but some people rely on it, and it's not clear to me right now
      # whether telling them to use IdeaGen instead is feasible.
      classpath = self.ivy_resolve(group_targets,
                                   executor=executor,
                                   symlink_ivyxml=True,
                                   workunit_name='ivy-resolve')
      if self.context.products.is_required_data('ivy_jar_products'):
        self._populate_ivy_jar_products(group_targets)
      for conf in self._confs:
        # It's important we add the full classpath as an (ordered) unit for code that is classpath
        # order sensitive
        classpath_entries = map(lambda entry: (conf, entry), classpath)
        groups.update_compatible_classpaths(group_key, classpath_entries)

      if self._report:
        self._generate_ivy_report(group_targets)

    create_jardeps_for = self.context.products.isrequired('jar_dependencies')
    if create_jardeps_for:
      genmap = self.context.products.get('jar_dependencies')
      for target in filter(create_jardeps_for, targets):
        self._ivy_utils.mapjars(genmap, target, executor=executor,
                                workunit_factory=self.context.new_workunit)

  def check_artifact_cache_for(self, invalidation_check):
    # Ivy resolution is an output dependent on the entire target set, and is not divisible
    # by target. So we can only cache it keyed by the entire target set.
    global_vts = VersionedTargetSet.from_versioned_targets(invalidation_check.all_vts)
    return [global_vts]

  def _populate_ivy_jar_products(self, targets):
    """Populate the build products with an IvyInfo object for each generated ivy report."""
    ivy_products = self.context.products.get_data('ivy_jar_products') or defaultdict(list)
    for conf in self._confs:
      ivyinfo = self._ivy_utils.parse_xml_report(targets, conf)
      if ivyinfo:
        # Value is a list, to accommodate multiple exclusives groups.
        ivy_products[conf].append(ivyinfo)
    self.context.products.safe_create_data('ivy_jar_products', lambda: ivy_products)

  def _generate_ivy_report(self, targets):
    def make_empty_report(report, organisation, module, conf):
      no_deps_xml_template = """
        <?xml version="1.0" encoding="UTF-8"?>
        <?xml-stylesheet type="text/xsl" href="ivy-report.xsl"?>
        <ivy-report version="1.0">
          <info
            organisation="%(organisation)s"
            module="%(module)s"
            revision="latest.integration"
            conf="%(conf)s"
            confs="%(conf)s"
            date="%(timestamp)s"/>
        </ivy-report>
      """
      no_deps_xml = no_deps_xml_template % dict(organisation=organisation,
                                                module=module,
                                                conf=conf,
                                                timestamp=time.strftime('%Y%m%d%H%M%S'))
      with open(report, 'w') as report_handle:
        print(no_deps_xml, file=report_handle)

    classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._ivy_bootstrap_key,
                                                                   self.create_java_executor())

    reports = []
    org, name = self._ivy_utils.identify(targets)
    xsl = os.path.join(self._cachedir, 'ivy-report.xsl')

    # Xalan needs this dir to exist - ensure that, but do no more - we have no clue where this
    # points.
    safe_mkdir(self._outdir, clean=False)

    for conf in self._confs:
      params = dict(org=org, name=name, conf=conf)
      xml = self._ivy_utils.xml_report_path(targets, conf)
      if not os.path.exists(xml):
        make_empty_report(xml, org, name, conf)
      out = os.path.join(self._outdir, '%(org)s-%(name)s-%(conf)s.html' % params)
      args = ['-IN', xml, '-XSL', xsl, '-OUT', out]
      if 0 != self.runjava(classpath=classpath, main='org.apache.xalan.xslt.Process',
                           args=args, workunit_name='report'):
        raise TaskError
      reports.append(out)

    css = os.path.join(self._outdir, 'ivy-report.css')
    if os.path.exists(css):
      os.unlink(css)
    shutil.copy(os.path.join(self._cachedir, 'ivy-report.css'), self._outdir)

    if self._open:
      binary_util.ui_open(*reports)

########NEW FILE########
__FILENAME__ = ivy_utils
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import xml
import pkgutil
import re
import threading
import errno

from collections import namedtuple, defaultdict
from contextlib import contextmanager

from twitter.common.collections import OrderedDict, OrderedSet
from twitter.common.dirutil import safe_mkdir, safe_open

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.generator import Generator, TemplateData
from twitter.pants.base.revision import Revision
from twitter.pants.base.target import Target
from twitter.pants.ivy.bootstrapper import Bootstrapper
from twitter.pants.ivy.ivy import Ivy
from twitter.pants.java import util
from twitter.pants.tasks.task_error import TaskError


IvyModuleRef = namedtuple('IvyModuleRef', ['org', 'name', 'rev'])
IvyArtifact = namedtuple('IvyArtifact', ['path', 'classifier'])
IvyModule = namedtuple('IvyModule', ['ref', 'artifacts', 'callers'])


class IvyInfo(object):
  def __init__(self):
    self.modules_by_ref = {}  # Map from ref to referenced module.
    # Map from ref of caller to refs of modules required by that caller.
    self.deps_by_caller = defaultdict(OrderedSet)

  def add_module(self, module):
    self.modules_by_ref[module.ref] = module
    for caller in module.callers:
      self.deps_by_caller[caller].add(module.ref)


class IvyUtils(object):
  """Useful methods related to interaction with ivy."""
  def __init__(self, config, options, log):
    self._log = log
    self._config = config
    self._options = options

    # TODO(pl): This is super awful, but options doesn't have a nice way to get out
    # attributes that might not be there, and even then the attribute value might be
    # None, which we still want to override
    # Benjy thinks we should probably hoist these options to the global set of options,
    # rather than just keeping them within IvyResolve.setup_parser
    self._mutable_pattern = (getattr(options, 'ivy_mutable_pattern', None) or
                             config.get('ivy-resolve', 'mutable_pattern', default=None))

    self._transitive = config.getbool('ivy-resolve', 'transitive', default=True)
    self._args = config.getlist('ivy-resolve', 'args', default=[])
    self._jvm_options = config.getlist('ivy-resolve', 'jvm_args', default=[])
    # Disable cache in File.getCanonicalPath(), makes Ivy work with -symlink option properly on ng.
    self._jvm_options.append('-Dsun.io.useCanonCaches=false')
    self._work_dir = config.get('ivy-resolve', 'workdir')
    self._template_path = os.path.join('templates', 'ivy_resolve', 'ivy.mustache')

    if self._mutable_pattern:
      try:
        self._mutable_pattern = re.compile(self._mutable_pattern)
      except re.error as e:
        raise TaskError('Invalid mutable pattern specified: %s %s' % (self._mutable_pattern, e))

    def parse_override(override):
      match = re.match(r'^([^#]+)#([^=]+)=([^\s]+)$', override)
      if not match:
        raise TaskError('Invalid dependency override: %s' % override)

      org, name, rev_or_url = match.groups()

      def fmt_message(message, template):
        return message % dict(
            overridden='%s#%s;%s' % (template.org, template.module, template.version),
            rev=rev_or_url,
            url=rev_or_url)

      def replace_rev(template):
        self._log.info(fmt_message('Overrode %(overridden)s with rev %(rev)s', template))
        return template.extend(version=rev_or_url, url=None, force=True)

      def replace_url(template):
        self._log.info(fmt_message('Overrode %(overridden)s with snapshot at %(url)s', template))
        return template.extend(version='SNAPSHOT', url=rev_or_url, force=True)

      replace = replace_url if re.match(r'^\w+://.+', rev_or_url) else replace_rev
      return (org, name), replace
    self._overrides = {}
    # TODO(pl): See above comment wrt options
    if hasattr(options, 'ivy_resolve_overrides') and options.ivy_resolve_overrides:
      self._overrides.update(parse_override(o) for o in options.ivy_resolve_overrides)

  @staticmethod
  @contextmanager
  def cachepath(path):
    if not os.path.exists(path):
      yield ()
    else:
      with safe_open(path, 'r') as cp:
        yield (path.strip() for path in cp.read().split(os.pathsep) if path.strip())

  @staticmethod
  def symlink_cachepath(ivy_home, inpath, symlink_dir, outpath):
    """Symlinks all paths listed in inpath that are under ivy_home into symlink_dir.

    Preserves all other paths. Writes the resulting paths to outpath.
    Returns a map of path -> symlink to that path.
    """
    safe_mkdir(symlink_dir)
    with safe_open(inpath, 'r') as infile:
      paths = filter(None, infile.read().strip().split(os.pathsep))
    new_paths = []
    for path in paths:
      if not path.startswith(ivy_home):
        new_paths.append(path)
        continue
      symlink = os.path.join(symlink_dir, os.path.relpath(path, ivy_home))
      try:
        os.makedirs(os.path.dirname(symlink))
      except OSError as e:
        if e.errno != errno.EEXIST:
          raise
      # Note: The try blocks cannot be combined. It may be that the dir exists but the link doesn't.
      try:
        os.symlink(path, symlink)
      except OSError as e:
        # We don't delete and recreate the symlink, as this may break concurrently executing code.
        if e.errno != errno.EEXIST:
          raise
      new_paths.append(symlink)
    with safe_open(outpath, 'w') as outfile:
      outfile.write(':'.join(new_paths))
    symlink_map = dict(zip(paths, new_paths))
    return symlink_map

  def identify(self, targets):
    targets = list(targets)
    if len(targets) == 1 and hasattr(targets[0], 'provides') and targets[0].provides:
      return targets[0].provides.org, targets[0].provides.name
    else:
      return 'internal', Target.maybe_readable_identify(targets)

  def xml_report_path(self, targets, conf):
    """The path to the xml report ivy creates after a retrieve."""
    org, name = self.identify(targets)
    cachedir = Bootstrapper.instance().ivy_cache_dir
    return os.path.join(cachedir, '%s-%s-%s.xml' % (org, name, conf))

  def parse_xml_report(self, targets, conf):
    """Returns the IvyInfo representing the info in the xml report, or None if no report exists."""
    path = self.xml_report_path(targets, conf)
    if not os.path.exists(path):
      return None

    ret = IvyInfo()
    etree = xml.etree.ElementTree.parse(self.xml_report_path(targets, conf))
    doc = etree.getroot()
    for module in doc.findall('dependencies/module'):
      org = module.get('organisation')
      name = module.get('name')
      for revision in module.findall('revision'):
        rev = revision.get('name')
        artifacts = []
        for artifact in revision.findall('artifacts/artifact'):
          artifacts.append(IvyArtifact(path=artifact.get('location'),
                                       classifier=artifact.get('extra-classifier')))
        callers = []
        for caller in revision.findall('caller'):
          callers.append(IvyModuleRef(caller.get('organisation'),
                                      caller.get('name'),
                                      caller.get('callerrev')))
        ret.add_module(IvyModule(IvyModuleRef(org, name, rev), artifacts, callers))
    return ret

  def _extract_classpathdeps(self, targets):
    """Subclasses can override to filter out a set of targets that should be resolved for classpath
    dependencies.
    """
    def is_classpath(target):
      return (target.is_jar or
              target.is_internal and any(jar for jar in target.jar_dependencies if jar.rev))

    classpath_deps = OrderedSet()
    for target in targets:
      classpath_deps.update(t for t in target.resolve() if t.is_concrete and is_classpath(t))
    return classpath_deps

  def _generate_ivy(self, targets, jars, excludes, ivyxml, confs):
    org, name = self.identify(targets)
    template_data = TemplateData(
        org=org,
        module=name,
        version='latest.integration',
        publications=None,
        configurations=confs,
        dependencies=[self._generate_jar_template(jar, confs) for jar in jars],
        excludes=[self._generate_exclude_template(exclude) for exclude in excludes])

    safe_mkdir(os.path.dirname(ivyxml))
    with open(ivyxml, 'w') as output:
      generator = Generator(pkgutil.get_data(__name__, self._template_path),
                            root_dir=get_buildroot(),
                            lib=template_data)
      generator.write(output)

  def _calculate_classpath(self, targets):

    def is_jardependant(target):
      return target.is_jar or target.is_jvm

    jars = OrderedDict()
    excludes = set()

    # Support the ivy force concept when we sanely can for internal dep conflicts.
    # TODO(John Sirois): Consider supporting / implementing the configured ivy revision picking
    # strategy generally.
    def add_jar(jar):
      coordinate = (jar.org, jar.name)
      existing = jars.get(coordinate)
      jars[coordinate] = jar if not existing else (
        self._resolve_conflict(existing=existing, proposed=jar)
      )

    def collect_jars(target):
      if target.is_jar:
        add_jar(target)
      elif target.jar_dependencies:
        for jar in target.jar_dependencies:
          if jar.rev:
            add_jar(jar)

      # Lift jvm target-level excludes up to the global excludes set
      if target.is_jvm and target.excludes:
        excludes.update(target.excludes)

    for target in targets:
      target.walk(collect_jars, is_jardependant)

    return jars.values(), excludes

  def _resolve_conflict(self, existing, proposed):
    if proposed == existing:
      return existing
    elif existing.force and proposed.force:
      raise TaskError('Cannot force %s#%s to both rev %s and %s' % (
        proposed.org, proposed.name, existing.rev, proposed.rev
      ))
    elif existing.force:
      self._log.debug('Ignoring rev %s for %s#%s already forced to %s' % (
        proposed.rev, proposed.org, proposed.name, existing.rev
      ))
      return existing
    elif proposed.force:
      self._log.debug('Forcing %s#%s from %s to %s' % (
        proposed.org, proposed.name, existing.rev, proposed.rev
      ))
      return proposed
    else:
      try:
        if Revision.lenient(proposed.rev) > Revision.lenient(existing.rev):
          self._log.debug('Upgrading %s#%s from rev %s  to %s' % (
            proposed.org, proposed.name, existing.rev, proposed.rev,
          ))
          return proposed
        else:
          return existing
      except Revision.BadRevision as e:
        raise TaskError('Failed to parse jar revision', e)

  def _is_mutable(self, jar):
    if jar.mutable is not None:
      return jar.mutable
    if self._mutable_pattern:
      return self._mutable_pattern.match(jar.rev)
    return False

  def _generate_jar_template(self, jar, confs):
    template = TemplateData(
        org=jar.org,
        module=jar.name,
        version=jar.rev,
        mutable=self._is_mutable(jar),
        force=jar.force,
        excludes=[self._generate_exclude_template(exclude) for exclude in jar.excludes],
        transitive=jar.transitive,
        artifacts=jar.artifacts,
        configurations=[conf for conf in jar.configurations if conf in confs])
    override = self._overrides.get((jar.org, jar.name))
    return override(template) if override else template

  def _generate_exclude_template(self, exclude):
    return TemplateData(org=exclude.org, name=exclude.name)

  def is_classpath_artifact(self, path):
    """Subclasses can override to determine whether a given artifact represents a classpath
    artifact."""
    return path.endswith('.jar') or path.endswith('.war')

  def is_mappable_artifact(self, org, name, path):
    """Subclasses can override to determine whether a given artifact represents a mappable
    artifact."""
    return self.is_classpath_artifact(path)

  def mapto_dir(self):
    """Subclasses can override to establish an isolated jar mapping directory."""
    return os.path.join(self._work_dir, 'mapped-jars')

  def mapjars(self, genmap, target, executor, workunit_factory=None):
    """
    Parameters:
      genmap: the jar_dependencies ProductMapping entry for the required products.
      target: the target whose jar dependencies are being retrieved.
    """
    mapdir = os.path.join(self.mapto_dir(), target.id)
    safe_mkdir(mapdir, clean=True)
    ivyargs = [
      '-retrieve', '%s/[organisation]/[artifact]/[conf]/'
                   '[organisation]-[artifact]-[revision](-[classifier]).[ext]' % mapdir,
      '-symlink',
    ]
    self.exec_ivy(mapdir,
                  [target],
                  ivyargs,
                  confs=target.configurations,
                  ivy=Bootstrapper.default_ivy(executor),
                  workunit_factory=workunit_factory,
                  workunit_name='map-jars')

    for org in os.listdir(mapdir):
      orgdir = os.path.join(mapdir, org)
      if os.path.isdir(orgdir):
        for name in os.listdir(orgdir):
          artifactdir = os.path.join(orgdir, name)
          if os.path.isdir(artifactdir):
            for conf in os.listdir(artifactdir):
              confdir = os.path.join(artifactdir, conf)
              for f in os.listdir(confdir):
                if self.is_mappable_artifact(org, name, f):
                  # TODO(John Sirois): kill the org and (org, name) exclude mappings in favor of a
                  # conf whitelist
                  genmap.add(org, confdir).append(f)
                  genmap.add((org, name), confdir).append(f)

                  genmap.add(target, confdir).append(f)
                  genmap.add((target, conf), confdir).append(f)
                  genmap.add((org, name, conf), confdir).append(f)

  ivy_lock = threading.RLock()

  def exec_ivy(self,
               target_workdir,
               targets,
               args,
               confs=None,
               ivy=None,
               workunit_name='ivy',
               workunit_factory=None,
               symlink_ivyxml=False):

    ivy = ivy or Bootstrapper.default_ivy()
    if not isinstance(ivy, Ivy):
      raise ValueError('The ivy argument supplied must be an Ivy instance, given %s of type %s'
                       % (ivy, type(ivy)))

    ivyxml = os.path.join(target_workdir, 'ivy.xml')
    jars, excludes = self._calculate_classpath(targets)

    ivy_args = ['-ivy', ivyxml]

    confs_to_resolve = confs or ['default']
    ivy_args.append('-confs')
    ivy_args.extend(confs_to_resolve)

    ivy_args.extend(args)
    if not self._transitive:
      ivy_args.append('-notransitive')
    ivy_args.extend(self._args)

    def safe_link(src, dest):
      if os.path.exists(dest):
        os.unlink(dest)
      os.symlink(src, dest)

    with IvyUtils.ivy_lock:
      self._generate_ivy(targets, jars, excludes, ivyxml, confs_to_resolve)
      runner = ivy.runner(jvm_options=self._jvm_options, args=ivy_args)
      try:
        result = util.execute_runner(runner,
                                     workunit_factory=workunit_factory,
                                     workunit_name=workunit_name)

        # Symlink to the current ivy.xml file (useful for IDEs that read it).
        if symlink_ivyxml:
          ivyxml_symlink = os.path.join(self._work_dir, 'ivy.xml')
          safe_link(ivyxml, ivyxml_symlink)

        if result != 0:
          raise TaskError('Ivy returned %d' % result)
      except runner.executor.Error as e:
        raise TaskError(e)

########NEW FILE########
__FILENAME__ = jar_create
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import functools
import os

from contextlib import contextmanager
from zipfile import ZIP_DEFLATED, ZIP_STORED

from twitter.common.dirutil import safe_mkdir

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.fs.fs import safe_filename
from twitter.pants.java.jar import Manifest, open_jar
from twitter.pants.targets.jvm_binary import JvmBinary

from .javadoc_gen import javadoc
from .scaladoc_gen import scaladoc

from . import Task, TaskError


DEFAULT_CONFS = ['default']


def is_binary(target):
  return isinstance(target, JvmBinary)


def is_java_library(target):
  return target.has_sources('.java') and not is_binary(target)


def is_scala_library(target):
  return target.has_sources('.scala') and not is_binary(target)


def is_jvm_library(target):
  return is_java_library(target) or is_scala_library(target)


def jarname(target, extension='.jar'):
  # TODO(John Sirois): incorporate version
  _, id_, _ = target.get_artifact_info()
  # Cap jar names quite a bit lower than the standard fs limit of 255 characters since these
  # artifacts will often be used outside pants and those uses may manipulate (expand) the jar
  # filenames blindly.
  return safe_filename(id_, extension, max_length=200)


class JarCreate(Task):
  """Jars jvm libraries and optionally their sources and their docs."""

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag('outdir'), dest='jar_create_outdir',
                            help='Emit jars in to this directory.')

    option_group.add_option(mkflag('compressed'), mkflag('compressed', negate=True),
                            dest='jar_create_compressed', default=True,
                            action='callback', callback=mkflag.set_bool,
                            help='[%default] Create compressed jars.')

    option_group.add_option(mkflag('transitive'), mkflag('transitive', negate=True),
                            dest='jar_create_transitive', default=True,
                            action='callback', callback=mkflag.set_bool,
                            help='[%default] Create jars for the transitive closure of internal '
                                 'targets reachable from the roots specified on the command line.')

    option_group.add_option(mkflag('classes'), mkflag('classes', negate=True),
                            dest='jar_create_classes', default=True,
                            action='callback', callback=mkflag.set_bool,
                            help='[%default] Create class jars.')
    option_group.add_option(mkflag('sources'), mkflag('sources', negate=True),
                            dest='jar_create_sources', default=False,
                            action='callback', callback=mkflag.set_bool,
                            help='[%default] Create source jars.')
    #TODO tdesai: Think about a better way to set defaults per goal basis.
    javadoc_defaults = True if option_group.title.split(':')[0] == 'publish' else False
    option_group.add_option(mkflag('javadoc'), mkflag('javadoc', negate=True),
                            dest='jar_create_javadoc',
                            default=javadoc_defaults,
                            action='callback', callback=mkflag.set_bool,
                            help='[%default] Create javadoc jars.')

  def __init__(self, context):
    Task.__init__(self, context)

    options = context.options
    products = context.products

    self._output_dir = (options.jar_create_outdir or
                        self.get_workdir(section='jar-create', workdir='jars'))
    self.transitive = options.jar_create_transitive
    self.confs = context.config.getlist('jar-create', 'confs', default=DEFAULT_CONFS)
    self.compression = ZIP_DEFLATED if options.jar_create_compressed else ZIP_STORED

    self.jar_classes = options.jar_create_classes or products.isrequired('jars')
    if self.jar_classes:
      products.require_data('classes_by_target')
      products.require_data('resources_by_target')

    definitely_create_javadoc = options.jar_create_javadoc or products.isrequired('javadoc_jars')
    definitely_dont_create_javadoc = options.jar_create_javadoc is False
    create_javadoc = options.jar_create_javadoc
    if definitely_create_javadoc and definitely_dont_create_javadoc:
      self.context.log.warn('javadoc jars are required but you have requested they not be created, '
                            'creating anyway')
    self.jar_javadoc = (True if definitely_create_javadoc else
                        False if definitely_dont_create_javadoc else
                        create_javadoc)
    if self.jar_javadoc:
      products.require(javadoc.product_type)
      products.require(scaladoc.product_type)

    self.jar_sources = products.isrequired('source_jars') or options.jar_create_sources

    self._jars = {}

  def execute(self, targets):
    safe_mkdir(self._output_dir)

    def jar_targets(predicate):
      return filter(predicate, (targets if self.transitive else self.context.target_roots))

    def add_genjar(typename, target, name):
      self.context.products.get(typename).add(target, self._output_dir).append(name)

    if self.jar_classes:
      self._jar(jar_targets(is_jvm_library), functools.partial(add_genjar, 'jars'))

    if self.jar_sources:
      self.sourcejar(jar_targets(is_jvm_library), functools.partial(add_genjar, 'source_jars'))

    if self.jar_javadoc:
      javadoc_add_genjar = functools.partial(add_genjar, 'javadoc_jars')
      self.javadocjar(jar_targets(is_java_library),
                      self.context.products.get(javadoc.product_type),
                      javadoc_add_genjar)
      self.javadocjar(jar_targets(is_scala_library),
                      self.context.products.get(scaladoc.product_type),
                      javadoc_add_genjar)

  @contextmanager
  def create_jar(self, target, path):
    existing = self._jars.setdefault(path, target)
    if target != existing:
      raise TaskError('Duplicate name: target %s tried to write %s already mapped to target %s' % (
        target, path, existing
      ))
    self._jars[path] = target
    with open_jar(path, 'w', compression=self.compression) as jar:
      yield jar

  def _jar(self, jvm_targets, add_genjar):
    classes_by_target = self.context.products.get_data('classes_by_target')
    resources_by_target = self.context.products.get_data('resources_by_target')

    for target in jvm_targets:
      target_classes = classes_by_target.get(target)

      target_resources = []
      if target.has_resources:
        target_resources.extend(resources_by_target.get(r) for r in target.resources)

      if target_classes or target_resources:
        jar_name = jarname(target)
        add_genjar(target, jar_name)
        jar_path = os.path.join(self._output_dir, jar_name)
        with self.create_jar(target, jar_path) as jarfile:
          def add_to_jar(target_products):
            if target_products:
              for root, products in target_products.rel_paths():
                for prod in products:
                  jarfile.write(os.path.join(root, prod), prod)
          add_to_jar(target_classes)
          for resources_target in target_resources:
            add_to_jar(resources_target)
          if target.is_java_agent:
            self.write_agent_manifest(target, jarfile)

  def sourcejar(self, jvm_targets, add_genjar):
    for target in jvm_targets:
      jar_name = jarname(target, '-sources.jar')
      add_genjar(target, jar_name)
      jar_path = os.path.join(self._output_dir, jar_name)
      with self.create_jar(target, jar_path) as jar:
        for source in target.sources:
          jar.write(os.path.join(get_buildroot(), target.target_base, source), source)

        if target.has_resources:
          for resources in target.resources:
            for resource in resources.sources:
              jar.write(os.path.join(get_buildroot(), resources.target_base, resource), resource)

  def javadocjar(self, java_targets, genmap, add_genjar):
    for target in java_targets:
      generated = genmap.get(target)
      if generated:
        jar_name = jarname(target, '-javadoc.jar')
        add_genjar(target, jar_name)
        jar_path = os.path.join(self._output_dir, jar_name)
        with self.create_jar(target, jar_path) as jar:
          for basedir, javadocfiles in generated.items():
            for javadocfile in javadocfiles:
              jar.write(os.path.join(basedir, javadocfile), javadocfile)

  def write_agent_manifest(self, agent, jarfile):
    # TODO(John Sirois): refactor an agent model to suport 'Boot-Class-Path' properly.
    manifest = Manifest()
    manifest.addentry(Manifest.MANIFEST_VERSION, '1.0')
    if agent.premain:
      manifest.addentry('Premain-Class', agent.premain)
    if agent.agent_class:
      manifest.addentry('Agent-Class', agent.agent_class)
    if agent.can_redefine:
      manifest.addentry('Can-Redefine-Classes', 'true')
    if agent.can_retransform:
      manifest.addentry('Can-Retransform-Classes', 'true')
    if agent.can_set_native_method_prefix:
      manifest.addentry('Can-Set-Native-Method-Prefix', 'true')
    jarfile.writestr(Manifest.PATH, manifest.contents())

########NEW FILE########
__FILENAME__ = jar_publish
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import functools
import getpass
import hashlib
import logging
import os
import pkgutil
import shutil
import sys

from collections import defaultdict

from twitter.common.collections import OrderedDict, OrderedSet
from twitter.common.config import Properties
from twitter.common.dirutil import safe_open, safe_rmtree
from twitter.common.log.options import LogOptions

from twitter.pants.base.build_environment import get_buildroot, get_scm
from twitter.pants.base.address import Address
from twitter.pants.base.target import Target
from twitter.pants.base.generator import Generator, TemplateData
from twitter.pants.ivy.bootstrapper import Bootstrapper
from twitter.pants.ivy.ivy import Ivy
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.targets.resources import Resources
from twitter.pants.tasks.scm_publish import ScmPublish, Semver

from . import Task, TaskError


class PushDb(object):
  @staticmethod
  def load(path):
    """Loads a pushdb maintained in a properties file at the given path."""
    with open(path, 'r') as props:
      properties = Properties.load(props)
      return PushDb(properties)

  def __init__(self, props):
    self._props = props

  def as_jar_with_version(self, target):
    """
      Given an internal target, return a JarDependency with the last published revision filled in.
    """
    jar_dep, db_get, _ = self._accessors_for_target(target)

    major = int(db_get('revision.major', '0'))
    minor = int(db_get('revision.minor', '0'))
    patch = int(db_get('revision.patch', '0'))
    snapshot = db_get('revision.snapshot', 'false').lower() == 'true'
    sha = db_get('revision.sha', None)
    fingerprint = db_get('revision.fingerprint', None)
    semver = Semver(major, minor, patch, snapshot=snapshot)
    jar_dep.rev = semver.version()
    return jar_dep, semver, sha, fingerprint

  def set_version(self, target, version, sha, fingerprint):
    version = version if isinstance(version, Semver) else Semver.parse(version)
    _, _, db_set = self._accessors_for_target(target)
    db_set('revision.major', version.major)
    db_set('revision.minor', version.minor)
    db_set('revision.patch', version.patch)
    db_set('revision.snapshot', str(version.snapshot).lower())
    db_set('revision.sha', sha)
    db_set('revision.fingerprint', fingerprint)

  def _accessors_for_target(self, target):
    jar_dep, _, exported = target.get_artifact_info()
    if not exported:
      raise ValueError

    def key(prefix):
      return '%s.%s%%%s' % (prefix, jar_dep.org, jar_dep.name)

    def getter(prefix, default=None):
      return self._props.get(key(prefix), default)

    def setter(prefix, value):
      self._props[key(prefix)] = value

    return jar_dep, getter, setter

  def dump(self, path):
    """Saves the pushdb as a properties file to the given path."""
    with open(path, 'w') as props:
      Properties.dump(self._props, props)


class DependencyWriter(object):
  """
    Builds up a template data representing a target and applies this to a template to produce a
    dependency descriptor.
  """

  @staticmethod
  def create_exclude(exclude):
    return TemplateData(org=exclude.org, name=exclude.name)

  def __init__(self, get_db, template_relpath):
    self.get_db = get_db
    self.template_relpath = template_relpath

  def write(self, target, path, confs=None):
    def as_jar(internal_target):
      jar, _, _, _ = self.get_db(internal_target).as_jar_with_version(internal_target)
      return jar

    # TODO(John Sirois): a dict is used here to de-dup codegen targets which have both the original
    # codegen target - say java_thrift_library - and the synthetic generated target (java_library)
    # Consider reworking codegen tasks to add removal of the original codegen targets when rewriting
    # the graph
    dependencies = OrderedDict()
    internal_codegen = {}
    configurations = set()
    for dep in target_internal_dependencies(target):
      jar = as_jar(dep)
      dependencies[(jar.org, jar.name)] = self.internaldep(jar, dep)
      if dep.is_codegen:
        internal_codegen[jar.name] = jar.name
    for jar in target.jar_dependencies:
      if jar.rev:
        dependencies[(jar.org, jar.name)] = self.jardep(jar)
        configurations |= set(jar._configurations)

    target_jar = self.internaldep(
                     as_jar(target),
                     configurations=list(configurations)).extend(dependencies=dependencies.values())

    template_kwargs = self.templateargs(target_jar, confs)
    with safe_open(path, 'w') as output:
      template = pkgutil.get_data(__name__, self.template_relpath)
      Generator(template, **template_kwargs).write(output)

  def templateargs(self, target_jar, confs=None):
    """
      Subclasses must return a dict for use by their template given the target jar template data
      and optional specific ivy configurations.
    """
    raise NotImplementedError()

  def internaldep(self, jar_dependency, dep=None, configurations=None):
    """
      Subclasses must return a template data for the given internal target (provided in jar
      dependency form).
    """
    raise NotImplementedError()

  def jardep(self, jar_dependency):
    """Subclasses must return a template data for the given external jar dependency."""
    raise NotImplementedError()


class PomWriter(DependencyWriter):
  def __init__(self, get_db):
    super(PomWriter, self).__init__(
        get_db,
        os.path.join('templates', 'jar_publish', 'pom.mustache'))

  def templateargs(self, target_jar, confs=None):
    return dict(artifact=target_jar)

  def jardep(self, jar):
    return TemplateData(
        org=jar.org,
        name=jar.name,
        rev=jar.rev,
        scope='compile',
        excludes=[self.create_exclude(exclude) for exclude in jar.excludes if exclude.name])

  def internaldep(self, jar_dependency, dep=None, configurations=None):
    return self.jardep(jar_dependency)


class IvyWriter(DependencyWriter):
  def __init__(self, get_db):
    super(IvyWriter, self).__init__(
        get_db,
        os.path.join('templates', 'ivy_resolve', 'ivy.mustache'))

  def templateargs(self, target_jar, confs=None):
    return dict(lib=target_jar.extend(
        publications=set(confs) if confs else set(),
        overrides=None))

  def _jardep(self, jar, transitive=True, configurations='default'):
    return TemplateData(
        org=jar.org,
        module=jar.name,
        version=jar.rev,
        mutable=False,
        force=jar.force,
        excludes=[self.create_exclude(exclude) for exclude in jar.excludes],
        transitive=transitive,
        artifacts=jar.artifacts,
        configurations=configurations)

  def jardep(self, jar):
    return self._jardep(jar,
        transitive=jar.transitive,
        configurations=jar._configurations)

  def internaldep(self, jar_dependency, dep=None, configurations=None):
    return self._jardep(jar_dependency, configurations=configurations)


def coordinate(org, name, rev=None):
  return '%s#%s;%s' % (org, name, rev) if rev else '%s#%s' % (org, name)


def jar_coordinate(jar, rev=None):
  return coordinate(jar.org, jar.name, rev or jar.rev)


def target_internal_dependencies(target):
  return filter(lambda tgt: not isinstance(tgt, Resources), target.internal_dependencies)


class JarPublish(ScmPublish, Task):
  """Publish jars to a maven repository.

  At a high-level, pants uses `Apache Ivy <http://ant.apache.org/ivy/>`_ to
  publish artifacts to Maven-style repositories. Pants performs prerequisite
  tasks like compiling, creating jars, and generating ``pom.xml`` files then
  invokes Ivy to actually publish the artifacts, so publishing is largely
  configured in ``ivysettings.xml``. ``BUILD`` and ``pants.ini`` files
  primarily provide linkage between publishable targets and the
  Ivy ``resolvers`` used to publish them.

  The following target types are publishable: :ref:`bdict_java_library`,
  :ref:`bdict_scala_library`, :ref:`bdict_java_thrift_library`,
  :ref:`bdict_annotation_processor`.
  Targets to publish and their dependencies must be publishable target
  types and specify the ``provides`` argument. One exception is
  :ref:`bdict_jar`\s - pants will generate a pom file that
  depends on the already-published jar.

  Example usage: ::

     # By default pants will perform a dry-run.
     ./pants goal clean-all publish src/java/com/twitter/mybird

     # Actually publish.
     ./pants goal clean-all publish src/java/com/twitter/mybird --no-publish-dryrun

  Please see ``./pants goal publish -h`` for a detailed description of all
  publishing options.

  Publishing can be configured in ``pants.ini`` as follows.

  ``jar-publish`` section:

  * ``repos`` - Required dictionary of settings for repos that may be pushed to.
  * ``ivy_jvmargs`` - Optional list of JVM command-line args when invoking Ivy.
  * ``restrict_push_branches`` - Optional list of branches to restrict publishing to.

  Example pants.ini jar-publish repos dictionary: ::

     repos = {
       # repository target name is paired with this key
       'myrepo': {
         # ivysettings.xml resolver to use for publishing
         'resolver': 'maven.twttr.com',
         # ivy configurations to publish
         'confs': ['default', 'sources', 'docs'],
         # address of a Credentials target to use when publishing
         'auth': 'address/of/credentials/BUILD:target',
         # help message if unable to initialize the Credentials target.
         'help': 'Please check your credentials and try again.',
       },
     }

  Additionally the ``ivy`` section ``ivy_settings`` property specifies which
  Ivy settings file to use when publishing is required.
  """

  _CONFIG_SECTION = 'jar-publish'

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    # TODO(John Sirois): Support a preview mode that outputs a file with entries like:
    # artifact id:
    # revision:
    # publish: (true|false)
    # changelog:
    #
    # Allow re-running this goal with the file as input to support forcing an arbitrary set of
    # revisions and supply of hand endited changelogs.

    option_group.add_option(mkflag("dryrun"), mkflag("dryrun", negate=True),
                            dest="jar_publish_dryrun", default=True,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Runs through a push without actually pushing "
                                 "artifacts, editing publish dbs or otherwise writing data")

    option_group.add_option(mkflag("commit", negate=True),
                            dest="jar_publish_commit", default=True,
                            action="callback", callback=mkflag.set_bool,
                            help="Turns off commits of the push db for local testing.")

    local_flag = mkflag("local")
    option_group.add_option(local_flag, dest="jar_publish_local",
                            help="Publishes jars to a maven repository on the local filesystem at "
                                 "the specified path.")

    option_group.add_option(mkflag("local-snapshot"), mkflag("local-snapshot", negate=True),
                            dest="jar_publish_local_snapshot", default=True,
                            action="callback", callback=mkflag.set_bool,
                            help="[%%default] If %s is specified, publishes jars with '-SNAPSHOT' "
                                 "revisions." % local_flag)

    option_group.add_option(mkflag("transitive"), mkflag("transitive", negate=True),
                            dest="jar_publish_transitive", default=True,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Publishes the specified targets and all their "
                                 "internal dependencies transitively.")

    option_group.add_option(mkflag("force"), mkflag("force", negate=True),
                            dest="jar_publish_force", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Forces pushing jars even if there have been no "
                                 "changes since the last push.")

    flag = mkflag('override')
    option_group.add_option(flag, action='append', dest='jar_publish_override',
                            help='''Specifies a published jar revision override in the form:
                            ([org]#[name]|[target spec])=[new revision]

                            For example, to specify 2 overrides:
                            %(flag)s=com.twitter.common#quantity=0.1.2 \\
                            %(flag)s=src/java/com/twitter/common/base=1.0.0 \\
                            ''' % dict(flag=flag))

    flag = mkflag("restart-at")
    option_group.add_option(flag, dest="jar_publish_restart_at",
                            help='''Restart a fail push at the given jar.  Jars can be identified by
                            maven coordinate [org]#[name] or target.

                            For example:
                            %(flag)s=com.twitter.common#quantity

                            Or:
                            %(flag)s=src/java/com/twitter/common/base
                            ''' % dict(flag=flag))

  def __init__(self, context, scm=None):
    Task.__init__(self, context)
    ScmPublish.__init__(self, scm or get_scm(),
                        self.context.config.getlist(
                          JarPublish._CONFIG_SECTION, 'restrict_push_branches'))
    self.outdir = os.path.join(context.config.getdefault('pants_workdir'), 'publish')
    self.cachedir = os.path.join(self.outdir, 'cache')

    self._jvmargs = context.config.getlist(JarPublish._CONFIG_SECTION, 'ivy_jvmargs', default=[])

    if context.options.jar_publish_local:
      local_repo = dict(
        resolver='publish_local',
        path=os.path.abspath(os.path.expanduser(context.options.jar_publish_local)),
        confs=['*'],
        auth=None
      )
      self.repos = defaultdict(lambda: local_repo)
      self.commit = False
      self.snapshot = context.options.jar_publish_local_snapshot
    else:
      self.repos = context.config.getdict(JarPublish._CONFIG_SECTION, 'repos')
      if not self.repos:
        raise TaskError("This repo is not yet set for publishing to the world! Please re-run with --publish-local")
      for repo, data in self.repos.items():
        auth = data.get('auth')
        if auth:
          credentials = context.resolve(auth).next()
          user = credentials.username(data['resolver'])
          password = credentials.password(data['resolver'])
          self.context.log.debug('Found auth for repo=%s user=%s' % (repo, user))
          self.repos[repo]['username'] = user
          self.repos[repo]['password'] = password
      self.commit = context.options.jar_publish_commit
      self.snapshot = False

    self.ivycp = context.config.getlist('ivy', 'classpath')
    self.ivysettings = context.config.get('jar-publish', 'ivy_settings')

    self.dryrun = context.options.jar_publish_dryrun
    self.transitive = context.options.jar_publish_transitive
    self.force = context.options.jar_publish_force

    def parse_jarcoordinate(coordinate):
      components = coordinate.split('#', 1)
      if len(components) == 2:
        org, name = components
        return org, name
      else:
        try:
          address = Address.parse(get_buildroot(), coordinate)
          try:
            target = Target.get(address)
            if not target:
              siblings = Target.get_all_addresses(address.buildfile)
              prompt = 'did you mean' if len(siblings) == 1 else 'maybe you meant one of these'
              raise TaskError('%s => %s?:\n    %s' % (address, prompt,
                                                      '\n    '.join(str(a) for a in siblings)))
            if not target.is_exported:
              raise TaskError('%s is not an exported target' % coordinate)
            return target.provides.org, target.provides.name
          except (ImportError, SyntaxError, TypeError):
            raise TaskError('Failed to parse %s' % address.buildfile.relpath)
        except IOError:
          raise TaskError('No BUILD file could be found at %s' % coordinate)

    self.overrides = {}
    if context.options.jar_publish_override:
      def parse_override(override):
        try:
          coordinate, rev = override.split('=', 1)
          try:
            rev = Semver.parse(rev)
          except ValueError as e:
            raise TaskError('Invalid version %s: %s' % (rev, e))
          return parse_jarcoordinate(coordinate), rev
        except ValueError:
          raise TaskError('Invalid override: %s' % override)

      self.overrides.update(parse_override(o) for o in context.options.jar_publish_override)

    self.restart_at = None
    if context.options.jar_publish_restart_at:
      self.restart_at = parse_jarcoordinate(context.options.jar_publish_restart_at)

    context.products.require('jars')
    context.products.require('source_jars')

  def execute(self, targets):
    self.check_clean_master(commit=(not self.dryrun and self.commit))

    exported_targets = self.exported_targets()
    self.check_targets(exported_targets)

    pushdbs = {}

    def get_db(tgt):
      # TODO(tdesai) Handle resource type in get_db.
      if tgt.provides is None:
        raise TaskError('trying to publish target %r which does not provide an artifact' % tgt)
      dbfile = tgt.provides.repo.push_db
      result = pushdbs.get(dbfile)
      if not result:
        db = PushDb.load(dbfile)
        repo = self.repos[tgt.provides.repo.name]
        result = (db, dbfile, repo)
        pushdbs[dbfile] = result
      return result

    def get_pushdb(tgt):
      return get_db(tgt)[0]

    def fingerprint_internal(tgt):
      if not tgt.is_internal:
        raise ValueError('Expected an internal target for fingerprinting, got %s' % tgt)
      pushdb, _, _ = get_db(tgt)
      _, _, _, fingerprint = pushdb.as_jar_with_version(tgt)
      return fingerprint or '0.0.0'

    def artifact_path(jar, version, name=None, suffix='', extension='jar', artifact_ext=''):
      return os.path.join(self.outdir, jar.org, jar.name + artifact_ext,
                          '%s%s-%s%s.%s' % ((name or jar.name),
                                            artifact_ext if name != 'ivy' else '',
                                            version,
                                            suffix,
                                            extension))

    def stage_artifact(tgt, jar, version, changelog, confs=None, artifact_ext=''):
      def path(name=None, suffix='', extension='jar'):
        return artifact_path(jar, version, name=name, suffix=suffix, extension=extension,
                             artifact_ext=artifact_ext)

      with safe_open(path(suffix='-CHANGELOG', extension='txt'), 'w') as changelog_file:
        changelog_file.write(changelog)
      ivyxml = path(name='ivy', extension='xml')

      IvyWriter(get_pushdb).write(tgt, ivyxml, confs=confs)
      PomWriter(get_pushdb).write(tgt, path(extension='pom'))

      return ivyxml

    def copy_artifact(tgt, version, typename, suffix='', artifact_ext=''):
      genmap = self.context.products.get(typename)
      for basedir, jars in genmap.get(tgt).items():
        for artifact in jars:
          path = artifact_path(jar, version, suffix=suffix, artifact_ext=artifact_ext)
          shutil.copy(os.path.join(basedir, artifact), path)

    def stage_artifacts(tgt, jar, version, changelog, confs=None):
      ivyxml_path = stage_artifact(tgt, jar, version, changelog, confs)
      copy_artifact(tgt, version, typename='jars')
      copy_artifact(tgt, version, typename='source_jars', suffix='-sources')

      jarmap = self.context.products.get('javadoc_jars')
      if not jarmap.empty() and (tgt.is_java or tgt.is_scala):
        copy_artifact(tgt, version, typename='javadoc_jars', suffix='-javadoc')

      return ivyxml_path

    if self.overrides:
      print('Publishing with revision overrides:\n  %s' % '\n  '.join(
        '%s=%s' % (coordinate(org, name), rev) for (org, name), rev in self.overrides.items()
      ))

    head_sha = self.scm.commit_id

    safe_rmtree(self.outdir)
    published = []
    skip = (self.restart_at is not None)
    for target in exported_targets:
      pushdb, dbfile, repo = get_db(target)
      jar, semver, sha, fingerprint = pushdb.as_jar_with_version(target)

      published.append(jar)

      if skip and (jar.org, jar.name) == self.restart_at:
        skip = False

      newver = self.overrides.get((jar.org, jar.name)) or semver.bump()
      if self.snapshot:
        newver = newver.make_snapshot()

      if newver <= semver:
        raise TaskError('Requested version %s must be greater than the current version %s' % (
          newver.version(), semver.version()
        ))

      newfingerprint = self.fingerprint(target, fingerprint_internal)
      no_changes = newfingerprint == fingerprint

      if no_changes:
        changelog = 'No changes for %s - forced push.\n' % jar_coordinate(jar, semver.version())
      else:
        changelog = self.changelog(target, sha) or 'Direct dependencies changed.\n'

      if no_changes and not self.force:
        print('No changes for %s' % jar_coordinate(jar, semver.version()))
        stage_artifacts(target, jar, (newver if self.force else semver).version(), changelog)
      elif skip:
        print('Skipping %s to resume at %s' % (
          jar_coordinate(jar, (newver if self.force else semver).version()),
          coordinate(self.restart_at[0], self.restart_at[1])
        ))
        stage_artifacts(target, jar, semver.version(), changelog)
      else:
        if not self.dryrun:
          # Confirm push looks good
          if no_changes:
            print(changelog)
          else:
            print('\nChanges for %s since %s @ %s:\n\n%s' % (
              coordinate(jar.org, jar.name), semver.version(), sha, changelog
            ))
          if os.isatty(sys.stdin.fileno()):
            push = raw_input('Publish %s with revision %s ? [y|N] ' % (
              coordinate(jar.org, jar.name), newver.version()
            ))
            print('\n')
            if push.strip().lower() != 'y':
              raise TaskError('User aborted push')

        pushdb.set_version(target, newver, head_sha, newfingerprint)

        ivyxml = stage_artifacts(target, jar, newver.version(), changelog, confs=repo['confs'])

        if self.dryrun:
          print('Skipping publish of %s in test mode.' % jar_coordinate(jar, newver.version()))
        else:
          resolver = repo['resolver']
          path = repo.get('path')

          # Get authentication for the publish repo if needed
          jvm_args = self._jvmargs
          if repo.get('auth'):
            user = repo.get('username')
            password = repo.get('password')
            if user and password:
              jvm_args.append('-Dlogin=%s' % user)
              jvm_args.append('-Dpassword=%s' % password)
            else:
              raise TaskError('Unable to publish to %s. %s' %
                              (repo['resolver'], repo.get('help', '')))

          # Do the publish
          def publish(ivyxml_path):
            ivysettings = self.generate_ivysettings(published, publish_local=path)
            args = [
              '-settings', ivysettings,
              '-ivy', ivyxml_path,
              '-deliverto', '%s/[organisation]/[module]/ivy-[revision].xml' % self.outdir,
              '-publish', resolver,
              '-publishpattern', '%s/[organisation]/[module]/'
                                 '[artifact]-[revision](-[classifier]).[ext]' % self.outdir,
              '-revision', newver.version(),
              '-m2compatible',
            ]

            if LogOptions.stderr_log_level() == logging.DEBUG:
              args.append('-verbose')

            if self.snapshot:
              args.append('-overwrite')

            try:
              ivy = Bootstrapper.default_ivy()
              ivy.execute(jvm_options=jvm_args, args=args,
                          workunit_factory=self.context.new_workunit, workunit_name='jar-publish')
            except (Bootstrapper.Error, Ivy.Error) as e:
              raise TaskError('Failed to push %s! %s' % (jar_coordinate(jar, newver.version()), e))

          publish(ivyxml)

          if self.commit:
            org = jar.org
            name = jar.name
            rev = newver.version()
            args = dict(
              org=org,
              name=name,
              rev=rev,
              coordinate=coordinate(org, name, rev),
              user=getpass.getuser(),
              cause='with forced revision' if (org, name) in self.overrides else '(autoinc)'
            )

            pushdb.dump(dbfile)
            self.commit_push(coordinate(org, name, rev))
            self.scm.refresh()
            self.scm.tag('%(org)s-%(name)s-%(rev)s' % args,
                         message='Publish of %(coordinate)s initiated by %(user)s %(cause)s' % args)

  def check_targets(self, targets):
    invalid = defaultdict(lambda: defaultdict(set))
    derived_by_target = dict()

    def collect(publish_target, walked_target):
      derived_by_target[walked_target.derived_from] = walked_target
      if not walked_target.has_sources() or not walked_target.sources:
        invalid[publish_target][walked_target].add('No sources.')
      if not walked_target.is_exported:
        invalid[publish_target][walked_target].add('Does not provide an artifact.')

    for target in targets:
      target.walk(functools.partial(collect, target), predicate=lambda t: t.is_concrete)

    # When walking the graph of a publishable target, we may encounter families of sibling targets
    # that form a derivation chain.  As long as one of these siblings is publishable, we can
    # proceed and publish a valid graph.
    # TODO(John Sirois): This does not actually handle derivation chains longer than 2 with the
    # exported item in the most derived position - fix this.
    for publish_target, invalid_targets in list(invalid.items()):
      for invalid_target, reasons in list(invalid_targets.items()):
        derived_target = derived_by_target[invalid_target]
        if derived_target not in invalid_targets:
          invalid_targets.pop(invalid_target)
      if not invalid_targets:
        invalid.pop(publish_target)

    if invalid:
      msg = list()

      def first_address(pair):
        first, _ = pair
        return str(first.address)

      for publish_target, invalid_targets in sorted(invalid.items(), key=first_address):
        msg.append('\n  Cannot publish %s due to:' % publish_target.address)
        for invalid_target, reasons in sorted(invalid_targets.items(), key=first_address):
          for reason in sorted(reasons):
            msg.append('\n    %s - %s' % (invalid_target.address, reason))

      raise TaskError('The following errors must be resolved to publish.%s' % ''.join(msg))

  def exported_targets(self):
    candidates = set()
    if self.transitive:
      candidates.update(self.context.targets())
    else:
      candidates.update(self.context.target_roots)

      def get_synthetic(lang, target):
        mappings = self.context.products.get(lang).get(target)
        if mappings:
          for key, generated in mappings.items():
            for synthetic in generated:
              yield synthetic

      # Handle the case where a code gen target is in the listed roots and the thus the publishable
      # target is a synthetic twin generated by a code gen task upstream.
      for candidate in self.context.target_roots:
        candidates.update(get_synthetic('java', candidate))
        candidates.update(get_synthetic('scala', candidate))

    def exportable(tgt):
      return tgt in candidates and tgt.is_exported

    return OrderedSet(filter(exportable,
                             reversed(InternalTarget.sort_targets(filter(exportable, candidates)))))

  def fingerprint(self, target, fingerprint_internal):
    sha = hashlib.sha1()

    for source in sorted(target.sources):
      path = os.path.join(target.target_base, source)
      with open(path) as fd:
        sha.update(source)
        sha.update(fd.read())

    # TODO(John Sirois): handle resources and circular dep scala_library java_sources

    for jarsig in sorted([jar_coordinate(j) for j in target.jar_dependencies if j.rev]):
      sha.update(jarsig)

    # TODO(tdesai) Handle resource type in get_db.
    internal_dependencies = sorted(target_internal_dependencies(target), key=lambda t: t.id)
    for internal_target in internal_dependencies:
      fingerprint = fingerprint_internal(internal_target)
      sha.update(fingerprint)

    return sha.hexdigest()

  def changelog(self, target, sha):
    return self.scm.changelog(from_commit=sha,
                              files=[os.path.join(target.target_base, source)
                                     for source in target.sources])

  def generate_ivysettings(self, publishedjars, publish_local=None):
    template_relpath = os.path.join('templates', 'jar_publish', 'ivysettings.mustache')
    template = pkgutil.get_data(__name__, template_relpath)
    with safe_open(os.path.join(self.outdir, 'ivysettings.xml'), 'w') as wrapper:
      generator = Generator(template,
                            ivysettings=self.ivysettings,
                            dir=self.outdir,
                            cachedir=self.cachedir,
                            published=[TemplateData(org=jar.org, name=jar.name)
                                       for jar in publishedjars],
                            publish_local=publish_local)
      generator.write(wrapper)
      return wrapper.name

########NEW FILE########
__FILENAME__ = javadoc_gen
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.tasks.jvmdoc_gen import Jvmdoc, JvmdocGen


javadoc = Jvmdoc(tool_name='javadoc', product_type='javadoc')


def is_java(target):
  return target.has_sources('.java')


class JavadocGen(JvmdocGen):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    cls.generate_setup_parser(option_group, args, mkflag, javadoc)

  def __init__(self, context, output_dir=None, confs=None, active=True):
    super(JavadocGen, self).__init__(context, javadoc, output_dir, confs, active)

  def execute(self, targets):
    self.generate_execute(targets, is_java, create_javadoc_command)


def create_javadoc_command(classpath, gendir, *targets):
  sources = []
  for target in targets:
    sources.extend(target.sources_relative_to_buildroot())

  if not sources:
    return None

  # TODO(John Sirois): try com.sun.tools.javadoc.Main via ng
  command = [
    'javadoc',
    '-quiet',
    '-encoding', 'UTF-8',
    '-notimestamp',
    '-use',
    '-classpath', ':'.join(classpath),
    '-d', gendir,
  ]

  # Always provide external linking for java API
  offlinelinks = set(['http://download.oracle.com/javase/6/docs/api/'])

  def link(target):
    for jar in target.jar_dependencies:
      if jar.apidocs:
        offlinelinks.add(jar.apidocs)
  for target in targets:
    target.walk(link, lambda t: t.is_jvm)

  for link in offlinelinks:
    command.extend(['-linkoffline', link, link])

  command.extend(sources)
  return command

########NEW FILE########
__FILENAME__ = junit_run
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import re
import sys

from twitter.common.dirutil import safe_mkdir, safe_open
from twitter.pants import binary_util
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.java.util import execute_java
from twitter.pants.targets.java_tests import JavaTests as junit_tests

from .jvm_task import JvmTask
from . import TaskError


class JUnitRun(JvmTask):
  _MAIN = 'com.twitter.common.junit.runner.ConsoleRunner'

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag('skip'), mkflag('skip', negate=True), dest = 'junit_run_skip',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%default] Skip running tests')

    option_group.add_option(mkflag('debug'), mkflag('debug', negate=True), dest = 'junit_run_debug',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%default] Run junit tests with a debugger')

    option_group.add_option(mkflag('fail-fast'), mkflag('fail-fast', negate=True),
                            dest = 'junit_run_fail_fast',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%default] Fail fast on the first test failure in a suite')

    option_group.add_option(mkflag('batch-size'), type = 'int', default=sys.maxint,
                            dest = 'junit_run_batch_size',
                            help = '[ALL] Runs at most this many tests in a single test process.')

    # TODO: Rename flag to jvm-options.
    option_group.add_option(mkflag('jvmargs'), dest = 'junit_run_jvmargs', action='append',
                            help = 'Runs junit tests in a jvm with these extra jvm args.')

    option_group.add_option(mkflag('test'), dest = 'junit_run_tests', action='append',
                            help = '[%default] Force running of just these tests.  Tests can be '
                                   'specified using any of: [classname], [classname]#[methodname], '
                                   '[filename] or [filename]#[methodname]')

    outdir = mkflag('outdir')
    option_group.add_option(outdir, dest='junit_run_outdir',
                            help='Emit output in to this directory.')

    xmlreport = mkflag('xmlreport')
    option_group.add_option(xmlreport, mkflag('xmlreport', negate=True),
                            dest = 'junit_run_xmlreport',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%default] Causes an xml report to be output for each test '
                                   'class that is run.')

    option_group.add_option(mkflag('per-test-timer'), mkflag('per-test-timer', negate=True),
                            dest = 'junit_run_per_test_timer',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%default] Shows progress and timer for each test '
                                   'class that is run.')

    option_group.add_option(mkflag('default-parallel'), mkflag('default-parallel', negate=True),
                            dest = 'junit_run_default_parallel',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%default] Whether to run classes without @TestParallel or '
                                   '@TestSerial annotations in parallel.')

    option_group.add_option(mkflag('parallel-threads'), type = 'int', default=0,
                            dest = 'junit_run_parallel_threads',
                            help = 'Number of threads to run tests in parallel. 0 for autoset.')

    option_group.add_option(mkflag("test-shard"), dest = "junit_run_test_shard",
                            help = "Subset of tests to run, in the form M/N, 0 <= M < N."
                                   "For example, 1/3 means run tests number 2, 5, 8, 11, ...")

    option_group.add_option(mkflag('coverage'), mkflag('coverage', negate=True),
                            dest = 'junit_run_coverage',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%default] Collects code coverage data')

    coverage_patterns = mkflag('coverage-patterns')
    option_group.add_option(coverage_patterns, dest='junit_run_coverage_patterns',
                            action='append',
                            help='By default all non-test code depended on by the selected tests '
                                 'is measured for coverage during the test run.  By specifying '
                                 'coverage patterns you can select which classes and packages '
                                 'should be counted.  Values should be class name prefixes in '
                                 'dotted form with ? and * wildcard support. If preceded with a - '
                                 'the pattern is excluded. '
                                 'For example, to include all code in com.twitter.raven except '
                                 'claws and the eye you would use: '
                                 '%(flag)s=com.twitter.raven.* '
                                 '%(flag)s=-com.twitter.raven.claw '
                                 '%(flag)s=-com.twitter.raven.Eye'
                                 'This option can be specified multiple times. ' % dict(
                                    flag=coverage_patterns
                                 ))

    option_group.add_option(mkflag('coverage-console'), mkflag('coverage-console', negate=True),
                            dest = 'junit_run_coverage_console',
                            action='callback', callback=mkflag.set_bool, default=True,
                            help = '[%default] Outputs a simple coverage report to the console.')

    option_group.add_option(mkflag('coverage-xml'), mkflag('coverage-xml', negate=True),
                            dest = 'junit_run_coverage_xml',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%%default] Produces an xml coverage report in %s.' % outdir)

    coverage_html_flag = mkflag('coverage-html')
    option_group.add_option(coverage_html_flag, mkflag('coverage-html', negate=True),
                            dest = 'junit_run_coverage_html',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%%default] Produces an html coverage report in %s.' % outdir)

    option_group.add_option(mkflag('coverage-html-open'), mkflag('coverage-html-open', negate=True),
                            dest = 'junit_run_coverage_html_open',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help = '[%%default] Tries to open the generated html coverage report, '
                                   'implies %s.' % coverage_html_flag)

    option_group.add_option(mkflag('suppress-output'), mkflag('suppress-output', negate=True),
                            dest = 'junit_run_suppress_output',
                            action='callback', callback=mkflag.set_bool, default=True,
                            help = '[%%default] Redirects test output to files in %s.  '
                                   'Implied by %s' % (outdir, xmlreport))

    option_group.add_option(mkflag("arg"), dest="junit_run_arg",
                            action="append",
                            help = "An arbitrary argument to pass directly to the test runner. "
                                   "This option can be specified multiple times.")

  def __init__(self, context):
    super(JUnitRun, self).__init__(context)

    context.products.require_data('exclusives_groups')

    self.confs = context.config.getlist('junit-run', 'confs', default=['default'])

    self._junit_bootstrap_key = 'junit'
    junit_bootstrap_tools = context.config.getlist('junit-run', 'junit-bootstrap-tools',
                                                   default=[':junit'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._junit_bootstrap_key, junit_bootstrap_tools)

    self._emma_bootstrap_key = 'emma'
    emma_bootstrap_tools = context.config.getlist('junit-run', 'emma-bootstrap-tools',
                                                  default=[':emma'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._emma_bootstrap_key, emma_bootstrap_tools)

    self.jvm_args = context.config.getlist('junit-run', 'jvm_args', default=[])
    if context.options.junit_run_jvmargs:
      self.jvm_args.extend(context.options.junit_run_jvmargs)
    if context.options.junit_run_debug:
      self.jvm_args.extend(context.config.getlist('jvm', 'debug_args'))

    # List of FQCN, FQCN#method, sourcefile or sourcefile#method.
    self.tests_to_run = context.options.junit_run_tests
    self.context.products.require_data('classes_by_target')
    self.context.products.require_data('classes_by_source')

    self.outdir = (
      context.options.junit_run_outdir
      or context.config.get('junit-run', 'workdir')
    )

    self.batch_size = context.options.junit_run_batch_size
    self.fail_fast = context.options.junit_run_fail_fast

    self.coverage = context.options.junit_run_coverage
    self.coverage_filters = context.options.junit_run_coverage_patterns or []
    self.coverage_dir = os.path.join(self.outdir, 'coverage')
    self.coverage_instrument_dir = os.path.join(self.coverage_dir, 'classes')
    self.coverage_metadata_file = os.path.join(self.coverage_dir, 'coverage.em')
    self.coverage_file = os.path.join(self.coverage_dir, 'coverage.ec')

    self.coverage_report_console = context.options.junit_run_coverage_console
    self.coverage_console_file = os.path.join(self.coverage_dir, 'coverage.txt')

    self.coverage_report_xml = context.options.junit_run_coverage_xml
    self.coverage_xml_file = os.path.join(self.coverage_dir, 'coverage.xml')

    self.coverage_report_html_open = context.options.junit_run_coverage_html_open
    self.coverage_report_html = (
      self.coverage_report_html_open
      or context.options.junit_run_coverage_html
    )
    self.coverage = self.coverage or self.coverage_report_html_open
    self.coverage_html_file = os.path.join(self.coverage_dir, 'html', 'index.html')

    self.opts = []
    if context.options.junit_run_xmlreport or context.options.junit_run_suppress_output:
      if self.fail_fast:
        self.opts.append('-fail-fast')
      if context.options.junit_run_xmlreport:
        self.opts.append('-xmlreport')
      self.opts.append('-suppress-output')
      self.opts.append('-outdir')
      self.opts.append(self.outdir)

    if context.options.junit_run_per_test_timer:
      self.opts.append('-per-test-timer')
    if context.options.junit_run_default_parallel:
      self.opts.append('-default-parallel')
    self.opts.append('-parallel-threads')
    self.opts.append(str(context.options.junit_run_parallel_threads))

    if context.options.junit_run_test_shard:
      self.opts.append('-test-shard')
      self.opts.append(context.options.junit_run_test_shard)

    if context.options.junit_run_arg:
      self.opts.extend(context.options.junit_run_arg)

  def _partition(self, tests):
    stride = min(self.batch_size, len(tests))
    for i in xrange(0, len(tests), stride):
      yield tests[i:i+stride]

  def execute(self, targets):
    if not self.context.options.junit_run_skip:
      tests = list(self.get_tests_to_run() if self.tests_to_run
                   else self.calculate_tests_from_targets(targets))
      if tests:
        bootstrapped_cp = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
            self._junit_bootstrap_key)
        junit_classpath = self.classpath(
            bootstrapped_cp,
            confs=self.confs,
            exclusives_classpath=self.get_base_classpath_for_target(targets[0]))

        def run_tests(classpath, main, jvm_args=None):
          # TODO(John Sirois): Integrated batching with the test runner.  As things stand we get
          # results summaries for example for each batch but no overall summary.
          # http://jira.local.twitter.com/browse/AWESOME-1114
          result = 0
          for batch in self._partition(tests):
            with binary_util.safe_args(batch) as batch_tests:
              result += abs(execute_java(
                classpath=classpath,
                main=main,
                jvm_options=(jvm_args or []) + self.jvm_args,
                args=self.opts + batch_tests,
                workunit_factory=self.context.new_workunit,
                workunit_name='run',
                workunit_labels=[WorkUnit.TEST]
              ))
              if result != 0 and self.fail_fast:
                break
          if result != 0:
            raise TaskError('java %s ... exited non-zero (%i)' % (main, result))

        if self.coverage:
          emma_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
              self._emma_bootstrap_key)

          def instrument_code():
            safe_mkdir(self.coverage_instrument_dir, clean=True)
            with binary_util.safe_args(self.get_coverage_patterns(targets)) as patterns:
              args = [
                'instr',
                '-out', self.coverage_metadata_file,
                '-d', self.coverage_instrument_dir,
                '-cp', os.pathsep.join(junit_classpath),
                '-exit'
              ]
              for pattern in patterns:
                args.extend(['-filter', pattern])
              main = 'emma'
              result = execute_java(classpath=emma_classpath, main=main, args=args,
                                    workunit_factory=self.context.new_workunit,
                                    workunit_name='emma-instrument')
              if result != 0:
                raise TaskError("java %s ... exited non-zero (%i)"
                                " 'failed to instrument'" % (main, result))

          def generate_reports():
            args = [
              'report',
              '-in', self.coverage_metadata_file,
              '-in', self.coverage_file,
              '-exit'
            ]
            source_bases = set()
            def collect_source_base(target):
              if self.is_coverage_target(target):
                source_bases.add(target.target_base)
            for target in self.test_target_candidates(targets):
              target.walk(collect_source_base)
            for source_base in source_bases:
              args.extend(['-sp', source_base])

            sorting = ['-Dreport.sort', '+name,+class,+method,+block']
            if self.coverage_report_console:
              args.extend(['-r', 'txt',
                           '-Dreport.txt.out.file=%s' % self.coverage_console_file] + sorting)
            if self.coverage_report_xml:
              args.extend(['-r', 'xml','-Dreport.xml.out.file=%s' % self.coverage_xml_file])
            if self.coverage_report_html:
              args.extend(['-r', 'html',
                           '-Dreport.html.out.file=%s' % self.coverage_html_file,
                           '-Dreport.out.encoding=UTF-8'] + sorting)

            main = 'emma'
            result = execute_java(classpath=emma_classpath, main=main, args=args,
                                  workunit_factory=self.context.new_workunit,
                                  workunit_name='emma-report')
            if result != 0:
              raise TaskError("java %s ... exited non-zero (%i)"
                              " 'failed to generate code coverage reports'" % (main, result))

            if self.coverage_report_console:
              with safe_open(self.coverage_console_file) as console_report:
                sys.stdout.write(console_report.read())
            if self.coverage_report_html_open:
              binary_util.ui_open(self.coverage_html_file)

          instrument_code()
          try:
            # Coverage runs over instrumented classes require the instrumented classes come 1st in
            # the classpath followed by the normal classpath.  The instrumentation also adds a
            # dependency on emma libs that must be satisfied on the classpath.
            run_tests([self.coverage_instrument_dir] + junit_classpath + emma_classpath,
                      JUnitRun._MAIN,
                      jvm_args=['-Demma.coverage.out.file=%s' % self.coverage_file])
          finally:
            generate_reports()
        else:
          self.context.lock.release()
          run_tests(junit_classpath, JUnitRun._MAIN)

  def is_coverage_target(self, tgt):
    return (tgt.is_java or tgt.is_scala) and not tgt.is_test and not tgt.is_codegen

  def get_coverage_patterns(self, targets):
    if self.coverage_filters:
      return self.coverage_filters
    else:
      classes_under_test = set()
      classes_by_source = self.context.products.get_data('classes_by_source')
      def add_sources_under_test(tgt):
        if self.is_coverage_target(tgt):
          for source in tgt.sources_relative_to_buildroot():
            source_products = classes_by_source.get(source)
            if source_products:
              for _, classes in source_products.rel_paths():
                classes_under_test.update(JUnitRun.classfile_to_classname(cls) for cls in classes)

      for target in targets:
        target.walk(add_sources_under_test)
      return classes_under_test

  def get_tests_to_run(self):
    for test_spec in self.tests_to_run:
      for c in self.interpret_test_spec(test_spec):
        yield c

  def test_target_candidates(self, targets):
    for target in targets:
      if isinstance(target, junit_tests):
        yield target

  def calculate_tests_from_targets(self, targets):
    targets_to_classes = self.context.products.get_data('classes_by_target')
    for target in self.test_target_candidates(targets):
      target_products = targets_to_classes.get(target)
      if target_products:
        for _, classes in target_products.rel_paths():
          for cls in classes:
            yield JUnitRun.classfile_to_classname(cls)

  def classnames_from_source_file(self, srcfile):
    relsrc = os.path.relpath(srcfile, get_buildroot()) if os.path.isabs(srcfile) else srcfile
    source_products = self.context.products.get_data('classes_by_source').get(relsrc)
    if not source_products:
      # It's valid - if questionable - to have a source file with no classes when, for
      # example, the source file has all its code commented out.
      self.context.log.warn('Source file %s generated no classes' % srcfile)
    else:
      for _, classes in source_products.rel_paths():
        for cls in classes:
          yield JUnitRun.classfile_to_classname(cls)

  @staticmethod
  def classfile_to_classname(cls):
    clsname, _ = os.path.splitext(cls.replace('/', '.'))
    return clsname

  def interpret_test_spec(self, test_spec):
    components = test_spec.split('#', 2)
    classname_or_srcfile = components[0]
    methodname = '#' + components[1] if len(components) == 2 else ''

    if os.path.exists(classname_or_srcfile):  # It's a source file.
      srcfile = classname_or_srcfile  # Alias for clarity.
      for cls in self.classnames_from_source_file(srcfile):
        # Tack the methodname onto all classes in the source file, as we
        # can't know which method the user intended.
        yield cls + methodname
    else:  # It's a classname.
      classname = classname_or_srcfile
      yield classname + methodname


PACKAGE_PARSER = re.compile(r'^\s*package\s+([\w.]+)\s*;?\s*')


def calculate_basedir(filepath):
  with open(filepath, 'r') as source:
    for line in source:
      match = PACKAGE_PARSER.match(line)
      if match:
        package = match.group(1)
        packagedir = package.replace('.', '/')
        dirname = os.path.dirname(filepath)
        if not dirname.endswith(packagedir):
          raise TaskError('File %s declares a mismatching package %s' % (file, package))
        return dirname[:-len(packagedir)]

  raise TaskError('Could not calculate a base dir for: %s' % file)

########NEW FILE########
__FILENAME__ = jvmdoc_gen
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import collections
import contextlib
import multiprocessing
import os
import subprocess

from twitter.common.dirutil import safe_mkdir

from twitter.pants import binary_util
from twitter.pants.tasks import Task, TaskError

Jvmdoc = collections.namedtuple('Jvmdoc', ['tool_name', 'product_type'])

ParserConfig = collections.namedtuple(
                'JvmdocGenParserConfig',
                ['outdir_opt', 'include_codegen_opt', 'transitive_opt', 'open_opt', 'combined_opt',
                 'ignore_failure_opt'])


class JvmdocGen(Task):
  @classmethod
  def setup_parser_config(cls):
    opts = ['%s_%s' % (cls.__name__, opt) for opt in ParserConfig._fields]
    return ParserConfig(*opts)

  @classmethod
  def generate_setup_parser(cls, option_group, args, mkflag, jvmdoc):
    parser_config = cls.setup_parser_config()
    option_group.add_option(
      mkflag('outdir'),
      dest=parser_config.outdir_opt,
      help='Emit %s in this directory.' % jvmdoc.tool_name)

    option_group.add_option(
      mkflag('include-codegen'),
      mkflag('include-codegen', negate=True),
      dest=parser_config.include_codegen_opt,
      default=None,
      action='callback',
      callback=mkflag.set_bool,
      help='[%%default] Create %s for generated code.' % jvmdoc.tool_name)

    option_group.add_option(
      mkflag('transitive'),
      mkflag('transitive', negate=True),
      dest=parser_config.transitive_opt,
      default=True,
      action='callback',
      callback=mkflag.set_bool,
      help='[%%default] Create %s for the transitive closure of internal '
           'targets reachable from the roots specified on the command line.'
           % jvmdoc.tool_name)

    combined_flag = mkflag('combined')
    option_group.add_option(
      combined_flag,
      mkflag('combined', negate=True),
      dest=parser_config.combined_opt,
      default=False,
      action='callback',
      callback=mkflag.set_bool,
      help='[%%default] Generate %s for all targets combined instead of '
           'each target individually.'
           % jvmdoc.tool_name)

    option_group.add_option(
      mkflag('open'),
      mkflag('open', negate=True),
      dest=parser_config.open_opt,
      default=False,
      action='callback',
      callback=mkflag.set_bool,
      help='[%%default] Attempt to open the generated %s in a browser '
           '(implies %s).' % (jvmdoc.tool_name, combined_flag))

    option_group.add_option(
      mkflag('ignore-failure'),
      mkflag('ignore-failure', negate=True),
      dest=parser_config.ignore_failure_opt,
      default=False,
      action='callback',
      callback=mkflag.set_bool,
      help='Specifies that %s errors should not cause build errors'
           % jvmdoc.tool_name)

  def __init__(self, context, jvmdoc, output_dir, confs, active):
    def getattr_options(option):
      return getattr(context.options, option)

    super(JvmdocGen, self).__init__(context)

    self._jvmdoc = jvmdoc
    jvmdoc_tool_name = self._jvmdoc.tool_name

    config_section = '%s-gen' % jvmdoc_tool_name
    parser_config = self.setup_parser_config()

    pants_workdir = context.config.getdefault('pants_workdir')
    self._output_dir = (
      output_dir
      or getattr_options(parser_config.outdir_opt)
      or context.config.get(config_section,
                            'workdir',
                            default=os.path.join(pants_workdir, jvmdoc_tool_name))
    )

    flagged_codegen = getattr_options(parser_config.include_codegen_opt)
    self._include_codegen = (flagged_codegen if flagged_codegen is not None
                             else context.config.getbool(config_section, 'include_codegen',
                                                         default=False))

    self.transitive = getattr_options(parser_config.transitive_opt)
    self.confs = confs or context.config.getlist(config_section, 'confs', default=['default'])
    self.active = active
    self.open = getattr_options(parser_config.open_opt)
    self.combined = self.open or getattr_options(parser_config.combined_opt)
    self.ignore_failure = getattr_options(parser_config.ignore_failure_opt)

  def invalidate_for(self):
    return (self.combined, self.transitive, self._output_dir, self.confs, self._include_codegen)

  def generate_execute(self, targets, language_predicate, create_jvmdoc_command):
    """
    Generate an execute method given a language predicate and command to create documentation

    language_predicate: a function that accepts a target and returns True if the target is of that
                        language
    create_jvmdoc_command: (classpath, directory, *targets) -> command (string) that will generate
                           documentation documentation for targets
    """
    catalog = self.context.products.isrequired(self._jvmdoc.product_type)
    if catalog and self.combined:
      raise TaskError(
          'Cannot provide %s target mappings for combined output' % self._jvmdoc.product_type)
    elif catalog or self.active:
      def docable(target):
        return language_predicate(target) and (self._include_codegen or not target.is_codegen)

      with self.invalidated(filter(docable, targets)) as invalidation_check:
        safe_mkdir(self._output_dir)
        with self.context.state('classpath', []) as cp:
          classpath = [jar for conf, jar in cp if conf in self.confs]

          def find_jvmdoc_targets():
            invalid_targets = set()
            for vt in invalidation_check.invalid_vts:
              invalid_targets.update(vt.targets)

            if self.transitive:
              return invalid_targets
            else:
              return set(invalid_targets).intersection(set(self.context.target_roots))

          jvmdoc_targets = list(filter(docable, find_jvmdoc_targets()))
          if self.combined:
            self._generate_combined(classpath, jvmdoc_targets, create_jvmdoc_command)
          else:
            self._generate_individual(classpath, jvmdoc_targets, create_jvmdoc_command)

      if catalog:
        for target in targets:
          gendir = self._gendir(target)
          jvmdocs = []
          for root, dirs, files in os.walk(gendir):
            jvmdocs.extend(os.path.relpath(os.path.join(root, f), gendir) for f in files)
          self.context.products.get(self._jvmdoc.product_type).add(target, gendir, jvmdocs)

  def _generate_combined(self, classpath, targets, create_jvmdoc_command):
    gendir = os.path.join(self._output_dir, 'combined')
    if targets:
      safe_mkdir(gendir, clean=True)
      command = create_jvmdoc_command(classpath, gendir, *targets)
      if command:
        create_jvmdoc(command, gendir)
    if self.open:
      binary_util.ui_open(os.path.join(gendir, 'index.html'))

  def _generate_individual(self, classpath, targets, create_jvmdoc_command):
    jobs = {}
    for target in targets:
      gendir = self._gendir(target)
      command = create_jvmdoc_command(classpath, gendir, target)
      if command:
        jobs[gendir] = (target, command)

    if jobs:
      with contextlib.closing(
            multiprocessing.Pool(processes=min(len(jobs), multiprocessing.cpu_count()))) as pool:
        # map would be a preferable api here but fails after the 1st batch with an internal:
        # ...
        #  File "...src/python/twitter/pants/tasks/jar_create.py", line 170, in javadocjar
        #      pool.map(createjar, jobs)
        #    File "...lib/python2.6/multiprocessing/pool.py", line 148, in map
        #      return self.map_async(func, iterable, chunksize).get()
        #    File "...lib/python2.6/multiprocessing/pool.py", line 422, in get
        #      raise self._value
        #  NameError: global name 'self' is not defined
        futures = []
        for gendir, (target, command) in jobs.items():
          futures.append(pool.apply_async(create_jvmdoc, args=(command, gendir)))

        for future in futures:
          result, gendir = future.get()
          target, command = jobs[gendir]
          if result != 0:
            message = 'Failed to process %s for %s [%d]: %s' % (
                      self._jvmdoc.tool_name, target, result, command)
            if self.ignore_failure:
              self.context.log.warn(message)
            else:
              raise TaskError(message)

  def _gendir(self, target):
    return os.path.join(self._output_dir, target.id)


def create_jvmdoc(command, gendir):
  safe_mkdir(gendir, clean=True)
  process = subprocess.Popen(command)
  result = process.wait()
  return result, gendir

########NEW FILE########
__FILENAME__ = jvm_binary_task
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'John Sirois'

import os

from twitter.common.collections.ordereddict import OrderedDict
from twitter.common.collections.orderedset import OrderedSet

from twitter.pants.targets.jvm_binary import JvmBinary
from twitter.pants.tasks import Task


class JvmBinaryTask(Task):

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("outdir"), dest="jvm_binary_create_outdir",
                            help="Create bundles and archives in this directory.")

    option_group.add_option(mkflag("deployjar"), mkflag("deployjar", negate=True),
                            dest="jvm_binary_create_deployjar", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Create a monolithic deploy jar containing this "
                                 "binaries classfiles as well as all classfiles it depends on "
                                 "transitively.")

  def __init__(self, context):
    Task.__init__(self, context)

  def is_binary(self, target):
    return isinstance(target, JvmBinary)

  def require_jar_dependencies(self, predicate=None):
    self.context.products.require('jar_dependencies', predicate=predicate or self.is_binary)

  def list_jar_dependencies(self, binary, confs=None):
    jardepmap = self.context.products.get('jar_dependencies') or {}

    if confs:
      return self._mapped_dependencies(jardepmap, binary, confs)
    else:
      return self._unexcluded_dependencies(jardepmap, binary)

  def _mapped_dependencies(self, jardepmap, binary, confs):
    # TODO(John Sirois): rework product mapping towards well known types

    # Generate a map of jars for each unique artifact (org, name)
    externaljars = OrderedDict()
    visited = set()
    for conf in confs:
      mapped = jardepmap.get((binary, conf))
      if mapped:
        for basedir, jars in mapped.items():
          for externaljar in jars:
            if (basedir, externaljar) not in visited:
              visited.add((basedir, externaljar))
              keys = jardepmap.keys_for(basedir, externaljar)
              for key in keys:
                if isinstance(key, tuple) and len(key) == 3:
                  org, name, configuration = key
                  classpath_entry = externaljars.get((org, name))
                  if not classpath_entry:
                    classpath_entry = {}
                    externaljars[(org, name)] = classpath_entry
                  classpath_entry[conf] = os.path.join(basedir, externaljar)
    return externaljars.values()

  def _unexcluded_dependencies(self, jardepmap, binary):
    # TODO(John Sirois): Kill this and move jar exclusion to use confs
    excludes = set()
    for exclude_key in ((e.org, e.name) if e.name else e.org for e in binary.deploy_excludes):
      exclude = jardepmap.get(exclude_key)
      if exclude:
        for basedir, jars in exclude.items():
          for jar in jars:
            excludes.add((basedir, jar))
    self.context.log.debug('Calculated excludes:\n\t%s' % '\n\t'.join(str(e) for e in excludes))

    externaljars = OrderedSet()

    def add_jars(target):
      mapped = jardepmap.get(target)
      if mapped:
        for basedir, jars in mapped.items():
          for externaljar in jars:
            if (basedir, externaljar) not in excludes:
              externaljars.add((basedir, externaljar))
            else:
              self.context.log.debug('Excluding %s from binary' % externaljar)

    binary.walk(add_jars, lambda t: t.is_internal)
    return externaljars

########NEW FILE########
__FILENAME__ = analysis

class Analysis(object):
  """Parsed representation of an analysis for some JVM language.

  An analysis provides information on the src -> class product mappings
  and on the src -> {src|class|jar} file dependency mappings.
  """
  @classmethod
  def merge(cls, analyses):
    """Merge multiple analysis instances into one."""
    raise NotImplementedError()

  def split(self, splits, catchall=False):
    """Split the analysis according to splits, which is a list of K iterables of source files.

    If catchall is False, returns a list of K ZincAnalysis objects, one for each of the splits, in order.
    If catchall is True, returns K+1 ZincAnalysis objects, the last one containing the analysis for any
    remainder sources not mentioned in the K splits.
    """
    raise NotImplementedError()

  def write_to_path(self, outfile_path, rebasings=None):
    with open(outfile_path, 'w') as outfile:
      self.write(outfile, rebasings)

  def write(self, outfile, rebasings=None):
    """Write this Analysis to outfile.

    rebasings: A list of path prefix pairs [from_prefix, to_prefix] to rewrite.
               to_prefix may be None, in which case matching paths are removed entirely.
    """
    raise NotImplementedError()

########NEW FILE########
__FILENAME__ = analysis_parser
import os

from twitter.pants.tasks.task_error import TaskError


class ParseError(TaskError):
  pass


class AnalysisParser(object):
  """Parse a file containing representation of an analysis for some JVM language."""
  def __init__(self, classes_dir):
    self.classes_dir = classes_dir  # The output dir for classes in this analysis.

  def is_nonempty_analysis(self, path):
    """Returns whether an analysis at a specified path is nontrivial."""
    if not os.path.exists(path):
      return False
    empty_prefix = self.empty_prefix()
    with open(path, 'r') as infile:
      prefix = infile.read(len(empty_prefix))
    return prefix != empty_prefix

  def empty_prefix(self):
    """Returns a prefix indicating a trivial analysis file.

    I.e., this prefix is present at the begnning of an analysis file iff the analysis is trivial.
    """
    raise NotImplementedError()

  def parse_from_path(self, infile_path):
    """Parse an analysis instance from a text file."""
    with open(infile_path, 'r') as infile:
      return self.parse(infile)

  def parse(self, infile):
    """Parse an analysis instance from an open file."""
    raise NotImplementedError()

  def parse_products_from_path(self, infile_path):
    """An efficient parser of just the src->class mappings.

    Returns a map of src -> list of classfiles. All paths are absolute.
    """
    with open(infile_path, 'r') as infile:
      return self.parse_products(infile)

  def parse_products(self, infile):
    """An efficient parser of just the src->class mappings.

    Returns a map of src -> list of classfiles. All paths are absolute.
    """
    raise NotImplementedError()

  def parse_deps_from_path(self, infile_path, classpath_indexer):
    """An efficient parser of just the src->dep mappings.

    classpath_indexer - a no-arg method that an implementation may call if it needs a mapping
                        of class->element on the classpath that provides that class.
                        We use this indirection to avoid unnecessary precomputation.
    """
    with open(infile_path, 'r') as infile:
      return self.parse_deps(infile, classpath_indexer)

  def parse_deps(self, infile, classpath_indexer):
    """An efficient parser of just the binary, source and external deps sections.

    classpath_indexer - a no-arg method that an implementation may call if it needs a mapping
                        of class->element on the classpath that provides that class.
                        We use this indirection to avoid unnecessary precomputation.

    Returns a dict of src -> iterable of deps, where each item in deps is either a binary dep,
    source dep or external dep, i.e., either a source file, a class file or a jar file.

    All paths are absolute.
    """
    raise NotImplementedError()


########NEW FILE########
__FILENAME__ = analysis_tools
import os
import shutil
from twitter.common.contextutil import temporary_dir
from twitter.pants.base.build_environment import get_buildroot


class AnalysisTools(object):
  """Analysis manipulation methods required by JvmCompile."""
  _IVY_HOME_PLACEHOLDER = '/_IVY_HOME_PLACEHOLDER'
  _PANTS_HOME_PLACEHOLDER = '/_PANTS_HOME_PLACEHOLDER'

  def __init__(self, context, parser, analysis_cls):
    self.parser = parser
    self._java_home = context.java_home
    self._ivy_home = context.ivy_home
    self._pants_home = get_buildroot()
    self._analysis_cls = analysis_cls

  def split_to_paths(self, analysis_path, split_path_pairs, catchall_path=None):
    """Split an analysis file.

    split_path_pairs: A list of pairs (split, output_path) where split is a list of source files
    whose analysis is to be split out into output_path. The source files may either be
    absolute paths, or relative to the build root.

    If catchall_path is specified, the analysis for any sources not mentioned in the splits is
    split out to that path.
    """
    analysis = self.parser.parse_from_path(analysis_path)
    splits, output_paths = zip(*split_path_pairs)
    split_analyses = analysis.split(splits, catchall_path is not None)
    if catchall_path is not None:
      output_paths = output_paths + (catchall_path, )
    for analysis, path in zip(split_analyses, output_paths):
      analysis.write_to_path(path)

  def merge_from_paths(self, analysis_paths, merged_analysis_path):
    """Merge multiple analysis files into one."""
    analyses = [self.parser.parse_from_path(path) for path in analysis_paths]
    merged_analysis = self._analysis_cls.merge(analyses)
    merged_analysis.write_to_path(merged_analysis_path)

  def relativize(self, src_analysis, relativized_analysis):
    with temporary_dir() as tmp_analysis_dir:
      tmp_analysis_file = os.path.join(tmp_analysis_dir, 'analysis.relativized')

      # NOTE: We can't port references to deps on the Java home. This is because different JVM
      # implementations on different systems have different structures, and there's not
      # necessarily a 1-1 mapping between Java jars on different systems. Instead we simply
      # drop those references from the analysis file.
      #
      # In practice the JVM changes rarely, and it should be fine to require a full rebuild
      # in those rare cases.
      rebasings = [
        (self._java_home, None),
        (self._ivy_home, self._IVY_HOME_PLACEHOLDER),
        (self._pants_home, self._PANTS_HOME_PLACEHOLDER),
        ]
      # Work on a tmpfile, for safety.
      self._rebase_from_path(src_analysis, tmp_analysis_file, rebasings)
      shutil.move(tmp_analysis_file, relativized_analysis)

  def localize(self, src_analysis, localized_analysis):
    with temporary_dir() as tmp_analysis_dir:
      tmp_analysis_file = os.path.join(tmp_analysis_dir, 'analysis')
      rebasings = [
        (AnalysisTools._IVY_HOME_PLACEHOLDER, self._ivy_home),
        (AnalysisTools._PANTS_HOME_PLACEHOLDER, self._pants_home),
        ]
      # Work on a tmpfile, for safety.
      self._rebase_from_path(src_analysis, tmp_analysis_file, rebasings)
      shutil.move(tmp_analysis_file, localized_analysis)

  def _rebase_from_path(self, input_analysis_path, output_analysis_path, rebasings):
    """Rebase file paths in an analysis file.

    rebasings: A list of path prefix pairs [from_prefix, to_prefix] to rewrite.
               to_prefix may be None, in which case matching paths are removed entirely.
    """
    analysis = self.parser.parse_from_path(input_analysis_path)
    analysis.write_to_path(output_analysis_path, rebasings=rebasings)

########NEW FILE########
__FILENAME__ = java_compile

import os
import shlex

from twitter.common.dirutil import safe_open

from twitter.pants.base.target import Target
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.jvm_compile.analysis_tools import AnalysisTools
from twitter.pants.tasks.jvm_compile.java.jmake_analysis import JMakeAnalysis
from twitter.pants.tasks.jvm_compile.java.jmake_analysis_parser import JMakeAnalysisParser
from twitter.pants.tasks.jvm_compile.jvm_compile import JvmCompile


# From http://kenai.com/projects/jmake/sources/mercurial/content/src/com/sun/tools/jmake/Main.java?rev=26
# Main.mainExternal docs.
_JMAKE_ERROR_CODES = {
   -1: 'invalid command line option detected',
   -2: 'error reading command file',
   -3: 'project database corrupted',
   -4: 'error initializing or calling the compiler',
   -5: 'compilation error',
   -6: 'error parsing a class file',
   -7: 'file not found',
   -8: 'I/O exception',
   -9: 'internal jmake exception',
  -10: 'deduced and actual class name mismatch',
  -11: 'invalid source file extension',
  -12: 'a class in a JAR is found dependent on a class with the .java source',
  -13: 'more than one entry for the same class is found in the project',
  -20: 'internal Java error (caused by java.lang.InternalError)',
  -30: 'internal Java error (caused by java.lang.RuntimeException).'
}
# When executed via a subprocess return codes will be treated as unsigned
_JMAKE_ERROR_CODES.update((256+code, msg) for code, msg in _JMAKE_ERROR_CODES.items())


class JavaCompile(JvmCompile):
  _language = 'java'
  _file_suffix = '.java'
  _config_section = 'java-compile'

    # Well known metadata file to auto-register annotation processors with a java 1.6+ compiler
  _PROCESSOR_INFO_FILE = 'META-INF/services/javax.annotation.processing.Processor'


  _JMAKE_MAIN = 'com.sun.tools.jmake.Main'

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    JvmCompile.setup_parser(JavaCompile, option_group, args, mkflag)

    option_group.add_option(mkflag("args"), dest="java_compile_args", action="append",
                            help="Pass these extra args to javac.")

  def __init__(self, context):
    super(JavaCompile, self).__init__(context, jdk=True)

    self._depfile = os.path.join(self._analysis_dir, 'global_depfile')

    self._jmake_bootstrap_key = 'jmake'
    external_tools = context.config.getlist('java-compile', 'jmake-bootstrap-tools', default=[':jmake'])
    self.register_jvm_tool(self._jmake_bootstrap_key, external_tools)

    self._compiler_bootstrap_key = 'java-compiler'
    compiler_bootstrap_tools = context.config.getlist('java-compile', 'compiler-bootstrap-tools',
                                                      default=[':java-compiler'])
    self.register_jvm_tool(self._compiler_bootstrap_key, compiler_bootstrap_tools)

    self._javac_opts = []
    if context.options.java_compile_args:
      for arg in context.options.java_compile_args:
        self._javac_opts.extend(shlex.split(arg))
    else:
      self._javac_opts.extend(context.config.getlist('java-compile', 'javac_args', default=[]))

  def create_analysis_tools(self):
    return AnalysisTools(self.context, JMakeAnalysisParser(self._classes_dir), JMakeAnalysis)

  def extra_products(self, target):
    ret = []
    if target.is_apt and target.processors:
      root = os.path.join(self._resources_dir, Target.maybe_readable_identify([target]))
      processor_info_file = os.path.join(root, JavaCompile._PROCESSOR_INFO_FILE)
      self._write_processor_info(processor_info_file, target.processors)
      ret.append((root, [processor_info_file]))
    return ret

  def compile(self, args, classpath, sources, classes_output_dir, analysis_file):
    jmake_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._jmake_bootstrap_key)
    args = [
      '-classpath', ':'.join(classpath + [self._classes_dir]),
      '-d', self._classes_dir,
      '-pdb', analysis_file,
      '-pdb-text-format',
      ]

    compiler_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
        self._compiler_bootstrap_key)
    args.extend([
      '-jcpath', ':'.join(compiler_classpath),
      '-jcmainclass', 'com.twitter.common.tools.Compiler',
      ])
    args.extend(map(lambda arg: '-C%s' % arg, self._javac_opts))

    args.extend(self._args)
    args.extend(sources)
    result = self.runjava(classpath=jmake_classpath,
                          main=JavaCompile._JMAKE_MAIN,
                          jvm_options=self._jvm_options,
                          args=args,
                          workunit_name='jmake',
                          workunit_labels=[WorkUnit.COMPILER])
    if result:
      default_message = 'Unexpected error - JMake returned %d' % result
      raise TaskError(_JMAKE_ERROR_CODES.get(result, default_message))

  def post_process(self, relevant_targets):
      # Produce a monolithic apt processor service info file for further compilation rounds
      # and the unit test classpath.
      # This is distinct from the per-target ones we create in extra_products().
      all_processors = set()
      for target in relevant_targets:
        if target.is_apt and target.processors:
          all_processors.update(target.processors)
      processor_info_file = os.path.join(self._classes_dir, JavaCompile._PROCESSOR_INFO_FILE)
      if os.path.exists(processor_info_file):
        with safe_open(processor_info_file, 'r') as f:
          for processor in f:
            all_processors.add(processor)
      self._write_processor_info(processor_info_file, all_processors)

  def _write_processor_info(self, processor_info_file, processors):
    with safe_open(processor_info_file, 'w') as f:
      for processor in processors:
        f.write('%s\n' % processor.strip())

########NEW FILE########
__FILENAME__ = jmake_analysis
import os

from collections import defaultdict
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.tasks.jvm_compile.analysis import Analysis


class JMakeAnalysis(Analysis):
  """Parsed representation of a jmake pdb.

  We use the term 'analysis' for uniformity with Zinc etc.
  """
  @classmethod
  def merge(cls, analyses):
    merged_pcd_entries = []
    merged_src_to_deps = {}
    for analysis in analyses:
      merged_pcd_entries.extend(analysis.pcd_entries)
      merged_src_to_deps.update(analysis.src_to_deps)
    return JMakeAnalysis(merged_pcd_entries, merged_src_to_deps)

  def __init__(self, pcd_entries, src_to_deps):
    self.pcd_entries = pcd_entries  # Note that second item in tuple is the source file.
    self.src_to_deps = src_to_deps

  def split(self, splits, catchall=False):
    buildroot = get_buildroot()
    src_to_split_idx = {}
    for i, split in enumerate(splits):
      for s in split:
        src_to_split_idx[s if os.path.isabs(s) else os.path.join(buildroot, s)] = i
    num_outputs = len(splits) + 1 if catchall else len(splits)
    catchall_idx = len(splits) if catchall else -1

    split_pcd_entries = []
    split_src_to_deps = []
    for _ in xrange(0, num_outputs):
      split_pcd_entries.append([])
      split_src_to_deps.append({})

    for pcd_entry in self.pcd_entries:
      split_idx = src_to_split_idx.get(pcd_entry[1], catchall_idx)
      if split_idx != -1:
        split_pcd_entries[split_idx].append(pcd_entry)
    for src, deps in self.src_to_deps.items():
      split_idx = src_to_split_idx.get(src, catchall_idx)
      if split_idx != -1:
        split_src_to_deps[split_idx][src] = deps

    return [JMakeAnalysis(x, y) for x, y in zip(split_pcd_entries, split_src_to_deps)]

  def write(self, outfile, rebasings=None):
    # Note that the only paths in a jmake analysis are source files.
    def rebase_path(path):
      if rebasings:
        for rebase_from, rebase_to in rebasings:
          if rebase_to is None:
            if path.startswith(rebase_from):
              return None
          else:
            path = path.replace(rebase_from, rebase_to)
      return path

    outfile.write('pcd entries:\n')
    outfile.write('%d items\n' % len(self.pcd_entries))
    for pcd_entry in self.pcd_entries:
      rebased_src = rebase_path(pcd_entry[1])
      if rebased_src:
        outfile.write(pcd_entry[0])
        outfile.write('\t')
        outfile.write(rebased_src)
        for x in pcd_entry[2:]:
          outfile.write('\t')
          outfile.write(x)
          # Note that last element already includes \n.

    outfile.write('dependencies:\n')
    outfile.write('%d items\n' % len(self.src_to_deps))
    for src, deps in self.src_to_deps.items():
      rebased_src = rebase_path(src)
      if rebased_src:
        outfile.write(rebased_src)
        for dep in deps:
          outfile.write('\t')
          outfile.write(dep)
        outfile.write('\n')

  def compute_products(self):
    """Returns the products in this analysis.

    Returns a map of <src file full path> -> list of classfiles, relative to the classes dir.

    Note that we don't currently use this method: We use JMakeAnalysisParser.parse_products()
    to more efficiently read just the products out of the file. However we leave this
    here for documentation of the meaning of the useful fields in pcd_entries.
    """
    src_to_classfiles = defaultdict(list)
    for pcd_entry in self.pcd_entries:
      srcfile = pcd_entry[1]
      # In the file classes are represented with slashes, not dots. E.g., com/foo/bar/Baz.
      src_to_classfiles[srcfile].append(pcd_entry[0] + '.class')
    return src_to_classfiles

########NEW FILE########
__FILENAME__ = jmake_analysis_parser
from collections import defaultdict
import os
import re

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.tasks.jvm_compile.analysis_parser import ParseError, AnalysisParser
from twitter.pants.tasks.jvm_compile.java.jmake_analysis import JMakeAnalysis


class JMakeAnalysisParser(AnalysisParser):
  """Parse a file containing representation of an analysis for some JVM language."""

  def empty_prefix(self):
    return 'pcd entries:\n0 items\n'

  def parse(self, infile):
    self._expect_header(infile.readline(), 'pcd entries')
    num_pcd_entries = self._parse_num_items(infile.readline())
    pcd_entries = []
    for i in xrange(0, num_pcd_entries):
      line = infile.readline()
      tpl = line.split('\t')
      if len(tpl) != 5:
        raise ParseError('Line must contain 5 tab-separated fields: %s' % line)
      pcd_entries.append(tpl)  # Note: we preserve the \n on the last entry.
    src_to_deps = self._parse_deps_at_position(infile)
    return JMakeAnalysis(pcd_entries, src_to_deps)

  def parse_products(self, infile):
    self._expect_header(infile.readline(), 'pcd entries')
    num_pcd_entries = self._parse_num_items(infile.readline())
    ret = defaultdict(list)
    # Parse more efficiently than above, since we only care about
    # the first two elements in the line.
    for _ in xrange(0, num_pcd_entries):
      line = infile.readline()
      p1 = line.find('\t')
      clsfile = os.path.join(self.classes_dir, line[0:p1] + '.class')
      p2 = line.find('\t', p1 + 1)
      src = line[p1+1:p2]
      ret[src].append(clsfile)
    return ret

  def parse_deps(self, infile, classpath_indexer):
    buildroot = get_buildroot()
    classpath_elements_by_class = classpath_indexer()
    self._expect_header(infile.readline(), 'pcd entries')
    num_pcd_entries = self._parse_num_items(infile.readline())
    for _ in xrange(0, num_pcd_entries):
      infile.readline()  # Skip these lines.
    src_to_deps = self._parse_deps_at_position(infile)
    ret = defaultdict(set)
    for src, deps in src_to_deps.items():
      for dep in deps:
        rel_classfile = dep + '.class'
        classpath_element = classpath_elements_by_class.get(rel_classfile, None)
        if classpath_element:  # Dep is on an external jar/classes dir.
          ret[src].add(classpath_element)
        else:  # Dep is on an internal class.
          classfile = os.path.join(buildroot, self.classes_dir, rel_classfile)
          ret[src].add(classfile)
    return ret

  def _parse_deps_at_position(self, infile):
    self._expect_header(infile.readline(), 'dependencies')
    num_deps = self._parse_num_items(infile.readline())
    src_to_deps = {}
    for i in xrange(0, num_deps):
      tpl = infile.readline().split('\t')
      src = tpl[0]
      deps = tpl[1:]
      deps[-1] = deps[-1][0:-1]  # Trim off the \n.
      src_to_deps[src] = deps
    return src_to_deps

  num_items_re = re.compile(r'(\d+) items\n')

  def _parse_num_items(self, line):
    """Parse a line of the form '<num> items' and returns <num> as an int."""
    matchobj = JMakeAnalysisParser.num_items_re.match(line)
    if not matchobj:
      raise ParseError('Expected: "<num> items". Found: "%s"' % line)
    return int(matchobj.group(1))

  def _expect_header(self, line, header):
    expected = header + ':\n'
    if line != expected:
      raise ParseError('Expected: %s. Found: %s' % (expected, line))

########NEW FILE########
__FILENAME__ = jvm_compile
import itertools
import os
import shutil
import uuid

from collections import defaultdict
from itertools import groupby

from twitter.common import contextutil
from twitter.common.collections import OrderedSet
from twitter.common.contextutil import open_zip
from twitter.common.dirutil import safe_rmtree, safe_mkdir
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.target import Target
from twitter.pants.base.worker_pool import Work
from twitter.pants.goal.products import MultipleRootedProducts
from twitter.pants.reporting.reporting_utils import items_to_report_element
from twitter.pants.tasks.jvm_compile.jvm_dependency_analyzer import JvmDependencyAnalyzer
from twitter.pants.tasks.nailgun_task import NailgunTask
from twitter.pants.tasks import Task


class JvmCompile(NailgunTask):
  """A common framework for JVM compilation.

  To subclass for a specific JVM language, implement the static values and methods
  mentioned below under "Subclasses must implement".
  """

  @staticmethod
  def setup_parser(subcls, option_group, args, mkflag):
    NailgunTask.setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag('warnings'), mkflag('warnings', negate=True),
                            dest=subcls._language+'_compile_warnings',
                            default=True,
                            action='callback',
                            callback=mkflag.set_bool,
                            help='[%default] Compile with all configured warnings enabled.')

    option_group.add_option(mkflag('partition-size-hint'),
                            dest=subcls._language+'_partition_size_hint',
                            action='store',
                            type='int',
                            default=-1,
                            help='Roughly how many source files to attempt to compile together. '
                                 'Set to a large number to compile all sources together. Set this '
                                 'to 0 to compile target-by-target. Default is set in pants.ini.')

    option_group.add_option(mkflag('missing-deps'),
                            dest=subcls._language+'_missing_deps',
                            choices=['off', 'warn', 'fatal'],
                            default='warn',
                            help='[%default] One of off, warn, fatal. '
                                 'Check for missing dependencies in ' +  subcls._language + 'code. '
                                 'Reports actual dependencies A -> B where there is no '
                                 'transitive BUILD file dependency path from A to B.'
                                 'If fatal, missing deps are treated as a build error.')

    option_group.add_option(mkflag('missing-direct-deps'),
                            dest=subcls._language+'_missing_direct_deps',
                            choices=['off', 'warn', 'fatal'],
                            default='off',
                            help='[%default] One of off, warn, fatal. '
                                 'Check for missing direct dependencies in ' + subcls._language +
                                 ' code. Reports actual dependencies A -> B where there is no '
                                 'direct BUILD file dependency path from A to B. This is a very '
                                 'strict check, as in practice it is common to rely on transitive, '
                                 'non-direct dependencies, e.g., due to type inference or when the '
                                 'main target in a BUILD file is modified to depend on other '
                                 'targets in the same BUILD file as an implementation detail. It '
                                 'may still be useful to set it to fatal temorarily, to detect '
                                 'these.')

    option_group.add_option(mkflag('unnecessary-deps'),
                            dest=subcls._language+'_unnecessary_deps',
                            choices=['off', 'warn', 'fatal'],
                            default='off',
                            help='[%default] One of off, warn, fatal. Check for declared '
                                 'dependencies in ' +  subcls._language + ' code that are not '
                                 'needed. This is a very strict check. For example, generated code '
                                 'will often legitimately have BUILD dependencies that are unused '
                                 'in practice.')

    option_group.add_option(mkflag('delete-scratch'), mkflag('delete-scratch', negate=True),
                            dest=subcls._language+'_delete_scratch',
                            default=True,
                            action='callback',
                            callback=mkflag.set_bool,
                            help='[%default] Leave intermediate scratch files around, '
                                 'for debugging build problems.')

  # Subclasses must implement.
  # --------------------------

  _language = None
  _file_suffix = None
  _config_section = None

  def create_analysis_tools(self):
    """Returns an AnalysisTools implementation.

    Subclasses must implement.
    """
    raise NotImplementedError()

  def compile(self, args, classpath, sources, classes_output_dir, analysis_file):
    """Invoke the compiler.

    Must raise TaskError on compile failure.

    Subclasses must implement."""
    raise NotImplementedError()


  # Subclasses may override.
  # ------------------------

  def extra_classpath_elements(self):
    """Extra classpath elements common to all compiler invocations.

    E.g., jars for compiler plugins.
    """
    return []

  def extra_products(self, target):
    """Any extra, out-of-band products created for a target.

    E.g., targets that produce scala compiler plugins produce an info file.
    Returns a list of pairs (root, [absolute paths of files under root]).
    """
    return []

  def post_process(self, relevant_targets):
    """Any extra post-execute work."""
    pass


  # Common code.
  # ------------

  @staticmethod
  def _analysis_for_target(analysis_dir, target):
    return os.path.join(analysis_dir, target.id + '.analysis')

  @staticmethod
  def _portable_analysis_for_target(analysis_dir, target):
    return JvmCompile._analysis_for_target(analysis_dir, target) + '.portable'

  def __init__(self, context, minimum_version=None, jdk=False):
    # TODO(John Sirois): XXX plumb minimum_version via config or flags
    super(JvmCompile, self).__init__(context, minimum_version=minimum_version, jdk=jdk)
    concrete_class = type(self)
    config_section = concrete_class._config_section

    def get_lang_specific_option(opt):
      full_opt_name = self._language + '_' + opt
      return getattr(context.options, full_opt_name, None)

    # Global workdir.
    self._pants_workdir = context.config.getdefault('pants_workdir')

    # Various working directories.
    workdir = context.config.get(config_section, 'workdir')
    self._classes_dir = os.path.join(workdir, 'classes')
    self._resources_dir = os.path.join(workdir, 'resources')
    self._analysis_dir = os.path.join(workdir, 'analysis')

    self._delete_scratch = get_lang_specific_option('delete_scratch')

    safe_mkdir(self._classes_dir)
    safe_mkdir(self._analysis_dir)

    self._analysis_file = os.path.join(self._analysis_dir, 'global_analysis.valid')
    self._invalid_analysis_file = os.path.join(self._analysis_dir, 'global_analysis.invalid')

    # A temporary, but well-known, dir in which to munge analysis/dependency files in before
    # caching. It must be well-known so we know where to find the files when we retrieve them from
    # the cache.
    self._analysis_tmpdir = os.path.join(self._analysis_dir, 'artifact_cache_tmpdir')

    # We can't create analysis tools until after construction.
    self._lazy_analysis_tools = None

    # Compiler options.
    self._args = context.config.getlist(config_section, 'args')
    if get_lang_specific_option('compile_warnings'):
      self._args.extend(context.config.getlist(config_section, 'warning_args'))
    else:
      self._args.extend(context.config.getlist(config_section, 'no_warning_args'))

    # The rough number of source files to build in each compiler pass.
    self._partition_size_hint = get_lang_specific_option('partition_size_hint')
    if self._partition_size_hint == -1:
      self._partition_size_hint = context.config.getint(config_section, 'partition_size_hint',
                                                        default=1000)

    # JVM options for running the compiler.
    self._jvm_options = context.config.getlist(config_section, 'jvm_args')

    # The ivy confs for which we're building.
    self._confs = context.config.getlist(config_section, 'confs', default=['default'])

    # Set up dep checking if needed.
    def munge_flag(flag):
      return None if flag == 'off' else flag
    check_missing_deps = munge_flag(get_lang_specific_option('missing_deps'))
    check_missing_direct_deps = munge_flag(get_lang_specific_option('missing_direct_deps'))
    check_unnecessary_deps = munge_flag(get_lang_specific_option('unnecessary_deps'))

    if check_missing_deps or check_missing_direct_deps or check_unnecessary_deps:
      # Must init it here, so it can set requirements on the context.
      self._dep_analyzer = JvmDependencyAnalyzer(self.context,
                                                 check_missing_deps,
                                                 check_missing_direct_deps,
                                                 check_unnecessary_deps)
    else:
      self._dep_analyzer = None

    self._class_to_jarfile = None  # Computed lazily as needed.

    self.context.products.require_data('exclusives_groups')
    self.setup_artifact_cache_from_config(config_section=config_section)

    # Sources (relative to buildroot) present in the last analysis that have since been deleted.
    # Generated lazily, so do not access directly. Call self._get_deleted_sources().
    self._lazy_deleted_sources = None

  def product_type(self):
    return 'classes'

  def can_dry_run(self):
    return True

  def move(self, src, dst):
    if self._delete_scratch:
      shutil.move(src, dst)
    else:
      shutil.copy(src, dst)

  # TODO(benjy): Break this monstrosity up? Previous attempts to do so
  #              turned out to be more trouble than it was worth.
  def execute(self, targets):
    # TODO(benjy): Add a pre-execute phase for injecting deps into targets, so e.g.,
    # we can inject a dep on the scala runtime library and still have it ivy-resolve.

    # In case we have no relevant targets and return early.
    self._create_empty_products()

    relevant_targets = [t for t in targets if t.has_sources(self._file_suffix)]

    if not relevant_targets:
      return

    # Get the exclusives group for the targets to compile.
    # Group guarantees that they'll be a single exclusives key for them.
    egroups = self.context.products.get_data('exclusives_groups')
    group_id = egroups.get_group_key_for_target(relevant_targets[0])

    # Add resource dirs to the classpath for us and for downstream tasks.
    for conf in self._confs:
      egroups.update_compatible_classpaths(group_id, [(conf, self._resources_dir)])

    # Get the classpath generated by upstream JVM tasks (including previous calls to execute()).
    classpath = egroups.get_classpath_for_group(group_id)

    # Add any extra classpath elements.
    for conf in self._confs:
      for jar in self.extra_classpath_elements():
        classpath.insert(0, (conf, jar))

    # Target -> sources (relative to buildroot).
    sources_by_target = self._compute_sources_by_target(relevant_targets)

    # Invalidation check. Everything inside the with block must succeed for the
    # invalid targets to become valid.
    with self.invalidated(relevant_targets,
                          invalidate_dependents=True,
                          partition_size_hint=self._partition_size_hint) as invalidation_check:
      if invalidation_check.invalid_vts and not self.dry_run:
        # The analysis for invalid and deleted sources is no longer valid.
        invalid_targets = [vt.target for vt in invalidation_check.invalid_vts]
        invalid_sources_by_target = {}
        for tgt in invalid_targets:
          invalid_sources_by_target[tgt] = sources_by_target[tgt]
        invalid_sources = list(itertools.chain.from_iterable(invalid_sources_by_target.values()))
        deleted_sources = self._deleted_sources()

        # Work in a tmpdir so we don't stomp the main analysis files on error.
        # The tmpdir is cleaned up in a shutdown hook, because background work
        # may need to access files we create here even after this method returns.
        self._ensure_analysis_tmpdir()
        tmpdir = os.path.join(self._analysis_tmpdir, str(uuid.uuid4()))
        os.mkdir(tmpdir)
        valid_analysis_tmp = os.path.join(tmpdir, 'valid_analysis')
        newly_invalid_analysis_tmp = os.path.join(tmpdir, 'newly_invalid_analysis')
        invalid_analysis_tmp = os.path.join(tmpdir, 'invalid_analysis')
        if self._analysis_parser.is_nonempty_analysis(self._analysis_file):
          with self.context.new_workunit(name='prepare-analysis'):
            self._analysis_tools.split_to_paths(self._analysis_file,
              [(invalid_sources + deleted_sources, newly_invalid_analysis_tmp)], valid_analysis_tmp)
            if self._analysis_parser.is_nonempty_analysis(self._invalid_analysis_file):
              self._analysis_tools.merge_from_paths(
                [self._invalid_analysis_file, newly_invalid_analysis_tmp], invalid_analysis_tmp)
            else:
              invalid_analysis_tmp = newly_invalid_analysis_tmp

            # Now it's OK to overwrite the main analysis files with the new state.
            self.move(valid_analysis_tmp, self._analysis_file)
            self.move(invalid_analysis_tmp, self._invalid_analysis_file)

        # Register products for all the valid targets.
        # We register as we go, so dependency checking code can use this data.
        valid_targets = list(set(relevant_targets) - set(invalid_targets))
        self._register_products(valid_targets, sources_by_target, self._analysis_file)

        # Figure out the sources and analysis belonging to each partition.
        partitions = []  # Each element is a triple (vts, sources_by_target, analysis).
        for vts in invalidation_check.invalid_vts_partitioned:
          partition_tmpdir = os.path.join(tmpdir, Target.maybe_readable_identify(vts.targets))
          os.mkdir(partition_tmpdir)
          sources = list(itertools.chain.from_iterable(
              [invalid_sources_by_target.get(t, []) for t in vts.targets]))
          de_duped_sources = list(OrderedSet(sources))
          if len(sources) != len(de_duped_sources):
            counts = [(src, len(list(srcs))) for src, srcs in groupby(sorted(sources))]
            self.context.log.warn(
                'De-duped the following sources:\n\t%s' %
                '\n\t'.join(sorted('%d %s' % (cnt, src) for src, cnt in counts if cnt > 1)))
          analysis_file = os.path.join(partition_tmpdir, 'analysis')
          partitions.append((vts, de_duped_sources, analysis_file))

        # Split per-partition files out of the global invalid analysis.
        if self._analysis_parser.is_nonempty_analysis(self._invalid_analysis_file) and partitions:
          with self.context.new_workunit(name='partition-analysis'):
            splits = [(x[1], x[2]) for x in partitions]
            # We have to pass the analysis for any deleted files through zinc, to give it
            # a chance to delete the relevant class files.
            if splits:
              splits[0] = (splits[0][0] + deleted_sources, splits[0][1])
            self._analysis_tools.split_to_paths(self._invalid_analysis_file, splits)

        # Now compile partitions one by one.
        for partition in partitions:
          (vts, sources, analysis_file) = partition
          cp_entries = [entry for conf, entry in classpath if conf in self._confs]
          self._process_target_partition(partition, cp_entries)
          # No exception was thrown, therefore the compile succeded and analysis_file is now valid.
          if os.path.exists(analysis_file):  # The compilation created an analysis.
            # Merge the newly-valid analysis with our global valid analysis.
            new_valid_analysis = analysis_file + '.valid.new'
            if self._analysis_parser.is_nonempty_analysis(self._analysis_file):
              with self.context.new_workunit(name='update-upstream-analysis'):
                self._analysis_tools.merge_from_paths([self._analysis_file, analysis_file],
                                                      new_valid_analysis)
            else:  # We need to keep analysis_file around. Background tasks may need it.
              shutil.copy(analysis_file, new_valid_analysis)

            # Move the merged valid analysis to its proper location.
            # We do this before checking for missing dependencies, so that we can still
            # enjoy an incremental compile after fixing missing deps.
            self.move(new_valid_analysis, self._analysis_file)

            # Update the products with the latest classes. Must happen before the
            # missing dependencies check.
            self._register_products(vts.targets, sources_by_target, analysis_file)
            if self._dep_analyzer:
              # Check for missing dependencies.
              actual_deps = self._analysis_parser.parse_deps_from_path(analysis_file,
                  lambda: self._compute_classpath_elements_by_class(cp_entries))
              with self.context.new_workunit(name='find-missing-dependencies'):
                self._dep_analyzer.check(sources, actual_deps)

            # Kick off the background artifact cache write.
            if self.artifact_cache_writes_enabled():
              self._write_to_artifact_cache(analysis_file, vts, invalid_sources_by_target)

          if self._analysis_parser.is_nonempty_analysis(self._invalid_analysis_file):
            with self.context.new_workunit(name='trim-downstream-analysis'):
              # Trim out the newly-valid sources from our global invalid analysis.
              new_invalid_analysis = analysis_file + '.invalid.new'
              discarded_invalid_analysis = analysis_file + '.invalid.discard'
              self._analysis_tools.split_to_paths(self._invalid_analysis_file,
                [(sources, discarded_invalid_analysis)], new_invalid_analysis)
              self.move(new_invalid_analysis, self._invalid_analysis_file)

          # Now that all the analysis accounting is complete, and we have no missing deps,
          # we can safely mark the targets as valid.
          vts.update()
      else:
        # Nothing to build. Register products for all the targets in one go.
        self._register_products(relevant_targets, sources_by_target, self._analysis_file)

    # Update the classpath for downstream tasks.
    for conf in self._confs:
      egroups.update_compatible_classpaths(group_id, [(conf, self._classes_dir)])

    self.post_process(relevant_targets)

  def _process_target_partition(self, partition, classpath):
    """Needs invoking only on invalid targets.

    partition - a triple (vts, sources_by_target, analysis_file).
    classpath - a list of classpath entries.

    May be invoked concurrently on independent target sets.

    Postcondition: The individual targets in vts are up-to-date, as if each were
                   compiled individually.
    """
    (vts, sources, analysis_file) = partition

    if not sources:
      self.context.log.warn('Skipping %s compile for targets with no sources:\n  %s'
                            % (self._language, vts.targets))
    else:
      # Do some reporting.
      self.context.log.info(
        'Compiling a partition containing ',
        items_to_report_element(sources, 'source'),
        ' in ',
        items_to_report_element([t.address.reference() for t in vts.targets], 'target'), '.')
      with self.context.new_workunit('compile'):
        # The compiler may delete classfiles, then later exit on a compilation error. Then if the
        # change triggering the error is reverted, we won't rebuild to restore the missing
        # classfiles. So we force-invalidate here, to be on the safe side.
        vts.force_invalidate()
        self.compile(self._args, classpath, sources, self._classes_dir, analysis_file)

  def check_artifact_cache(self, vts):
    # Special handling for scala analysis files. Class files are retrieved directly into their
    # final locations in the global classes dir.

    def post_process_cached_vts(cached_vts):
      # Merge the localized analysis with the global one (if any).
      analyses_to_merge = []
      for vt in cached_vts:
        for target in vt.targets:
          analysis_file = JvmCompile._analysis_for_target(self._analysis_tmpdir, target)
          portable_analysis_file = JvmCompile._portable_analysis_for_target(self._analysis_tmpdir,
                                                                            target)
          if os.path.exists(portable_analysis_file):
            self._analysis_tools.localize(portable_analysis_file, analysis_file)
          if os.path.exists(analysis_file):
            analyses_to_merge.append(analysis_file)

      if len(analyses_to_merge) > 0:
        if os.path.exists(self._analysis_file):
          analyses_to_merge.append(self._analysis_file)
        with contextutil.temporary_dir() as tmpdir:
          tmp_analysis = os.path.join(tmpdir, 'analysis')
          with self.context.new_workunit(name='merge_analysis'):
            self._analysis_tools.merge_from_paths(analyses_to_merge, tmp_analysis)
          self.move(tmp_analysis, self._analysis_file)

    self._ensure_analysis_tmpdir()
    return Task.do_check_artifact_cache(self, vts, post_process_cached_vts=post_process_cached_vts)

  def _write_to_artifact_cache(self, analysis_file, vts, sources_by_target):
    vt_by_target = dict([(vt.target, vt) for vt in vts.versioned_targets])

    split_analysis_files = [
        JvmCompile._analysis_for_target(self._analysis_tmpdir, t) for t in vts.targets]
    portable_split_analysis_files = [
        JvmCompile._portable_analysis_for_target(self._analysis_tmpdir, t) for t in vts.targets]

    # Set up args for splitting the analysis into per-target files.
    splits = zip([sources_by_target.get(t, []) for t in vts.targets], split_analysis_files)
    splits_args_tuples = [(analysis_file, splits)]

    # Set up args for rebasing the splits.
    relativize_args_tuples = zip(split_analysis_files, portable_split_analysis_files)

    # Set up args for artifact cache updating.
    vts_artifactfiles_pairs = []
    classes_by_source = self._compute_classes_by_source(analysis_file)
    for target, sources in sources_by_target.items():
      artifacts = []
      for source in sources:
        artifacts.extend(classes_by_source.get(source, []))
      vt = vt_by_target.get(target)
      if vt is not None:
        # NOTE: analysis_file doesn't exist yet.
        vts_artifactfiles_pairs.append(
            (vt,
             artifacts + [JvmCompile._portable_analysis_for_target(self._analysis_tmpdir, target)]))

    update_artifact_cache_work = self.get_update_artifact_cache_work(vts_artifactfiles_pairs)
    if update_artifact_cache_work:
      work_chain = [
        Work(self._analysis_tools.split_to_paths, splits_args_tuples, 'split'),
        Work(self._analysis_tools.relativize, relativize_args_tuples, 'relativize'),
        update_artifact_cache_work
      ]
      self.context.submit_background_work_chain(work_chain, parent_workunit_name='cache')

  def _compute_classes_by_source(self, analysis_file=None):
    """Compute src->classes.

    Srcs are relative to buildroot. Classes are absolute paths.
    """
    if analysis_file is None:
      analysis_file = self._analysis_file

    if not os.path.exists(analysis_file):
      return {}
    buildroot = get_buildroot()
    products = self._analysis_parser.parse_products_from_path(analysis_file)
    classes_by_src = {}
    for src, classes in products.items():
      relsrc = os.path.relpath(src, buildroot)
      classes_by_src[relsrc] = classes
    return classes_by_src

  def _deleted_sources(self):
    """Returns the list of sources present in the last analysis that have since been deleted.

    This is a global list. We have no way of associating them to individual targets.
    Paths are relative to buildroot.
    """
    # We compute the list lazily.
    if self._lazy_deleted_sources is None:
      with self.context.new_workunit('find-deleted-sources'):
        if os.path.exists(self._analysis_file):
          products = self._analysis_parser.parse_products_from_path(self._analysis_file)
          buildroot = get_buildroot()
          old_sources = products.keys()  # Absolute paths.
          self._lazy_deleted_sources = [os.path.relpath(src, buildroot) for src in old_sources
                                        if not os.path.exists(src)]
        else:
          self._lazy_deleted_sources = []
    return self._lazy_deleted_sources

  def _compute_sources_by_target(self, targets):
    """Returns map target -> list of sources (relative to buildroot)."""
    def calculate_sources(target):
      sources = [s for s in target.sources_relative_to_buildroot() if s.endswith(self._file_suffix)]
      # TODO: Make this less hacky. Ideally target.java_sources will point to sources, not targets.
      if hasattr(target, 'java_sources') and target.java_sources:
        sources.extend(self._resolve_target_sources(target.java_sources, '.java'))
      return sources
    return dict([(t, calculate_sources(t)) for t in targets])

  def _resolve_target_sources(self, target_sources, extension=None, relative_to_target_base=False):
    """Given a list of pants targets, extract their sources as a list.

    Filters against the extension if given and optionally returns the paths relative to the target
    base.
    """
    resolved_sources = []
    for resolved in Target.resolve_all(target_sources):
      if hasattr(resolved, 'sources'):
        resolved_sources.extend(
          source if relative_to_target_base else os.path.join(resolved.target_base, source)
          for source in resolved.sources if not extension or source.endswith(extension)
        )
    return resolved_sources

  def _compute_classpath_elements_by_class(self, classpath):
    # Don't consider loose classes dirs in our classpath. Those will be considered
    # separately, by looking at products.
    def non_product(path):
      return not (path.startswith(self._pants_workdir) and os.path.isdir(path))
    classpath_jars = filter(non_product, classpath)
    if self._class_to_jarfile is None:
      self._class_to_jarfile = {}
      for jarpath in self.find_all_bootstrap_jars() + classpath_jars:
        # Per the classloading spec, a 'jar' in this context can also be a .zip file.
        if os.path.isfile(jarpath) and ((jarpath.endswith('.jar') or jarpath.endswith('.zip'))):
          with open_zip(jarpath, 'r') as jar:
            for cls in jar.namelist():
              # First jar with a given class wins, just like when classloading.
              if cls.endswith('.class') and not cls in self._class_to_jarfile:
                self._class_to_jarfile[cls] = jarpath
        elif os.path.isdir(jarpath):
          for dirpath, _, filenames in os.walk(jarpath, followlinks=True):
            for f in filter(lambda x: x.endswith('.class'), filenames):
              cls = os.path.relpath(os.path.join(dirpath, f), jarpath)
              if not cls in self._class_to_jarfile:
                self._class_to_jarfile[cls] = jarpath
    return self._class_to_jarfile

  def find_all_bootstrap_jars(self):
    def get_path(key):
      return self.context.java_sysprops.get(key, '').split(':')

    def find_jars_in_dirs(dirs):
      ret = []
      for d in dirs:
        if os.path.isdir(d):
          ret.extend(filter(lambda s: s.endswith('.jar'), os.listdir(d)))
      return ret

    # Note: assumes HotSpot, or some JVM that supports sun.boot.class.path.
    # TODO: Support other JVMs? Not clear if there's a standard way to do so.
    # May include loose classes dirs.
    boot_classpath = get_path('sun.boot.class.path')

    # Note that per the specs, overrides and extensions must be in jars.
    # Loose class files will not be found by the JVM.
    override_jars = find_jars_in_dirs(get_path('java.endorsed.dirs'))
    extension_jars = find_jars_in_dirs(get_path('java.ext.dirs'))

    # Note that this order matters: it reflects the classloading order.
    bootstrap_jars = filter(os.path.isfile, override_jars + boot_classpath + extension_jars)
    return bootstrap_jars  # Technically, may include loose class dirs from boot_classpath.

  @property
  def _analysis_tools(self):
    if self._lazy_analysis_tools is None:
      self._lazy_analysis_tools = self.create_analysis_tools()
    return self._lazy_analysis_tools

  @property
  def _analysis_parser(self):
    return self._analysis_tools.parser

  def _ensure_analysis_tmpdir(self):
    # Do this lazily, so we don't trigger creation of a worker pool unless we need it.
    if not os.path.exists(self._analysis_tmpdir):
      os.makedirs(self._analysis_tmpdir)
      if self._delete_scratch:
        self.context.background_worker_pool().add_shutdown_hook(
            lambda: safe_rmtree(self._analysis_tmpdir))

  def _create_empty_products(self):
    make_products = lambda: defaultdict(MultipleRootedProducts)
    if self.context.products.is_required_data('classes_by_source'):
      self.context.products.safe_create_data('classes_by_source', make_products)
    if self.context.products.is_required_data('classes_by_target'):
      self.context.products.safe_create_data('classes_by_target', make_products)
    if self.context.products.is_required_data('resources_by_target'):
      self.context.products.safe_create_data('resources_by_target', make_products)

  def _register_products(self, targets, sources_by_target, analysis_file):
    classes_by_source = self.context.products.get_data('classes_by_source')
    classes_by_target = self.context.products.get_data('classes_by_target')
    resources_by_target = self.context.products.get_data('resources_by_target')

    if classes_by_source is not None or classes_by_target is not None:
      computed_classes_by_source = self._compute_classes_by_source(analysis_file)
      for target in targets:
        target_products = classes_by_target[target] if classes_by_target is not None else None
        for source in sources_by_target[target]:  # Source is relative to buildroot.
          classes = computed_classes_by_source.get(source, [])  # Classes are absolute paths.
          if classes_by_target is not None:
            target_products.add_abs_paths(self._classes_dir, classes)
          if classes_by_source is not None:
            classes_by_source[source].add_abs_paths(self._classes_dir, classes)

    if resources_by_target is not None:
      for target in targets:
        target_resources = resources_by_target[target]
        for root, abs_paths in self.extra_products(target):
          target_resources.add_abs_paths(root, abs_paths)

########NEW FILE########
__FILENAME__ = jvm_dependency_analyzer

import os
from collections import defaultdict

from twitter.common.collections import OrderedSet

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.targets.jar_dependency import JarDependency
from twitter.pants.targets.jar_library import JarLibrary
from twitter.pants.targets.jvm_target import JvmTarget
from twitter.pants.tasks import TaskError, Task


class JvmDependencyAnalyzer(object):
  def __init__(self,
               context,
               check_missing_deps,
               check_missing_direct_deps,
               check_unnecessary_deps):

    self._context = context
    self._context.products.require_data('classes_by_target')
    self._context.products.require_data('ivy_jar_products')

    self._check_missing_deps = check_missing_deps
    self._check_missing_direct_deps = check_missing_direct_deps
    self._check_unnecessary_deps = check_unnecessary_deps

  def _compute_targets_by_file(self):
    """Returns a map from abs path of source, class or jar file to an OrderedSet of targets.

    The value is usually a singleton, because a source or class file belongs to a single target.
    However a single jar may be provided (transitively or intransitively) by multiple JarLibrary
    targets. But if there is a JarLibrary target that depends on a jar directly, then that
    "canonical" target will be the first one in the list of targets.
    """
    targets_by_file = defaultdict(OrderedSet)

    # Multiple JarLibrary targets can provide the same (org, name).
    jarlibs_by_id = defaultdict(set)

    # Compute src -> target.
    with self._context.new_workunit(name='map_sources'):
      buildroot = get_buildroot()
      # Look at all targets in-play for this pants run. Does not include synthetic targets,
      for target in self._context.targets():
        if isinstance(target, JvmTarget):
          for src in target.sources_relative_to_buildroot():
            targets_by_file[os.path.join(buildroot, src)].add(target)
        elif isinstance(target, JarLibrary):
          for jardep in target.dependencies:
            if isinstance(jardep, JarDependency):
              jarlibs_by_id[(jardep.org, jardep.name)].add(target)

    # Compute class -> target.
    with self._context.new_workunit(name='map_classes'):
      classes_by_target = self._context.products.get_data('classes_by_target')
      for tgt, target_products in classes_by_target.items():
        for _, classes in target_products.abs_paths():
          for cls in classes:
            targets_by_file[cls].add(tgt)

    # Compute jar -> target.
    with self._context.new_workunit(name='map_jars'):
      with Task.symlink_map_lock:
        all_symlinks_map = self._context.products.get_data('symlink_map').copy()
        # We make a copy, so it's safe to use outside the lock.

      def register_transitive_jars_for_ref(ivyinfo, ref):
        deps_by_ref_memo = {}

        def get_transitive_jars_by_ref(ref1, visited=None):
          if ref1 in deps_by_ref_memo:
            return deps_by_ref_memo[ref1]
          else:
            visited = visited or set()
            if ref1 in visited:
              return set()  # Ivy allows circular deps.
            visited.add(ref1)
            jars = set()
            jars.update(ivyinfo.modules_by_ref[ref1].artifacts)
            for dep in ivyinfo.deps_by_caller.get(ref1, []):
              jars.update(get_transitive_jars_by_ref(dep, visited))
            deps_by_ref_memo[ref1] = jars
            return jars

        target_key = (ref.org, ref.name)
        if target_key in jarlibs_by_id:
          # These targets provide all the jars in ref, and all the jars ref transitively depends on.
          jarlib_targets = jarlibs_by_id[target_key]

          for jar in get_transitive_jars_by_ref(ref):
            # Register that each jarlib_target provides jar (via all its symlinks).
            symlinks = all_symlinks_map.get(os.path.realpath(jar.path), [])
            for symlink in symlinks:
              for jarlib_target in jarlib_targets:
                targets_by_file[symlink].add(jarlib_target)

      ivy_products = self._context.products.get_data('ivy_jar_products')
      if ivy_products:
        for ivyinfos in ivy_products.values():
          for ivyinfo in ivyinfos:
            for ref in ivyinfo.modules_by_ref:
              register_transitive_jars_for_ref(ivyinfo, ref)

    return targets_by_file

  def _compute_transitive_deps_by_target(self):
    """Map from target to all the targets it depends on, transitively."""
    # Sort from least to most dependent.
    sorted_targets = reversed(InternalTarget.sort_targets(self._context.targets()))
    transitive_deps_by_target = defaultdict(set)
    # Iterate in dep order, to accumulate the transitive deps for each target.
    for target in sorted_targets:
      transitive_deps = set()
      if hasattr(target, 'dependencies'):
        for dep in target.dependencies:
          transitive_deps.update(transitive_deps_by_target.get(dep, []))
          transitive_deps.add(dep)
        transitive_deps_by_target[target] = transitive_deps
    return transitive_deps_by_target

  def check(self, srcs, actual_deps):
    """Check for missing deps.

    See docstring for _compute_missing_deps for details.
    """
    if self._check_missing_deps or self._check_missing_direct_deps or self._check_unnecessary_deps:
      missing_file_deps, missing_tgt_deps, missing_direct_tgt_deps = \
        self._compute_missing_deps(srcs, actual_deps)

      buildroot = get_buildroot()
      def shorten(path):  # Make the output easier to read.
        for prefix in [buildroot, self._context.ivy_home]:
          if path.startswith(prefix):
            return os.path.relpath(path, prefix)
        return path

      if self._check_missing_deps and (missing_file_deps or missing_tgt_deps):
        for (tgt_pair, evidence) in missing_tgt_deps:
          evidence_str = '\n'.join(['    %s uses %s' % (shorten(e[0]), shorten(e[1]))
                                    for e in evidence])
          self._context.log.error(
              'Missing BUILD dependency %s -> %s because:\n%s'
              % (tgt_pair[0].address.reference(), tgt_pair[1].address.reference(), evidence_str))
        for (src_tgt, dep) in missing_file_deps:
          self._context.log.error('Missing BUILD dependency %s -> %s'
                                  % (src_tgt.address.reference(), shorten(dep)))
        if self._check_missing_deps == 'fatal':
          raise TaskError('Missing deps.')

      if self._check_missing_direct_deps:
        for (tgt_pair, evidence) in missing_direct_tgt_deps:
          evidence_str = '\n'.join(['    %s uses %s' % (shorten(e[0]), shorten(e[1]))
                                    for e in evidence])
          self._context.log.warn('Missing direct BUILD dependency %s -> %s because:\n%s' %
                                  (tgt_pair[0].address, tgt_pair[1].address, evidence_str))
        if self._check_missing_direct_deps == 'fatal':
          raise TaskError('Missing direct deps.')

      if self._check_unnecessary_deps:
        raise TaskError('Unnecessary dep warnings not implemented yet.')

  def _compute_missing_deps(self, srcs, actual_deps):
    """Computes deps that are used by the compiler but not specified in a BUILD file.

    These deps are bugs waiting to happen: the code may happen to compile because the dep was
    brought in some other way (e.g., by some other root target), but that is obviously fragile.

    Note that in practice we're OK with reliance on indirect deps that are only brought in
    transitively. E.g., in Scala type inference can bring in such a dep subtly. Fortunately these
    cases aren't as fragile as a completely missing dependency. It's still a good idea to have
    explicit direct deps where relevant, so we optionally warn about indirect deps, to make them
    easy to find and reason about.

    - actual_deps: a map src -> list of actual deps (source, class or jar file) as noted by the
      compiler.

    Returns a triple (missing_file_deps, missing_tgt_deps, missing_direct_tgt_deps) where:

    - missing_file_deps: a list of pairs (src_tgt, dep_file) where src_tgt requires dep_file, and
      we're unable to map to a target (because its target isn't in the total set of targets in play,
      and we don't want to parse every BUILD file in the workspace just to find it).

    - missing_tgt_deps: a list of pairs (src_tgt, dep_tgt) where src_tgt is missing a necessary
                        transitive dependency on dep_tgt.

    - missing_direct_tgt_deps: a list of pairs (src_tgt, dep_tgt) where src_tgt is missing a direct
                               dependency on dep_tgt but has a transitive dep on it.

    All paths in the input and output are absolute.
    """
    def must_be_explicit_dep(dep):
      # We don't require explicit deps on the java runtime, so we shouldn't consider that
      # a missing dep.
      return not dep.startswith(self._context.java_home)

    # TODO: If recomputing these every time becomes a performance issue, memoize for
    # already-seen targets and incrementally compute for new targets not seen in a previous
    # partition, in this or a previous chunk.
    targets_by_file = self._compute_targets_by_file()
    transitive_deps_by_target = self._compute_transitive_deps_by_target()

    # Find deps that are actual but not specified.
    with self._context.new_workunit(name='scan_deps'):
      missing_file_deps = OrderedSet()  # (src, src).
      missing_tgt_deps_map = defaultdict(list)  # (tgt, tgt) -> a list of (src, src) as evidence.
      missing_direct_tgt_deps_map = defaultdict(list)  # The same, but for direct deps.

      buildroot = get_buildroot()
      abs_srcs = [os.path.join(buildroot, src) for src in srcs]
      for src in abs_srcs:
        src_tgt = next(iter(targets_by_file.get(src)))
        if src_tgt is not None:
          for actual_dep in filter(must_be_explicit_dep, actual_deps.get(src, [])):
            actual_dep_tgts = targets_by_file.get(actual_dep)
            # actual_dep_tgts is usually a singleton. If it's not, we only need one of these
            # to be in our declared deps to be OK.
            if actual_dep_tgts is None:
              missing_file_deps.add((src_tgt, actual_dep))
            elif src_tgt not in actual_dep_tgts:  # Obviously intra-target deps are fine.
              canonical_actual_dep_tgt = next(iter(actual_dep_tgts))
              if actual_dep_tgts.isdisjoint(transitive_deps_by_target.get(src_tgt, [])):
                missing_tgt_deps_map[(src_tgt, canonical_actual_dep_tgt)].append((src, actual_dep))
              elif canonical_actual_dep_tgt not in src_tgt.dependencies:
                # The canonical dep is the only one a direct dependency makes sense on.
                missing_direct_tgt_deps_map[(src_tgt, canonical_actual_dep_tgt)].append(
                    (src, actual_dep))
        else:
          raise TaskError('Requested dep info for unknown source file: %s' % src)

    return (list(missing_file_deps),
            missing_tgt_deps_map.items(),
            missing_direct_tgt_deps_map.items())

########NEW FILE########
__FILENAME__ = scala_compile
import os
from twitter.pants.targets.scala_library import ScalaLibrary
from twitter.pants.tasks.jvm_compile.analysis_tools import AnalysisTools
from twitter.pants.tasks.jvm_compile.jvm_compile import JvmCompile
from twitter.pants.tasks.jvm_compile.scala.zinc_analysis import ZincAnalysis
from twitter.pants.tasks.jvm_compile.scala.zinc_analysis_parser import ZincAnalysisParser
from twitter.pants.tasks.jvm_compile.scala.zinc_utils import ZincUtils


class ScalaCompile(JvmCompile):
  _language = 'scala'
  _file_suffix = '.scala'
  _config_section = 'scala-compile'

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    JvmCompile.setup_parser(ScalaCompile, option_group, args, mkflag)

    option_group.add_option(mkflag('plugins'), dest='plugins', default=None,
      action='append', help='Use these scalac plugins. Default is set in pants.ini.')

  def __init__(self, context):
    JvmCompile.__init__(self, context, jdk=False)

    # Set up the zinc utils.
    color = not context.options.no_color
    self._zinc_utils = ZincUtils(context=context,
                                 nailgun_task=self,
                                 jvm_options = self._jvm_options,
                                 color=color,
                                 jvm_tool_bootstrapper=self._jvm_tool_bootstrapper)

    # If we are compiling scala libraries with circular deps on java libraries we need to
    # make sure those cycle deps are present.
    for scala_target in self.context.targets(lambda t: isinstance(t, ScalaLibrary)):
      for java_target in scala_target.java_sources:
        self.context.add_target(java_target)

  def create_analysis_tools(self):
    return AnalysisTools(self.context, ZincAnalysisParser(self._classes_dir), ZincAnalysis)

  def extra_classpath_elements(self):
    # Classpath entries necessary for our compiler plugins.
    return self._zinc_utils.plugin_jars()

  def extra_products(self, target):
      ret = []
      if target.is_scalac_plugin and target.classname:
        root, plugin_info_file = ZincUtils.write_plugin_info(self._resources_dir, target)
        ret.append((root, [plugin_info_file]))
      return ret

  def compile(self, args, classpath, sources, classes_output_dir, analysis_file):
    # We have to treat our output dir as an upstream element, so zinc can find valid
    # analysis for previous partitions. We use the global valid analysis for the upstream.
    upstream = { classes_output_dir: self._analysis_file } if os.path.exists(self._analysis_file) else {}
    return self._zinc_utils.compile(args, classpath + [self._classes_dir], sources,
                                    classes_output_dir, analysis_file, upstream)

########NEW FILE########
__FILENAME__ = zinc_analysis
from collections import defaultdict
import json
import os
import re
import itertools
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.tasks.jvm_compile.analysis import Analysis


class ZincAnalysisElement(object):
  """Encapsulates one part of the analysis.

  Subclasses specify which section headers comprise this part. Note that data in these objects is
  just text, possibly split on lines or '->'.
  """
  headers = ()  # Override in subclasses.

  @classmethod
  def from_json_obj(cls, obj):
    return cls([obj[header] for header in cls.headers])

  def __init__(self, args):
    # Subclasses can alias the elements of self.args in their own __init__, for convenience.
    self.args = args

  def write(self, outfile, inline_vals=True, rebasings=None):
    self._write_multiple_sections(outfile, self.headers, self.args, inline_vals, rebasings)

  def _write_multiple_sections(self, outfile, headers, reps, inline_vals=True, rebasings=None):
    """Write multiple sections."""
    for header, rep in zip(headers, reps):
      self._write_section(outfile, header, rep, inline_vals, rebasings)

  def _write_section(self, outfile, header, rep, inline_vals=True, rebasings=None):
    """Write a single section.

    Items are sorted, for ease of testing.
    """
    def rebase(txt):
      for rebase_from, rebase_to in rebasings:
        if rebase_to is None:
          if rebase_from in txt:
            return None
        else:
          txt = txt.replace(rebase_from, rebase_to)
      return txt

    rebasings = rebasings or []
    items = []
    for k, vals in rep.iteritems():
      for v in vals:
        item = rebase('%s -> %s%s' % (k, '' if inline_vals else '\n', v))
        if item:
          items.append(item)
    items.sort()
    outfile.write(header + ':\n')
    outfile.write('%d items\n' % len(items))
    for item in items:
      outfile.write(item)
      outfile.write('\n')


class ZincAnalysis(Analysis):
  """Parsed representation of a zinc analysis.

  Note also that all files in keys/values are full-path, just as they appear in the analysis file.
  If you want paths relative to the build root or the classes dir or whatever, you must compute
  those yourself.
  """

  # Implementation of class method required by Analysis.

  FORMAT_VERSION_LINE = 'format version: 4\n'

  @staticmethod
  def merge_dicts(dicts):
    """Merges multiple dicts into one.

    Assumes keys don't overlap.
    """
    ret = defaultdict(list)
    for d in dicts:
      ret.update(d)
    return ret

  @classmethod
  def merge(cls, analyses):
    # Note: correctly handles "internalizing" external deps that must be internal post-merge.

    # Merge relations.
    src_prod = ZincAnalysis.merge_dicts([a.relations.src_prod for a in analyses])
    binary_dep = ZincAnalysis.merge_dicts([a.relations.binary_dep for a in analyses])
    classes = ZincAnalysis.merge_dicts([a.relations.classes for a in analyses])
    used = ZincAnalysis.merge_dicts([a.relations.used for a in analyses])

    class_to_source = dict((v, k) for k, vs in classes.iteritems() for v in vs)

    def merge_dependencies(internals, externals):
      internal = ZincAnalysis.merge_dicts(internals)
      naive_external = ZincAnalysis.merge_dicts(externals)
      external = defaultdict(list)
      for k, vs in naive_external.iteritems():
        for v in vs:
          vfile = class_to_source.get(v)
          if vfile and vfile in src_prod:
            internal[k].append(vfile)  # Internalized.
          else:
            external[k].append(v)  # Remains external.
      return internal, external

    internal, external = merge_dependencies(
      [a.relations.internal_src_dep for a in analyses],
      [a.relations.external_dep for a in analyses])

    internal_pi, external_pi = merge_dependencies(
      [a.relations.internal_src_dep_pi for a in analyses],
      [a.relations.external_dep_pi for a in analyses])

    member_ref_internal, member_ref_external = merge_dependencies(
      [a.relations.member_ref_internal_dep for a in analyses],
      [a.relations.member_ref_external_dep for a in analyses])

    inheritance_internal, inheritance_external = merge_dependencies(
      [a.relations.inheritance_internal_dep for a in analyses],
      [a.relations.inheritance_external_dep for a in analyses])

    relations = Relations((src_prod, binary_dep,
                           internal, external,
                           internal_pi, external_pi,
                           member_ref_internal, member_ref_external,
                           inheritance_internal, inheritance_external,
                           classes, used))

    # Merge stamps.
    products = ZincAnalysis.merge_dicts([a.stamps.products for a in analyses])
    sources = ZincAnalysis.merge_dicts([a.stamps.sources for a in analyses])
    binaries = ZincAnalysis.merge_dicts([a.stamps.binaries for a in analyses])
    classnames = ZincAnalysis.merge_dicts([a.stamps.classnames for a in analyses])
    stamps = Stamps((products, sources, binaries, classnames))

    # Merge APIs.
    internal_apis = ZincAnalysis.merge_dicts([a.apis.internal for a in analyses])
    naive_external_apis = ZincAnalysis.merge_dicts([a.apis.external for a in analyses])
    external_apis = defaultdict(list)
    for k, vs in naive_external_apis.iteritems():
      kfile = class_to_source.get(k)
      if kfile and kfile in src_prod:
        internal_apis[kfile] = vs  # Internalized.
      else:
        external_apis[k] = vs  # Remains external.
    apis = APIs((internal_apis, external_apis))

    # Merge source infos.
    source_infos = SourceInfos((ZincAnalysis.merge_dicts([a.source_infos.source_infos for a in analyses]), ))

    # Merge compilations.
    compilation_vals = sorted(set([x[0] for a in analyses for x in a.compilations.compilations.itervalues()]))
    compilations_dict = defaultdict(list)
    for i, v in enumerate(compilation_vals):
      compilations_dict['%03d' % i] = [v]
    compilations = Compilations((compilations_dict, ))

    compile_setup = analyses[0].compile_setup if len(analyses) > 0 else CompileSetup((defaultdict(list), ))
    return ZincAnalysis(relations, stamps, apis, source_infos, compilations, compile_setup)

  def __init__(self, relations, stamps, apis, source_infos, compilations, compile_setup):
    (self.relations, self.stamps, self.apis, self.source_infos, self.compilations, self.compile_setup) = \
      (relations, stamps, apis, source_infos, compilations, compile_setup)

  # Impelementation of methods required by Analysis.

  def split(self, splits, catchall=False):
    # Note: correctly handles "externalizing" internal deps that must be external post-split.
    buildroot = get_buildroot()
    splits = [set([s if os.path.isabs(s) else os.path.join(buildroot, s) for s in x]) for x in splits]
    if catchall:
      # Even empty sources with no products have stamps.
      remainder_sources = set(self.stamps.sources.keys()).difference(*splits)
      splits.append(remainder_sources)  # The catch-all

    # Split relations.
    src_prod_splits = self._split_dict(self.relations.src_prod, splits)
    binary_dep_splits = self._split_dict(self.relations.binary_dep, splits)
    classes_splits = self._split_dict(self.relations.classes, splits)

    # For historical reasons, external deps are specified as src->class while internal deps are
    # specified as src->src. So we pick a representative class for each src.
    representatives = dict((k, min(vs)) for k, vs in self.relations.classes.iteritems())

    def split_dependencies(all_internal, all_external):
      naive_internals = self._split_dict(all_internal, splits)
      naive_externals = self._split_dict(all_external, splits)

      internals = []
      externals = []
      for naive_internal, external, split in zip(naive_internals, naive_externals, splits):
        internal = defaultdict(list)
        for k, vs in naive_internal.iteritems():
          for v in vs:
            if v in split:
              internal[k].append(v)  # Remains internal.
            else:
              external[k].append(representatives[v])  # Externalized.
        internals.append(internal)
        externals.append(external)
      return internals, externals

    internal_splits, external_splits = \
      split_dependencies(self.relations.internal_src_dep, self.relations.external_dep)
    internal_pi_splits, external_pi_splits = \
      split_dependencies(self.relations.internal_src_dep_pi, self.relations.external_dep_pi)

    member_ref_internal_splits, member_ref_external_splits = \
      split_dependencies(self.relations.member_ref_internal_dep, self.relations.member_ref_external_dep)
    inheritance_internal_splits, inheritance_external_splits = \
      split_dependencies(self.relations.inheritance_internal_dep, self.relations.inheritance_external_dep)
    used_splits = self._split_dict(self.relations.used, splits)

    relations_splits = []
    for args in zip(src_prod_splits, binary_dep_splits,
                    internal_splits, external_splits,
                    internal_pi_splits, external_pi_splits,
                    member_ref_internal_splits, member_ref_external_splits,
                    inheritance_internal_splits, inheritance_external_splits,
                    classes_splits, used_splits):
      relations_splits.append(Relations(args))

    # Split stamps.
    stamps_splits = []
    for src_prod, binary_dep, split in zip(src_prod_splits, binary_dep_splits, splits):
      products_set = set(itertools.chain(*src_prod.values()))
      binaries_set = set(itertools.chain(*binary_dep.values()))
      products = dict((k, v) for k, v in self.stamps.products.iteritems() if k in products_set)
      sources = dict((k, v) for k, v in self.stamps.sources.iteritems() if k in split)
      binaries = dict((k, v) for k, v in self.stamps.binaries.iteritems() if k in binaries_set)
      classnames = dict((k, v) for k, v in self.stamps.classnames.iteritems() if k in binaries_set)
      stamps_splits.append(Stamps((products, sources, binaries, classnames)))

    # Split apis.

    # The splits, but expressed via class representatives of the sources (see above).
    representative_splits = [filter(None, [representatives.get(s) for s in srcs]) for srcs in splits]
    representative_to_internal_api = {}
    for src, rep in representatives.items():
      representative_to_internal_api[rep] = self.apis.internal.get(src)

    # Note that the keys in self.apis.external are classes, not sources.
    internal_api_splits = self._split_dict(self.apis.internal, splits)
    external_api_splits = self._split_dict(self.apis.external, representative_splits)

    # All externalized deps require a copy of the relevant api.
    for external, external_api in zip(external_splits, external_api_splits):
      for vs in external.values():
        for v in vs:
          if v in representative_to_internal_api:
            external_api[v] = representative_to_internal_api[v]

    apis_splits = []
    for args in zip(internal_api_splits, external_api_splits):
      apis_splits.append(APIs(args))

    # Split source infos.
    source_info_splits = \
      [SourceInfos((x, )) for x in self._split_dict(self.source_infos.source_infos, splits)]

    analyses = []
    for relations, stamps, apis, source_infos in zip(relations_splits, stamps_splits, apis_splits, source_info_splits):
      analyses.append(ZincAnalysis(relations, stamps, apis, source_infos, self.compilations, self.compile_setup))

    return analyses

  def write(self, outfile, rebasings=None):
    outfile.write(ZincAnalysis.FORMAT_VERSION_LINE)
    self.relations.write(outfile, rebasings=rebasings)
    self.stamps.write(outfile, rebasings=rebasings)
    self.apis.write(outfile, inline_vals=False, rebasings=rebasings)
    self.source_infos.write(outfile, inline_vals=False, rebasings=rebasings)
    self.compilations.write(outfile, inline_vals=True, rebasings=rebasings)
    self.compile_setup.write(outfile, inline_vals=True, rebasings=rebasings)

  # Extra methods re json.

  def write_json_to_path(self, outfile_path):
    with open(outfile_path, 'w') as outfile:
      self.write_json(outfile)

  def write_json(self, outfile):
    obj = dict(zip(('relations', 'stamps', 'apis', 'source_infos', 'compilations', 'compile_setup'),
                     (self.relations, self.stamps, self.apis, self.source_infos, self.compilations, self.compile_setup)))
    json.dump(obj, outfile, cls=ZincAnalysisJSONEncoder, sort_keys=True, indent=2)

  def _split_dict(self, d, splits):
    """Split a dict by its keys.

    splits: A list of lists of keys.
    Returns one dict per split.
    """
    ret = []
    for split in splits:
      dict_split = defaultdict(list)
      for f in split:
        if f in d:
          dict_split[f] = d[f]
      ret.append(dict_split)
    return ret


class Relations(ZincAnalysisElement):
  headers = ('products', 'binary dependencies',
             # TODO: The following 4 headers will go away after SBT completes the
             # transition to the new headers (the 4 after that).
             'direct source dependencies', 'direct external dependencies',
             'public inherited source dependencies', 'public inherited external dependencies',
             'member reference internal dependencies', 'member reference external dependencies',
             'inheritance internal dependencies', 'inheritance external dependencies',
             'class names', 'used names')

  def __init__(self, args):
    super(Relations, self).__init__(args)
    (self.src_prod, self.binary_dep,
     self.internal_src_dep, self.external_dep,
     self.internal_src_dep_pi, self.external_dep_pi,
     self.member_ref_internal_dep, self.member_ref_external_dep,
     self.inheritance_internal_dep, self.inheritance_external_dep,
     self.classes, self.used) = self.args


class Stamps(ZincAnalysisElement):
  headers = ('product stamps', 'source stamps', 'binary stamps', 'class names')

  def __init__(self, args):
    super(Stamps, self).__init__(args)
    (self.products, self.sources, self.binaries, self.classnames) = self.args


class APIs(ZincAnalysisElement):
  headers = ('internal apis', 'external apis')

  def __init__(self, args):
    super(APIs, self).__init__(args)
    (self.internal, self.external) = self.args


class SourceInfos(ZincAnalysisElement):
  headers = ("source infos", )

  def __init__(self, args):
    super(SourceInfos, self).__init__(args)
    (self.source_infos, ) = self.args


class Compilations(ZincAnalysisElement):
  headers = ('compilations', )

  def __init__(self, args):
    super(Compilations, self).__init__(args)
    (self.compilations, ) = self.args


class CompileSetup(ZincAnalysisElement):
  headers = ('output mode', 'output directories','compile options','javac options',
             'compiler version', 'compile order')

  def __init__(self, args):
    super(CompileSetup, self).__init__(args)
    (self.output_mode, self.output_dirs, self.compile_options, self.javac_options,
     self.compiler_version, self.compile_order) = self.args


class ZincAnalysisJSONEncoder(json.JSONEncoder):
  """A custom encoder for writing analysis elements as JSON.

  Not currently used, but might be useful in the future, e.g., for creating javascript-y
  analysis browsing tools.
  """
  def default(self, obj):
    if isinstance(obj, ZincAnalysisElement):
      ret = {}
      for h, a in zip(type(obj).headers, obj.args):
        ret[h] = a
      return ret
    else:
      super(ZincAnalysisJSONEncoder, self).default(obj)

########NEW FILE########
__FILENAME__ = zinc_analysis_parser
from collections import defaultdict
import json
import os
import re

from twitter.pants.tasks.jvm_compile.analysis_parser import AnalysisParser, ParseError
from twitter.pants.tasks.jvm_compile.scala.zinc_analysis import (
    APIs,
    Compilations,
    CompileSetup,
    Relations,
    SourceInfos,
    Stamps,
    ZincAnalysis,
)


class ZincAnalysisParser(AnalysisParser):
  """Parses a zinc analysis file."""

  def empty_prefix(self):
    return 'products:\n0 items\n'

  def parse(self, infile):
    """Parse a ZincAnalysis instance from an open text file."""
    def parse_element(cls):
      parsed_sections = [self._parse_section(infile, header) for header in cls.headers]
      return cls(parsed_sections)

    self._verify_version(infile)
    relations = parse_element(Relations)
    stamps = parse_element(Stamps)
    apis = parse_element(APIs)
    source_infos = parse_element(SourceInfos)
    compilations = parse_element(Compilations)
    compile_setup = parse_element(CompileSetup)
    return ZincAnalysis(relations, stamps, apis, source_infos, compilations, compile_setup)

  def parse_products(self, infile):
    """An efficient parser of just the products section."""
    self._verify_version(infile)
    return self._find_repeated_at_header(infile, 'products')

  def parse_deps(self, infile, classpath_indexer):
    self._verify_version(infile)
    # Note: relies on the fact that these headers appear in this order in the file.
    bin_deps = self._find_repeated_at_header(infile, 'binary dependencies')
    src_deps = self._find_repeated_at_header(infile, 'direct source dependencies')
    ext_deps = self._find_repeated_at_header(infile, 'direct external dependencies')

    # TODO(benjy): Temporary hack until we inject a dep on the scala runtime jar.
    scalalib_re = re.compile(r'scala-library-\d+\.\d+\.\d+\.jar$')
    filtered_bin_deps = defaultdict(list)
    for src, deps in bin_deps.iteritems():
      filtered_bin_deps[src] = filter(lambda x: scalalib_re.search(x) is None, deps)

    transformed_ext_deps = {}
    def fqcn_to_path(fqcn):
      return os.path.join(self.classes_dir, fqcn.replace('.', os.sep) + '.class')
    for src, fqcns in ext_deps.items():
      transformed_ext_deps[src] = [fqcn_to_path(fqcn) for fqcn in fqcns]

    ret = defaultdict(list)
    for d in [filtered_bin_deps, src_deps, transformed_ext_deps]:
      ret.update(d)
    return ret

  # Extra zinc-specific methods re json.

  def parse_json_from_path(self, infile_path):
    """Parse a ZincAnalysis instance from a JSON file."""
    with open(infile_path, 'r') as infile:
      return self.parse_from_json(infile)

  def parse_from_json(self, infile):
    """Parse a ZincAnalysis instance from an open JSON file."""
    obj = json.load(infile)
    relations = Relations.from_json_obj(obj['relations'])
    stamps = Stamps.from_json_obj(obj['stamps'])
    apis = APIs.from_json_obj(obj['apis'])
    source_infos = SourceInfos.from_json_obj(obj['source infos'])
    compilations = Compilations.from_json_obj(obj['compilations'])
    compile_setup = Compilations.from_json_obj(obj['compile setup'])
    return ZincAnalysis(relations, stamps, apis, source_infos, compilations, compile_setup)

  def _find_repeated_at_header(self, lines_iter, header):
    header_line = header + ':\n'
    while lines_iter.next() != header_line:
      pass
    return self._parse_section(lines_iter, expected_header=None)

  def _verify_version(self, lines_iter):
    version_line = lines_iter.next()
    if version_line != ZincAnalysis.FORMAT_VERSION_LINE:
      raise ParseError('Unrecognized version line: ' + version_line)

  _num_items_re = re.compile(r'(\d+) items\n')

  def _parse_num_items(self, lines_iter):
    """Parse a line of the form '<num> items' and returns <num> as an int."""
    line = lines_iter.next()
    matchobj = self._num_items_re.match(line)
    if not matchobj:
      raise ParseError('Expected: "<num> items". Found: "%s"' % line)
    return int(matchobj.group(1))

  def _parse_section(self, lines_iter, expected_header=None):
    """Parse a single section."""
    if expected_header:
      line = lines_iter.next()
      if expected_header + ':\n' != line:
        raise ParseError('Expected: "%s:". Found: "%s"' % (expected_header, line))
    n = self._parse_num_items(lines_iter)
    relation = defaultdict(list)  # Values are lists, to accommodate relations.
    for i in xrange(n):
      k, _, v = lines_iter.next().partition(' -> ')
      if len(v) == 1:  # Value on its own line.
        v = lines_iter.next()
      relation[k].append(v[:-1])
    return relation

########NEW FILE########
__FILENAME__ = zinc_utils
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import textwrap

from contextlib import closing
from itertools import chain
from xml.etree import ElementTree

from twitter.common.collections import OrderedDict
from twitter.common.contextutil import open_zip as open_jar
from twitter.common.dirutil import  safe_open

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.hash_utils import hash_file
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.tasks import TaskError


# Well known metadata file required to register scalac plugins with nsc.
_PLUGIN_INFO_FILE = 'scalac-plugin.xml'

class ZincUtils(object):
  """Convenient wrapper around zinc invocations.

  Instances are immutable, and all methods are reentrant (assuming that the java_runner is).
  """
  _ZINC_MAIN = 'com.typesafe.zinc.Main'

  def __init__(self, context, nailgun_task, jvm_options, color, jvm_tool_bootstrapper):
    self.context = context
    self._nailgun_task = nailgun_task  # We run zinc on this task's behalf.
    self._jvm_options = jvm_options
    self._color = color
    self._jvm_tool_bootstrapper = jvm_tool_bootstrapper

    # The target scala version.
    self._compile_bootstrap_key = 'scalac'
    compile_bootstrap_tools = context.config.getlist('scala-compile', 'compile-bootstrap-tools',
                                                     default=[':scala-compile-2.9.3'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._compile_bootstrap_key, compile_bootstrap_tools)

    # The zinc version (and the scala version it needs, which may differ from the target version).
    self._zinc_bootstrap_key = 'zinc'
    zinc_bootstrap_tools = context.config.getlist('scala-compile', 'zinc-bootstrap-tools', default=[':zinc'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._zinc_bootstrap_key, zinc_bootstrap_tools)

    # Compiler plugins.
    plugins_bootstrap_tools = context.config.getlist('scala-compile', 'scalac-plugin-bootstrap-tools',
                                                     default=[])
    if plugins_bootstrap_tools:
      self._plugins_bootstrap_key = 'plugins'
      self._jvm_tool_bootstrapper.register_jvm_tool(self._plugins_bootstrap_key, plugins_bootstrap_tools)
    else:
      self._plugins_bootstrap_key = None

  @property
  def _zinc_classpath(self):
    return self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._zinc_bootstrap_key)

  @property
  def _compiler_classpath(self):
    return self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._compile_bootstrap_key)

  @property
  def _plugin_jars(self):
    if self._plugins_bootstrap_key:
      return self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._plugins_bootstrap_key)
    else:
      return []

  @property
  def _zinc_jar_args(self):
    zinc_jars = ZincUtils.identify_zinc_jars(self._zinc_classpath)
    # The zinc jar names are also the flag names.
    return (list(chain.from_iterable([['-%s' % name, jarpath]
                                     for (name, jarpath) in zinc_jars.items()])) +
            ['-scala-path', ':'.join(self._compiler_classpath)])

  def _plugin_args(self):
    # Allow multiple flags and also comma-separated values in a single flag.
    if self.context.options.plugins is not None:
      plugin_names = [p for val in self.context.options.plugins for p in val.split(',')]
    else:
      plugin_names = self.context.config.getlist('scala-compile', 'scalac-plugins', default=[])

    plugin_args = self.context.config.getdict('scala-compile', 'scalac-plugin-args', default={})
    active_plugins = self.find_plugins(plugin_names)

    ret = []
    for name, jar in active_plugins.items():
      ret.append('-S-Xplugin:%s' % jar)
      for arg in plugin_args.get(name, []):
        ret.append('-S-P:%s:%s' % (name, arg))
    return ret

  def plugin_jars(self):
    """The jars containing code for enabled plugins."""
    return self._plugin_jars

  def _run_zinc(self, args, workunit_name='zinc', workunit_labels=None):
    zinc_args = [
      '-log-level', self.context.options.log_level or 'info',
    ]
    if not self._color:
      zinc_args.append('-no-color')
    zinc_args.extend(self._zinc_jar_args)
    zinc_args.extend(args)
    return self._nailgun_task.runjava(classpath=self._zinc_classpath,
                                      main=ZincUtils._ZINC_MAIN,
                                      jvm_options=self._jvm_options,
                                      args=zinc_args,
                                      workunit_name=workunit_name,
                                      workunit_labels=workunit_labels)

  def compile(self, opts, classpath, sources, output_dir, analysis_file, upstream_analysis_files):
    args = list(opts)  # Make a copy

    args.extend(self._plugin_args())

    if upstream_analysis_files:
      args.extend(
        ['-analysis-map', ','.join(['%s:%s' % kv for kv in upstream_analysis_files.items()])])

    args.extend([
      '-analysis-cache', analysis_file,
      # We add compiler_classpath to ensure the scala-library jar is on the classpath.
      # TODO: This also adds the compiler jar to the classpath, which compiled code shouldn't
      # usually need. Be more selective?
      '-classpath', ':'.join(self._compiler_classpath + classpath),
      '-d', output_dir
    ])
    args.extend(sources)
    self.log_zinc_file(analysis_file)
    if self._run_zinc(args, workunit_labels=[WorkUnit.COMPILER]):
      raise TaskError('Zinc compile failed.')

  @staticmethod
  def write_plugin_info(resources_dir, target):
    root = os.path.join(resources_dir, target.id)
    plugin_info_file = os.path.join(root, _PLUGIN_INFO_FILE)
    with safe_open(plugin_info_file, 'w') as f:
      f.write(textwrap.dedent('''
        <plugin>
          <name>%s</name>
          <classname>%s</classname>
        </plugin>
      ''' % (target.plugin, target.classname)).strip())
    return root, plugin_info_file

  # These are the names of the various jars zinc needs. They are, conveniently and
  # non-coincidentally, the names of the flags used to pass the jar locations to zinc.
  zinc_jar_names = ['compiler-interface', 'sbt-interface' ]

  @staticmethod
  def identify_zinc_jars(zinc_classpath):
    """Find the named jars in the zinc classpath.

    TODO: Make these mappings explicit instead of deriving them by jar name heuristics.
    """
    ret = OrderedDict()
    ret.update(ZincUtils.identify_jars(ZincUtils.zinc_jar_names, zinc_classpath))
    return ret

  @staticmethod
  def identify_jars(names, jars):
    jars_by_name = {}
    jars_and_filenames = [(x, os.path.basename(x)) for x in jars]

    for name in names:
      jar_for_name = None
      for jar, filename in jars_and_filenames:
        if filename.startswith(name):
          jar_for_name = jar
          break
      if jar_for_name is None:
        raise TaskError('Couldn\'t find jar named %s' % name)
      else:
        jars_by_name[name] = jar_for_name
    return jars_by_name

  def find_plugins(self, plugin_names):
    """Returns a map from plugin name to plugin jar."""
    plugin_names = set(plugin_names)
    plugins = {}
    buildroot = get_buildroot()
    # plugin_jars is the universe of all possible plugins and their transitive deps.
    # Here we select the ones to actually use.
    for jar in self.plugin_jars():
      with open_jar(jar, 'r') as jarfile:
        try:
          with closing(jarfile.open(_PLUGIN_INFO_FILE, 'r')) as plugin_info_file:
            plugin_info = ElementTree.parse(plugin_info_file).getroot()
          if plugin_info.tag != 'plugin':
            raise TaskError(
              'File %s in %s is not a valid scalac plugin descriptor' % (_PLUGIN_INFO_FILE, jar))
          name = plugin_info.find('name').text
          if name in plugin_names:
            if name in plugins:
              raise TaskError('Plugin %s defined in %s and in %s' % (name, plugins[name], jar))
            # It's important to use relative paths, as the compiler flags get embedded in the zinc
            # analysis file, and we port those between systems via the artifact cache.
            plugins[name] = os.path.relpath(jar, buildroot)
        except KeyError:
          pass

    unresolved_plugins = plugin_names - set(plugins.keys())
    if unresolved_plugins:
      raise TaskError('Could not find requested plugins: %s' % list(unresolved_plugins))
    return plugins

  def log_zinc_file(self, analysis_file):
    self.context.log.debug('Calling zinc on: %s (%s)' % (analysis_file, hash_file(analysis_file).upper() if os.path.exists(analysis_file) else 'nonexistent'))

########NEW FILE########
__FILENAME__ = jvm_run
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import shlex

from twitter.common.dirutil import safe_open
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.java.executor import CommandLineGrabber
from twitter.pants.targets.jvm_binary import JvmBinary
from twitter.pants.java.util import execute_java

from .jvm_task import JvmTask

from . import Task, TaskError


def is_binary(target):
  return isinstance(target, JvmBinary)


class JvmRun(JvmTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("jvmargs"), dest = "run_jvmargs", action="append",
      help = "Run binary in a jvm with these extra jvm args.")

    option_group.add_option(mkflag("args"), dest = "run_args", action="append",
      help = "Run binary with these main() args.")

    option_group.add_option(mkflag("debug"), mkflag("debug", negate=True), dest = "run_debug",
      action="callback", callback=mkflag.set_bool, default=False,
      help = "[%default] Run binary with a debugger")

    option_group.add_option(mkflag('only-write-cmd-line'), dest = 'only_write_cmd_line',
      action='store', default=None,
      help = '[%default] Instead of running, just write the cmd line to this file')

  def __init__(self, context):
    Task.__init__(self, context)
    self.jvm_args = context.config.getlist('jvm-run', 'jvm_args', default=[])
    if context.options.run_jvmargs:
      for arg in context.options.run_jvmargs:
        self.jvm_args.extend(shlex.split(arg))
    self.args = []
    if context.options.run_args:
      for arg in context.options.run_args:
        self.args.extend(shlex.split(arg))
    if context.options.run_debug:
      self.jvm_args.extend(context.config.getlist('jvm', 'debug_args'))
    self.confs = context.config.getlist('jvm-run', 'confs', default=['default'])
    self.only_write_cmd_line = context.options.only_write_cmd_line
    context.products.require_data('exclusives_groups')

  def execute(self, targets):
    # The called binary may block for a while, allow concurrent pants activity during this pants
    # idle period.
    #
    # TODO(John Sirois): refactor lock so that I can do:
    # with self.context.lock.yield():
    #   - blocking code
    #
    # Currently re-acquiring the lock requires a path argument that was set up by the goal
    # execution engine.  I do not want task code to learn the lock location.
    # http://jira.local.twitter.com/browse/AWESOME-1317

    self.context.lock.release()
    # Run the first target that is a binary.
    binaries = filter(is_binary, targets)
    if len(binaries) > 0:  # We only run the first one.
      main = binaries[0].main
      egroups = self.context.products.get_data('exclusives_groups')
      group_key = egroups.get_group_key_for_target(binaries[0])
      group_classpath = egroups.get_classpath_for_group(group_key)

      executor = CommandLineGrabber() if self.only_write_cmd_line else None
      result = execute_java(
        classpath=(self.classpath(confs=self.confs, exclusives_classpath=group_classpath)),
        main=main,
        executor=executor,
        jvm_options=self.jvm_args,
        args=self.args,
        workunit_factory=self.context.new_workunit,
        workunit_name='run',
        workunit_labels=[WorkUnit.RUN]
      )

      if self.only_write_cmd_line:
        with safe_open(self.only_write_cmd_line, 'w') as outfile:
          outfile.write(' '.join(executor.cmd))
      elif result != 0:
        raise TaskError('java %s ... exited non-zero (%i)' % (main, result), exit_code=result)

########NEW FILE########
__FILENAME__ = jvm_task
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.tasks import Task


class JvmTask(Task):
  def get_base_classpath_for_target(self, target):
    """Note: to use this method, the exclusives_groups data product must be available. This should
    have been set by the prerequisite java/scala compile."""
    egroups = self.context.products.get_data('exclusives_groups')
    group_key = egroups.get_group_key_for_target(target)
    return egroups.get_classpath_for_group(group_key)

  def classpath(self, cp=None, confs=None, exclusives_classpath=None):
    classpath = list(cp) if cp else []
    exclusives_classpath = exclusives_classpath or []

    classpath.extend(path for conf, path in exclusives_classpath if not confs or conf in confs)

    def add_resource_paths(predicate):
      bases = set()
      for target in self.context.targets():
        if predicate(target):
          if target.target_base not in bases:
            sibling_resources_base = os.path.join(os.path.dirname(target.target_base), 'resources')
            classpath.append(os.path.join(get_buildroot(), sibling_resources_base))
            bases.add(target.target_base)

    if self.context.config.getbool('jvm', 'parallel_src_paths', default=False):
      add_resource_paths(lambda t: t.is_jvm and not t.is_test)

    if self.context.config.getbool('jvm', 'parallel_test_paths', default=False):
      add_resource_paths(lambda t: t.is_jvm and not t.is_test)

    return classpath

########NEW FILE########
__FILENAME__ = jvm_tool_bootstrapper
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.tasks.task_error import TaskError


class JvmToolBootstrapper(object):
  def __init__(self, products):
    self._products = products

  def get_jvm_tool_classpath(self, key, executor=None):
    """Get a classpath for the tool previously registered under the key.

    Returns a list of paths.
    """
    return self.get_lazy_jvm_tool_classpath(key, executor)()

  def get_lazy_jvm_tool_classpath(self, key, executor=None):
    """Get a lazy classpath for the tool previously registered under the key.

    Returns a no-arg callable. Invoking it returns a list of paths.
    """
    callback_product_map = self._products.get_data('jvm_build_tools_classpath_callbacks') or {}
    callback = callback_product_map.get(key)
    if not callback:
      raise TaskError('No bootstrap callback registered for %s' % key)
    return lambda: callback(executor=executor)

  def register_jvm_tool(self, key, tools):
    """Register a list of targets against a key.

    We can later use this key to get a callback that will resolve these targets.
    Note: Not reentrant. We assume that all registration is done in the main thread.
    """
    self._products.require_data('jvm_build_tools_classpath_callbacks')
    tool_product_map = self._products.get_data('jvm_build_tools') or {}
    existing = tool_product_map.get(key)
    # It's OK to re-register with the same value, but not to change the value.
    if existing is not None:
      if existing != tools:
        raise TaskError('Attemping to change tools under %s from %s to %s.'
                        % (key, existing, tools))
    else:
      tool_product_map[key] = tools
      self._products.safe_create_data('jvm_build_tools', lambda: tool_product_map)


########NEW FILE########
__FILENAME__ = listtargets
# =============================================================================
# Copyright 2012 Twitter, Inc.
# -----------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.build_file import BuildFile
from twitter.pants.base.target import Target

from .console_task import ConsoleTask
from .task_error import TaskError


class ListTargets(ConsoleTask):
  """
  Lists all BUILD targets in the system with no arguments, otherwise lists all
  the BUILD targets that reside in the the BUILD files hosting the specified
  targets.
  """
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(ListTargets, cls).setup_parser(option_group, args, mkflag)

    option_group.add_option(
        mkflag("provides"),
        action="store_true",
        dest="list_provides", default=False,
        help="Specifies only targets that provide an artifact should be "
        "listed. The output will be 2 columns in this case: "
        "[target address] [artifact id]")

    option_group.add_option(
        mkflag("provides-columns"),
        dest="list_provides_columns",
        default='address,artifact_id',
        help="Specifies the columns to include in listing output when "
        "restricting the listing to targets that provide an artifact. "
        "Available columns are: address, artifact_id, repo_name, repo_url "
        "and repo_db")

    option_group.add_option(
        mkflag("documented"),
        action="store_true",
        dest="list_documented",
        default=False,
        help="Prints only targets that are documented with a description.")

  def __init__(self, context, **kwargs):
    super(ListTargets, self).__init__(context, **kwargs)

    self._provides = context.options.list_provides
    self._provides_columns = context.options.list_provides_columns
    self._documented = context.options.list_documented
    self._root_dir = get_buildroot()

  def console_output(self, targets):
    if self._provides:
      def extract_artifact_id(target):
        provided_jar, _, _ = target.get_artifact_info()
        return "%s%s%s" % (provided_jar.org, '#', provided_jar.name)

      extractors = dict(
          address=lambda target: str(target.address),
          artifact_id=extract_artifact_id,
          repo_name=lambda target: target.provides.repo.name,
          repo_url=lambda target: target.provides.repo.url,
          repo_db=lambda target: target.provides.repo.push_db,
      )

      def print_provides(column_extractors, address):
        target = Target.get(address)
        if target.is_exported:
          return " ".join(extractor(target) for extractor in column_extractors)

      try:
        column_extractors = [extractors[col] for col in (self._provides_columns.split(','))]
      except KeyError:
        raise TaskError("Invalid columns specified %s. Valid ones include address, artifact_id, "
                        "repo_name, repo_url and repo_db." % self._provides_columns)

      print_fn = lambda address: print_provides(column_extractors, address)
    elif self._documented:
      def print_documented(address):
        target = Target.get(address)
        if target.description:
          return '%s\n  %s' % (address, '\n  '.join(target.description.strip().split('\n')))
      print_fn = print_documented
    else:
      print_fn = lambda addr: str(addr)

    visited = set()
    for address in self._addresses():
      result = print_fn(address)
      if result and result not in visited:
        visited.add(result)
        yield result

  def _addresses(self):
    if self.context.target_roots:
      for target in self.context.target_roots:
        yield target.address
    else:
      for buildfile in BuildFile.scan_buildfiles(self._root_dir):
        for address in Target.get_all_addresses(buildfile):
          yield address

########NEW FILE########
__FILENAME__ = list_goals
# =============================================================================
# Copyright 2014 Twitter, Inc.
# -----------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================

from twitter.pants.goal import Phase

from .console_task import ConsoleTask


class ListGoals(ConsoleTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(ListGoals, cls).setup_parser(option_group, args, mkflag)
    option_group.add_option(mkflag("all"),
                            dest="goal_list_all",
                            default=False,
                            action="store_true",
                            help="[%default] List all goals even if no description is available.")
    option_group.add_option(mkflag('graph'),
                            dest='goal_list_graph',
                            action='store_true',
                            help='[%default] Generate a graphviz graph of installed goals.')

  def console_output(self, targets):
    def report():
      yield 'Installed goals:'
      documented_rows = []
      undocumented = []
      max_width = 0
      for phase, _ in Phase.all():
        if phase.description:
          documented_rows.append((phase.name, phase.description))
          max_width = max(max_width, len(phase.name))
        elif self.context.options.goal_list_all:
          undocumented.append(phase.name)
      for name, description in documented_rows:
        yield '  %s: %s' % (name.rjust(max_width), description)
      if undocumented:
        yield ''
        yield 'Undocumented goals:'
        yield '  %s' % ' '.join(undocumented)

    def graph():
      def get_cluster_name(phase):
        return 'cluster_%s' % phase.name.replace('-', '_')

      def get_goal_name(phase, goal):
        name = '%s_%s' % (phase.name, goal.name)
        return name.replace('-', '_')

      phase_by_phasename = {}
      for phase, goals in Phase.all():
        phase_by_phasename[phase.name] = phase

      yield '\n'.join([
        'digraph G {',
        '  rankdir=LR;',
        '  graph [compound=true];',
        ])
      for phase, installed_goals in Phase.all():
        yield '\n'.join([
          '  subgraph %s {' % get_cluster_name(phase),
          '    node [style=filled];',
          '    color = blue;',
          '    label = "%s";' % phase.name,
        ])
        for installed_goal in installed_goals:
          yield '    %s [label="%s"];' % (get_goal_name(phase, installed_goal),
                                          installed_goal.name)
        yield '  }'

      edges = set()
      for phase, installed_goals in Phase.all():
        for installed_goal in installed_goals:
          for dependency in installed_goal.dependencies:
            tail_goal = phase_by_phasename.get(dependency.name).goals()[-1]
            edge = 'ltail=%s lhead=%s' % (get_cluster_name(phase),
                                          get_cluster_name(Phase.of(tail_goal)))
            if edge not in edges:
              yield '  %s -> %s [%s];' % (get_goal_name(phase, installed_goal),
                                          get_goal_name(Phase.of(tail_goal), tail_goal),
                                          edge)
            edges.add(edge)
      yield '}'

    return graph() if self.context.options.goal_list_graph else report()

########NEW FILE########
__FILENAME__ = markdown_to_html
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'John Sirois'

try:
  import markdown

  WIKILINKS_PATTERN = r'\[\[([^\]]+)\]\]'

  class WikilinksPattern(markdown.inlinepatterns.Pattern):
    def __init__(self, build_url, markdown_instance=None):
      markdown.inlinepatterns.Pattern.__init__(self, WIKILINKS_PATTERN, markdown_instance)
      self.build_url = build_url

    def handleMatch(self, m):
      alias, url = self.build_url(m.group(2).strip())
      el = markdown.util.etree.Element('a')
      el.set('href', url)
      el.text = markdown.util.AtomicString(alias)
      return el

  class WikilinksExtension(markdown.Extension):
    def __init__(self, build_url, configs=None):
      markdown.Extension.__init__(self, configs or {})
      self.build_url = build_url

    def extendMarkdown(self, md, md_globals):
      md.inlinePatterns['wikilinks'] = WikilinksPattern(self.build_url, md)

  HAS_MARKDOWN = True
except ImportError:
  HAS_MARKDOWN = False

try:
  from pygments.formatters.html import HtmlFormatter
  from pygments.styles import get_all_styles

  def configure_codehighlight_options(option_group, mkflag):
    all_styles = list(get_all_styles())
    option_group.add_option(mkflag("code-style"), dest="markdown_to_html_code_style",
                            type="choice", choices=all_styles,
                            help="Selects the stylesheet to use for code highlights, one of: "
                                 "%s." % ' '.join(all_styles))

  def emit_codehighlight_css(path, style):
    with safe_open(path, 'w') as css:
      css.write((HtmlFormatter(style=style)).get_style_defs('.codehilite'))
    return path
except ImportError:
  def configure_codehighlight_options(option_group, mkflag): pass
  def emit_codehighlight_css(path, style): pass


import codecs
import os
import re
import textwrap

from twitter.common.dirutil import safe_mkdir, safe_open

from twitter.pants import binary_util
from twitter.pants.base.address import Address
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.target import Target
from twitter.pants.targets.doc import Page
from twitter.pants.tasks import Task, TaskError


class MarkdownToHtml(Task):
  AVAILABLE = HAS_MARKDOWN

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    configure_codehighlight_options(option_group, mkflag)

    option_group.add_option(mkflag("open"), mkflag("open", negate=True),
                            dest = "markdown_to_html_open",
                            action="callback", callback=mkflag.set_bool, default=False,
                            help = "[%default] Open the generated documents in a browser.")

    option_group.add_option(mkflag("fragment"), mkflag("fragment", negate=True),
                            dest = "markdown_to_html_fragment",
                            action="callback", callback=mkflag.set_bool, default=False,
                            help = "[%default] Generate a fragment of html to embed in a page.")

    option_group.add_option(mkflag("outdir"), dest="markdown_to_html_outdir",
                            help="Emit generated html in to this directory.")

    option_group.add_option(mkflag("extension"), dest = "markdown_to_html_extensions",
                            action="append",
                            help = "Override the default markdown extensions and process pages "
                                   "whose source have these extensions instead.")

  def __init__(self, context):
    Task.__init__(self, context)

    self.open = context.options.markdown_to_html_open

    pants_workdir = context.config.getdefault('pants_workdir')
    self.outdir = (
      context.options.markdown_to_html_outdir
      or context.config.get('markdown-to-html',
                            'workdir',
                            default=os.path.join(pants_workdir, 'markdown'))
    )

    self.extensions = set(
      context.options.markdown_to_html_extensions
      or context.config.getlist('markdown-to-html', 'extensions', default=['.md', '.markdown'])
    )

    self.fragment = context.options.markdown_to_html_fragment

    self.code_style = context.config.get('markdown-to-html', 'code-style', default='friendly')
    if hasattr(context.options, 'markdown_to_html_code_style'):
      if context.options.markdown_to_html_code_style:
        self.code_style = context.options.markdown_to_html_code_style

  def execute(self, targets):
    if not MarkdownToHtml.AVAILABLE:
      raise TaskError('Cannot process markdown - no markdown lib on the sys.path')

    # TODO(John Sirois): consider adding change detection

    css_relpath = os.path.join('css', 'codehighlight.css')
    css = emit_codehighlight_css(os.path.join(self.outdir, css_relpath), self.code_style)
    if css:
      self.context.log.info('Emitted %s' % css)

    def is_page(target):
      return isinstance(target, Page)

    roots = set()
    interior_nodes = set()
    if self.open:
      dependencies_by_page = self.context.dependents(on_predicate=is_page, from_predicate=is_page)
      roots.update(dependencies_by_page.keys())
      for dependencies in dependencies_by_page.values():
        interior_nodes.update(dependencies)
        roots.difference_update(dependencies)
      for page in self.context.targets(is_page):
        # There are no in or out edges so we need to show show this isolated page.
        if not page.dependencies and page not in interior_nodes:
          roots.add(page)

    plaingenmap = self.context.products.get('markdown_html')
    wikigenmap = self.context.products.get('wiki_html')
    show = []
    for page in filter(is_page, targets):
      _, ext = os.path.splitext(page.source)
      if ext in self.extensions:
        def process_page(key, outdir, url_builder, config, genmap, fragment=False):
          html_path = self.process(
            outdir,
            page.target_base,
            page.source,
            self.fragment or fragment,
            url_builder,
            config,
            css=css
          )
          self.context.log.info('Processed %s to %s' % (page.source, html_path))
          relpath = os.path.relpath(html_path, outdir)
          genmap.add(key, outdir, [relpath])
          return html_path

        def url_builder(linked_page, config=None):
          path, ext = os.path.splitext(linked_page.source)
          return linked_page.name, os.path.relpath(path + '.html', os.path.dirname(page.source))

        page_path = os.path.join(self.outdir, 'html')
        html = process_page(page, page_path, url_builder, lambda p: None, plaingenmap)
        if css and not self.fragment:
          plaingenmap.add(page, self.outdir, list(css_relpath))
        if self.open and page in roots:
          show.append(html)

        for wiki in page.wikis():
          def get_config(page):
            return page.wiki_config(wiki)
          basedir = os.path.join(self.outdir, wiki.id)
          process_page((wiki, page), basedir, wiki.url_builder, get_config,
                       wikigenmap, fragment=True)

    if show:
      binary_util.ui_open(*show)

  PANTS_LINK = re.compile(r'''pants\(['"]([^)]+)['"]\)(#.*)?''')

  def process(self, outdir, base, source, fragmented, url_builder, get_config, css=None):
    def parse_url(spec):
      match = MarkdownToHtml.PANTS_LINK.match(spec)
      if match:
        page = Target.get(Address.parse(get_buildroot(), match.group(1)))
        anchor = match.group(2) or ''
        if not page:
          raise TaskError('Invalid link %s' % match.group(1))
        alias, url = url_builder(page, config=get_config(page))
        return alias, url + anchor
      else:
        return spec, spec

    def build_url(label):
      components = label.split('|', 1)
      if len(components) == 1:
        return parse_url(label.strip())
      else:
        alias, link = components
        _, url = parse_url(link.strip())
        return alias, url

    wikilinks = WikilinksExtension(build_url)

    path, ext = os.path.splitext(source)
    output_path = os.path.join(outdir, path + '.html')
    safe_mkdir(os.path.dirname(output_path))
    with codecs.open(output_path, 'w', 'utf-8') as output:
      with codecs.open(os.path.join(get_buildroot(), base, source), 'r', 'utf-8') as input:
        md_html = markdown.markdown(
          input.read(),
          extensions=['codehilite(guess_lang=False)', 'extra', 'tables', 'toc', wikilinks],
        )
        if fragmented:
          if css:
            with safe_open(css) as fd:
              output.write(textwrap.dedent('''
              <style type="text/css">
              %s
              </style>
              ''').strip() % fd.read())
              output.write('\n')
          output.write(md_html)
        else:
          if css:
            css_relpath = os.path.relpath(css, outdir)
            out_relpath = os.path.dirname(source)
            link_relpath = os.path.relpath(css_relpath, out_relpath)
            css = '<link rel="stylesheet" type="text/css" href="%s"/>' % link_relpath
          html = textwrap.dedent('''
          <html>
            <head>
              <meta charset="utf-8">
              %s
            </head>
            <body>
          <!-- generated by pants! -->
          %s
            </body>
          </html>
          ''').strip() % (css or '', md_html)
          output.write(html)
        return output.name

########NEW FILE########
__FILENAME__ = minimal_cover
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.tasks.console_task import ConsoleTask


class MinimalCover(ConsoleTask):
  """Outputs a minimal covering set of targets.

  For a given set of input targets, the output targets transitive dependency set will include all
  the input targets without gaps.
  """

  def console_output(self, _):
    internal_deps = set()
    for target in self.context.target_roots:
      internal_deps.update(self._collect_internal_deps(target))

    minimal_cover = set()
    for target in self.context.target_roots:
      if target not in internal_deps and target not in minimal_cover:
        minimal_cover.add(target)
        yield str(target.address)

  def _collect_internal_deps(self, target):
    internal_deps = set()
    target.walk(internal_deps.add)
    internal_deps.discard(target)
    return internal_deps

########NEW FILE########
__FILENAME__ = nailgun_task
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import time

from twitter.pants.java import util
from twitter.pants.java.distribution import Distribution
from twitter.pants.java.executor import SubprocessExecutor
from twitter.pants.java.nailgun_executor import NailgunExecutor

from . import Task, TaskError


class NailgunTask(Task):

  _DAEMON_OPTION_PRESENT = False

  @staticmethod
  def killall(logger=None, everywhere=False):
    """Kills all nailgun servers launched by pants in the current repo.

    Returns ``True`` if all nailguns were successfully killed, ``False`` otherwise.

    :param logger: a callable that accepts a message string describing the killed nailgun process
    :param bool everywhere: ``True`` to kill all nailguns servers launched by pants on this machine
    """
    if not NailgunExecutor.killall:
      return False
    else:
      return NailgunExecutor.killall(logger=logger, everywhere=everywhere)

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    if not NailgunTask._DAEMON_OPTION_PRESENT:
      option_group.parser.add_option("--ng-daemons", "--no-ng-daemons", dest="nailgun_daemon",
                                     default=True, action="callback", callback=mkflag.set_bool,
                                     help="[%default] Use nailgun daemons to execute java tasks.")
      NailgunTask._DAEMON_OPTION_PRESENT = True

  def __init__(self, context, minimum_version=None, jdk=False):
    super(NailgunTask, self).__init__(context)

    default_workdir_root = os.path.join(context.config.getdefault('pants_workdir'), 'ng')
    self._workdir = os.path.join(
        context.config.get('nailgun', 'workdir', default=default_workdir_root),
        self.__class__.__name__)

    self._nailgun_bootstrap_key = 'nailgun'
    self._jvm_tool_bootstrapper.register_jvm_tool(self._nailgun_bootstrap_key, [':nailgun-server'])

    start = time.time()
    try:
      self._dist = Distribution.cached(minimum_version=minimum_version, jdk=jdk)
      # TODO(John Sirois): Use a context timer when AWESOME-1265 gets merged.
      context.log.debug('Located java distribution in %.3fs' % (time.time() - start))
    except Distribution.Error as e:
      raise TaskError(e)

  def create_java_executor(self):
    """Create java executor that uses this task's ng daemon, if allowed.

    Call only in execute() or later. TODO: Enforce this.
    """
    if self.context.options.nailgun_daemon and not os.environ.get('PANTS_DEV'):
      classpath = os.pathsep.join(
        self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._nailgun_bootstrap_key))
      client = NailgunExecutor(self._workdir, classpath, distribution=self._dist)
    else:
      client = SubprocessExecutor(self._dist)
    return client

  @property
  def jvm_args(self):
    """Default jvm args the nailgun will be launched with.

    By default no special jvm args are used.  If a value for ``jvm_args`` is specified in pants.ini
    globally in the ``DEFAULT`` section or in the ``nailgun`` section, then that list will be used.
    """
    return self.context.config.getlist('nailgun', 'jvm_args', default=[])

  def runjava(self, classpath, main, jvm_options=None, args=None, workunit_name=None,
              workunit_labels=None):
    """Runs the java main using the given classpath and args.

    If --no-ng-daemons is specified then the java main is run in a freshly spawned subprocess,
    otherwise a persistent nailgun server dedicated to this Task subclass is used to speed up
    amortized run times.
    """
    executor = self.create_java_executor()
    try:
      return util.execute_java(classpath=classpath,
                               main=main,
                               jvm_options=jvm_options,
                               args=args,
                               executor=executor,
                               workunit_factory=self.context.new_workunit,
                               workunit_name=workunit_name,
                               workunit_labels=workunit_labels)
    except executor.Error as e:
      raise TaskError(e)

########NEW FILE########
__FILENAME__ = pathdeps
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.tasks.console_task import ConsoleTask

__author__ = 'Dave Buchfuhrer'

class PathDeps(ConsoleTask):
  def console_output(self, targets):
    return set(t.address.buildfile.parent_path for t in targets if hasattr(t, 'address'))

########NEW FILE########
__FILENAME__ = paths
from __future__ import print_function

from collections import defaultdict
import copy

from twitter.common.lang import Compatibility
from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.address import Address
from twitter.pants.base.target import Target
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.console_task import ConsoleTask


class PathFinder(ConsoleTask):
  def __init__(self, context):
    ConsoleTask.__init__(self, context)
    self.log = context.log
    self.target_roots = context.target_roots

  @classmethod
  def _coerce_to_targets(cls, from_str, to_str):
    if isinstance(from_str, Compatibility.string):
      if not isinstance(to_str, Compatibility.string):
        raise TaskError('Finding paths from string %s to non-string %s' % (from_str, str(to_str)))

      from_address = Address.parse(get_buildroot(), from_str)
      to_address = Address.parse(get_buildroot(), to_str)

      from_target = Target.get(from_address)
      to_target = Target.get(to_address)

      if not from_target:
        raise TaskError('Target %s doesn\'t exist' % from_address.reference())
      if not to_target:
        raise TaskError('Target %s doesn\'t exist' % to_address.reference())

      return from_target, to_target

    elif isinstance(to_str, Compatibility.string):
      raise TaskError('Finding paths from string %s to non-string %s' % (to_str, str(from_str)))
    return from_str, to_str

  @classmethod
  def _find_paths(cls, from_target, to_target, log):
    from_target, to_target = cls._coerce_to_targets(from_target, to_target)

    log.debug('Looking for all paths from %s to %s' % (from_target.address.reference(), to_target.address.reference()))

    paths = cls._find_paths_rec(from_target, to_target)
    print('Found %d paths' % len(paths))
    print('')
    for path in paths:
      log.debug('\t[%s]' % ', '.join([target.address.reference() for target in path]))

  all_paths = defaultdict(lambda: defaultdict(list))
  @classmethod
  def _find_paths_rec(cls, from_target, to_target):
    if from_target == to_target:
      return [[from_target]]

    if from_target not in cls.all_paths or to_target not in cls.all_paths[from_target]:
      paths = []
      if hasattr(from_target, 'dependency_addresses'):
        for address in from_target.dependency_addresses:
          dep = Target.get(address)
          for path in cls._find_paths_rec(dep, to_target):
            new_path = copy.copy(path)
            new_path.insert(0, from_target)
            paths.append(new_path)

      cls.all_paths[from_target][to_target] = paths

    return cls.all_paths[from_target][to_target]

  examined_targets = set()

  @classmethod
  def _find_path(cls, from_target, to_target, log):
    from_target, to_target = cls._coerce_to_targets(from_target, to_target)

    log.debug('Looking for path from %s to %s' % (from_target.address.reference(), to_target.address.reference()))

    queue = [([from_target], 0)]
    while True:
      if not queue:
        print('no path found from %s to %s!' % (from_target.address.reference(), to_target.address.reference()))
        break

      path, indent = queue.pop(0)
      next_target = path[-1]
      if next_target in cls.examined_targets:
        continue
      cls.examined_targets.add(next_target)

      log.debug('%sexamining %s' % ('  ' * indent, next_target))

      if next_target == to_target:
        print('')
        for target in path:
          print('%s' % target.address.reference())
        break

      if hasattr(next_target, 'dependency_addresses'):
        for address in next_target.dependency_addresses:
          dep = Target.get(address)
          queue.append((path + [dep], indent + 1))


class Path(PathFinder):
  def execute(self, targets):
    if len(self.target_roots) != 2:
      raise TaskError('Specify two targets please (found %d)' % len(self.target_roots))

    self._find_path(self.target_roots[0], self.target_roots[1], self.log)

class Paths(PathFinder):
  def execute(self, targets):
    if len(self.target_roots) != 2:
      raise TaskError('Specify two targets please (found %d)' % len(self.target_roots))

    self._find_paths(self.target_roots[0], self.target_roots[1], self.log)

########NEW FILE########
__FILENAME__ = prepare_resources
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================
from collections import defaultdict

import os
import shutil

from twitter.common.dirutil import safe_mkdir
from twitter.pants.goal.products import MultipleRootedProducts

from twitter.pants.tasks import Task


class PrepareResources(Task):

  def __init__(self, context):
    Task.__init__(self, context)

    self.workdir = context.config.get('prepare-resources', 'workdir')
    self.confs = context.config.getlist('prepare-resources', 'confs', default=['default'])
    self.context.products.require_data('exclusives_groups')

  def execute(self, targets):
    if self.context.products.is_required_data('resources_by_target'):
      self.context.products.safe_create_data('resources_by_target',
                                             lambda: defaultdict(MultipleRootedProducts))

    if len(targets) == 0:
      return
    def extract_resources(target):
      return target.resources if target.has_resources else ()
    all_resources_tgts = set()
    for resources_tgts in map(extract_resources, targets):
      all_resources_tgts.update(resources_tgts)

    def target_dir(resources_tgt):
      return os.path.join(self.workdir, resources_tgt.id)

    with self.invalidated(all_resources_tgts) as invalidation_check:
      invalid_targets = set()
      for vt in invalidation_check.invalid_vts:
        invalid_targets.update(vt.targets)

      for resources_tgt in invalid_targets:
        resources_dir = target_dir(resources_tgt)
        safe_mkdir(resources_dir, clean=True)
        for resource_path in resources_tgt.sources:
          basedir = os.path.dirname(resource_path)
          destdir = os.path.join(resources_dir, basedir)
          safe_mkdir(destdir)
          # TODO: Symlink instead?
          shutil.copy(os.path.join(resources_tgt.target_base, resource_path),
                      os.path.join(resources_dir, resource_path))

      resources_by_target = self.context.products.get_data('resources_by_target')
      egroups = self.context.products.get_data('exclusives_groups')
      group_key = egroups.get_group_key_for_target(targets[0])

      for resources_tgt in all_resources_tgts:
        resources_dir = target_dir(resources_tgt)
        for conf in self.confs:
          egroups.update_compatible_classpaths(group_key, [(conf, resources_dir)])
        if resources_by_target is not None:
          target_resources = resources_by_target[resources_tgt]
          target_resources.add_rel_paths(resources_dir, resources_tgt.sources)

########NEW FILE########
__FILENAME__ = protobuf_gen
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import re
import subprocess

from collections import defaultdict

from twitter.common import log
from twitter.common.collections import OrderedSet
from twitter.common.dirutil import safe_mkdir

from twitter.pants.binary_util import select_binary
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.java_protobuf_library import JavaProtobufLibrary
from twitter.pants.targets.python_library import PythonLibrary

from .code_gen import CodeGen

from . import TaskError


class ProtobufGen(CodeGen):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("outdir"), dest="protobuf_gen_create_outdir",
                            help="Emit generated code in to this directory.")

    option_group.add_option(mkflag("lang"), dest="protobuf_gen_langs", default=[],
                            action="append", type="choice", choices=['python', 'java'],
                            help="Force generation of protobuf code for these languages.  Both "
                                 "'python' and 'java' are supported")

  def __init__(self, context):
    CodeGen.__init__(self, context)

    self.protoc_supportdir = self.context.config.get('protobuf-gen', 'supportdir')
    self.protoc_version = self.context.config.get('protobuf-gen', 'version')
    self.output_dir = (context.options.protobuf_gen_create_outdir or
                       context.config.get('protobuf-gen', 'workdir'))

    def resolve_deps(key):
      deps = OrderedSet()
      for dep in context.config.getlist('protobuf-gen', key):
        deps.update(context.resolve(dep))
      return deps

    self.javadeps = resolve_deps('javadeps')
    self.java_out = os.path.join(self.output_dir, 'gen-java')

    self.pythondeps = resolve_deps('pythondeps')
    self.py_out = os.path.join(self.output_dir, 'gen-py')

    self.gen_langs = set(context.options.protobuf_gen_langs)
    for lang in ('java', 'python'):
      if self.context.products.isrequired(lang):
        self.gen_langs.add(lang)

    self.protobuf_binary = select_binary(
      self.protoc_supportdir,
      self.protoc_version,
      'protoc',
      context.config
    )

  def invalidate_for(self):
    return self.gen_langs

  def invalidate_for_files(self):
    return [self.protobuf_binary]

  def is_gentarget(self, target):
    return isinstance(target, JavaProtobufLibrary)

  def is_forced(self, lang):
    return lang in self.gen_langs

  def genlangs(self):
    return dict(java=lambda t: t.is_jvm, python=lambda t: t.is_python)

  def genlang(self, lang, targets):
    protobuf_binary = select_binary(
      self.protoc_supportdir,
      self.protoc_version,
      'protoc',
      self.context.config
    )

    bases, sources = self._calculate_sources(targets)

    if lang == 'java':
      safe_mkdir(self.java_out)
      gen = '--java_out=%s' % self.java_out
    elif lang == 'python':
      safe_mkdir(self.py_out)
      gen = '--python_out=%s' % self.py_out
    else:
      raise TaskError('Unrecognized protobuf gen lang: %s' % lang)

    args = [self.protobuf_binary, gen]

    for base in bases:
      args.append('--proto_path=%s' % base)

    args.extend(sources)
    log.debug('Executing: %s' % ' '.join(args))
    process = subprocess.Popen(args)
    result = process.wait()
    if result != 0:
      raise TaskError('%s ... exited non-zero (%i)' % (self.protobuf_binary, result))

  def _calculate_sources(self, targets):
    bases = set()
    sources = set()

    def collect_sources(target):
      if self.is_gentarget(target):
        bases.add(target.target_base)
        sources.update(target.sources_relative_to_buildroot())

    for target in targets:
      target.walk(collect_sources)
    return bases, sources

  def createtarget(self, lang, gentarget, dependees):
    if lang == 'java':
      return self._create_java_target(gentarget, dependees)
    elif lang == 'python':
      return self._create_python_target(gentarget, dependees)
    else:
      raise TaskError('Unrecognized protobuf gen lang: %s' % lang)

  def _create_java_target(self, target, dependees):
    genfiles = []
    for source in target.sources:
      path = os.path.join(target.target_base, source)
      genfiles.extend(calculate_genfiles(path, source).get('java', []))
    tgt = self.context.add_new_target(self.java_out,
                                      JavaLibrary,
                                      name=target.id,
                                      sources=genfiles,
                                      provides=target.provides,
                                      dependencies=self.javadeps,
                                      excludes=target.excludes)
    tgt.id = target.id + '.protobuf_gen'
    for dependee in dependees:
      dependee.update_dependencies([tgt])
    return tgt

  def _create_python_target(self, target, dependees):
    genfiles = []
    for source in target.sources:
      path = os.path.join(target.target_base, source)
      genfiles.extend(calculate_genfiles(path, source).get('py', []))
    tgt = self.context.add_new_target(self.py_out,
                                      PythonLibrary,
                                      name=target.id,
                                      sources=genfiles,
                                      dependencies=self.pythondeps)
    tgt.id = target.id
    for dependee in dependees:
      dependee.dependencies.add(tgt)
    return tgt


DEFAULT_PACKAGE_PARSER = re.compile(r'^\s*package\s+([^;]+)\s*;\s*$')
OPTION_PARSER = re.compile(r'^\s*option\s+([^ =]+)\s*=\s*([^\s]+)\s*;\s*$')
TYPE_PARSER = re.compile(r'^\s*(enum|message)\s+([^\s{]+).*')


def camelcase(string):
  """Convert snake casing where present to camel casing"""
  return ''.join(word.capitalize() for word in string.split('_'))


def calculate_genfiles(path, source):
  with open(path, 'r') as protobuf:
    lines = protobuf.readlines()
    package = ''
    filename = re.sub(r'\.proto$', '', os.path.basename(source))
    outer_class_name = camelcase(filename)
    multiple_files = False
    types = set()
    for line in lines:
      match = DEFAULT_PACKAGE_PARSER.match(line)
      if match:
        package = match.group(1)
      else:
        match = OPTION_PARSER.match(line)
        if match:
          name = match.group(1)
          value = match.group(2)

          def string_value():
            return value.lstrip('"').rstrip('"')

          def bool_value():
            return value == 'true'

          if 'java_package' == name:
            package = string_value()
          elif 'java_outer_classname' == name:
            outer_class_name = string_value()
          elif 'java_multiple_files' == name:
            multiple_files = bool_value()
        else:
          match = TYPE_PARSER.match(line)
          if match:
            type_ = match.group(2)
            types.add(type_)
            if match.group(1) == 'message':
              types.add('%sOrBuilder' % type_)

    genfiles = defaultdict(set)
    genfiles['py'].update(calculate_python_genfiles(source))
    genfiles['java'].update(calculate_java_genfiles(package,
                                                    outer_class_name,
                                                    types if multiple_files else []))
    return genfiles


def calculate_python_genfiles(source):
  yield re.sub(r'\.proto$', '_pb2.py', source)


def calculate_java_genfiles(package, outer_class_name, types):
  basepath = package.replace('.', '/')

  def path(name):
    return os.path.join(basepath, '%s.java' % name)

  yield path(outer_class_name)
  for type_ in types:
    yield path(type_)

########NEW FILE########
__FILENAME__ = provides
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Benjy Weinberger'

import os
import sys

from twitter.common.collections import OrderedSet
from twitter.common.contextutil import open_zip as open_jar
from twitter.pants.tasks import Task
from twitter.pants.tasks.ivy_utils import IvyModuleRef, IvyUtils
from twitter.pants.targets.jvm_binary import JvmBinary


class Provides(Task):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("outdir"), dest="provides_outdir",
      help="Emit provides outputs into this directory.")
    option_group.add_option(mkflag("transitive"), default=False,
      action="store_true", dest='provides_transitive',
      help="Shows the symbols provided not just by the specified targets but by all their transitive dependencies.")
    option_group.add_option(mkflag("also-write-to-stdout"), default=False,
      action="store_true", dest='provides_also_write_to_stdout',
      help="If set, also outputs the provides information to stdout.")

  def __init__(self, context):
    Task.__init__(self, context)
    self.ivy_utils = IvyUtils(config=context.config,
                              options=context.options,
                              log=context.log)
    self.confs = context.config.getlist('ivy', 'confs', default=['default'])
    self.target_roots = context.target_roots
    self.transitive = context.options.provides_transitive
    self.workdir = context.config.get('provides', 'workdir')
    self.outdir = context.options.provides_outdir or self.workdir
    self.also_write_to_stdout = context.options.provides_also_write_to_stdout or False
    # Create a fake target, in case we were run directly on a JarLibrary containing nothing but JarDependencies.
    # TODO(benjy): Get rid of this special-casing of jar dependencies.
    context.add_new_target(self.workdir,
      JvmBinary,
      name='provides',
      dependencies=self.target_roots,
      configurations=self.confs)
    context.products.require('jars')

  def execute(self, targets):
    for conf in self.confs:
      outpath = os.path.join(self.outdir, '%s.%s.provides' %
                             (self.ivy_utils.identify(targets)[1], conf))
      if self.transitive:
        outpath += '.transitive'
      ivyinfo = self.ivy_utils.parse_xml_report(self.context, conf)
      jar_paths = OrderedSet()
      for root in self.target_roots:
        jar_paths.update(self.get_jar_paths(ivyinfo, root, conf))

      with open(outpath, 'w') as outfile:
        def do_write(s):
          outfile.write(s)
          if self.also_write_to_stdout:
            sys.stdout.write(s)
        for jar in jar_paths:
          do_write('# from jar %s\n' % jar)
          for line in self.list_jar(jar):
            if line.endswith('.class'):
              class_name = line[:-6].replace('/', '.')
              do_write(class_name)
              do_write('\n')
      print('Wrote provides information to %s' % outpath)

  def get_jar_paths(self, ivyinfo, target, conf):
    jar_paths = OrderedSet()
    if target.is_jar_library:
      # Jar library proxies jar dependencies or jvm targets, so the jars are just those of the
      # dependencies.
      for paths in [ self.get_jar_paths(ivyinfo, dep, conf) for dep in target.dependencies ]:
        jar_paths.update(paths)
    elif target.is_jar_dependency:
      ref = IvyModuleRef(target.org, target.name, target.rev, conf)
      jar_paths.update(self.get_jar_paths_for_ivy_module(ivyinfo, ref))
    elif target.is_jvm:
      for basedir, jars in self.context.products.get('jars').get(target).items():
        jar_paths.update([os.path.join(basedir, jar) for jar in jars])
      if self.transitive:
        for dep in target.dependencies:
          jar_paths.update(self.get_jar_paths(ivyinfo, dep, conf))

    return jar_paths

  def get_jar_paths_for_ivy_module(self, ivyinfo, ref):
    jar_paths = OrderedSet()
    module = ivyinfo.modules_by_ref[ref]
    jar_paths.update([a.path for a in module.artifacts])
    if self.transitive:
      for dep in ivyinfo.deps_by_caller.get(ref, []):
        jar_paths.update(self.get_jar_paths_for_ivy_module(ivyinfo, dep))
    return jar_paths

  def list_jar(self, path):
    with open_jar(path, 'r') as jar:
      return jar.namelist()


########NEW FILE########
__FILENAME__ = target
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import OrderedSet

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target, TargetDefinitionException
from twitter.pants.targets.python_requirement import PythonRequirement


def is_python_root(target):
  return isinstance(target, PythonRoot)


class PythonRoot(Target):
  """
    Internal target for managing python chroot state.
  """
  @classmethod
  def synthetic_name(cls, targets):
    return list(targets)[0].name if len(targets) > 0 else 'empty'

  @classmethod
  def union(cls, targets, name=None):
    name = name or (cls.synthetic_name(targets) + '-union')
    with ParseContext.temp():
      return cls(name, dependencies=targets)

  @classmethod
  def of(cls, target):
    with ParseContext.temp():
      return cls(target.name, dependencies=[target])

  def __init__(self, name, dependencies=None):
    self.dependencies = OrderedSet(dependencies) if dependencies else OrderedSet()
    self.internal_dependencies = OrderedSet()
    self.interpreters = []
    self.distributions = {} # interpreter => distributions
    self.chroots = {}       # interpreter => chroots
    super(PythonRoot, self).__init__(name)

  def closure(self):
    os = OrderedSet()
    for target in self.dependencies | self.internal_dependencies:
      os.update(target.closure())
    return os

  def select(self, target_class):
    return OrderedSet(target for target in self.closure() if isinstance(target, target_class))

  @property
  def requirements(self):
    return self.select(PythonRequirement)

########NEW FILE########
__FILENAME__ = roots
from twitter.pants.targets.sources import SourceRoot

from .console_task import ConsoleTask


class ListRoots(ConsoleTask):
  """
  List the registered source roots of the repo.
  """

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(ListRoots, cls).setup_parser(option_group, args, mkflag)

  def console_output(self, targets):
    for src_root, targets in SourceRoot.all_roots().items():
      all_targets = ','.join(sorted([tgt.__name__ for tgt in targets]))
      yield '%s: %s' % (src_root, all_targets or '*')

########NEW FILE########
__FILENAME__ = scaladoc_gen
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.tasks.jvmdoc_gen import Jvmdoc, JvmdocGen


scaladoc = Jvmdoc(tool_name='scaladoc', product_type='scaladoc')


def is_scala(target):
  return target.has_sources('.scala')


class ScaladocGen(JvmdocGen):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    cls.generate_setup_parser(option_group, args, mkflag, scaladoc)

  def __init__(self, context, output_dir=None, confs=None, active=True):
    super(ScaladocGen, self).__init__(context, scaladoc, output_dir, confs, active)

  def execute(self, targets):
    self.generate_execute(targets, lambda t: t.is_scala, create_scaladoc_command)


def create_scaladoc_command(classpath, gendir, *targets):
  sources = []
  for target in targets:
    sources.extend(target.sources_relative_to_buildroot())

  if not sources:
    return None

  # TODO(John Chee): try scala.tools.nsc.ScalaDoc via ng
  command = [
    'scaladoc',
    '-usejavacp',
    '-classpath', ':'.join(classpath),
    '-d', gendir,
  ]

  command.extend(sources)
  return command

########NEW FILE########
__FILENAME__ = scalastyle
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import re

from twitter.pants.base.config import Config
from twitter.pants.base.target import Target
from twitter.pants.process.xargs import Xargs
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.nailgun_task import NailgunTask


class Scalastyle(NailgunTask):
  """Checks scala source files to ensure they're stylish.

  Scalastyle is configured via the 'scalastyle' pants.ini section.

  * ``config`` - Required path of the scalastyle configuration file.
  * ``excludes`` - Optional path of an excludes file that contains
    lines of regular expressions used to exclude matching files
    from style checks. File names matched against these regular
    expressions are relative to the repository root
    (e.g.: com/twitter/mybird/MyBird.scala).
  """

  _CONFIG_SECTION = 'scalastyle'
  _MAIN = 'org.scalastyle.Main'

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    NailgunTask.setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag("skip"), mkflag("skip", negate=True),
                            dest="scalastyle_skip", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Skip scalastyle.")

  def __init__(self, context):
    NailgunTask.__init__(self, context)
    self._scalastyle_config = self.context.config.get_required(
      Scalastyle._CONFIG_SECTION, 'config')
    if not os.path.exists(self._scalastyle_config):
      raise Config.ConfigError(
          'Scalastyle config file does not exist: %s' % self._scalastyle_config)

    excludes_file = self.context.config.get(Scalastyle._CONFIG_SECTION, 'excludes')
    self._excludes = set()
    if excludes_file:
      if not os.path.exists(excludes_file):
        raise Config.ConfigError('Scalastyle excludes file does not exist: %s' % excludes_file)
      self.context.log.debug('Using scalastyle excludes file %s' % excludes_file)
      with open(excludes_file) as fh:
        for pattern in fh.readlines():
          self._excludes.add(re.compile(pattern.strip()))

    self._scalastyle_bootstrap_key = 'scalastyle'
    self.register_jvm_tool(self._scalastyle_bootstrap_key, [':scalastyle'])

  def execute(self, targets):
    if self.context.options.scalastyle_skip:
      self.context.log.debug('Skipping checkstyle.')
      return

    check_targets = list()
    for target in targets:
      for tgt in target.resolve():
        if isinstance(tgt, Target) and tgt.has_sources('.scala'):
          check_targets.append(tgt)

    def filter_excludes(filename):
      if self._excludes:
        for exclude in self._excludes:
          if exclude.match(filename):
            return False
      return True

    scala_sources = list()
    for target in check_targets:
      def collect(filename):
        if filename.endswith('.scala'):
          scala_sources.append(os.path.join(target.target_base, filename))
      map(collect, filter(filter_excludes, target.sources))

    if scala_sources:
      def call(srcs):
        cp = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._scalastyle_bootstrap_key)
        return self.runjava(classpath=cp,
                            main=Scalastyle._MAIN,
                            args=['-c', self._scalastyle_config] + srcs)
      result = Xargs(call).execute(scala_sources)
      if result != 0:
        raise TaskError('java %s ... exited non-zero (%i)' % (Scalastyle._MAIN, result))

########NEW FILE########
__FILENAME__ = scala_repl
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import shlex
import subprocess

from twitter.pants.base.workunit import WorkUnit
from twitter.pants.java.util import execute_java
from .jvm_task import JvmTask

from . import Task


class ScalaRepl(JvmTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("jvmargs"), dest = "run_jvmargs", action="append",
      help = "Run the repl in a jvm with these extra jvm args.")
    option_group.add_option(mkflag('args'), dest = 'run_args', action='append',
                            help = 'run the repl in a jvm with extra args.')

  def __init__(self, context):
    Task.__init__(self, context)
    self.jvm_args = context.config.getlist('scala-repl', 'jvm_args', default=[])
    if context.options.run_jvmargs:
      for arg in context.options.run_jvmargs:
        self.jvm_args.extend(shlex.split(arg))
    self.confs = context.config.getlist('scala-repl', 'confs', default=['default'])
    self._bootstrap_key = 'scala-repl'
    bootstrap_tools = context.config.getlist('scala-repl', 'bootstrap-tools')
    self._jvm_tool_bootstrapper.register_jvm_tool(self._bootstrap_key, bootstrap_tools)
    self.main = context.config.get('scala-repl', 'main')
    self.args = context.config.getlist('scala-repl', 'args', default=[])
    if context.options.run_args:
      for arg in context.options.run_args:
        self.args.extend(shlex.split(arg))

  def execute(self, targets):
    # The repl session may last a while, allow concurrent pants activity during this pants idle
    # period.
    tools_classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(self._bootstrap_key)

    self.context.lock.release()
    self.save_stty_options()

    classpath = self.classpath(tools_classpath,
                               confs=self.confs,
                               exclusives_classpath=self.get_base_classpath_for_target(targets[0]))

    print('')  # Start REPL output on a new line.
    try:
      execute_java(classpath=classpath,
                   main=self.main,
                   jvm_options=self.jvm_args,
                   args=self.args,
                   workunit_factory=self.context.new_workunit,
                   workunit_name='repl',
                   workunit_labels=[WorkUnit.REPL, WorkUnit.JVM])
    except KeyboardInterrupt:
      # TODO(John Sirois): Confirm with Steve Gury that finally does not work on mac and an
      # explicit catch of KeyboardInterrupt is required.
      pass
    self.restore_ssty_options()

  def save_stty_options(self):
    """
    The scala REPL changes some stty parameters and doesn't save/restore them after
    execution, so if you have a terminal with non-default stty options, you end
    up to a broken terminal (need to do a 'reset').
    """
    self.stty_options = self.run_cmd('stty -g 2>/dev/null')

  def restore_ssty_options(self):
    self.run_cmd('stty ' + self.stty_options)

  def run_cmd(self, cmd):
    po = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
    stdout, _ = po.communicate()
    return stdout

########NEW FILE########
__FILENAME__ = scm_publish
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from . import Task, TaskError


class Semver(object):
  @staticmethod
  def parse(version):
    components = version.split('.', 3)
    if len(components) != 3:
      raise ValueError
    major, minor, patch = components

    def to_i(component):
      try:
        return int(component)
      except (TypeError, ValueError):
        raise ValueError('Invalid revision component %s in %s - '
                         'must be an integer' % (component, version))
    return Semver(to_i(major), to_i(minor), to_i(patch))

  def __init__(self, major, minor, patch, snapshot=False):
    self.major = major
    self.minor = minor
    self.patch = patch
    self.snapshot = snapshot

  def bump(self):
    # A bump of a snapshot discards snapshot status
    return Semver(self.major, self.minor, self.patch + 1)

  def make_snapshot(self):
    return Semver(self.major, self.minor, self.patch, snapshot=True)

  def version(self):
    return '%s.%s.%s' % (
      self.major,
      self.minor,
      ('%s-SNAPSHOT' % self.patch) if self.snapshot else self.patch
    )

  def __eq__(self, other):
    return self.__cmp__(other) == 0

  def __cmp__(self, other):
    diff = self.major - other.major
    if not diff:
      diff = self.minor - other.minor
      if not diff:
        diff = self.patch - other.patch
        if not diff:
          if self.snapshot and not other.snapshot:
            diff = 1
          elif not self.snapshot and other.snapshot:
            diff = -1
          else:
            diff = 0
    return diff

  def __repr__(self):
    return 'Semver(%s)' % self.version()


class ScmPublish(object):
  def __init__(self, scm, restrict_push_branches):
    self.restrict_push_branches = frozenset(restrict_push_branches or ())
    self.scm = scm

  def check_clean_master(self, commit=False):
    if commit:
      if self.restrict_push_branches:
        branch = self.scm.branch_name
        if branch not in self.restrict_push_branches:
          raise TaskError('Can only push from %s, currently on branch: %s' % (
            ' '.join(sorted(self.restrict_push_branches)), branch
          ))

      changed_files = self.scm.changed_files()
      if changed_files:
        raise TaskError('Can only push from a clean branch, found : %s' % ' '.join(changed_files))
    else:
      print('Skipping check for a clean %s in test mode.' % self.scm.branch_name)

  def commit_push(self, coordinates):
    self.scm.refresh()
    self.scm.commit('pants build committing publish data for push of %s' % coordinates)


########NEW FILE########
__FILENAME__ = scrooge_gen
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import hashlib
import os
import re
import tempfile

from collections import defaultdict, namedtuple

from twitter.common.collections import OrderedSet
from twitter.common.dirutil import safe_mkdir, safe_open

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.java_thrift_library import JavaThriftLibrary
from twitter.pants.targets.scala_library import ScalaLibrary
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.nailgun_task import NailgunTask
from twitter.pants.thrift_util import (
    calculate_compile_sources,
    calculate_compile_sources_HACK_FOR_SCROOGE_LEGACY)

CompilerConfig = namedtuple('CompilerConfig', ['name', 'config_section', 'profile',
                                               'main', 'calc_srcs', 'langs'])


class Compiler(namedtuple('CompilerConfigWithContext', ('context',) + CompilerConfig._fields)):
  @classmethod
  def fromConfig(cls, context, config):
    return cls(context, **config._asdict())

  @property
  def jvm_args(self):
    args = self.context.config.getlist(self.config_section, 'jvm_args', default=[])
    args.append('-Dfile.encoding=UTF-8')
    return args

  @property
  def outdir(self):
    pants_workdir_fallback = os.path.join(get_buildroot(), '.pants.d')
    workdir_fallback = os.path.join(self.context.config.getdefault('pants_workdir',
                                                                   default=pants_workdir_fallback),
                                    self.name)
    outdir = (self.context.options.scrooge_gen_create_outdir
              or self.context.config.get(self.config_section, 'workdir', default=workdir_fallback))
    return os.path.relpath(outdir)

  @property
  def verbose(self):
    if self.context.options.scrooge_gen_quiet is not None:
      return not self.context.options.scrooge_gen_quiet
    else:
      return self.context.config.getbool(self.config_section, 'verbose', default=False)

  @property
  def strict(self):
    return self.context.config.getbool(self.config_section, 'strict', default=False)


_COMPILERS = [
    CompilerConfig(name='scrooge',
                   config_section='scrooge-gen',
                   profile='scrooge-gen',
                   main='com.twitter.scrooge.Main',
                   calc_srcs=calculate_compile_sources,
                   langs=frozenset(['scala', 'java'])),
    CompilerConfig(name='scrooge-legacy',
                   config_section='scrooge-legacy-gen',
                   profile='scrooge-legacy-gen',
                   main='com.twitter.scrooge.Main',
                   calc_srcs=calculate_compile_sources_HACK_FOR_SCROOGE_LEGACY,
                   langs=frozenset(['scala']))
]

_CONFIG_FOR_COMPILER = dict((compiler.name, compiler) for compiler in _COMPILERS)

_TARGET_TYPE_FOR_LANG = dict(scala=ScalaLibrary, java=JavaLibrary)


class ScroogeGen(NailgunTask):
  GenInfo = namedtuple('GenInfo', ['gen', 'deps'])

  class PartialCmd(namedtuple('PC', ['compiler', 'language', 'rpc_style', 'namespace_map'])):
    @property
    def outdir(self):
      namespace_sig = None
      if self.namespace_map:
        sha = hashlib.sha1()
        for ns_from, ns_to in sorted(self.namespace_map):
          sha.update(ns_from)
          sha.update(ns_to)
        namespace_sig = sha.hexdigest()
      output_style = '-'.join(filter(None, (self.language, self.rpc_style, namespace_sig)))
      return os.path.join(self.compiler.outdir, output_style)

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("outdir"), dest="scrooge_gen_create_outdir",
                            help="Emit generated code in to this directory.")
    option_group.add_option(mkflag("quiet"), dest="scrooge_gen_quiet",
                            action="callback", callback=mkflag.set_bool, default=None,
                            help="[%default] Suppress output, overrides verbose flag in pants.ini.")

  def __init__(self, context):
    super(ScroogeGen, self).__init__(context)
    self.compiler_for_name = dict((name, Compiler.fromConfig(context, config))
                                  for name, config in _CONFIG_FOR_COMPILER.items())

    for name, compiler in self.compiler_for_name.items():
      bootstrap_tools = context.config.getlist(compiler.config_section, 'bootstrap-tools',
                                               default=[':%s' % compiler.profile])
      self._jvm_tool_bootstrapper.register_jvm_tool(compiler.name, bootstrap_tools)

  def _tempname(self):
    # don't assume the user's cwd is buildroot
    pants_workdir = self.context.config.getdefault('pants_workdir')
    tmp_dir = os.path.join(pants_workdir, 'tmp')
    safe_mkdir(tmp_dir)
    fd, path = tempfile.mkstemp(dir=tmp_dir, prefix='')
    os.close(fd)
    return path

  def execute(self, targets):
    gentargets_by_dependee = self.context.dependents(
        on_predicate=self.is_gentarget,
        from_predicate=lambda t: not self.is_gentarget(t))

    dependees_by_gentarget = defaultdict(set)
    for dependee, tgts in gentargets_by_dependee.items():
      for gentarget in tgts:
        dependees_by_gentarget[gentarget].add(dependee)

    partial_cmds = defaultdict(set)
    gentargets = filter(self.is_gentarget, targets)

    for target in gentargets:
      partial_cmd = self.PartialCmd(
          compiler=self.compiler_for_name[target.compiler],
          language=target.language,
          rpc_style=target.rpc_style,
          namespace_map=tuple(sorted(target.namespace_map.items()) if target.namespace_map else ()))
      partial_cmds[partial_cmd].add(target)

    for partial_cmd, tgts in partial_cmds.items():
      gen_files_for_source = self.gen(partial_cmd, tgts)

      outdir = partial_cmd.outdir
      langtarget_by_gentarget = {}
      for target in tgts:
        dependees = dependees_by_gentarget.get(target, [])
        langtarget_by_gentarget[target] = self.createtarget(target, dependees, outdir,
                                                            gen_files_for_source)

      genmap = self.context.products.get(partial_cmd.language)
      for gentarget, langtarget in langtarget_by_gentarget.items():
        genmap.add(gentarget, get_buildroot(), [langtarget])
        for dep in gentarget.internal_dependencies:
          if self.is_gentarget(dep):
            langtarget.update_dependencies([langtarget_by_gentarget[dep]])

  def gen(self, partial_cmd, targets):
    with self.invalidated(targets, invalidate_dependents=True) as invalidation_check:
      invalid_targets = []
      for vt in invalidation_check.invalid_vts:
        invalid_targets.extend(vt.targets)

      compiler = partial_cmd.compiler
      import_paths, changed_srcs = compiler.calc_srcs(invalid_targets, self.is_gentarget)
      outdir = partial_cmd.outdir
      if changed_srcs:
        args = []

        for import_path in import_paths:
          args.extend(['--import-path', import_path])

        args.extend(['--language', partial_cmd.language])

        for lhs, rhs in partial_cmd.namespace_map:
          args.extend(['--namespace-map', '%s=%s' % (lhs, rhs)])

        if partial_cmd.rpc_style == 'ostrich':
          args.append('--finagle')
          args.append('--ostrich')
        elif partial_cmd.rpc_style == 'finagle':
          args.append('--finagle')

        args.extend(['--dest', outdir])
        safe_mkdir(outdir)

        if not compiler.strict:
          args.append('--disable-strict')

        if compiler.verbose:
          args.append('--verbose')

        gen_file_map_path = os.path.relpath(self._tempname())
        args.extend(['--gen-file-map', gen_file_map_path])

        args.extend(changed_srcs)

        classpath = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(compiler.name)
        returncode = self.runjava(classpath=classpath,
                                  main=compiler.main,
                                  jvm_options=compiler.jvm_args,
                                  args=args,
                                  workunit_name=compiler.name)
        try:
          if 0 == returncode:
            gen_files_for_source = self.parse_gen_file_map(gen_file_map_path, outdir)
        finally:
          os.remove(gen_file_map_path)

        if 0 != returncode:
          raise TaskError('java %s ... exited non-zero (%i)' % (compiler.main, returncode))
        self.write_gen_file_map(gen_files_for_source, invalid_targets, outdir)

    return self.gen_file_map(targets, outdir)

  def createtarget(self, gentarget, dependees, outdir, gen_files_for_source):
    assert self.is_gentarget(gentarget)

    def create_target(files, deps, target_type):
      return self.context.add_new_target(outdir,
                                         target_type,
                                         name=gentarget.id,
                                         sources=files,
                                         provides=gentarget.provides,
                                         dependencies=deps,
                                         excludes=gentarget.excludes)

    def create_geninfo(key):
      compiler = self.compiler_for_name[gentarget.compiler]
      gen_info = self.context.config.getdict(compiler.config_section, key,
                                             default={'gen': key,
                                                      'deps': {'service': [], 'structs': []}})
      gen = gen_info['gen']
      deps = dict()
      for category, depspecs in gen_info['deps'].items():
        dependencies = OrderedSet()
        deps[category] = dependencies
        for depspec in depspecs:
          dependencies.update(self.context.resolve(depspec))
      return self.GenInfo(gen, deps)

    return self._inject_target(gentarget, dependees,
                               create_geninfo(gentarget.language),
                               gen_files_for_source,
                               create_target)

  def _inject_target(self, target, dependees, geninfo, gen_files_for_source, create_target):
    files = []
    has_service = False
    for source in target.sources_relative_to_buildroot():
      services = calculate_services(source)
      genfiles = gen_files_for_source[source]
      has_service = has_service or services
      files.extend(genfiles)
    deps = OrderedSet(geninfo.deps['service' if has_service else 'structs'])
    deps.update(target.dependencies)
    target_type = _TARGET_TYPE_FOR_LANG[target.language]
    tgt = create_target(files, deps, target_type)
    tgt.derived_from = target
    tgt.add_labels('codegen', 'synthetic')
    for dependee in dependees:
      if isinstance(dependee, InternalTarget):
        dependee.update_dependencies((tgt,))
      else:
        # TODO(John Sirois): rationalize targets with dependencies.
        # JarLibrary or PythonTarget dependee on the thrift target
        dependee.dependencies.add(tgt)
    return tgt

  def parse_gen_file_map(self, gen_file_map_path, outdir):
    d = defaultdict(set)
    with open(gen_file_map_path, 'r') as deps:
      for dep in deps:
        src, cls = dep.strip().split('->')
        src = os.path.relpath(src.strip())
        cls = os.path.relpath(cls.strip(), outdir)
        d[src].add(cls)
    return d

  def gen_file_map_path_for_target(self, target, outdir):
    return os.path.join(outdir, 'gen-file-map-by-target', target.id)

  def gen_file_map_for_target(self, target, outdir):
    gen_file_map = self.gen_file_map_path_for_target(target, outdir)
    return self.parse_gen_file_map(gen_file_map, outdir)

  def gen_file_map(self, targets, outdir):
    gen_file_map = defaultdict(set)
    for target in targets:
      target_gen_file_map = self.gen_file_map_for_target(target, outdir)
      gen_file_map.update(target_gen_file_map)
    return gen_file_map

  def write_gen_file_map_for_target(self, gen_file_map, target, outdir):
    def calc_srcs(target):
      _, srcs = calculate_compile_sources([target], self.is_gentarget)
      return srcs
    with safe_open(self.gen_file_map_path_for_target(target, outdir), 'w') as f:
      for src in sorted(calc_srcs(target)):
        clss = gen_file_map[src]
        for cls in sorted(clss):
          print('%s -> %s' % (src, os.path.join(outdir, cls)), file=f)

  def write_gen_file_map(self, gen_file_map, targets, outdir):
    for target in targets:
      self.write_gen_file_map_for_target(gen_file_map, target, outdir)

  def is_gentarget(self, target):
    result = (isinstance(target, JavaThriftLibrary)
              and target.compiler in self.compiler_for_name.keys())

    if result and target.language not in self.compiler_for_name[target.compiler].langs:
      raise TaskError("%s can not generate %s" % (target.compiler, target.language))
    return result

  @staticmethod
  def _validate(targets):
    ValidateCompilerConfig = namedtuple('ValidateCompilerConfig', ['language', 'rpc_style'])

    def compiler_config(tgt):
      # Note compiler is not present in this signature. At this time
      # Scrooge and the Apache thrift generators produce identical
      # java sources, and the Apache generator does not produce scala
      # sources. As there's no permutation allowing the creation of
      # incompatible sources with the same language+rpc_style we omit
      # the compiler from the signature at this time.
      return ValidateCompilerConfig(language=tgt.language, rpc_style=tgt.rpc_style)

    mismatched_compiler_configs = defaultdict(set)

    for target in filter(lambda t: isinstance(t, JavaThriftLibrary), targets):
      mycompilerconfig = compiler_config(target)
      def collect(dep):
        if mycompilerconfig != compiler_config(dep):
          mismatched_compiler_configs[target].add(dep)
      target.walk(collect, predicate=lambda t: isinstance(t, JavaThriftLibrary))

    if mismatched_compiler_configs:
      msg = ['Thrift dependency trees must be generated with a uniform compiler configuration.\n\n']
      for tgt in sorted(mismatched_compiler_configs.keys()):
        msg.append('%s - %s\n' % (tgt, compiler_config(tgt)))
        for dep in mismatched_compiler_configs[tgt]:
          msg.append('    %s - %s\n' % (dep, compiler_config(dep)))
      raise TaskError(''.join(msg))


NAMESPACE_PARSER = re.compile(r'^\s*namespace\s+([^\s]+)\s+([^\s]+)\s*$')
TYPE_PARSER = re.compile(r'^\s*(const|enum|exception|service|struct|union)\s+([^\s{]+).*')


# TODO(John Sirois): consolidate thrift parsing to 1 pass instead of 2
def calculate_services(source):
  """Calculates the services generated for the given thrift IDL source.
  Returns an interable of services
  """

  with open(source, 'r') as thrift:
    namespaces = dict()
    types = defaultdict(set)
    for line in thrift:
      match = NAMESPACE_PARSER.match(line)
      if match:
        lang = match.group(1)
        namespace = match.group(2)
        namespaces[lang] = namespace
      else:
        match = TYPE_PARSER.match(line)
        if match:
          typename = match.group(1)
          name = match.group(2)
          types[typename].add(name)

    return types['service']

########NEW FILE########
__FILENAME__ = sorttargets
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict

from twitter.common.util import topological_sort

from twitter.pants.base.target import Target
from twitter.pants.tasks.console_task import ConsoleTask


class SortTargets(ConsoleTask):
  @staticmethod
  def _is_target(item):
    return isinstance(item, Target)

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(SortTargets, cls).setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag("reverse"), mkflag("reverse", negate=True),
                            dest="sort_targets_reverse", default=False,
                            action="callback", callback=mkflag.set_bool,
                            help="[%default] Sort least depenendent to most.")

  def __init__(self, *args, **kwargs):
    super(SortTargets, self).__init__(*args, **kwargs)
    self._reverse = self.context.options.sort_targets_reverse

  def console_output(self, targets):
    depmap = defaultdict(set)

    def map_deps(target):
      # TODO(John Sirois): rationalize target hierarchies - this is the only 'safe' way to treat
      # both python and jvm targets today.
      if hasattr(target, 'dependencies'):
        deps = depmap[str(target.address)]
        for dep in target.dependencies:
          for resolved in filter(self._is_target, dep.resolve()):
            deps.add(str(resolved.address))

    for root in self.context.target_roots:
      root.walk(map_deps, self._is_target)

    tsorted = []
    for group in topological_sort(depmap):
      tsorted.extend(group)
    if self._reverse:
      tsorted = reversed(tsorted)

    roots = set(str(root.address) for root in self.context.target_roots)
    for address in tsorted:
      if address in roots:
        yield address

########NEW FILE########
__FILENAME__ = specs_run
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.collections import OrderedSet

from twitter.pants.base.workunit import WorkUnit
from twitter.pants.binary_util import safe_args
from twitter.pants.java.util import execute_java
from .jvm_task import JvmTask

from . import TaskError


class SpecsRun(JvmTask):
  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag('skip'), mkflag('skip', negate=True), dest = 'specs_run_skip',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help='[%default] Skip running specs')

    option_group.add_option(mkflag('debug'), mkflag('debug', negate=True), dest = 'specs_run_debug',
                            action='callback', callback=mkflag.set_bool, default=False,
                            help='[%default] Run specs with a debugger')

    option_group.add_option(mkflag('jvmargs'), dest='specs_run_jvm_options', action='append',
                            help='Runs specs in a jvm with these extra jvm options.')

    option_group.add_option(mkflag('test'), dest='specs_run_tests', action='append',
                            help='[%default] Force running of just these specs.  Tests can be '
                                 'specified either by fully qualified classname or '
                                 'full file path.')

    option_group.add_option(mkflag('color'), mkflag('color', negate=True),
                            dest='specs_run_color', default=True,
                            action='callback', callback=mkflag.set_bool,
                            help='[%default] Emit test result with ANSI terminal color codes.')

  def __init__(self, context):
    super(SpecsRun, self).__init__(context)

    self._specs_bootstrap_key = 'specs'
    bootstrap_tools = context.config.getlist('specs-run', 'bootstrap-tools',
                                             default=[':scala-specs-2.9.3'])
    self._jvm_tool_bootstrapper.register_jvm_tool(self._specs_bootstrap_key, bootstrap_tools)

    self.confs = context.config.getlist('specs-run', 'confs', default=['default'])

    self._jvm_options = context.config.getlist('specs-run', 'jvm_args', default=[])
    if context.options.specs_run_jvm_options:
      self._jvm_options.extend(context.options.specs_run_jvm_options)
    if context.options.specs_run_debug:
      self._jvm_options.extend(context.config.getlist('jvm', 'debug_args'))

    self.skip = context.options.specs_run_skip
    self.color = context.options.specs_run_color

    self.workdir = context.config.get('specs-run', 'workdir')

    self.tests = context.options.specs_run_tests

  def execute(self, targets):
    if not self.skip:
      def run_tests(tests):
        args = ['--color'] if self.color else []
        args.append('--specs=%s' % ','.join(tests))
        specs_runner_main = 'com.twitter.common.testing.ExplicitSpecsRunnerMain'

        bootstrapped_cp = self._jvm_tool_bootstrapper.get_jvm_tool_classpath(
            self._specs_bootstrap_key)
        classpath = self.classpath(
            bootstrapped_cp,
            confs=self.confs,
            exclusives_classpath=self.get_base_classpath_for_target(targets[0]))

        result = execute_java(
          classpath=classpath,
          main=specs_runner_main,
          jvm_options=self._jvm_options,
          args=args,
          workunit_factory=self.context.new_workunit,
          workunit_name='specs',
          workunit_labels=[WorkUnit.TEST]
        )
        if result != 0:
          raise TaskError('java %s ... exited non-zero (%i)' % (specs_runner_main, result))

      if self.tests:
        run_tests(self.tests)
      else:
        with safe_args(self.calculate_tests(targets)) as tests:
          if tests:
            run_tests(tests)

  def calculate_tests(self, targets):
    tests = OrderedSet()
    for target in targets:
      if target.is_scala and target.is_test:
        tests.update(target.sources_relative_to_buildroot())
    return tests

########NEW FILE########
__FILENAME__ = targets_help
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import inspect
import textwrap

from string import Template

from twitter.pants.base.target import Target
from twitter.pants.targets.sources import SourceRoot
from twitter.pants.tasks.console_task import ConsoleTask


class TargetsHelp(ConsoleTask):
  """Provides online help for installed targets.

  This task provides online help modes for installed targets. Without args,
  all installed targets are listed with their one-line description.
  An optional flag allows users to specify a target they want detailed
  help about."""

  INSTALLED_TARGETS_HEADER = '\n'.join([
    'For details about a specific target, try: ./pants goal targets --targets-details=target_name',
    'Installed target types:\n',
  ])

  DETAILS_HEADER = Template('TARGET NAME\n\n  $name -- $desc\n\nTARGET ARGUMENTS\n')

  # TODO(Travis Crawford): Eliminate this mapping once pants has been moved to Github.
  # Since pants already aliases Target names, the ideal way of doing this would be:
  #
  #   (a) Add a method to all Target objects that provides their alias, rather
  #       than renaming elsewhere. This way Target instances are self-contained.
  #       Of course, if BUILD files used a template system this would not be necessary.
  #
  #       @classmethod
  #       def get_alias(cls):
  #         raise ValueError('subclasses must override alias name.')
  #
  #   (b) Replace aliases with something like:
  #       https://cgit.twitter.biz/science/tree/src/python/twitter/pants/__init__.py#n88
  #
  #       to_alias = [AnnotationProcessor, ...]
  #       for t in to_alias:
  #         vars()[t.get_alias()] = t
  TARGET_TO_ALIAS = {
    'AnnotationProcessor': 'annotation_processor',
    'Artifact': 'artifact',
    'Benchmark': 'benchmark',
    'Bundle': 'bundle',
    'Credentials': 'credentials',
    'JarLibrary': 'dependencies',
    'PythonEgg': 'egg',
    'Exclude': 'exclude',
    'Pants': 'fancy_pants',
    'JarDependency': 'jar',
    'JavaLibrary': 'java_library',
    'JavaAntlrLibrary': 'java_antlr_library',
    'JavaProtobufLibrary': 'java_protobuf_library',
    'JavaTests': 'junit_tests',
    'JavaThriftLibrary': 'java_thrift_library',
    'JavaThriftstoreDMLLibrary': 'java_thriftstore_dml_library',
    'JvmBinary': 'jvm_binary',
    'JvmApp': 'jvm_app',
    # For testing. When targets define their own alias (or we use a template
    # system for BUILD files) this need to register targets goes away.
    'MyTarget': 'my_target',
    'OinkQuery': 'oink_query',
    'Page': 'page',
    'PythonArtifact': 'python_artifact',
    'PythonBinary': 'python_binary',
    'PythonLibrary': 'python_library',
    'PythonAntlrLibrary': 'python_antlr_library',
    'PythonRequirement': 'python_requirement',
    'PythonThriftLibrary': 'python_thrift_library',
    'PythonTests': 'python_tests',
    'PythonTestSuite': 'python_test_suite',
    'Repository': 'repo',
    'Resources': 'resources',
    'ScalaLibrary': 'scala_library',
    'ScalaTests': 'scala_specs',
    'ScalacPlugin': 'scalac_plugin',
    'SourceRoot': 'source_root',
    'ThriftJar': 'thrift_jar',
    'ThriftLibrary': 'thrift_library',
    'Wiki': 'wiki',
  }

  ALIAS_TO_TARGET = {}
  MAX_ALIAS_LEN = 0

  for k, v in TARGET_TO_ALIAS.items():
    ALIAS_TO_TARGET[v] = k
    MAX_ALIAS_LEN = max(MAX_ALIAS_LEN, len(v))

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(TargetsHelp, cls).setup_parser(option_group, args, mkflag)
    option_group.add_option(mkflag("details"), dest="goal_targets_details", default=None,
                            help='Display detailed information about the specific target type.')

  def console_output(self, targets):
    """Display a list of installed target types, or details about a specific target type."""
    target_types = {}
    for target_type in SourceRoot._ROOTS_BY_TYPE.keys():
      target_types[target_type.__name__] = target_type

    if self.context.options.goal_targets_details is None:
      return self._get_installed_targets(target_types)
    else:
      return self._get_details(target_types[self.ALIAS_TO_TARGET[self.context.options.goal_targets_details]])

  @staticmethod
  def _get_arg_help(docstring):
    """Given a docstring, return a map of arg to help string.

    Pants target constructor docstrings should document arguments as follows.
    Note constructor docstrings only document arguments. All documentation about
    the class itself belong in the class docstring.

    myarg: the description
    anotherarg: this description is continued
      on the next line"""
    arg_help = {}

    if docstring is None:
      return arg_help

    last = None
    import re
    for line in docstring.split('\n'):
      if line == '':
        continue
      match = re.search('^\s*:param[\w ]* (\w+):\s(.*)$', line)
      if match:
        last = match.group(1)
        arg_help[last] = match.group(2)
      else:
        arg_help[last] += ' %s' % line.strip()
    return arg_help

  @staticmethod
  def _get_installed_targets(target_types):
    """List installed targets and their one-line description."""
    lines = [TargetsHelp.INSTALLED_TARGETS_HEADER]
    for target_type in sorted(target_types.keys()):
      if target_types[target_type].__doc__ is None:
        desc = 'Description unavailable.'
      else:
        desc = target_types[target_type].__doc__.split('\n')[0]
      lines.append('  %s: %s' % (
        TargetsHelp.TARGET_TO_ALIAS[target_type].rjust(TargetsHelp.MAX_ALIAS_LEN), desc))
    return lines

  @staticmethod
  def _get_details(target):
    """Get detailed help for the given target."""
    assert target is not None and issubclass(target, Target)

    arg_spec = inspect.getargspec(target.__init__)
    arg_help = TargetsHelp._get_arg_help(target.__init__.__doc__)

    min_default_idx = 0
    if arg_spec.defaults is None:
      min_default_idx = len(arg_spec.args)
    elif len(arg_spec.args) > len(arg_spec.defaults):
      min_default_idx = len(arg_spec.args) - len(arg_spec.defaults)

    lines = [TargetsHelp.DETAILS_HEADER.substitute(
      name=TargetsHelp.TARGET_TO_ALIAS[target.__name__], desc=target.__doc__)]

    max_width = 0
    for arg in arg_spec.args:
      max_width = max(max_width, len(arg))

    wrapper = textwrap.TextWrapper(subsequent_indent=' '*(max_width+4))

    for idx, val in enumerate(arg_spec.args):
      has_default = False
      default_val = None

      if idx >= min_default_idx:
        has_default = True
        default_val = arg_spec.defaults[idx-min_default_idx]

      if val == 'self':
        continue
      help_str = 'No help available for this argument.'
      try:
        help_str = arg_help[val]
      except KeyError:
        pass
      if has_default:
        help_str += ' (default: %s) ' % str(default_val)
      lines.append('  %s: %s' % (val.rjust(max_width), '\n'.join(wrapper.wrap(help_str))))
    return lines

########NEW FILE########
__FILENAME__ = task
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict

import itertools
import os
import shutil
import sys
import threading

from contextlib import contextmanager

from twitter.common.collections.orderedset import OrderedSet

from twitter.pants.base.build_invalidator import BuildInvalidator, CacheKeyGenerator
from twitter.pants.base.config import Config
from twitter.pants.base.hash_utils import hash_file
from twitter.pants.base.worker_pool import Work
from twitter.pants.base.workunit import WorkUnit
from twitter.pants.cache.cache_setup import create_artifact_cache
from twitter.pants.cache.read_write_artifact_cache import ReadWriteArtifactCache
from twitter.pants.ivy.bootstrapper import Bootstrapper  # XXX
from twitter.pants.java.executor import Executor  # XXX
from twitter.pants.reporting.reporting_utils import items_to_report_element

from .jvm_tool_bootstrapper import JvmToolBootstrapper  # XXX
from .cache_manager import CacheManager, InvalidationCheck, VersionedTargetSet
from .ivy_utils import IvyUtils  # XXX
from .task_error import TaskError


class Task(object):
  # Protect writes to the global map of jar path -> symlinks to that jar.
  symlink_map_lock = threading.Lock()

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    """Set up the cmd-line parser.

    Subclasses can add flags to the pants command line using the given option group.
    Flag names should be created with mkflag([name]) to ensure flags are properly name-spaced
    amongst other tasks.
    """

  def __init__(self, context):
    self.context = context
    self.dry_run = self.can_dry_run() and context.options.dry_run
    self._pants_workdir = self.context.config.getdefault('pants_workdir')
    self._cache_key_generator = CacheKeyGenerator(
        context.config.getdefault('cache_key_gen_version', default=None))
    self._read_artifact_cache_spec = None
    self._write_artifact_cache_spec = None
    self._artifact_cache = None
    self._artifact_cache_setup_lock = threading.Lock()

    default_invalidator_root = os.path.join(self.context.config.getdefault('pants_workdir'),
                                            'build_invalidator')
    self._build_invalidator_dir = os.path.join(
        context.config.get('tasks', 'build_invalidator', default=default_invalidator_root),
        self.product_type())
    self._jvm_tool_bootstrapper = JvmToolBootstrapper(self.context.products)

  def register_jvm_tool(self, key, target_addrs):
    self._jvm_tool_bootstrapper.register_jvm_tool(key, target_addrs)

  def tool_classpath(self, key, executor=None):
    return self._jvm_tool_bootstrapper.get_jvm_tool_classpath(key, executor)

  def lazy_tool_classpath(self, key, executor=None):
    return self._jvm_tool_bootstrapper.get_lazy_jvm_tool_classpath(key, executor)

  def setup_artifact_cache_from_config(self, config_section=None):
    """Subclasses can call this in their __init__() to set up artifact caching for that task type.

    Uses standard config file keys to find the cache spec.
    The cache is created lazily, as needed.
    """
    section = config_section or Config.DEFAULT_SECTION
    read_spec = self.context.config.getlist(section, 'read_artifact_caches', default=[])
    write_spec = self.context.config.getlist(section, 'write_artifact_caches', default=[])
    self.setup_artifact_cache(read_spec, write_spec)

  def setup_artifact_cache(self, read_spec, write_spec):
    """Subclasses can call this in their __init__() to set up artifact caching for that task type.

    See docstring for pants.cache.create_artifact_cache() for details on the spec format.
    The cache is created lazily, as needed.

    """
    self._read_artifact_cache_spec = read_spec
    self._write_artifact_cache_spec = write_spec

  def _create_artifact_cache(self, spec, action):
    if len(spec) > 0:
      pants_workdir = self.context.config.getdefault('pants_workdir')
      my_name = self.__class__.__name__
      return create_artifact_cache(self.context.log, pants_workdir, spec, my_name, action)
    else:
      return None

  def get_artifact_cache(self):
    with self._artifact_cache_setup_lock:
      if (self._artifact_cache is None
          and (self._read_artifact_cache_spec or self._write_artifact_cache_spec)):
        self._artifact_cache = ReadWriteArtifactCache(
            self._create_artifact_cache(self._read_artifact_cache_spec, 'will read from'),
            self._create_artifact_cache(self._write_artifact_cache_spec, 'will write to'))
      return self._artifact_cache

  def artifact_cache_reads_enabled(self):
    return bool(self._read_artifact_cache_spec) and self.context.options.read_from_artifact_cache

  def artifact_cache_writes_enabled(self):
    return bool(self._write_artifact_cache_spec) and self.context.options.write_to_artifact_cache

  def product_type(self):
    """Set the product type for this task.

    By default, each task is considered as creating a unique product type.
    Subclasses can override this to specify a shared product type, e.g., 'classes'.

    Tasks with the same product type can invalidate each other's targets, e.g., if a ScalaLibrary
    depends on a JavaLibrary, a change to the JavaLibrary will invalidate the ScalaLibrary because
    they both have the same product type.
    """
    return self.__class__.__name__

  def can_dry_run(self):
    """Subclasses can override this to indicate that they respect the --dry-run flag.

    It's the subclass task's responsibility to do the right thing if this flag is set.

    Note that tasks such as codegen and ivy resolution cannot dry-run, because subsequent
    cache key computation will fail on missing sources/external deps.
    """
    return False

  def execute(self, targets):
    """Executes this task against targets, which may be a subset of the current context targets."""
    raise TaskError('execute() not implemented')

  def invalidate_for(self):
    """Provides extra objects that participate in invalidation.

    Subclasses can override and return an object that should be checked for changes when
    managing target invalidation.  If the pickled form of returned object changes
    between runs all targets will be invalidated.
    """
    return None

  def invalidate_for_files(self):
    """Provides extra files that participate in invalidation.

    Subclasses can override and return a list of full paths to extra, non-source files that should
    be checked for changes when managing target invalidation. This is useful for tracking
    changes to pre-built build tools, e.g., the thrift compiler.
    """
    return []

  def invalidate(self):
    """Invalidates all targets for this task."""
    BuildInvalidator(self._build_invalidator_dir).force_invalidate_all()

  @contextmanager
  def invalidated(self, targets, only_buildfiles=False, invalidate_dependents=False,
                  partition_size_hint=sys.maxint, silent=False):
    """Checks targets for invalidation, first checking the artifact cache.
    Subclasses call this to figure out what to work on.

    targets:               The targets to check for changes.
    only_buildfiles:       If True, then only the target's BUILD files are checked for changes, not
                           its sources.
    invalidate_dependents: If True then any targets depending on changed targets are invalidated.
    partition_size_hint:   Each VersionedTargetSet in the yielded list will represent targets
                           containing roughly this number of source files, if possible. Set to
                           sys.maxint for a single VersionedTargetSet. Set to 0 for one
                           VersionedTargetSet per target. It is up to the caller to do the right
                           thing with whatever partitioning it asks for.

    Yields an InvalidationCheck object reflecting the (partitioned) targets.

    If no exceptions are thrown by work in the block, the build cache is updated for the targets.
    Note: the artifact cache is not updated. That must be done manually.
    """
    extra_data = [self.invalidate_for()]

    for f in self.invalidate_for_files():
      extra_data.append(hash_file(f))

    cache_manager = CacheManager(self._cache_key_generator,
                                 self._build_invalidator_dir,
                                 invalidate_dependents,
                                 extra_data,
                                 only_externaldeps=only_buildfiles)

    invalidation_check = cache_manager.check(targets, partition_size_hint)

    if invalidation_check.invalid_vts and self.artifact_cache_reads_enabled():
      with self.context.new_workunit('cache'):
        cached_vts, uncached_vts = \
          self.check_artifact_cache(self.check_artifact_cache_for(invalidation_check))
      if cached_vts:
        cached_targets = [vt.target for vt in cached_vts]
        for t in cached_targets:
          self.context.run_tracker.artifact_cache_stats.add_hit('default', t)
        if not silent:
          self._report_targets('Using cached artifacts for ', cached_targets, '.')
      if uncached_vts:
        uncached_targets = [vt.target for vt in uncached_vts]
        for t in uncached_targets:
          self.context.run_tracker.artifact_cache_stats.add_miss('default', t)
        if not silent:
          self._report_targets('No cached artifacts for ', uncached_targets, '.')
      # Now that we've checked the cache, re-partition whatever is still invalid.
      invalidation_check = \
        InvalidationCheck(invalidation_check.all_vts, uncached_vts, partition_size_hint)

    if not silent:
      targets = []
      sources = []
      num_invalid_partitions = len(invalidation_check.invalid_vts_partitioned)
      for vt in invalidation_check.invalid_vts_partitioned:
        targets.extend(vt.targets)
        sources.extend(vt.cache_key.sources)
      if len(targets):
        msg_elements = ['Invalidated ',
                        items_to_report_element([t.address.reference() for t in targets], 'target')]
        if len(sources) > 0:
          msg_elements.append(' containing ')
          msg_elements.append(items_to_report_element(sources, 'source file'))
        if num_invalid_partitions > 1:
          msg_elements.append(' in %d target partitions' % num_invalid_partitions)
        msg_elements.append('.')
        self.context.log.info(*msg_elements)

    # Yield the result, and then mark the targets as up to date.
    yield invalidation_check
    if not self.dry_run:
      for vt in invalidation_check.invalid_vts:
        vt.update()  # In case the caller doesn't update.

  def check_artifact_cache_for(self, invalidation_check):
    """Decides which VTS to check the artifact cache for.

    By default we check for each invalid target. Can be overridden, e.g., to
    instead check only for a single artifact for the entire target set.
    """
    return invalidation_check.invalid_vts

  def check_artifact_cache(self, vts):
    """Checks the artifact cache for the specified list of VersionedTargetSets.

    Returns a pair (cached, uncached) of VersionedTargets that were
    satisfied/unsatisfied from the cache.
    """
    return self.do_check_artifact_cache(vts)

  def do_check_artifact_cache(self, vts, post_process_cached_vts=None):
    """Checks the artifact cache for the specified list of VersionedTargetSets.

    Returns a pair (cached, uncached) of VersionedTargets that were
    satisfied/unsatisfied from the cache.
    """
    if not vts:
      return [], []

    cached_vts = []
    uncached_vts = OrderedSet(vts)

    with self.context.new_workunit(name='check', labels=[WorkUnit.MULTITOOL]) as parent:
      res = self.context.submit_foreground_work_and_wait(
        Work(lambda vt: bool(self.get_artifact_cache().use_cached_files(vt.cache_key)),
             [(vt, ) for vt in vts], 'fetch'), workunit_parent=parent)
    for vt, was_in_cache in zip(vts, res):
      if was_in_cache:
        cached_vts.append(vt)
        uncached_vts.discard(vt)
    # Note that while the input vts may represent multiple targets (for tasks that overrride
    # check_artifact_cache_for), the ones we return must represent single targets.
    def flatten(vts):
      return list(itertools.chain.from_iterable([vt.versioned_targets for vt in vts]))
    all_cached_vts, all_uncached_vts = flatten(cached_vts), flatten(uncached_vts)
    if post_process_cached_vts:
      post_process_cached_vts(all_cached_vts)
    for vt in all_cached_vts:
      vt.update()
    return all_cached_vts, all_uncached_vts

  def update_artifact_cache(self, vts_artifactfiles_pairs):
    """Write to the artifact cache, if we're configured to.

    vts_artifactfiles_pairs - a list of pairs (vts, artifactfiles) where
      - vts is single VersionedTargetSet.
      - artifactfiles is a list of absolute paths to artifacts for the VersionedTargetSet.
    """
    update_artifact_cache_work = self.get_update_artifact_cache_work(vts_artifactfiles_pairs)
    if update_artifact_cache_work:
      self.context.submit_background_work_chain([update_artifact_cache_work],
                                                parent_workunit_name='cache')

  def get_update_artifact_cache_work(self, vts_artifactfiles_pairs, cache=None):
    """Create a Work instance to update the artifact cache, if we're configured to.

    vts_artifactfiles_pairs - a list of pairs (vts, artifactfiles) where
      - vts is single VersionedTargetSet.
      - artifactfiles is a list of paths to artifacts for the VersionedTargetSet.
    """
    cache = cache or self.get_artifact_cache()
    if cache:
      if len(vts_artifactfiles_pairs) == 0:
        return None
        # Do some reporting.
      targets = set()
      for vts, _ in vts_artifactfiles_pairs:
        targets.update(vts.targets)
      self._report_targets('Caching artifacts for ', list(targets), '.')
      # Cache the artifacts.
      args_tuples = []
      for vts, artifactfiles in vts_artifactfiles_pairs:
        if self.context.options.verify_artifact_cache:
          pass  # TODO: Verify that the artifact we just built is identical to the cached one?
        args_tuples.append((vts.cache_key, artifactfiles))
      return Work(lambda *args: cache.insert(*args), args_tuples, 'insert')
    else:
      return None

  def _report_targets(self, prefix, targets, suffix):
    self.context.log.info(
      prefix,
      items_to_report_element([t.address.reference() for t in targets], 'target'),
      suffix)

  def ivy_resolve(self, targets, executor=None, symlink_ivyxml=False, silent=False,
                  workunit_name=None, workunit_labels=None):

    if executor and not isinstance(executor, Executor):
      raise ValueError('The executor must be an Executor instance, given %s of type %s'
                       % (executor, type(executor)))
    ivy = Bootstrapper.default_ivy(java_executor=executor,
                                   bootstrap_workunit_factory=self.context.new_workunit)

    targets = set(targets)

    if not targets:
      return []

    work_dir = self.context.config.get('ivy-resolve', 'workdir')
    ivy_utils = IvyUtils(config=self.context.config,
                         options=self.context.options,
                         log=self.context.log)

    with self.invalidated(targets,
                          only_buildfiles=True,
                          invalidate_dependents=True,
                          silent=silent) as invalidation_check:
      global_vts = VersionedTargetSet.from_versioned_targets(invalidation_check.all_vts)
      target_workdir = os.path.join(work_dir, global_vts.cache_key.hash)
      target_classpath_file = os.path.join(target_workdir, 'classpath')
      raw_target_classpath_file = target_classpath_file + '.raw'
      raw_target_classpath_file_tmp = raw_target_classpath_file + '.tmp'
      # A common dir for symlinks into the ivy2 cache. This ensures that paths to jars
      # in artifact-cached analysis files are consistent across systems.
      # Note that we have one global, well-known symlink dir, again so that paths are
      # consistent across builds.
      symlink_dir = os.path.join(work_dir, 'jars')

      # Note that it's possible for all targets to be valid but for no classpath file to exist at
      # target_classpath_file, e.g., if we previously built a superset of targets.
      if invalidation_check.invalid_vts or not os.path.exists(raw_target_classpath_file):
        args = ['-cachepath', raw_target_classpath_file_tmp]

        def exec_ivy():
          ivy_utils.exec_ivy(
              target_workdir=target_workdir,
              targets=targets,
              args=args,
              ivy=ivy,
              workunit_name='ivy',
              workunit_factory=self.context.new_workunit,
              symlink_ivyxml=symlink_ivyxml)

        if workunit_name:
          with self.context.new_workunit(name=workunit_name, labels=workunit_labels or []):
            exec_ivy()
        else:
          exec_ivy()

        if not os.path.exists(raw_target_classpath_file_tmp):
          raise TaskError('Ivy failed to create classpath file at %s'
                          % raw_target_classpath_file_tmp)
        shutil.move(raw_target_classpath_file_tmp, raw_target_classpath_file)

        if self.artifact_cache_writes_enabled():
          self.update_artifact_cache([(global_vts, [raw_target_classpath_file])])

    # Make our actual classpath be symlinks, so that the paths are uniform across systems.
    # Note that we must do this even if we read the raw_target_classpath_file from the artifact
    # cache. If we cache the target_classpath_file we won't know how to create the symlinks.
    symlink_map = IvyUtils.symlink_cachepath(self.context.ivy_home, raw_target_classpath_file,
                                             symlink_dir, target_classpath_file)
    with Task.symlink_map_lock:
      all_symlinks_map = self.context.products.get_data('symlink_map') or defaultdict(list)
      for path, symlink in symlink_map.items():
        all_symlinks_map[os.path.realpath(path)].append(symlink)
      self.context.products.safe_create_data('symlink_map', lambda: all_symlinks_map)

    with IvyUtils.cachepath(target_classpath_file) as classpath:
      stripped_classpath = [path.strip() for path in classpath]
      return [path for path in stripped_classpath if ivy_utils.is_classpath_artifact(path)]

  def get_workdir(self, section="default", key="workdir", workdir=None):
    return self.context.config.get(section,
                                   key,
                                   default=os.path.join(self._pants_workdir,
                                                        workdir or self.__class__.__name__.lower()))

########NEW FILE########
__FILENAME__ = task_error
from twitter.pants.base.build_manual import manual


@manual.builddict()
class TaskError(Exception):
  """
    Raised to indicate a task has failed.

    :param int exit_code: an optional exit code (1, by default)
  """
  def __init__(self, *args, **kwargs):
    self._exit_code = kwargs.pop('exit_code', 1)
    super(TaskError, self).__init__(*args, **kwargs)

  @property
  def exit_code(self):
    return self._exit_code

########NEW FILE########
__FILENAME__ = thrift_gen
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import errno
import os
import re
import subprocess

from collections import defaultdict, namedtuple

from twitter.common import log
from twitter.common.collections import OrderedSet
from twitter.common.dirutil import safe_mkdir

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.java_thrift_library import JavaThriftLibrary
from twitter.pants.targets.python_library import PythonLibrary
from twitter.pants.targets.python_thrift_library import PythonThriftLibrary
from twitter.pants.tasks import TaskError
from twitter.pants.thrift_util import calculate_compile_roots, select_thrift_binary

from .code_gen import CodeGen


def _copytree(from_base, to_base):
  def abort(error):
    raise TaskError('Failed to copy from %s to %s: %s' % (from_base, to_base, error))

  # TODO(John Sirois): Consider adding a unit test and lifting this to common/dirutils or similar
  def safe_link(src, dst):
    try:
      os.link(src, dst)
    except OSError as e:
      if e.errno != errno.EEXIST:
        raise e

  for dirpath, dirnames, filenames in os.walk(from_base, topdown=True, onerror=abort):
    to_path = os.path.join(to_base, os.path.relpath(dirpath, from_base))
    for dirname in dirnames:
      safe_mkdir(os.path.join(to_path, dirname))
    for filename in filenames:
      safe_link(os.path.join(dirpath, filename), os.path.join(to_path, filename))


class ThriftGen(CodeGen):
  GenInfo = namedtuple('GenInfo', ['gen', 'deps'])
  ThriftSession = namedtuple('ThriftSession', ['outdir', 'cmd', 'process'])

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    option_group.add_option(mkflag("outdir"), dest="thrift_gen_create_outdir",
                            help="Emit generated code in to this directory.")

    option_group.add_option(mkflag("version"), dest="thrift_version",
                            help="Thrift compiler version.")

    option_group.add_option(mkflag("lang"), dest="thrift_gen_langs",  default=[],
                            action="append", type="choice", choices=['python', 'java'],
                            help="Force generation of thrift code for these languages.  Both "
                                 "'python' and 'java' are supported")

  def __init__(self, context):
    CodeGen.__init__(self, context)

    output_dir = (
      context.options.thrift_gen_create_outdir
      or context.config.get('thrift-gen', 'workdir')
    )
    self.combined_dir = os.path.join(output_dir, 'combined')
    self.session_dir = os.path.join(output_dir, 'sessions')

    self.strict = context.config.getbool('thrift-gen', 'strict')
    self.verbose = context.config.getbool('thrift-gen', 'verbose')

    def create_geninfo(key):
      gen_info = context.config.getdict('thrift-gen', key)
      gen = gen_info['gen']
      deps = {}
      for category, depspecs in gen_info['deps'].items():
        dependencies = OrderedSet()
        deps[category] = dependencies
        for depspec in depspecs:
          dependencies.update(context.resolve(depspec))
      return self.GenInfo(gen, deps)

    self.gen_java = create_geninfo('java')
    self.gen_python = create_geninfo('python')

    self.gen_langs = set(context.options.thrift_gen_langs)
    for lang in ('java', 'python'):
      if self.context.products.isrequired(lang):
        self.gen_langs.add(lang)

    self.thrift_binary = select_thrift_binary(context.config,
                                              version=context.options.thrift_version)

  def invalidate_for(self):
    return self.gen_langs

  def invalidate_for_files(self):
    # TODO: This will prevent artifact caching across platforms.
    # Find some cross-platform way to assert the thrift binary version.
    return [self.thrift_binary]

  def is_gentarget(self, target):
    return ((isinstance(target, JavaThriftLibrary) and target.compiler == 'thrift')
            or isinstance(target, PythonThriftLibrary))

  def is_forced(self, lang):
    return lang in self.gen_langs

  def genlangs(self):
    return dict(java=lambda t: t.is_jvm, python=lambda t: t.is_python)

  def genlang(self, lang, targets):
    bases, sources = calculate_compile_roots(targets, self.is_gentarget)

    if lang == 'java':
      gen = self.gen_java.gen
    elif lang == 'python':
      gen = self.gen_python.gen
    else:
      raise TaskError('Unrecognized thrift gen lang: %s' % lang)

    args = [
      self.thrift_binary,
      '--gen', gen,
      '-recurse',
    ]

    if self.strict:
      args.append('-strict')
    if self.verbose:
      args.append('-verbose')
    for base in bases:
      args.extend(('-I', base))

    sessions = []
    for source in sources:
      self.context.log.info('Generating thrift for %s\n' % source)
      # Create a unique session dir for this thrift root.  Sources may be full paths but we only
      # need the path relative to the build root to ensure uniqueness.
      # TODO(John Sirois): file paths should be normalized early on and uniformly, fix the need to
      # relpath here at all.
      relsource = os.path.relpath(source, get_buildroot())
      outdir = os.path.join(self.session_dir, '.'.join(relsource.split(os.path.sep)))
      safe_mkdir(outdir)

      cmd = args[:]
      cmd.extend(('-o', outdir))
      cmd.append(source)
      log.debug('Executing: %s' % ' '.join(cmd))
      sessions.append(self.ThriftSession(outdir, cmd, subprocess.Popen(cmd)))

    result = 0
    for session in sessions:
      if result != 0:
        session.process.kill()
      else:
        result = session.process.wait()
        if result != 0:
          self.context.log.error('Failed: %s' % ' '.join(session.cmd))
        else:
          _copytree(session.outdir, self.combined_dir)
    if result != 0:
      raise TaskError('%s ... exited non-zero (%i)' % (self.thrift_binary, result))

  def createtarget(self, lang, gentarget, dependees):
    if lang == 'java':
      return self._create_java_target(gentarget, dependees)
    elif lang == 'python':
      return self._create_python_target(gentarget, dependees)
    else:
      raise TaskError('Unrecognized thrift gen lang: %s' % lang)

  def _create_java_target(self, target, dependees):
    def create_target(files, deps):
       return self.context.add_new_target(os.path.join(self.combined_dir, 'gen-java'),
                                          JavaLibrary,
                                          name=target.id,
                                          sources=files,
                                          provides=target.provides,
                                          dependencies=deps,
                                          excludes=target.excludes)
    return self._inject_target(target, dependees, self.gen_java, 'java', create_target)

  def _create_python_target(self, target, dependees):
    def create_target(files, deps):
     return self.context.add_new_target(os.path.join(self.combined_dir, 'gen-py'),
                                        PythonLibrary,
                                        name=target.id,
                                        sources=files,
                                        dependencies=deps)
    return self._inject_target(target, dependees, self.gen_python, 'py', create_target)

  def _inject_target(self, target, dependees, geninfo, namespace, create_target):
    files = []
    has_service = False
    for src in target.sources_relative_to_buildroot():
      services, genfiles = calculate_gen(src)
      has_service = has_service or services
      files.extend(genfiles.get(namespace, []))
    deps = geninfo.deps['service' if has_service else 'structs']
    tgt = create_target(files, deps)
    tgt.id = target.id + '.thrift_gen'
    for dependee in dependees:
      if isinstance(dependee, InternalTarget):
        dependee.update_dependencies((tgt,))
      else:
        # TODO(John Sirois): rationalize targets with dependencies.
        # JarLibrary or PythonTarget dependee on the thrift target
        dependee.dependencies.add(tgt)
    return tgt


NAMESPACE_PARSER = re.compile(r'^\s*namespace\s+([^\s]+)\s+([^\s]+)\s*$')
TYPE_PARSER = re.compile(r'^\s*(const|enum|exception|service|struct|union)\s+([^\s{]+).*')


# TODO(John Sirois): consolidate thrift parsing to 1 pass instead of 2
def calculate_gen(source):
  """Calculates the service types and files generated for the given thrift IDL source.

  Returns a tuple of (service types, generated files).
  """

  with open(source, 'r') as thrift:
    lines = thrift.readlines()
    namespaces = {}
    types = defaultdict(set)
    for line in lines:
      match = NAMESPACE_PARSER.match(line)
      if match:
        lang = match.group(1)
        namespace = match.group(2)
        namespaces[lang] = namespace
      else:
        match = TYPE_PARSER.match(line)
        if match:
          typename = match.group(1)
          name = match.group(2)
          types[typename].add(name)

    genfiles = defaultdict(set)

    namespace = namespaces.get('py')
    if namespace:
      genfiles['py'].update(calculate_python_genfiles(namespace, types))

    namespace = namespaces.get('java')
    if namespace:
      genfiles['java'].update(calculate_java_genfiles(namespace, types))

    return types['service'], genfiles


def calculate_python_genfiles(namespace, types):
  basepath = namespace.replace('.', '/')
  def path(name):
    return os.path.join(basepath, '%s.py' % name)
  yield path('__init__')
  if 'const' in types:
    yield path('constants')
  if set(['enum', 'exception', 'struct', 'union']) & set(types.keys()):
    yield path('ttypes')
  for service in types['service']:
    yield path(service)
    yield os.path.join(basepath, '%s-remote' % service)


def calculate_java_genfiles(namespace, types):
  basepath = namespace.replace('.', '/')
  def path(name):
    return os.path.join(basepath, '%s.java' % name)
  if 'const' in types:
    yield path('Constants')
  for typename in ['enum', 'exception', 'service', 'struct', 'union']:
    for name in types[typename]:
      yield path(name)

########NEW FILE########
__FILENAME__ = what_changed
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import sys

from abc import abstractmethod
from collections import defaultdict

from twitter.common.lang import AbstractClass, Compatibility

from twitter.pants.base.build_environment import get_buildroot, get_scm
from twitter.pants.base.build_file import BuildFile
from twitter.pants.base.target import Target
from twitter.pants.scm import Scm

from .console_task import ConsoleTask
from .task_error import TaskError


class WhatChanged(ConsoleTask):
  """Emits the targets that have been modified since a given commit."""

  @classmethod
  def setup_parser(cls, option_group, args, mkflag):
    super(WhatChanged, cls).setup_parser(option_group, args, mkflag)

    option_group.add_option(mkflag('parent'), dest='what_changed_create_prefix', default='HEAD',
                            help='[%default] Identifies the parent tree-ish to calculate changes '
                                 'against.')

    option_group.add_option(mkflag("files"), mkflag("files", negate=True), default=False,
                            action="callback", callback=mkflag.set_bool,
                            dest='what_changed_show_files',
                            help='[%default] Shows changed files instead of the targets that own '
                                 'them.')

  def __init__(self, context, workspace, outstream=sys.stdout):
    if not isinstance(workspace, Workspace):
      raise ValueError('WhatChanged requires a Workspace, given %s' % workspace)

    super(WhatChanged, self).__init__(context, outstream)

    self._workspace = workspace

    self._parent = context.options.what_changed_create_prefix
    self._show_files = context.options.what_changed_show_files

    self._filemap = defaultdict(set)

  def console_output(self, _):
    touched_files = self._get_touched_files()
    if self._show_files:
      for path in touched_files:
        yield path
    else:
      touched_targets = set()
      for path in touched_files:
        for touched_target in self._owning_targets(path):
          if touched_target not in touched_targets:
            touched_targets.add(touched_target)
            yield str(touched_target.address)

  def _get_touched_files(self):
    try:
      return self._workspace.touched_files(self._parent)
    except Workspace.WorkspaceError as e:
      raise TaskError(e)

  def _owning_targets(self, path):
    for build_file in self._candidate_owners(path):
      is_build_file = (build_file.full_path == os.path.join(get_buildroot(), path))
      for address in Target.get_all_addresses(build_file):
        target = Target.get(address)

        # A synthesized target can never own permanent files on disk
        if target != target.derived_from:
          # TODO(John Sirois): tighten up the notion of targets written down in a BUILD by a user
          # vs. targets created by pants at runtime.
          continue

        if target and (is_build_file or ((target.has_sources() or target.has_resources)
                                         and self._owns(target, path))):
          yield target

  def _candidate_owners(self, path):
    build_file = BuildFile(get_buildroot(), relpath=os.path.dirname(path), must_exist=False)
    if build_file.exists():
      yield build_file
    for sibling in build_file.siblings():
      yield sibling
    for ancestor in build_file.ancestors():
      yield ancestor

  def _owns(self, target, path):
    if target not in self._filemap:
      files = self._filemap[target]
      files_owned_by_target = target.sources if target.has_sources() else []
      # TODO (tdesai): This case to handle resources in PythonTarget.
      # Remove this when we normalize resources handling across python and jvm targets.
      if target.has_resources:
        for resource in target.resources:
          if isinstance(resource, Compatibility.string):
            files_owned_by_target.extend(target.resources)
      for owned_file in files_owned_by_target:
        owned_path = os.path.join(target.target_base, owned_file)
        files.add(owned_path)
    return path in self._filemap[target]


class Workspace(AbstractClass):
  """Tracks the state of the current workspace."""

  class WorkspaceError(Exception):
    """Indicates a problem reading the local workspace."""

  @abstractmethod
  def touched_files(self, parent):
    """Returns the set of paths modified between the given parent commit and the current local
    workspace state.
    """


class ScmWorkspace(Workspace):
  """A workspace that uses an Scm to determine the touched files."""

  def __init__(self, scm):
    super(ScmWorkspace, self).__init__()

    self._scm = scm or get_scm()

    if self._scm is None:
      raise self.WorkspaceError('Cannot figure out what changed without a configured '
                                'source-control system.')

  def touched_files(self, parent):
    try:
      return self._scm.changed_files(from_commit=parent, include_untracked=True)
    except Scm.ScmException as e:
      raise self.WorkspaceError("Problem detecting changed files.", e)


class ScmWhatChanged(WhatChanged):
  def __init__(self, context, scm=None, outstream=sys.stdout):
    """Creates a WhatChanged task that uses an Scm to determine changed files.

    context:    The pants execution context.
    scm:        The scm to use, taken from the globally configured scm if None.
    outstream:  The stream to write changed files or targets to.
    """
    super(ScmWhatChanged, self).__init__(context, ScmWorkspace(scm or get_scm()), outstream)

########NEW FILE########
__FILENAME__ = thrift_util
import os
import re

from twitter.pants.binary_util import select_binary


INCLUDE_PARSER = re.compile(r'^\s*include\s+"([^"]+)"\s*([\/\/|\#].*)*$')


def find_includes(basedirs, source, log=None):
  """Finds all thrift files included by the given thrift source.

  :basedirs: A set of thrift source file base directories to look for includes in.
  :source: The thrift source file to scan for includes.
  :log: An optional logger
  """

  all_basedirs = [os.path.dirname(source)]
  all_basedirs.extend(basedirs)

  includes = set()
  with open(source, 'r') as thrift:
    for line in thrift.readlines():
      match = INCLUDE_PARSER.match(line)
      if match:
        capture = match.group(1)
        added = False
        for basedir in all_basedirs:
          include = os.path.join(basedir, capture)
          if os.path.exists(include):
            if log:
              log.debug('%s has include %s' % (source, include))
            includes.add(include)
            added = True
        if not added:
          raise ValueError("%s included in %s not found in bases %s"
                           % (include, source, all_basedirs))
  return includes


def find_root_thrifts(basedirs, sources, log=None):
  """Finds the root thrift files in the graph formed by sources and their recursive includes.

  :basedirs: A set of thrift source file base directories to look for includes in.
  :sources: Seed thrift files to examine.
  :log: An optional logger.
  """

  root_sources = set(sources)
  for source in sources:
    root_sources.difference_update(find_includes(basedirs, source, log=log))
  return root_sources


def calculate_compile_sources_HACK_FOR_SCROOGE_LEGACY(targets, is_thrift_target):
  """Calculates the set of thrift source files that need to be compiled
  as well as their associated import/include directories.
  It does not exclude sources that are included in other sources.

  A tuple of (include dirs, thrift sources) is returned.

  :targets: The targets to examine.
  :is_thrift_target: A predicate to pick out thrift targets for consideration in the analysis.
  """

  dirs = set()
  sources = set()

  def collect_sources(target):
    for source in target.sources:
      dirs.add(os.path.normpath(os.path.join(target.target_base, os.path.dirname(source))))
      sources.add(os.path.join(target.target_base, source))
  for target in targets:
    target.walk(collect_sources, predicate=is_thrift_target)

  return dirs, sources


def calculate_compile_sources(targets, is_thrift_target):
  """Calculates the set of thrift source files that need to be compiled.
  It does not exclude sources that are included in other sources.

  A tuple of (include basedirs, thrift sources) is returned.

  :targets: The targets to examine.
  :is_thrift_target: A predicate to pick out thrift targets for consideration in the analysis.
  """

  basedirs = set()
  sources = set()
  def collect_sources(target):
    basedirs.add(target.target_base)
    sources.update(target.sources_relative_to_buildroot())
  for target in targets:
    target.walk(collect_sources, predicate=is_thrift_target)
  return basedirs, sources


def calculate_compile_roots(targets, is_thrift_target):
  """Calculates the minimal set of thrift source files that need to be compiled.

  A tuple of (include basedirs, root thrift sources) is returned.

  :targets: The targets to examine.
  :is_thrift_target: A predicate to pick out thrift targets for consideration in the analysis.
  """

  basedirs, sources = calculate_compile_sources(targets, is_thrift_target)
  sources = find_root_thrifts(basedirs, sources)
  return basedirs, sources


def select_thrift_binary(config, version=None):
  """Selects a thrift compiler binary matching the current os and architecture.

  By default uses the repo default thrift compiler version specified in the pants config.

  config: The pants config containing thrift thrift binary selection data.
  version: An optional thrift compiler binary version override.
  """
  thrift_supportdir = config.get('thrift-gen', 'supportdir')
  thrift_version = version or config.get('thrift-gen', 'version')
  return select_binary(thrift_supportdir, thrift_version, 'thrift', config)

########NEW FILE########
__FILENAME__ = thrift_parser
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import json
import sys

import antlr3
from antlrgen.twitter.thrift.descriptors.AntlrThriftLexer import AntlrThriftLexer
from antlrgen.twitter.thrift.descriptors.AntlrThriftParser import AntlrThriftParser
from antlrgen.twitter.thrift.descriptors.AntlrThriftTreeWalker import AntlrThriftTreeWalker

from twitter.thrift.text import thrift_json_encoder
from twitter.thrift.descriptors.thrift_parser_error import ThriftParserError

class ThriftParser(object):
  """Parses a thrift file and creates descriptors for all the entities in that file.

  Each of the parse methods below returns an instance of thrift_descriptors.Program."""
  def __init__(self):
    pass

  def parse_string(self, data):
    """Parse from a string."""
    return self._parse(antlr3.ANTLRStringStream(data))

  def parse_file(self, path):
    """Parse from a file specififed by path."""
    return self._parse(antlr3.ANTLRFileStream(path))

  def parse_input(self, input):
    """Parse from a file-like object."""
    return self._parse(antlr3.ANTLRInputStream(input))

  def _parse(self, char_stream):
    # Parse the raw input to an AST.
    lexer = AntlrThriftLexer(char_stream)
    tokens = antlr3.CommonTokenStream(lexer)
    parser = AntlrThriftParser(tokens)
    root = parser.program().tree
    if parser.getNumberOfSyntaxErrors() > 0:
      raise ThriftParserError('Thrift parse failed')

    # Walk the AST.
    nodes = antlr3.tree.CommonTreeNodeStream(root)
    nodes.setTokenStream(tokens)
    walker = AntlrThriftTreeWalker(nodes)
    return walker.program()


########NEW FILE########
__FILENAME__ = thrift_parser_error
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

class ThriftParserError(Exception):
  """An error while parsing a .thrift file."""
  pass

########NEW FILE########
__FILENAME__ = thrift_json_decoder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import json

from thrift.Thrift import TType

class ThriftJSONDecoder(json.JSONDecoder):
  """A decoder that makes python thrift structs JSON deserializable via the
  standard python json module.

  Pass this decoder when reading json, like this:

  json.loads(str, cls=text.ThriftJSONDecoder, <other kwargs>)

  Note that this is not a full protocol implementation in the thrift sense. This
  is just a quick-and-easy parser for unittests etc.
  """
  ROOT_THRIFT_CLASS = 'root_thrift_class'

  def __init__(self, *args, **kwargs):
    self.root_thrift_class = kwargs[ThriftJSONDecoder.ROOT_THRIFT_CLASS]
    del kwargs[ThriftJSONDecoder.ROOT_THRIFT_CLASS]
    super(ThriftJSONDecoder, self).__init__(*args, **kwargs)

  def decode(self, json_str):
    dict = super(ThriftJSONDecoder, self).decode(json_str)
    return self._convert(dict, TType.STRUCT,
                         (self.root_thrift_class, self.root_thrift_class.thrift_spec))

  def _convert(self, val, ttype, ttype_info):
    if ttype == TType.STRUCT:
      (thrift_class, thrift_spec) = ttype_info
      ret = thrift_class()
      for field in thrift_spec:
        if field is not None:
          (tag, field_ttype, field_name, field_ttype_info, dummy) = field
          if field_name in val:
            converted_val = self._convert(val[field_name], field_ttype, field_ttype_info)
            setattr(ret, field_name, converted_val)
    elif ttype == TType.LIST:
      (element_ttype, element_ttype_info) = ttype_info
      ret = [self._convert(x, element_ttype, element_ttype_info) for x in val]
    elif ttype == TType.SET:
      (element_ttype, element_ttype_info) = ttype_info
      ret = set([self._convert(x, element_ttype, element_ttype_info) for x in val])
    elif ttype == TType.MAP:
      (key_ttype, key_ttype_info, val_ttype, val_ttype_info) = ttype_info
      ret = dict([(self._convert(k, key_ttype, key_ttype_info),
                   self._convert(v, val_ttype, val_ttype_info)) for (k, v) in val.iteritems()])
    elif ttype == TType.STRING:
      ret = unicode(val)
    elif ttype == TType.DOUBLE:
      ret = float(val)
    elif ttype == TType.I64:
      ret = long(val)
    elif ttype == TType.I32 or ttype == TType.I16 or ttype == TType.BYTE:
      ret = int(val)
    elif ttype == TType.BOOL:
      ret = not not val
    else:
      raise Exception, 'Unrecognized thrift field type: %d' % ttype
    return ret

def json_to_thrift(json_str, root_thrift_class):
  """A utility shortcut function to parse a thrift json object of the specified class."""
  return json.loads(json_str, cls=ThriftJSONDecoder, root_thrift_class=root_thrift_class)


########NEW FILE########
__FILENAME__ = thrift_json_encoder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import json

class ThriftJSONEncoder(json.JSONEncoder):
  """An encoder that makes python thrift structs JSON serializable via the
  standard python json module.

  Pass this encoder when writing json, like this:

  json.dumps(thriftobj, cls=text.ThriftJSONEncoder, <other kwargs>)

  Note that this is not a full protocol implementation in the thrift sense. This
  is just a quick-and-easy pretty-printer for unittests, debugging etc.
  """
  THRIFT_SPEC = 'thrift_spec'

  def __init__(self, *args, **kwargs):
    super(ThriftJSONEncoder, self).__init__(*args, **kwargs)

  def default(self, o):
    # Handle sets (the value type of a thrift set field). We emit them as lists,
    # sorted by element.
    if isinstance(o, set):
      ret = list(o)
      ret.sort()
      return ret

    # Handle everything that isn't a thrift struct.
    if not hasattr(o, ThriftJSONEncoder.THRIFT_SPEC):
      return super(ThriftJSONEncoder, self).default(o)

    # Handle thrift structs.
    spec = getattr(o, ThriftJSONEncoder.THRIFT_SPEC)
    ret = {}
    for (field_number, type, name, type_info, default) in \
          [field_spec for field_spec in spec if field_spec is not None]:
      if name in o.__dict__:
        val = o.__dict__[name]
        if val != default:
          ret[name] = val
    return ret

def thrift_to_json(o):
  """A utility shortcut function to return a pretty-printed JSON thrift object.

  Map keys will be sorted, making the returned string suitable for use in test comparisons."""
  # ident=2 tells python to pretty-print, and put a newline after each comma. Therefore we
  # set the comma separator to have no space after it. Otherwise it'll be difficult to embed
  # a golden string for comparisons (since our editors strip spaces at the ends of lines).
  return json.dumps(o, sort_keys=True, cls=ThriftJSONEncoder, indent=2, separators=(',', ': '))


########NEW FILE########
__FILENAME__ = generic_struct_parser
# Copyright 2011 Twitter Inc. All rights reserved

__author__ = 'ugo'  # Ugo Di Girolamo

import traceback
from thrift.Thrift import TType

def _type_name(ftype):
  return TType._VALUES_TO_NAMES[ftype]

def read(iprot):
  """Reads arbitrary thrift structs from a thrift protocol.

  Returns a dictionary where:
    - keys are constructed from the id of the field as "FIELD_" + id
    - values are a tuple (type_name, data) where data is:
      - in the case of STRUCTs the dictionary created calling this function
      - in the case of simple data (I*, STRING, DOUBLE, ...) just the value
      - for containters, respectively:
        - a tuple ((key_type_name, value_type_name), dict)
        - a tuple (value_type_name, list)
        - a tuple (value_type_name, set)
        where the elements in the dict, list and set are parsed using the same
        rules.
  """
  data = {}
  iprot.readStructBegin()
  while True:
    (fname, ftype, fid) = iprot.readFieldBegin()
    if ftype == TType.STOP:
      break
    else:
      try:
        val = _read_one_field(iprot, ftype)
        data["FIELD_%d" % fid] = (_type_name(ftype), val)
      except:
        traceback.print_exc()
    iprot.readFieldEnd()
  iprot.readStructEnd()
  return data

def _read_one_field(iprot, ftype):
  if ftype == TType.BOOL:
    return iprot.readBool()
  elif ftype == TType.BYTE:
    return iprot.readByte()
  elif ftype == TType.I08:
    return iprot.readI08()
  elif ftype == TType.I16:
    return iprot.readI16()
  elif ftype == TType.I32:
    return iprot.readI32()
  elif ftype == TType.I64:
    return iprot.readI64()
  elif ftype == TType.DOUBLE:
    return iprot.readDouble()
  elif ftype == TType.STRING:
    return iprot.readString()
  elif ftype == TType.UTF8:
    return iprot.readUtf8()
  elif ftype == TType.UTF16:
    return iprot.readUtf16()
  elif ftype == TType.STRUCT:
    return read(iprot)
  elif ftype == TType.MAP:
    (ktype, vtype, size) = iprot.readMapBegin()
    data = {}
    for i in range(size):
      k = _read_one_field(iprot, ktype)
      v = _read_one_field(iprot, vtype)
      data[k] = v
    iprot.readMapEnd()
    return ((_type_name(ktype), _type_name(vtype)), data)
  elif ftype == TType.SET:
    (etype, size) = iprot.readSetBegin()
    data = set()
    for i in range(size):
      data.add(_read_one_field(iprot, etype))
    iprot.readSetEnd()
    return (_type_name(etype), data)
  elif ftype == TType.LIST:
    (etype, size) = iprot.readListBegin()
    data = list()
    for i in range(size):
      data.append(_read_one_field(iprot, etype))
    iprot.readListEnd()
    return (_type_name(etype), data)

########NEW FILE########
__FILENAME__ = test_class_factoring
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.class_factoring import ClassFactoring


BAD_CLASS = PythonFile.from_statement("""
class Distiller(object):
  CONSTANT = "foo"

  def foo(self, value):
    return os.path.join(Distiller.CONSTANT, value)
""")


def test_class_factoring():
  plugin = ClassFactoring(BAD_CLASS)
  nits = list(plugin.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T800'
  assert nits[0].severity == Nit.WARNING

########NEW FILE########
__FILENAME__ = test_except_statements
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.except_statements import ExceptStatements


EXCEPT_TEMPLATE = """
try:                  # 001
  1 / 0
%s
  pass
"""


def nits_from(clause):
  return list(ExceptStatements(PythonFile.from_statement(EXCEPT_TEMPLATE % clause)).nits())


def test_except_statements():
  for clause in ('except:', 'except :', 'except\t:'):
    nits = nits_from(clause)
    assert len(nits) == 1
    assert nits[0].code == 'T803'
    assert nits[0].severity == Nit.ERROR

  for clause in (
      'except KeyError, e:',
      'except (KeyError, ValueError), e:',
      'except KeyError, e :',
      'except (KeyError, ValueError), e\t:'):
    nits = nits_from(clause)
    assert len(nits) == 1
    assert nits[0].code == 'T601'
    assert nits[0].severity == Nit.ERROR

  for clause in (
      'except KeyError:',
      'except KeyError as e:',
      'except (KeyError, ValueError) as e:',
      'except (KeyError, ValueError) as e:'):
    nits = nits_from(clause)
    assert len(nits) == 0

########NEW FILE########
__FILENAME__ = test_future_compatibility
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.future_compatibility import FutureCompatibility


BAD_CLASS = PythonFile.from_statement("""
class Distiller(object):
  CONSTANT = "foo"

  def foo(self, value):
    return os.path.join(Distiller.CONSTANT, value)
""")


def exemplar_fail(code, severity, statement):
  nits = list(FutureCompatibility(PythonFile.from_statement(statement)).nits())
  assert len(nits) == 1
  assert nits[0].code == code
  assert nits[0].severity == severity
  return nits[0]


def exemplar_pass(statement):
  nits = list(FutureCompatibility(PythonFile.from_statement(statement)).nits())
  assert len(nits) == 0


def test_xrange():
  exemplar_fail('T603', Nit.ERROR, """
    for k in range(5):
      pass
    for k in xrange(10):
      pass
  """)

  exemplar_pass("""
    for k in obj.xrange(10):
      pass
  """)


def test_iters():
  for function_name in FutureCompatibility.BAD_ITERS:
    exemplar_fail('T602', Nit.ERROR, """
      d = {1: 2, 2: 3, 3: 4}
      for k in d.%s():
        pass
      for k in d.values():
        pass
    """ % function_name)


def test_names():
  for class_name in FutureCompatibility.BAD_NAMES:
    exemplar_fail('T604', Nit.ERROR, """
      if isinstance(k, %s):
        pass
      if isinstance(k, str):
        pass
    """ % class_name)


def test_metaclass():
  exemplar_fail('T605', Nit.WARNING, """
    class Singleton(object):
      __metaclass__ = SingletonMetaclass
      CONSTANT = 2 + 3

      def __init__(self):
        pass
  """)

########NEW FILE########
__FILENAME__ = test_import_order
import ast
import textwrap

from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.import_order import ImportOrder, ImportType


def strip_newline(stmt):
  return textwrap.dedent('\n'.join(filter(None, stmt.splitlines())))


IMPORT_CHUNKS = {
  ImportType.STDLIB: strip_newline("""
  import ast
  from collections import namedtuple
  import time
  """),

  ImportType.TWITTER: strip_newline("""
  from twitter.common import app
  from twitter.common.dirutil import (
      safe_mkdtemp,
      safe_open,
      safe_rmtree)
  """),

  ImportType.GEN: strip_newline("""
  from gen.twitter.aurora.ttypes import TwitterTaskInfo
  """),

  ImportType.PACKAGE: strip_newline("""
  from .import_order import (
      ImportOrder,
      ImportType
  )
  """),

  ImportType.THIRD_PARTY: strip_newline("""
  from kazoo.client import KazooClient
  import zookeeper
  """),
}


def stitch_chunks(newlines, *chunks):
  stitched = ('\n' * newlines).join(map(IMPORT_CHUNKS.get, chunks))
  return stitched


def get_import_chunk_types(import_type):
  chunks = list(ImportOrder(PythonFile(IMPORT_CHUNKS[import_type])).iter_import_chunks())
  assert len(chunks) == 1
  return tuple(map(type, chunks[0]))


def test_classify_import_chunks():
  assert get_import_chunk_types(ImportType.STDLIB) == (ast.Import, ast.ImportFrom, ast.Import)
  assert get_import_chunk_types(ImportType.TWITTER) == (ast.ImportFrom, ast.ImportFrom)
  assert get_import_chunk_types(ImportType.GEN) == (ast.ImportFrom,)
  assert get_import_chunk_types(ImportType.PACKAGE) == (ast.ImportFrom,)
  assert get_import_chunk_types(ImportType.THIRD_PARTY) == (ast.ImportFrom, ast.Import)


def test_classify_import():
  for import_type, chunk in IMPORT_CHUNKS.items():
    io = ImportOrder(PythonFile(chunk))
    import_chunks = list(io.iter_import_chunks())
    assert len(import_chunks) == 1
    module_types, chunk_errors = io.classify_imports(import_chunks[0])
    assert len(module_types) == 1
    assert module_types.pop() == import_type
    assert chunk_errors == []


PAIRS = (
  (ImportType.STDLIB, ImportType.TWITTER),
  (ImportType.TWITTER, ImportType.GEN),
  (ImportType.PACKAGE, ImportType.THIRD_PARTY),
)


def test_pairwise_classify():
  for first, second in PAIRS:
    io = ImportOrder(PythonFile(stitch_chunks(1, first, second)))
    import_chunks = list(io.iter_import_chunks())
    assert len(import_chunks) == 2

    module_types, chunk_errors = io.classify_imports(import_chunks[0])
    assert len(module_types) == 1
    assert len(chunk_errors) == 0
    assert module_types.pop() == first

    module_types, chunk_errors = io.classify_imports(import_chunks[1])
    assert len(module_types) == 1
    assert len(chunk_errors) == 0
    assert module_types.pop() == second

  for second, first in PAIRS:
    io = ImportOrder(PythonFile(stitch_chunks(1, first, second)))
    import_chunks = list(io.iter_import_chunks())
    assert len(import_chunks) == 2
    nits = list(io.nits())
    assert len(nits) == 1
    assert nits[0].code == 'T406'
    assert nits[0].severity == Nit.ERROR


def test_multiple_imports_error():
  io = ImportOrder(PythonFile(stitch_chunks(0, ImportType.STDLIB, ImportType.TWITTER)))
  import_chunks = list(io.iter_import_chunks())
  assert len(import_chunks) == 1

  module_types, chunk_errors = io.classify_imports(import_chunks[0])
  assert len(chunk_errors) == 1
  assert chunk_errors[0].code == 'T405'
  assert chunk_errors[0].severity == Nit.ERROR
  assert set(module_types) == set([ImportType.STDLIB, ImportType.TWITTER])

  io = ImportOrder(PythonFile('import time, pkg_resources'))
  import_chunks = list(io.iter_import_chunks())
  assert len(import_chunks) == 1
  module_types, chunk_errors = io.classify_imports(import_chunks[0])
  assert len(chunk_errors) == 3
  assert set(chunk_error.code for chunk_error in chunk_errors) == set(['T403', 'T405', 'T402'])
  assert set(module_types) == set([ImportType.STDLIB, ImportType.THIRD_PARTY])


def test_import_lexical_order():
  io = ImportOrder(PythonFile.from_statement("""
    from twitter.common.dirutil import safe_rmtree, safe_mkdtemp
  """))
  nits = list(io.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T401'
  assert nits[0].severity == Nit.ERROR


def test_import_wildcard():
  io = ImportOrder(PythonFile.from_statement("""
    from twitter.common.dirutil import *
  """))
  nits = list(io.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T400'
  assert nits[0].severity == Nit.ERROR

########NEW FILE########
__FILENAME__ = test_indentation
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.indentation import Indentation


def test_indentation():
  ind = Indentation(PythonFile.from_statement("""
    def foo():
        pass
  """))
  nits = list(ind.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T100'
  assert nits[0].severity == Nit.ERROR

  ind = Indentation(PythonFile.from_statement("""
    def foo():
      pass
  """))
  nits = list(ind.nits())
  assert len(nits) == 0

  ind = Indentation(PythonFile.from_statement("""
    def foo():
      baz = (
          "this "
          "is "
          "ok")
  """))
  nits = list(ind.nits())
  assert len(nits) == 0

########NEW FILE########
__FILENAME__ = test_missing_contextmanager
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.missing_contextmanager import MissingContextManager


def test_missing_contextmanager():
  mcm = MissingContextManager(PythonFile.from_statement("""
    with open("derp.txt"):
      pass
    
    with open("herp.txt") as fp:
      fp.read()
  """))
  nits = list(mcm.nits())
  assert len(nits) == 0

  mcm = MissingContextManager(PythonFile.from_statement("""
    foo = open("derp.txt")
  """))
  nits = list(mcm.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T802'
  assert nits[0].severity == Nit.WARNING

  # TODO(wickman) In these cases suggest using contextlib.closing
  mcm = MissingContextManager(PythonFile.from_statement("""
    from urllib2 import urlopen
    the_googs = urlopen("http://www.google.com").read()
  """))
  nits = list(mcm.nits())
  assert len(nits) == 0

########NEW FILE########
__FILENAME__ = test_newlines
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.newlines import Newlines

TOPLEVEL = """
def foo():
  pass%s
%s
  pass
"""


def test_newlines():
  for toplevel_def in ('def bar():', 'class Bar(object):'):
    for num_newlines in (0, 1, 3, 4):
      newlines = Newlines(PythonFile.from_statement(TOPLEVEL % ('\n' * num_newlines, toplevel_def)))
      nits = list(newlines.nits())
      assert len(nits) == 1
      assert nits[0].code == 'T302'
      assert nits[0].severity == Nit.ERROR
    newlines = Newlines(PythonFile.from_statement(TOPLEVEL % ('\n\n', toplevel_def)))
    assert len(list(newlines.nits())) == 0


GOOD_CLASS_DEF_1 = """
class Foo(object):
  def __init__(self):
    pass

  def bar(self):
    pass
"""

GOOD_CLASS_DEF_2 = """
class Foo(object):
  def __init__(self):
    pass

  # this should be fine
  def bar(self):
    pass
"""


GOOD_CLASS_DEF_3 = """
class Foo(object):
  class Error(Exception): pass
  class SomethingError(Error): pass

  def __init__(self):
    pass

  def bar(self):
    pass
"""


BAD_CLASS_DEF_1 = """
class Foo(object):
  class Error(Exception): pass
  class SomethingError(Error): pass
  def __init__(self):
    pass

  def bar(self):
    pass
"""

BAD_CLASS_DEF_2 = """
class Foo(object):
  class Error(Exception): pass
  class SomethingError(Error): pass

  def __init__(self):
    pass
  def bar(self):
    pass
"""


def test_classdefs():
  newlines = Newlines(PythonFile.from_statement(GOOD_CLASS_DEF_1))
  assert len(list(newlines.nits())) == 0

  newlines = Newlines(PythonFile.from_statement(GOOD_CLASS_DEF_2))
  assert len(list(newlines.nits())) == 0

  newlines = Newlines(PythonFile.from_statement(BAD_CLASS_DEF_1))
  nits = list(newlines.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T301'
  assert nits[0]._line_number == 4
  assert nits[0].severity == Nit.ERROR

  newlines = Newlines(PythonFile.from_statement(BAD_CLASS_DEF_2))
  nits = list(newlines.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T301'
  assert nits[0]._line_number == 7
  assert nits[0].severity == Nit.ERROR

########NEW FILE########
__FILENAME__ = test_new_style_classes
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.new_style_classes import NewStyleClasses


def test_new_style_classes():
  nsc = NewStyleClasses(PythonFile.from_statement("""
    class OldStyle:
      pass
    
    class NewStyle(object):
      pass
  """))
  nits = list(nsc.nits())
  assert len(nits) == 1
  assert nits[0]._line_number == 1
  assert nits[0].code == 'T606'
  assert nits[0].severity == Nit.ERROR

  nsc = NewStyleClasses(PythonFile.from_statement("""
    class NewStyle(OtherThing, ThatThing, WhatAmIDoing):
      pass
  """))
  nits = list(nsc.nits())
  assert len(nits) == 0

  nsc = NewStyleClasses(PythonFile.from_statement("""
    class OldStyle():  # unspecified mro
      pass
  """))
  nits = list(nsc.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T606'

########NEW FILE########
__FILENAME__ = test_print_statements
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.print_statements import PrintStatements


def test_print_statements():
  ps = PrintStatements(PythonFile.from_statement("""
    from __future__ import print_function
    print("I do what I want")
    
    class Foo(object):
      def print(self):
        "I can do this because it's not a reserved word."
  """))
  assert len(list(ps.nits())) == 0

  ps = PrintStatements(PythonFile.from_statement("""
    print("I do what I want")
  """))
  assert len(list(ps.nits())) == 0

  ps = PrintStatements(PythonFile.from_statement("""
    print["I do what I want"]
  """))
  nits = list(ps.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T607'
  assert nits[0].severity == Nit.ERROR

########NEW FILE########
__FILENAME__ = test_trailing_whitespace
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.trailing_whitespace import TrailingWhitespace


def test_exception_map():
  tw = TrailingWhitespace(PythonFile.from_statement("""
  test_string_001 = ""
  test_string_002 = " "
  test_string_003 = \"\"\"  
    foo   
  \"\"\"
  test_string_006 = ("   "
                     "   ")
  class Foo(object):
    pass
  # comment 010  
  test_string_011 = ''
  # comment 012
  # comment 013
  """))
  assert len(list(tw.nits())) == 0
  assert not tw.has_exception(9, 0, 0)
  assert not tw.has_exception(3, 0, 1)
  assert not tw.has_exception(3, 17, 17)
  assert tw.has_exception(3, 18, 18)
  assert tw.has_exception(3, 18, 10000)  # """ continuated strings have no ends. 
  assert not tw.has_exception(6, 8, 8)
  assert tw.has_exception(6, 19, 19)
  assert tw.has_exception(6, 19, 23)
  assert not tw.has_exception(6, 23, 25)  # ("  " continuations have string termination


def test_continuation_with_exception():
  tw = TrailingWhitespace(PythonFile.from_statement("""
  test_string_001 = ("   "  
                     "   ")
  """))
  nits = list(tw.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T200'
  assert nits[0].severity == Nit.ERROR


def test_trailing_slash():
  tw = TrailingWhitespace(PythonFile.from_statement("""
  foo = \\
    123
  bar = \"\"\"
    bin/bash foo \\
             bar \\
             baz
  \"\"\"
  """))
  nits = list(tw.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T201'
  assert nits[0].severity == Nit.ERROR
  assert nits[0]._line_number == 1

########NEW FILE########
__FILENAME__ = test_variable_names
from twitter.checkstyle.common import Nit, PythonFile
from twitter.checkstyle.plugins.variable_names import (
  allow_underscores,
  is_builtin_name,
  is_lower_snake,
  is_reserved_name,
  is_reserved_with_trailing_underscore,
  is_upper_camel,
  PEP8VariableNames,
)


def test_allow_underscores():
  @allow_underscores(0)
  def no_underscores(name):
    return name
  assert no_underscores('foo') == 'foo'
  assert no_underscores('foo_') == 'foo_'
  assert no_underscores('_foo') is False
  assert no_underscores('__foo') is False

  @allow_underscores(1)
  def one_underscore(name):
    return name
  assert one_underscore('foo') == 'foo'
  assert one_underscore('_foo') == 'foo'
  assert one_underscore('_foo_') == 'foo_'
  assert one_underscore('__foo') is False
  assert one_underscore('___foo') is False


UPPER_CAMEL = (
  'Rate',
  'HTTPRate',
  'HttpRate',
  'Justastringofwords'
)

LOWER_SNAKE = (
  'quiet',
  'quiet_noises',
)


def test_is_upper_camel():
  for word in UPPER_CAMEL:
    assert is_upper_camel(word)
    assert is_upper_camel('_' + word)
    assert not is_upper_camel('__' + word)
    assert not is_upper_camel(word + '_')
  for word in LOWER_SNAKE:
    assert not is_upper_camel(word)
    assert not is_upper_camel('_' + word)
    assert not is_upper_camel(word + '_')


def test_is_lower_snake():
  for word in LOWER_SNAKE:
    assert is_lower_snake(word)
    assert is_lower_snake('_' + word)
    assert is_lower_snake('__' + word)
  for word in UPPER_CAMEL:
    assert not is_lower_snake(word)
    assert not is_lower_snake('_' + word)


def test_is_builtin_name():
  assert is_builtin_name('__foo__')
  assert not is_builtin_name('__fo_o__')
  assert not is_builtin_name('__Foo__')
  assert not is_builtin_name('__fOo__')
  assert not is_builtin_name('__foo')
  assert not is_builtin_name('foo__')


def test_is_reserved_name():
  for name in ('for', 'super', 'id', 'type', 'class'):
    assert is_reserved_name(name)
  assert not is_reserved_name('none')


def test_is_reserved_with_trailing_underscore():
  for name in ('super', 'id', 'type', 'class'):
    assert is_reserved_with_trailing_underscore(name + '_')
    assert not is_reserved_with_trailing_underscore(name + '__')
  for name in ('garbage', 'slots', 'metaclass'):
    assert not is_reserved_with_trailing_underscore(name + '_')


def test_class_names():
  p8 = PEP8VariableNames(PythonFile.from_statement("""
    class dhis_not_right(object):
      pass
  """))
  nits = list(p8.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T000'
  assert nits[0]._line_number == 1
  assert nits[0].severity == Nit.ERROR


def test_class_globals():
  p8 = PEP8VariableNames(PythonFile.from_statement("""
    class DhisRight(object):
      RIGHT = 123
      notRight = 321
  """))
  nits = list(p8.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T001'
  assert nits[0]._line_number == 3
  assert nits[0].severity == Nit.ERROR


def test_builtin_overrides():
  p8 = PEP8VariableNames(PythonFile.from_statement("""
    def range():
      print("Not in a class body")
    
    class DhisRight(object):
      def any(self):
        print("In a class body")
  """))
  nits = list(p8.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T801'
  assert nits[0]._line_number == 1
  assert nits[0].severity == Nit.ERROR


def test_lower_snake_method_names():
  p8 = PEP8VariableNames(PythonFile.from_statement("""
    def totally_fine():
      print("Not in a class body")
    
    class DhisRight(object):
      def clearlyNotThinking(self):
        print("In a class body")
  """))
  nits = list(p8.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T002'
  assert nits[0]._line_number == 5
  assert nits[0].severity == Nit.ERROR

  p8 = PEP8VariableNames(PythonFile.from_statement("""
    class DhisRight:
      def clearlyNotThinking(self):
        print("In a class body")
  """))
  nits = list(p8.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T002'
  assert nits[0]._line_number == 2
  assert nits[0].severity == Nit.ERROR

  # Allow derivations from other modules to be ok.
  p8 = PEP8VariableNames(PythonFile.from_statement("""
    class TestCase(unittest.TestCase):
      def setUp(self):
        pass
  """))
  nits = list(p8.nits())
  assert len(list(p8.nits())) == 0

  p8 = PEP8VariableNames(PythonFile.from_statement("""
    def clearlyNotThinking():
      print("Not in a class body")
    
    class DhisRight(object):
      def totally_fine(self):
        print("In a class body")
  """))
  nits = list(p8.nits())
  assert len(nits) == 1
  assert nits[0].code == 'T002'
  assert nits[0]._line_number == 1
  assert nits[0].severity == Nit.ERROR

########NEW FILE########
__FILENAME__ = test_common
import ast
import textwrap

from twitter.checkstyle.common import (
    CheckstylePlugin,
    Nit,
    OffByOneList,
    PythonFile
)

import pytest


def make_statement(statement):
  return '\n'.join(textwrap.dedent(statement).splitlines()[1:])


PYTHON_STATEMENT = make_statement("""
import ast
from os.path import (
    join,
    split,
)

import zookeeper


class Keeper(object):
  def __init__(self):
    self._session = None

  def session(self):
    return self._session
""")


def test_python_file():
  pf = PythonFile(PYTHON_STATEMENT, 'keeper.py')
  assert pf.filename == 'keeper.py'
  assert pf.logical_lines == {
    1: (1, 2, 0),
    2: (2, 6, 0),
    7: (7, 8, 0),
    10: (10, 11, 0),
    11: (11, 12, 2),
    12: (12, 13, 4),
    14: (14, 15, 2),
    15: (15, 16, 4)
  }
  with pytest.raises(IndexError):
    pf[0]
  with pytest.raises(IndexError):
    pf[len(PYTHON_STATEMENT.splitlines()) + 1]
  assert pf[1] == ["import ast"]
  assert pf[2] == ["from os.path import (", "    join,", "    split,", ")"]
  assert pf[3] == ["    join,"]
  assert '\n'.join(pf) == PYTHON_STATEMENT
  assert list(pf.enumerate()) == list(enumerate(PYTHON_STATEMENT.splitlines(), 1))


def test_style_error():
  pf = PythonFile(PYTHON_STATEMENT, 'keeper.py')

  class ActualCheckstylePlugin(CheckstylePlugin):
    def nits(self):
      return []

  cp = ActualCheckstylePlugin(pf)

  se = cp.error('A123', 'You have a terrible taste in libraries.')
  assert se.line_number is None
  assert se.code == 'A123'
  str(se)

  se = cp.error('A123', 'You have a terrible taste in libraries.', 7)
  assert se.line_number == '007'
  str(se)

  se = cp.error('A123', 'You have a terrible taste in libraries.', 2)
  assert se.line_number == '002-005'
  assert se.severity == Nit.ERROR
  str(se)

  sw = cp.warning('A321', 'You have a terrible taste in libraries.', 2)
  assert sw.severity == Nit.WARNING
  assert sw.code == 'A321'
  str(sw)

  import_from = None
  for node in ast.walk(pf.tree):
    if isinstance(node, ast.ImportFrom):
      import_from = node
  assert import_from is not None

  ase = cp.error('B380', "I don't like your from import!", import_from)
  assert ase.severity == Nit.ERROR

  se = cp.error('B380', "I don't like your from import!", 2)
  assert str(se) == str(ase)


def test_off_by_one():
  obl = OffByOneList([])
  for index in (-1, 0, 1):
    with pytest.raises(IndexError):
      obl[index]
  for s in (slice(1, 1), slice(1, 2), slice(-2, -1)):
    assert obl[s] == []
  for s in (slice(-1, 0), slice(0, 1)):
    with pytest.raises(IndexError):
      obl[s]

  obl = OffByOneList([1, 2, 3])
  for k in (1, 2, 3):
    assert obl[k] == k
    assert obl[k:k + 1] == [k]
    assert obl.index(k) == k
    assert obl.count(k) == 1
  assert list(reversed(obl)) == [3, 2, 1]
  for k in (0, 4):
    with pytest.raises(IndexError):
      obl[k]

  for value in (None, 2.0, type):
    with pytest.raises(TypeError):
      obl[value]

########NEW FILE########
__FILENAME__ = test_git_iterators
import os

from twitter.checkstyle.iterators import git_iterator

import pytest
import git


non_python_filename = "non-python-file"
python_filename = "python-file.py"
other_branch = 'other_branch'
feature_branch = 'feature_branch'


class Options(object):
  def __init__(self, diff=None):
    self.diff = diff


@pytest.fixture
def repo(tmpdir):
  os.chdir(tmpdir.strpath)
  repo = git.Repo.init(tmpdir.strpath)

  tmpdir.join(non_python_filename).write("content")
  tmpdir.join(python_filename).write("content")

  repo.index.add([python_filename, non_python_filename])
  repo.index.commit('initial commit')

  # Create two branch and set head to feature_branch
  repo.create_head(other_branch)
  repo.create_head(feature_branch)
  repo.head.reference = repo.create_head(feature_branch)

  return repo


def test_no_py_file_changed(tmpdir, repo):
  tmpdir.join(non_python_filename).write('some more')
  repo.index.add([non_python_filename])
  repo.index.commit("modify a non-python file")

  assert [f[0] for f in git_iterator(None, Options())] == []


def test_py_changed_in_branch(tmpdir, repo):
  # Create a commit in other branch and diff against it
  repo.heads.other_branch.checkout()
  tmpdir.join(python_filename).write('some more')
  repo.index.add([python_filename])
  repo.index.commit("modify a python file in another branch")
  repo.heads.feature_branch.checkout()

  assert [f[0] for f in git_iterator(None, Options(other_branch))] == [python_filename]


def test_py_file_changed(tmpdir, repo):
  tmpdir.join(python_filename).write('some python to check')
  repo.index.add([python_filename])
  repo.index.commit("modify a python file")

  assert [f[0] for f in git_iterator(None, Options())] == [python_filename]


def test_py_file_added(tmpdir, repo):
  repo.head.reference.commit
  new_python_filename = "newfile.py"
  tmpdir.join(new_python_filename).write('some python to check')
  repo.index.add([new_python_filename])
  repo.index.commit("add a python file")

  assert [f[0] for f in git_iterator(None, Options())] == [new_python_filename]


def test_py_file_renamed(tmpdir, repo):
  new_python_filename = "newfile.py"
  tmpdir.join(new_python_filename).write(tmpdir.join(python_filename).read())
  repo.index.remove([python_filename])
  repo.index.add([new_python_filename])
  repo.index.commit("rename a python file")

  assert [f[0] for f in git_iterator(None, Options())] == [new_python_filename]

########NEW FILE########
__FILENAME__ = test_iterators
from StringIO import StringIO
from textwrap import dedent

from twitter.checkstyle.iterators import diff_lines


class Blob(object):
  def __init__(self, blob):
    self._blob = blob
    self.hexsha = 'ignore me'

  @property
  def data_stream(self):
    return StringIO(self._blob)


def make_blob(stmt):
  return Blob(dedent('\n'.join(stmt.splitlines()[1:])))


def test_diff_lines():
  blob_a = make_blob("""
    001 herp derp
  """)

  assert list(diff_lines(blob_a, blob_a)) == []

  blob_b = make_blob("""
    001 derp herp
  """)

  assert list(diff_lines(blob_a, blob_b)) == [1]

  blob_c = make_blob("""
    001 herp derp
    002 derp derp
  """)

  assert list(diff_lines(blob_a, blob_c)) == [2]
  assert list(diff_lines(blob_c, blob_a)) == []

  blob_d = make_blob("""
    001
    002
    003
    004
  """)

  blob_e = make_blob("""
    001
    004
  """)

  assert list(diff_lines(blob_d, blob_e)) == []
  assert list(diff_lines(blob_e, blob_d)) == [2, 3]

  blob_f = make_blob("""
    001
    002
    003
    004
  """)

  blob_g = make_blob("""
    002
    001
    004
    003
  """)

  assert list(diff_lines(blob_f, blob_g)) == [1, 3]
  assert list(diff_lines(blob_g, blob_f)) == [1, 3]

########NEW FILE########
__FILENAME__ = test_noqa
from twitter.checkstyle.checker import (
    apply_filter,
    noqa_file_filter,
    noqa_line_filter,
)
from twitter.checkstyle.common import CheckstylePlugin, PythonFile


class Rage(CheckstylePlugin):
  def nits(self):
    for line_no, _ in self.python_file.enumerate():
      yield self.error('T999', 'I hate everything!', line_no)


def test_noqa_line_filter():
  nits = apply_filter(PythonFile.from_statement("""
    print('This is not fine')
    print('This is fine')  # noqa
  """), Rage, None)
  
  nits = list(nits)
  assert len(nits) == 1, ('Actually got nits: %s' % (' '.join('%s:%s' % (nit._line_number, nit) for nit in nits)))
  assert nits[0].code == 'T999'


def test_noqa_file_filter():
  nits = apply_filter(PythonFile.from_statement("""
    # checkstyle: noqa
    print('This is not fine')
    print('This is fine')
  """), Rage, None)
  
  nits = list(nits)
  assert len(nits) == 0

########NEW FILE########
__FILENAME__ = test_app
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import defaultdict
from functools import partial
import threading
import time
import unittest
import sys

from twitter.common import options
from twitter.common.app import Module
from twitter.common.app.application import Application
from twitter.common.exceptions import ExceptionalThread

import pytest


class ModuleFactory(object):
  def __init__(self):
    self._values = defaultdict(int)

  def value(self, key):
    return self._values[key]

  def new_module(self, name, dependencies=None, with_setup=True, with_teardown=True):
    def setup_function(self):
      self._values['counter'] = self._values['counter'] + 1
      self._values[self.label()] = self._values['counter']
    def teardown_function(self):
      self._values['exit_counter'] = self._values['exit_counter'] + 1
      self._values[self.label() + '_exit'] = self._values['exit_counter']
    def __init__(self, name, values):
      self._values = values
      Module.__init__(self, label=name, dependencies=dependencies)

    class_methods = { '__init__': __init__ }
    if with_setup: class_methods.update(setup_function = setup_function)
    if with_teardown: class_methods.update(teardown_function = teardown_function)
    AnonymousModule = type('AnonymousModule', (Module,), class_methods)
    return AnonymousModule(name, self._values)


class TestApp(unittest.TestCase):
  def setUp(self):
    Module.clear_registry()
    self.factory = ModuleFactory()

  def test_app_registry_basic(self):
    self.factory.new_module('hello')
    app = Application(force_args=['--app_debug'])
    app.init()
    assert self.factory.value('hello') == 1, (
        'initialization functions should be invoked on app.init')

  def test_app_registry_dependencies_simple(self):
    self.factory.new_module('first')
    self.factory.new_module('second', dependencies='first')
    self.factory.new_module('third', dependencies='second')
    app = Application(force_args=[])
    app.init()
    assert self.factory.value('first') > 0, 'first callback should be called'
    assert self.factory.value('second') > 0, 'second callback should be called'
    assert self.factory.value('third') > 0, 'third callback should be called'
    assert self.factory.value('first') < self.factory.value('second'), (
        'second callback should be called after first')
    assert self.factory.value('second') < self.factory.value('third'), (
        'third callback should be called after second')

  # TODO(wickman) Add cyclic dependency detection and a test.
  def test_app_registry_dependencies_of_list(self):
    self.factory.new_module('first')
    self.factory.new_module('second', dependencies='first')
    self.factory.new_module('third', dependencies=['first', 'second'])
    app = Application(force_args=[])
    app.init()
    assert self.factory.value('first') > 0, 'first callback should be called'
    assert self.factory.value('second') > 0, 'second callback should be called'
    assert self.factory.value('third') > 0, 'third callback should be called'
    assert self.factory.value('first') < self.factory.value('third'), (
        'third callback should be called after first')
    assert self.factory.value('second') < self.factory.value('third'), (
        'third callback should be called after first')

  def test_app_registry_exit_functions(self):
    self.factory.new_module('first')
    self.factory.new_module('second', dependencies='first')
    self.factory.new_module('third', dependencies=['first', 'second'])
    app = Application(force_args=[])
    app.init()
    app._state = app.SHUTDOWN
    app._run_module_teardown()
    assert self.factory.value('third_exit') > 0 and (
      self.factory.value('second_exit') > 0 and self.factory.value('first_exit') > 0), (
      'all exit callbacks should have been called')
    assert self.factory.value('third_exit') < self.factory.value('second_exit')
    assert self.factory.value('third_exit') < self.factory.value('first_exit')

  def test_app_cyclic_dependencies(self):
    self.factory.new_module('first', dependencies='second')
    with pytest.raises(Module.DependencyCycle):
      self.factory.new_module('second', dependencies='first')

  def test_app_add_options_with_raw(self):
    # raw option
    app = Application(force_args=['--option1', 'option1value', 'extraargs'])
    app.add_option('--option1', dest='option1')
    app.init()
    assert app.get_options().option1 == 'option1value'
    assert app.argv() == ['extraargs']

  def test_app_add_options_with_Option(self):
    # options.Option
    app = Application(force_args=['--option1', 'option1value', 'extraargs'])
    opt = options.Option('--option1', dest='option1')
    app.add_option(opt)
    app.init()
    assert app.get_options().option1 == 'option1value'
    assert app.argv() == ['extraargs']

  def test_app_copy_command_options(self):
    option1 = options.TwitterOption('--test1')
    option2 = options.TwitterOption('--test2')

    app = Application()

    @app.command_option(option1)
    def test_command():
      pass

    @app.copy_command_options(test_command)
    @app.command_option(option2)
    def test_command_2():
      pass

    assert set([option1, option2]) == set(getattr(test_command_2, Application.OPTIONS_ATTR))

  def test_app_add_command_options(self):
    option_name = 'test_option_name'
    option = options.TwitterOption('--test', dest=option_name)

    app = Application()

    @app.command_option(option)
    def test_command():
      pass

    assert not hasattr(app.get_options(), option_name)
    app.add_command_options(test_command)
    assert hasattr(app.get_options(), option_name)


class TestApplication(Application):
  def __init__(self, main_method, force_args=[]):
    super(TestApplication, self).__init__(exit_function=self.__exit_function, force_args=force_args)
    self.__main_method = main_method
    self.exited = threading.Event()
    self.exited_rc = None

  def _find_main_method(self):
    return self.__main_method

  def __exit_function(self, rc):
    self.exited_rc = rc
    self.exited.set()


def test_application_basic():
  def excepting_method():
    1 / 0

  def exiting_method():
    sys.exit(2)

  def normal_method():
    return 3

  def run_app(method):
    app = TestApplication(method)
    app.main()
    return app

  app = run_app(excepting_method)
  assert app.exited_rc == 1

  app = run_app(exiting_method)
  assert app.exited_rc == 2

  app = run_app(normal_method)
  assert app.exited_rc == 3


def test_application_main_arguments():
  def main_no_args():
    return 0

  app = TestApplication(main_no_args, force_args=[])
  app.main()
  assert app.exited_rc == 0

  app = TestApplication(main_no_args, force_args=['a', 'b', 'c'])
  app.main()
  assert app.exited_rc == 1

  def main_with_args(args):
    assert args == ['a', 'b', 'c']
    return 0

  app = TestApplication(main_with_args, force_args=['a', 'b', 'c'])
  app.main()
  assert app.exited_rc == 0

  def main_with_args_and_options(args, options):
    assert args == ['a', 'b']
    assert options.foo == 'bar'
    return 0

  app = TestApplication(main_with_args_and_options, force_args=['--foo=bar', 'a', 'b'])
  app.add_option('--foo')
  app.main()
  assert app.exited_rc == 0

  def main_with_wrong_args(args, options, herp, derp):
    return 0
  app = TestApplication(main_with_wrong_args)
  app.main()
  assert app.exited_rc == 1

  broken_main = 'not a function'
  app = TestApplication(broken_main)
  app.main()
  assert app.exited_rc == 1


def test_application_commands():
  app = TestApplication(None)
  app.main()
  assert app.exited_rc == 1

  app = TestApplication(None)
  @app.default_command
  def not_main():
    return 0
  app.main()
  assert app.exited_rc == 0

  def real_main():
    return 0
  app = TestApplication(real_main)
  @app.default_command
  def not_main():
    return 0
  app.main()
  assert app.exited_rc == 1

  app = TestApplication(None)
  @app.command
  def command_but_not_default():
    return 0
  app = TestApplication(None)
  app.main()
  assert app.exited_rc == 1


def test_application_selects_command():
  def real_main():
    return 0

  def make_app_with_commands(*args, **kw):
    app = TestApplication(*args, **kw)

    @app.command
    def command_two(args):
      assert args == ['a', 'b']
      return 2

    @app.command
    def command_three():
      return 3

    @app.command
    @app.command_option('--foo', action='store_true', default=False)
    def command_four(args, options):
      return 100 + int(options.foo)

    return app

  app = make_app_with_commands(real_main, force_args=['a', 'b'])
  # real_main doesn't take extra arguments
  app.main()
  assert app.exited_rc == 1

  app = make_app_with_commands(real_main, force_args=['command_two', 'a', 'b'])
  app.main()
  assert app.exited_rc == 2

  app = make_app_with_commands(real_main, force_args=['command_three', 'a', 'b'])
  # command_three doesn't take args
  app.main()
  assert app.exited_rc == 1

  app = make_app_with_commands(real_main, force_args=['command_three'])
  app.main()
  assert app.exited_rc == 3

  app = make_app_with_commands(real_main, force_args=['command_four', '--foo'])
  app.main()
  assert app.exited_rc == 101

  app = make_app_with_commands(real_main, force_args=['command_four'])
  app.main()
  assert app.exited_rc == 100

  app = make_app_with_commands(None, force_args=[])
  # No main and no default command
  app.main()
  assert app.exited_rc == 1

  app = make_app_with_commands(None, force_args=[])
  @app.default_command
  def actual_main():
    return 0
  app.main()
  assert app.exited_rc == 0

  app = TestApplication(None, force_args=[])
  app.main()
  # No main defined at all
  assert app.exited_rc == 1


def test_shutdown_commands():
  shutdown1 = threading.Event()
  shutdown2 = threading.Event()
  shutdown_rc = []
  def shutdown_command(event, rc):
    shutdown_rc.append((rc, event))
    event.set()

  def simple_main():
    return 0

  app = TestApplication(simple_main)
  app.register_shutdown_command(partial(shutdown_command, shutdown1))
  app.register_shutdown_command(partial(shutdown_command, shutdown2))
  app.main()

  shutdown1.wait(timeout=1.0)
  shutdown2.wait(timeout=1.0)
  assert shutdown_rc == [(0, shutdown1), (0, shutdown2)]
  assert shutdown1.is_set()
  assert shutdown2.is_set()


def test_quitquitquit():
  def main():
    app.wait_forever()

  def wait_and_quit():
    time.sleep(0.5)
    app.quitquitquit()

  stop_thread = ExceptionalThread(target=wait_and_quit)
  stop_thread.start()

  app = TestApplication(main)
  app.main()

  assert app.exited_rc == 0


def test_shutdown_exception():
  def shutdown_command(rc):
    1 / 0
  app = TestApplication(lambda: 0)
  app.register_shutdown_command(shutdown_command)
  app.main()
  assert app.exited_rc == 0

########NEW FILE########
__FILENAME__ = test_maybe_list
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from functools import partial
import pytest

from twitter.common.collections import (
    maybe_list,
    OrderedDict,
    OrderedSet)


def test_default_maybe_list():
  HELLO_WORLD = ['hello', 'world']
  assert maybe_list('hello') == ['hello']
  assert maybe_list(('hello', 'world')) == HELLO_WORLD
  assert maybe_list(['hello', 'world']) == HELLO_WORLD
  assert maybe_list(OrderedSet(['hello', 'world', 'hello'])) == HELLO_WORLD
  assert maybe_list(s for s in ('hello', 'world')) == HELLO_WORLD
  od = OrderedDict(hello=1)
  od.update(world=2)
  assert maybe_list(od) == HELLO_WORLD
  assert maybe_list([]) == []
  assert maybe_list(()) == []
  assert maybe_list(set()) == []

  with pytest.raises(ValueError):
    maybe_list(123)
  with pytest.raises(ValueError):
    maybe_list(['hello', 123])

  assert maybe_list(['hello', 123], expected_type=(str, int)) == ['hello', 123]
  assert maybe_list(['hello', 123], expected_type=(int, str)) == ['hello', 123]


def test_maybe_list_types():
  iml = partial(maybe_list, expected_type=int)
  iml(1) == [1]
  iml([1,2]) == [1, 2]
  iml((1,2)) == [1, 2]
  iml(OrderedSet([1, 2, 1])) == [1, 2]
  iml(k for k in range(3)) == [0, 1, 2]
  iml([]) == []
  assert iml([]) == []
  assert iml(()) == []
  assert iml(set()) == []

  with pytest.raises(ValueError):
    iml('hello')
  with pytest.raises(ValueError):
    iml([123, 'hello'])
  with pytest.raises(ValueError):
    iml(['hello', 123])

########NEW FILE########
__FILENAME__ = test_ringbuffer
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest

from twitter.common.collections import RingBuffer

def test_append():
  r = RingBuffer(5)
  for i in xrange(0, 5):
    r.append(i)
  assert (r[0], r[1], r[2], r[3], r[4]) == (0, 1, 2, 3, 4)
  for i in xrange(5, 10):
    r.append(i)
  assert (r[0], r[1], r[2], r[3], r[4]) == (5, 6, 7, 8, 9)

def test_circularity():
  r = RingBuffer(3)
  r.append(1)
  r.append(2)
  r.append(3)
  assert 1 in r
  assert (r[0], r[3], r[6], r[-3]) == (1, 1, 1, 1)
  r.append(4)
  assert 1 not in r
  assert (r[0], r[3], r[6], r[-3]) == (2, 2, 2, 2)

def test_bad_operations():
  with pytest.raises(ValueError):
    RingBuffer(0)
  r = RingBuffer()
  with pytest.raises(IndexError):
    r[1]
  with pytest.raises(IndexError):
    r[-1]
  r.append(1)
  with pytest.raises(RingBuffer.InvalidOperation):
    del r[0]

########NEW FILE########
__FILENAME__ = test_concurrent
import time
from functools import partial
from Queue import Empty, Queue

import pytest
from twitter.common.concurrent import deadline, defer, Timeout
from twitter.common.contextutil import Timer


def test_deadline_default_timeout():
  timeout = partial(time.sleep, 0.5)
  with pytest.raises(Timeout):
    deadline(timeout)


def test_deadline_custom_timeout():
  timeout = partial(time.sleep, 0.2)
  with pytest.raises(Timeout):
    deadline(timeout, 0.1)


def test_deadline_no_timeout():
  assert 'success' == deadline(lambda: 'success')


def test_defer():
  DELAY = 0.5
  results = Queue(maxsize=1)
  def func():
    results.put_nowait('success')
  defer(func, delay=DELAY)
  with Timer() as timer:
    assert results.get() == 'success'
  assert timer.elapsed >= DELAY

########NEW FILE########
__FILENAME__ = test_deadline
import time
from functools import partial

import pytest
from twitter.common.concurrent import deadline, Timeout

def test_deadline_default_timeout():
  timeout = partial(time.sleep, 0.5)
  with pytest.raises(Timeout):
    deadline(timeout)


def test_deadline_custom_timeout():
  timeout = partial(time.sleep, 0.2)
  with pytest.raises(Timeout):
    deadline(timeout, 0.1)


def test_deadline_no_timeout():
  assert 'success' == deadline(lambda: 'success')

########NEW FILE########
__FILENAME__ = test_deferred
from Queue import Queue

import pytest
import time

from twitter.common.concurrent import defer
from twitter.common.contextutil import Timer
from twitter.common.testing.clock import ThreadedClock

def test_defer():
  clock = ThreadedClock()
  DELAY = 3
  results = Queue(maxsize=1)
  def func():
    results.put_nowait('success')
  defer(func, delay=DELAY, clock=clock)
  with Timer(clock=clock) as timer:
    clock.tick(4)
    assert results.get() == 'success'
  assert timer.elapsed >= DELAY

########NEW FILE########
__FILENAME__ = test_event_muxer
from threading import Event

from twitter.common.concurrent import EventMuxer

import pytest


def test_basic_muxer():
  # timeout, no events set
  muxer = EventMuxer(Event(), Event())
  assert not muxer.wait(timeout=0.1)

  # no re-entry
  with pytest.raises(RuntimeError):
    muxer.wait()

  # bad init
  with pytest.raises(ValueError):
    EventMuxer(Event(), 'not_an_event')


@pytest.mark.skipif("sys.version_info >= (2, 7)")
def test_wait_without_return_values():
  e1, e2 = Event(), Event()
  e1.set()
  assert not EventMuxer(e1, e2).wait()


@pytest.mark.skipif("sys.version_info < (2, 7)")
def test_wait_with_return_values():
  e1, e2 = Event(), Event()
  e1.set()
  assert EventMuxer(e1, e2).wait()

########NEW FILE########
__FILENAME__ = properties_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'John Sirois'

import unittest

from twitter.common.collections import OrderedDict
from twitter.common.config import Properties
from twitter.common.contextutil import temporary_file
from twitter.common.lang import Compatibility

class PropertiesTest(unittest.TestCase):
  def test_empty(self):
    self.assertLoaded('', {})
    self.assertLoaded(' ', {})
    self.assertLoaded('\t', {})
    self.assertLoaded('''

    ''', {})

  def test_comments(self):
    self.assertLoaded('''
# not=a prop
a=prop
 ! more non prop
    ''', {'a': 'prop'})

  def test_kv_sep(self):
    self.assertLoaded('''
    a=b
    c   d\=
    e\: :f
    jack spratt = \tbob barker
    g
    h=
    i :
    ''', {'a': 'b', 'c': 'd=', 'e:': 'f', 'jack spratt': 'bob barker', 'g': '', 'h': '', 'i': ''})

  def test_line_continuation(self):
    self.assertLoaded('''
    # A 3 line continuation
    a\\\\
        \\
           \\b
    c=\
    d
    e: \
    f
    g\
    :h
    i\
    = j
    ''', {'a\\': '\\b', 'c': 'd', 'e': 'f', 'g': 'h', 'i': 'j'})

  def test_stream(self):
    with temporary_file() as props_out:
      props_out.write('''
      it's a = file
      ''')
      props_out.close()
      with open(props_out.name, 'r') as props_in:
        self.assertLoaded(props_in, {'it\'s a': 'file'})

  def assertLoaded(self, contents, expected):
    self.assertEquals(expected, Properties.load(contents))

  def test_dump(self):
    props = OrderedDict()
    props['a'] = 1
    props['b'] = '''2
'''
    props['c'] =' 3 : ='
    out = Compatibility.StringIO()
    Properties.dump(props, out)
    self.assertEquals('a=1\nb=2\\\n\nc=\\ 3\\ \\:\\ \\=\n', out.getvalue())

########NEW FILE########
__FILENAME__ = test_environment_as
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import sys
import subprocess
from twitter.common.contextutil import environment_as, temporary_file

def test_empty_environment():
  with environment_as():
    pass

def test_override_single_variable():
  with temporary_file() as output:
    # test that the override takes place
    with environment_as(HORK = 'BORK'):
      subprocess.Popen([sys.executable, '-c', 'import os; print(os.environ["HORK"])'],
        stdout=output).wait()
      output.seek(0)
      assert output.read() == 'BORK\n'

    # test that the variable is cleared
    with temporary_file() as new_output:
      subprocess.Popen([sys.executable, '-c', 'import os; print("HORK" in os.environ)'],
        stdout=new_output).wait()
      new_output.seek(0)
      assert new_output.read() == 'False\n'

def test_environment_negation():
  with temporary_file() as output:
    with environment_as(HORK = 'BORK'):
      with environment_as(HORK = None):
      # test that the variable is cleared
        subprocess.Popen([sys.executable, '-c', 'import os; print("HORK" in os.environ)'],
          stdout=output).wait()
        output.seek(0)
        assert output.read() == 'False\n'

########NEW FILE########
__FILENAME__ = test_pushd
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
from twitter.common.contextutil import pushd, temporary_dir

def test_simple_pushd():
  pre_cwd = os.getcwd()
  with temporary_dir() as tempdir:
    with pushd(tempdir) as path:
      assert path == tempdir
      assert os.getcwd() == os.path.realpath(tempdir)
    assert os.getcwd() == pre_cwd
  assert os.getcwd() == pre_cwd

def test_nested_pushd():
  pre_cwd = os.getcwd()
  with temporary_dir() as tempdir1:
    with pushd(tempdir1) as path1:
      assert os.getcwd() == os.path.realpath(tempdir1)
      with temporary_dir(root_dir=tempdir1) as tempdir2:
        with pushd(tempdir2) as path2:
          assert os.getcwd() == os.path.realpath(tempdir2)
        assert os.getcwd() == os.path.realpath(tempdir1)
      assert os.getcwd() == os.path.realpath(tempdir1)
    assert os.getcwd() == pre_cwd
  assert os.getcwd() == pre_cwd

########NEW FILE########
__FILENAME__ = test_temps
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import shutil
from twitter.common.contextutil import temporary_file, temporary_dir

def test_temporary_file_no_args():
  with temporary_file() as fp:
    assert os.path.exists(fp.name), 'temporary file should exist within the context.'
  assert os.path.exists(fp.name) == False, 'temporary file should not exist outside of the context.'

def test_temporary_file_without_cleanup():
  with temporary_file(cleanup=False) as fp:
    assert os.path.exists(fp.name), 'temporary file should exist within the context.'
  assert os.path.exists(fp.name), 'temporary file should exist outside of context if cleanup=False.'
  os.unlink(fp.name)

def test_temporary_file_within_other_dir():
  with temporary_dir() as path:
    with temporary_file(root_dir=path) as f:
      assert os.path.realpath(f.name).startswith(os.path.realpath(path)), \
        'file should be created in root_dir if specified.'

def test_temporary_dir_no_args():
  with temporary_dir() as path:
    assert os.path.exists(path), 'temporary dir should exist within the context.'
    assert os.path.isdir(path), 'temporary dir should be a dir and not a file'
  assert os.path.exists(path) == False, 'temporary dir should not exist outside of the context.'

def test_temporary_dir_without_cleanup():
  with temporary_dir(cleanup=False) as path:
    assert os.path.exists(path), 'temporary dir should exist within the context.'
  assert os.path.exists(path), 'temporary dir should exist outside of context if cleanup=False.'
  shutil.rmtree(path)

def test_temporary_dir_with_root_dir():
  with temporary_dir() as path1:
    with temporary_dir(root_dir=path1) as path2:
      assert os.path.realpath(path2).startswith(os.path.realpath(path1)), \
        'nested temporary dir should be created within outer dir.'

########NEW FILE########
__FILENAME__ = test_timer
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import time 

from twitter.common.contextutil import Timer

def test_timer():
  with Timer() as t:
    assert t.start < time.time()
    assert t.elapsed > 0
    time.sleep(0.1)
    assert t.elapsed > 0.1
    time.sleep(0.1)
    assert t.finish is None
  assert t.elapsed > 0.2
  assert t.finish < time.time()


########NEW FILE########
__FILENAME__ = test_lru_cache
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.decorators import lru_cache

def test_basic():
  # just test the extra functionality that we added

  eviction_queue = []
  def on_eviction(element):
    eviction_queue.append(element)

  @lru_cache(10, on_eviction=on_eviction)
  def double(value):
    return value * 2

  for k in range(15):
    double(k)

  assert eviction_queue == [0, 2, 4, 6, 8]

########NEW FILE########
__FILENAME__ = test_threads
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import platform
import pytest
import threading

from twitter.common.decorators.threads import __gettid
from twitter.common.decorators.threads import identify_thread


SUPPORTED_PLATFORMS = (
  ('Linux', 'i386'),
  ('Linux', 'x86_64'),
)

PLATFORM_SUPPORTED = (platform.system(), platform.machine()) in SUPPORTED_PLATFORMS


class TestThread(threading.Thread):
  def __init__(self):
    threading.Thread.__init__(self)
    self.start_event = threading.Event()
    self.stop_event = threading.Event()
    self.daemon = True
  @identify_thread
  def run(self):
    self.start_event.set()
    self.stop_event.wait()


class TestNonthreadObject(object):
  @identify_thread
  def __init__(self):
    pass


def test_identified_nonthread_object():
  obj = TestNonthreadObject()
  assert hasattr(obj, '__thread_id')
  assert isinstance(obj.__thread_id, int) or obj.__thread_id == 'UNKNOWN'


@pytest.mark.skipif("not PLATFORM_SUPPORTED")
def test_gettid_supported_platform():
  assert __gettid() != -1


@pytest.mark.skipif("not PLATFORM_SUPPORTED")
def test_identified_thread_supported_platform():
  thread = TestThread()
  thread.start()
  # Non-zero delay between when a thread is started & when it's running; hence, we need to gate this
  thread.start_event.wait()
  assert isinstance(thread.__thread_id, int)
  assert thread.__thread_id != -1
  thread.stop_event.set()


@pytest.mark.skipif("PLATFORM_SUPPORTED")
def test_gettid_unsupported_platform():
  assert __gettid() == -1


@pytest.mark.skipif("PLATFORM_SUPPORTED")
def test_identified_thread_unsupported_platform():
  thread = TestThread()
  thread.start()
  # Non-zero delay between when a thread is started & when it's running; hence, we need to gate this
  thread.start_event.wait()
  assert thread.__thread_id == 'UNKNOWN'
  thread.stop_event.set()

########NEW FILE########
__FILENAME__ = fileset_test
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pytest

from contextlib import contextmanager

from twitter.common.contextutil import temporary_dir
from twitter.common.dirutil import Fileset as RealFileset


class Fileset(RealFileset):
  FILELIST = []

  @classmethod
  @contextmanager
  def over(cls, L):
    old, cls.FILELIST = cls.FILELIST, L
    yield
    cls.FILELIST = old

  @classmethod
  def walk(cls, path=None, allow_dirs=False, follow_links=False):
    for filename in cls.FILELIST:
      if isinstance(filename, Exception):
        raise filename
      elif filename.endswith('/'):
        path = os.path.normpath(filename)
        if allow_dirs:
          yield path
          yield path + '/'
      else:
        yield os.path.normpath(filename)


def ll(foo):
  return len(list(foo))


def leq(fs_foo, *bar):
  return set(fs_foo) == set(bar)


def test_add():
  with Fileset.over(['a', 'b']):
    assert leq(Fileset.rglobs('a') + ['b'], 'a', 'b')

  with Fileset.over(['a', 'b']):
    assert leq(Fileset.rglobs('a') + Fileset.rglobs('b'), 'a', 'b')


def test_subtract():
  with Fileset.over(['a', 'b']):
    assert leq(Fileset.rglobs('*') - ['b'], 'a')

  with Fileset.over(['a', 'b']):
    assert leq(Fileset.rglobs('*') - Fileset.rglobs('a'), 'b')


def test_lazy_raise():
  with pytest.raises(KeyError):
    with Fileset.over(['a', KeyError()]):
      apply(Fileset.rglobs('*'))


def test_zglobs():
  FILELIST = [
    'foo.txt',
    '.hidden_file',
    'a/',
    'a/foo.txt',
    'a/.hidden_file',
    'a/b/',
    'a/b/foo.txt',
    'a/b/.hidden_file',
    'foo/bar/baz.txt',
    'foo/',
    'foo/bar/',
    'a/b/c/',
  ]

  with Fileset.over(FILELIST):
    assert ll(Fileset.zglobs('*.txt')) == 1
    assert ll(Fileset.zglobs('.*')) == 1
    assert ll(Fileset.zglobs('*/*.txt')) == 1
    assert ll(Fileset.zglobs('*/.*')) == 1
    assert ll(Fileset.zglobs('*/*/*.txt')) == 2
    assert ll(Fileset.zglobs('*/*/.*')) == 1
    assert ll(Fileset.zglobs('???.txt')) == 1
    assert ll(Fileset.zglobs('?/*.txt')) == 1
    assert ll(Fileset.zglobs('?/?/*.txt')) == 1
    assert ll(Fileset.zglobs('a/*.txt')) == 1
    assert ll(Fileset.zglobs('a/b/*.txt')) == 1
    assert ll(Fileset.zglobs('a/???.txt')) == 1
    assert ll(Fileset.zglobs('a/?/*.txt')) == 1
    assert ll(Fileset.zglobs('*.txt', '*/.*')) == 2

  with Fileset.over(FILELIST):
    assert leq(Fileset.zglobs('*'), 'foo.txt', 'a', 'foo')
    assert leq(Fileset.zglobs('.*'), '.hidden_file')
    assert leq(Fileset.zglobs('**'), 'foo.txt', 'a', 'foo')
    assert leq(Fileset.zglobs('**/'), 'a/', 'a/b/', 'a/b/c/', 'foo/', 'foo/bar/')
    assert leq(Fileset.zglobs('**/*'),
        'foo.txt',
        'a',
        'a/foo.txt',
        'a/b',
        'a/b/foo.txt',
        'foo/bar/baz.txt',
        'foo',
        'foo/bar',
        'a/b/c'
    )
    assert leq(Fileset.zglobs('**/*.txt'), 'foo.txt', 'a/foo.txt', 'a/b/foo.txt',
        'foo/bar/baz.txt')
    assert leq(Fileset.zglobs('**/foo.txt'), 'foo.txt', 'a/foo.txt', 'a/b/foo.txt')
    assert leq(Fileset.zglobs('**/.*'),
        '.hidden_file', 'a/.hidden_file', 'a/b/.hidden_file')
    assert leq(Fileset.zglobs('*', 'a/*.txt'), 'foo.txt', 'a', 'foo', 'a/foo.txt')


def test_fnmatch():
  # We can't test Fileset.globs directly because it uses glob.glob and we
  # can't override its filelist.
  with Fileset.over(['.txt']):
    assert leq(Fileset.zglobs('*.txt'))
    assert leq(Fileset.zglobs('?.txt'))
    assert leq(Fileset.zglobs('[].txt'))
    assert leq(Fileset.zglobs('.*'), '.txt')
    assert leq(Fileset.zglobs('*.py', '.*'), '.txt')
    assert leq(Fileset.rglobs('*.txt'))
    assert leq(Fileset.rglobs('?.txt'))
    assert leq(Fileset.rglobs('[].txt'))
    assert leq(Fileset.rglobs('.*'), '.txt')
    assert leq(Fileset.rglobs('*.py', '.*'), '.txt')

  with Fileset.over(['a.txt']):
    for operation in (Fileset.rglobs, Fileset.zglobs):
      assert leq(operation('*.txt'), 'a.txt')
      assert leq(operation('?.txt'), 'a.txt')
      assert leq(operation('[abcd].txt'), 'a.txt')


def test_walk_altdir():
  files = []
  with temporary_dir() as td:
    for k in range(10):
      filename = os.path.join(td, '%010d' % k)
      with open(filename, 'w') as fp:
        fp.write('booyeah')
      files.append(filename)
    assert set(RealFileset.zglobs('*', root=td)) == set(os.path.basename(fn) for fn in files)

########NEW FILE########
__FILENAME__ = lock_test
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'John Sirois'

import os
import signal
import time

from twitter.common.contextutil import temporary_file, temporary_dir
from twitter.common.dirutil import Lock
from twitter.common.lang import Compatibility

if Compatibility.PY3:
  import unittest
else:
  import unittest2 as unittest

class LockTest(unittest.TestCase):
  def test_unlocked(self):
    lock1 = Lock.unlocked()
    lock2 = Lock.unlocked()
    self.assertFalse(lock1.release())
    self.assertFalse(lock1.release())
    self.assertFalse(lock2.release())

  def test_acquire_nowait(self):
    with temporary_file() as fd:
      def throw(pid):
        self.fail('Did not expect to wait for the 1st lock, held by %d' % pid)
      lock = Lock.acquire(fd.name, onwait=throw)
      try:
        def onwait(pid):
          self.assertEquals(os.getpid(), pid, "This process should hold the lock.")
          return False
        self.assertFalse(Lock.acquire(fd.name, onwait=onwait))
      finally:
        self.assertTrue(lock.release())
        self.assertFalse(lock.release())

  def test_acquire_wait(self):
    with temporary_dir() as path:
      lockfile = os.path.join(path, 'lock')

      childpid = os.fork()
      if childpid == 0:
        lock = Lock.acquire(lockfile)
        try:
          while True:
            time.sleep(1)
        except KeyboardInterrupt:
          lock.release()

      else:
        def childup():
          if not os.path.exists(lockfile):
            return False
          else:
            with open(lockfile) as fd:
              pid = fd.read().strip()
              return pid and pid == str(childpid)

        while not childup():
          time.sleep(0.1)

        # We should be blocked by the forked child lock owner
        def onwait(pid):
          self.assertEquals(childpid, pid)
          return False
        self.assertFalse(Lock.acquire(lockfile, onwait=onwait))

        # We should unblock after we 'kill' the forked child owner
        os.kill(childpid, signal.SIGINT)
        lock = Lock.acquire(lockfile)
        try:
          self.assertTrue(lock)
        finally:
          self.assertTrue(lock.release())

########NEW FILE########
__FILENAME__ = size_test
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import stat

from twitter.common.contextutil import temporary_file, temporary_dir
from twitter.common.dirutil import safe_size, safe_bsize, du


def create_files(tempdir, *filenames):
  for filename in filenames:
    with open(os.path.join(tempdir, filename), 'w') as fp:
      fp.close()


def test_size():
  with temporary_dir() as td:
    create_files(td, 'file1.txt')
    file1 = os.path.join(td, 'file1.txt')

    assert safe_size(file1) == 0
    with open(file1, 'w') as fp:
      fp.write('!' * 101)
    assert safe_size(file1) == 101

    f1stat = os.stat(file1)
    assert safe_bsize(file1) == 512 * f1stat.st_blocks
    assert du(td) == safe_bsize(file1)

    file2 = os.path.join(td, 'file2.txt')
    os.symlink('file1.txt', file2)
    assert safe_size(file2) == len('file1.txt')
    assert safe_bsize(file2) == len('file1.txt')
    assert du(td) == safe_bsize(file1) + len('file1.txt')

  assert safe_size(os.path.join(td, 'file3.txt')) == 0
  assert safe_bsize(os.path.join(td, 'file3.txt')) == 0

  errors = []
  def on_error(path, err):
    errors.append(path)

  safe_size(os.path.join(td, 'file3.txt'), on_error=on_error)
  assert errors == [os.path.join(td, 'file3.txt')]


########NEW FILE########
__FILENAME__ = tail_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os
import copy
import threading
import tempfile
import time
import sys
from twitter.common.lang import Compatibility
from twitter.common.dirutil import tail_f

if Compatibility.PY3:
  import unittest
else:
  import unittest2 as unittest

class TestClock(object):
  def __init__(self):
    self._cond = threading.Condition()
    self.reset()

  def time(self):
    return self._ticks

  def reset(self):
    self._ticks = 0

  def tick(self, ticks=1):
    for k in range(ticks):
      with self._cond:
        self._cond.notifyAll()

  def sleep(self, amount):
    with self._cond:
      for k in range(amount):
        self._cond.wait()
        self._ticks += 1

class TailThread(threading.Thread):
  def __init__(self, filename):
    self._filename = filename
    self._clock = TestClock()
    self._lines = []
    self._terminate = threading.Event()
    threading.Thread.__init__(self)

  def run(self):
    for line in tail_f(self._filename, clock=self._clock):
      self._lines.append(line)
      if self._terminate.is_set():
        break

  def clear(self):
    time.sleep(0.10)  # yield the thread.
    rc = copy.copy(self._lines)
    self._lines = []
    return rc

  def clock(self):
    return self._clock

  def terminate(self):
    self._terminate.set()

class TestTail(unittest.TestCase):
  @classmethod
  def write_to_fp(cls, msg):
    cls._fp.write(msg)
    cls._fp.flush()

  @classmethod
  def reset_fp(cls):
    filename = cls._fp.name
    cls._fp.close()
    os.unlink(filename)
    cls._fp = open(filename, 'w')

  @classmethod
  def setUpClass(cls):
    cls._fp = open(tempfile.mktemp(), 'w')
    cls._thread = TailThread(cls._fp.name)
    cls._thread.start()

  @classmethod
  def tearDownClass(cls):
    cls._thread.terminate()
    cls.write_to_fp('whee!')
    cls._thread.clock().tick()
    cls._thread.join()
    os.unlink(cls._fp.name)
    cls._fp.close()

  def test_simple_tail(self):
    self.write_to_fp('hello')
    self._thread.clock().tick()
    assert self._thread.clear() == ['hello']
    self.write_to_fp('hello 2')
    assert self._thread.clear() == []
    self._thread.clock().tick()
    assert self._thread.clear() == ['hello 2']

  def test_tail_through_reset(self):
    self._thread.clock().reset()
    assert self._thread.clock().time() == 0
    self.write_to_fp('hello')
    self._thread.clock().tick()
    assert self._thread.clear() == ['hello']
    self.reset_fp()
    self.write_to_fp('hello 2')
    self._thread.clock().tick()
    assert self._thread.clear() == ['hello 2']

  def test_tail_through_truncation(self):
    self.write_to_fp('hello')
    self._thread.clock().tick()
    assert self._thread.clear() == ['hello']

    self.write_to_fp('hello 2')
    self._thread.clock().tick()
    assert self._thread.clear() == ['hello 2']

    self._fp.truncate(0)
    self.write_to_fp('hello 3')
    self._thread.clock().tick()
    assert self._thread.clear() == ['hello 3']

########NEW FILE########
__FILENAME__ = test_dirutil
import atexit
import os
import tempfile

from twitter.common import dirutil

import mox
import pytest


def test_mkdtemp_setup_teardown():
  m = mox.Mox()

  def faux_cleaner():
    pass

  DIR1, DIR2 = 'fake_dir1__does_not_exist', 'fake_dir2__does_not_exist'
  m.StubOutWithMock(atexit, 'register')
  m.StubOutWithMock(os, 'getpid')
  m.StubOutWithMock(tempfile, 'mkdtemp')
  m.StubOutWithMock(dirutil, 'safe_rmtree')
  atexit.register(faux_cleaner) # ensure only called once
  tempfile.mkdtemp(dir='1').AndReturn(DIR1)
  tempfile.mkdtemp(dir='2').AndReturn(DIR2)
  os.getpid().MultipleTimes().AndReturn('unicorn')
  dirutil.safe_rmtree(DIR1)
  dirutil.safe_rmtree(DIR2)
  # make sure other "pids" are not cleaned
  dirutil._MKDTEMP_DIRS['fluffypants'].add('yoyo')

  try:
    m.ReplayAll()
    assert dirutil.safe_mkdtemp(dir='1', cleaner=faux_cleaner) == DIR1
    assert dirutil.safe_mkdtemp(dir='2', cleaner=faux_cleaner) == DIR2
    assert 'unicorn' in dirutil._MKDTEMP_DIRS
    assert dirutil._MKDTEMP_DIRS['unicorn'] == set([DIR1, DIR2])
    dirutil._mkdtemp_atexit_cleaner()
    assert 'unicorn' not in dirutil._MKDTEMP_DIRS
    assert dirutil._MKDTEMP_DIRS['fluffypants'] == set(['yoyo'])

  finally:
    dirutil._MKDTEMP_DIRS.pop('unicorn', None)
    dirutil._MKDTEMP_DIRS.pop('fluffypants', None)
    dirutil._mkdtemp_unregister_cleaner()

    m.UnsetStubs()
    m.VerifyAll()

########NEW FILE########
__FILENAME__ = test_pingpong_server
from twitter.common.quantity import Amount, Time
from twitter.common.testing.clock import ThreadedClock

from twitter.common.examples.pingpong import PingPongServer


class TestPingPongServer(PingPongServer):
  def __init__(self, *args, **kw):
    self._calls = []
    super(TestPingPongServer, self).__init__(*args, **kw)

  def send_request(self, endpoint, message, ttl):
    self._calls.append((endpoint, message, ttl))

  def expect_calls(self, *calls):
    for expected_call in calls:
      try:
        call = self._calls.pop(0)
        assert expected_call == call, (
            'Expected endpoint=%s message=%s ttl=%s, ' % expected_call,
            'got endpoint=%s message=%s ttl=%s.' % call)
      except IndexError:
        assert False, 'Expected endpoint=%s message=%s ttl=%s, got nothing' % expected_call


def test_ttl_decrement_works():
  clock = ThreadedClock()
  pps = TestPingPongServer('foo', 31337, clock=clock)

  pps.ping('hello world', ttl=1)
  clock.tick(TestPingPongServer.PING_DELAY.as_(Time.SECONDS))
  pps.expect_calls()


def test_ping():
  clock = ThreadedClock()
  pps = TestPingPongServer('foo', 31337, clock=clock)

  pps.ping('hello world', ttl=2)
  clock.tick(TestPingPongServer.PING_DELAY.as_(Time.SECONDS))
  pps.expect_calls(('ping', 'hello world', 1))

  pps.ping('hello world', ttl=3)
  clock.tick(TestPingPongServer.PING_DELAY.as_(Time.SECONDS))
  pps.expect_calls(('ping', 'hello world', 2))

########NEW FILE########
__FILENAME__ = test_exceptions
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import sys
import threading
from collections import namedtuple
from contextlib import contextmanager
from Queue import Queue

from twitter.common.exceptions import ExceptionalThread


class TestException(Exception):
  def __eq__(self, other):
    return (self.args == other.args and self.message == other.message)

class ExceptHookInfo(namedtuple('ExceptHookInfo', 'type value traceback')):
  def __eq__(self, other):
    return (self.type == other.type and self.value == other.value)


@contextmanager
def exception_handler():
  try:
    exception_queue = Queue()
    def queue_exceptions(type, value, traceback):
      exception_queue.put(ExceptHookInfo(type, value, traceback))
    sys.excepthook = queue_exceptions
    yield exception_queue
  finally:
    sys.excepthook = sys.__excepthook__


class TestExceptionalThread(ExceptionalThread):
  def __init__(self):
    super(TestExceptionalThread, self).__init__()
  def run(self):
    raise TestException('Something went wrong!')


class TestDefaultThread(threading.Thread):
  def run(self):
    raise TestException


def test_exception_caught():
  with exception_handler() as queue:
    thread = TestExceptionalThread()
    thread.start()
    thread.join()
    exception = queue.get()
    assert exception == ExceptHookInfo(TestException, TestException('Something went wrong!'), None)
  assert sys.excepthook == sys.__excepthook__


def test_exception_swallowed():
  with exception_handler() as queue:
    thread = TestDefaultThread()
    thread.start()
    thread.join()
    assert queue.empty()
  assert sys.excepthook == sys.__excepthook__

########NEW FILE########
__FILENAME__ = hdfs_test
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================
__author__ = 'Tejal Desai'

import sys
import unittest
from mock import patch

import fake_filesystem as pyfakefs

import twitter.common.fs
from twitter.common.fs import HDFSHelper


class MockCommandUtil:

  @staticmethod
  def execute(cmd, get_output=True):
    if (cmd[4] == '-lsr' or cmd[4] == '-ls') and cmd[5] =='path':
      return (0,"\n".join(["Found 1 items",
            "drwxr-xr-x   - tdesai staff         68 2012-08-06 13:51 hadoop_dir/test_dir",
            "-rwxrwxrwx   1 tdesai staff          6 2012-08-06 14:01 tejal.txt",
            "-rwxrwxrwx   1 tdesai staff          6 2012-08-06 14:01 tejal txt"]))

    if (cmd[4] == '-lsr' or cmd[4] == '-ls') and cmd[5] =='non_existing':
      return (255,"ls: File doesnot exists")

    if (cmd[4] == '-lsr' or cmd[4] == '-ls') and cmd[5] =='empty':
      return (0,None)

    if cmd[4] == '-test':
      return " ".join(cmd) == \
          'hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -test -e hadoop_dir'

    if cmd[4] == '-copyToLocal':
      if get_output:
        tmp_file = cmd[6]
        if " ".join(cmd) == \
            "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -copyToLocal " + \
             "somefile " + tmp_file:
          with open(tmp_file, "w") as f:
            f.write("read_test")
          return 0
        elif " ".join(cmd) == \
            "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -copyToLocal " + \
             "non_exist " + tmp_file:
          return 1

    if cmd[4] == '-copyFromLocal':
      if cmd[5] != 'text_file':
        tmp_file = cmd[5]
        with open(tmp_file, "r") as f:
          text1 = f.read()
        return (text1 == "write_text" and
          " ".join(cmd) == " ".join(["hadoop", "--config", "/etc/hadoop/hadoop-conf-tst-smf1",
                                     "dfs", "-copyFromLocal", tmp_file, "somefile"]))
    #For rest all cases return the command
    return " ".join(cmd)

  @staticmethod
  def execute_and_get_output(cmd):
    return MockCommandUtil.execute(cmd, True)

  @staticmethod
  def execute_suppress_stdout(cmd):
    return MockCommandUtil.execute(cmd, get_output=False)

class HdfsTest(unittest.TestCase):
  _config_dir = "/etc/hadoop/hadoop-conf-tst-smf1"
  _site_config = "%s/site.xml" % _config_dir
  _original_cwd = None

  def setUp(self):
    fake_fs = pyfakefs.FakeFilesystem()
    fake_os = pyfakefs.FakeOsModule(fake_fs)
    fake_fs.CreateFile(HdfsTest._site_config, contents="this is not a real file.")
    fake_fs.CreateFile("src", contents="heh. before pyfakefs this was unintentionally a dir.")

    self.original_os = twitter.common.fs.hdfs.os
    twitter.common.fs.hdfs.os = fake_os

  def tearDown(self):
    twitter.common.fs.hdfs.os = self.original_os

  def test_get_config_behavior(self):
    self.assertRaises(ValueError, HDFSHelper, "/this/does/not/exist",
                      command_class=MockCommandUtil)
    self.assertRaises(ValueError, HDFSHelper, HdfsTest._site_config,
                      command_class=MockCommandUtil)

  def test_get_config(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    self.assertEqual(hdfs_helper.config,'/etc/hadoop/hadoop-conf-tst-smf1')

  def test_get(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.get(['src'],"dst")
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -get src dst"
    self.assertEqual(cmd, expected_cmd)

  def test_put(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.put('src','dst')
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -put src dst"
    self.assertEqual(cmd, expected_cmd)

  def test_cat(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.cat('text_file', 'local')
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -cat " + \
      "text_file"
    self.assertEqual(cmd, expected_cmd)

  def test_hdfs_ls(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.ls('path', True)
    expected_output_dir = [['hadoop_dir/test_dir', 68]]
    expected_output = [['tejal.txt', 6], ['tejal txt',6]]
    self.assertEqual(cmd, expected_output_dir)
    cmd = hdfs_helper.ls('path')
    self.assertEqual(cmd, expected_output)
    #Empty path
    cmd = hdfs_helper.ls('empty', True)
    self.assertTrue(not cmd)
    #Return code 255
    self.assertRaises(HDFSHelper.InternalError,hdfs_helper.ls,'non_existing', True )


  def test_hdfs_lsr(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    expected_output = [['tejal.txt', 6], ['tejal txt',6]]
    expected_output_dir = [['hadoop_dir/test_dir', 68]]
    cmd = hdfs_helper.lsr('path')
    self.assertEqual(cmd, expected_output)
    cmd = hdfs_helper.lsr('path', True)
    self.assertEqual(cmd, expected_output_dir)

  def test_exists(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    self.assertEquals(0, hdfs_helper.exists('hadoop_dir'))

  def test_read(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    read_text = hdfs_helper.read('somefile')
    self.assertEqual("read_test", read_text)
    read_text = hdfs_helper.read('non_exist')
    self.assertEqual(None, read_text)

  def test_write(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    self.assertEqual(True, hdfs_helper.write('somefile',"write_text"))

  def test_mkdir(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.mkdir('dest')
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -mkdir dest"
    self.assertEqual(cmd, expected_cmd)

  def test_rm(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.rm('dest')
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -rm dest"
    self.assertEqual(cmd, expected_cmd)

  def test_cp(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.cp('src','dest')
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs -cp src dest"
    self.assertEqual(cmd, expected_cmd)

  def test_copy_from_local(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.copy_from_local('text_file','dest')
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs " + \
      "-copyFromLocal text_file dest"
    self.assertEqual(cmd, expected_cmd)

  def test_copy_to_local(self):
    hdfs_helper = HDFSHelper("/etc/hadoop/hadoop-conf-tst-smf1",
                             command_class=MockCommandUtil)
    cmd = hdfs_helper.copy_to_local('text_file','dest')
    expected_cmd = "hadoop --config /etc/hadoop/hadoop-conf-tst-smf1 dfs " + \
      "-copyToLocal text_file dest"
    self.assertEqual(cmd, expected_cmd)

########NEW FILE########
__FILENAME__ = test_git
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import subprocess

from twitter.common.contextutil import pushd
from twitter.common.git import branch, git

import git
import pytest


first_commit_file_content = 'first commit content'


@pytest.fixture
def repo(tmpdir):
  with pushd(tmpdir.strpath):
    repo = git.Repo.init(tmpdir.strpath)
    filename = 'test'

    tmpdir.join(filename).write(first_commit_file_content)
    repo.index.add([filename])
    repo.index.commit('initial commit')
    repo.create_head('a')

    tmpdir.join(filename).write('more content')
    repo.index.add([filename])
    repo.index.commit('second commit')
    return repo


def test_branch(tmpdir, repo):
  with branch('a', repo=repo):
    with pushd(tmpdir.strpath):
      with open('test') as f:
        assert f.read() == first_commit_file_content


def test_branch_throw(tmpdir, repo):
  with pytest.raises(ValueError):
    with branch('a', repo=repo):
      raise ValueError

########NEW FILE########
__FILENAME__ = test_building
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import functools
import wsgiref.util

from twitter.common.http import HttpServer

import pytest


skipifpy3k = pytest.mark.skipif('sys.version_info >= (3,0)')


def make_request(path):
  test_req = {'REQUEST_METHOD': 'GET', 'PATH_INFO': path}
  wsgiref.util.setup_testing_defaults(test_req)
  return test_req


def response_asserter(intended, status, headers):
  assert int(status.split()[0]) == intended


# TODO(wickman) Fix bind method delegation in py3x.  It's currently brittle
# and the new module might actually allow for binding in this fashion now.
@skipifpy3k
def test_basic_server_method_binding():
  class MyServer(HttpServer):
    def __init__(self):
      HttpServer.__init__(self)
    @HttpServer.route("/hello")
    @HttpServer.route("/hello/:first")
    @HttpServer.route("/hello/:first/:last")
    def hello(self, first = 'Zaphod', last = 'Beeblebrox'):
      return 'Hello, %s %s!' % (first, last)

  server = MyServer()
  assert server.app.handle('/hello') == 'Hello, Zaphod Beeblebrox!'
  assert server.app.handle('/hello/Brian') == 'Hello, Brian Beeblebrox!'
  assert server.app.handle('/hello/Brian/Horfgorf') == 'Hello, Brian Horfgorf!'



@skipifpy3k
def test_basic_server_error_binding():
  BREAKAGE = '*****breakage*****'
  class MyServer(object):
    @HttpServer.route('/broken')
    def broken_handler(self):
      raise Exception('unhandled exception!')

    @HttpServer.error(404)
    @HttpServer.error(500)
    def error_handler(self, error):
      return BREAKAGE

  server = HttpServer()
  mserver = MyServer()
  server.mount_routes(mserver)

  # Test 404 error handling.
  resp = server.app(make_request('/nonexistent_page'), functools.partial(response_asserter, 404))
  assert resp[0] == BREAKAGE

  # Test 500 error handling.
  resp = server.app(make_request('/broken'), functools.partial(response_asserter, 500))
  assert resp[0] == BREAKAGE


@skipifpy3k
def test_bind_method():
  class BaseServer(HttpServer):
    NAME = "heavens to murgatroyd!"
    def __init__(self):
      self._name = BaseServer.NAME
      HttpServer.__init__(self)

  class BaseServerNotSubclass(object):
    def method_one(self):
      return 'method_one'

  class BaseServerIsSubclass(BaseServer):
    def method_two(self):
      return 'method_two'

  bs = BaseServer()

  # make sure we properly raise un nonexistent methods
  with pytest.raises(ValueError):
    bs._bind_method(BaseServerIsSubclass, 'undefined_method_name')

  # properly raise on classes w/ divergent parents
  with pytest.raises(TypeError):
    bs._bind_method(BaseServerNotSubclass, 'method_one')

  # should be able to bind method to base class self
  bs._bind_method(BaseServerNotSubclass(), 'method_one')
  bs._bind_method(BaseServerIsSubclass(), 'method_two')
  assert bs.method_one() == 'method_one'
  assert bs.method_two() == 'method_two'

########NEW FILE########
__FILENAME__ = test_class_file
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pkgutil
import pytest
import sys
from twitter.common.java.class_file import ClassFile

import unittest2 as unittest

# Known golden file, com.google.protobuf.ByteString => ByteString.class
# named example_class because of science .gitignore
#
# resources don't properly work in python_tests yet, so this test is marked as expected fail.
_EXAMPLE_RESOURCE = 'resources/example_class'

@pytest.mark.skipif('sys.version_info >= (3,0)')
class ClassFileParserTest(unittest.TestCase):
  @classmethod
  def setUpClass(cls):
    cls._class_data = pkgutil.get_data('twitter.common.java', _EXAMPLE_RESOURCE)
    assert cls._class_data is not None
    cls._class_file = ClassFile(cls._class_data)

  def test_parsed(self):
    assert self._class_file is not None

  def test_parsed_maj_min(self):
    maj, min = self._class_file.version()
    assert maj == 50
    assert min == 0

  def test_parsed_this_class(self):
    assert self._class_file.this_class() == 'com/google/protobuf/ByteString'

  def test_parsed_super_class(self):
    assert self._class_file.super_class() == 'java/lang/Object'

  def test_parsed_access(self):
    access_flags = self._class_file.access_flags()
    assert access_flags.public()
    assert access_flags.final()
    assert access_flags.super_()
    assert not access_flags.interface()
    assert not access_flags.abstract()

########NEW FILE########
__FILENAME__ = test_perfdata
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import struct
import pkgutil

from twitter.common.java.perfdata import PerfData

import mock
import pytest


_EXAMPLE_RESOURCE = 'resources/example_hsperfdata'


def test_perfdata_integration():
  provider = lambda: pkgutil.get_data('twitter.common.java', _EXAMPLE_RESOURCE)
  perfdata = PerfData.get(provider)
  assert perfdata is not None
  perfdata.sample()

  assert len(perfdata) > 0
  keys = set(perfdata)
  for key in perfdata:
    assert key in keys
    assert perfdata[key] is not None


def test_struct_unpack_error():
  provider = lambda: pkgutil.get_data('twitter.common.java', _EXAMPLE_RESOURCE)
  perfdata = PerfData.get(provider)
  assert perfdata is not None

  with mock.patch('struct.unpack') as struct_unpack:
    struct_unpack.side_effect = struct.error('My shit got corrupted!')

    with pytest.raises(perfdata.ParseError):
      perfdata.sample()


def test_empty_ish_perfdata():
  provider = lambda: ''
  with pytest.raises(ValueError):
    perfdata = PerfData.get(provider)

  provider = lambda: PerfData.MAGIC
  with pytest.raises(ValueError):
    perfdata = PerfData.get(provider)

########NEW FILE########
__FILENAME__ = test_jira
import textwrap

from twitter.common.jira import Jira, JiraError

import mock
import pytest

GOOD_TRANSITIONS = textwrap.dedent('''{
                                        "transitions": [
                                          {
                                            "id": 1,
                                            "name": "Test"
                                          },
                                          {
                                            "id": 2,
                                            "name": "Resolve"
                                          }
                                        ]
                                      }''')

BAD_TRANSITIONS = textwrap.dedent('''{
                                        "transitions": [
                                          {
                                            "id": 1,
                                            "name": "Test"
                                          },
                                          {
                                            "id": 2,
                                            "name": "Fail"
                                          }
                                        ]
                                      }''')


@mock.patch("twitter.common.jira.jira.Jira.get_transitions")
def test_get_resolve_transition_id(mock_transitions):
  mock_transitions.return_value = GOOD_TRANSITIONS
  jira = Jira("test")
  assert jira._get_resolve_transition_id('test-7') == 2


@mock.patch("twitter.common.jira.jira.Jira.get_transitions")
def test_get_resolve_transition_id_error(mock_transitions):
  mock_transitions.return_value = BAD_TRANSITIONS
  jira = Jira("test")
  with pytest.raises(JiraError):
    jira._get_resolve_transition_id('test-7')

########NEW FILE########
__FILENAME__ = singleton_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.lang import Singleton

def test_basic_singleton():
  class Hello(Singleton):
    def __init__(self):
      pass
  h1 = Hello()
  h2 = Hello()
  assert id(h1) == id(h2), 'singleton mixin should memoize objects'

def test_singleton_names():
  class Hello(Singleton):
    def __init__(self, sig):
      self._sig = sig
  H1 = Hello

  class Hello(Singleton):
    def __init__(self, sig):
      self._sig = sig
  H2 = Hello

  h1 = H1('a')
  h2 = H2('b')
  assert id(h1) != id(h2), 'class names should not matter with singleton decorator'
  assert h1._sig != h2._sig

def test_cannot_supercede_constructors():
  class CountingSingleton(Singleton):
    VALUE=0
    @staticmethod
    def increment():
      CountingSingleton.VALUE += 1
    def __init__(self, value):
      self._value = value
      CountingSingleton.increment()
    def value(self):
      return self._value

  s1 = CountingSingleton('a')
  assert CountingSingleton.VALUE == 1, 'singleton constructor should be called on first invocation'

  s2 = CountingSingleton('b')
  assert s1.value() == s2.value()
  assert s2.value() == 'a'
  assert CountingSingleton.VALUE == 1, 'the constructor of a singleton should never be called twice'

def assert_factories_consistent(john, brian):
  j1 = john()
  j2 = john()
  b1 = brian()
  b2 = brian()
  assert id(j1) == id(j2)
  assert id(b1) == id(b2)
  assert id(j1) != id(b1)

def test_singleton_mixin_inheritance():
  class Named(object):
    def __init__(self, name):
      self._name = name
  class John(Named, Singleton):
    def __init__(self):
      Named.__init__(self, 'John')
  class Brian(Named, Singleton):
    def __init__(self):
      Named.__init__(self, 'Brian')
  assert_factories_consistent(John, Brian)
  # make sure ordering makes no difference
  class John(Singleton, Named):
    def __init__(self):
      Named.__init__(self, 'John')
  class Brian(Singleton, Named):
    def __init__(self):
      Named.__init__(self, 'Brian')
  assert_factories_consistent(John, Brian)


def test_singleton_metaclass_inheritance():
  class Named(Singleton):
    def __init__(self, name):
      self._name = name
  class John(Named):
    def __init__(self):
      Named.__init__(self, 'John')
  class Brian(Named):
    def __init__(self):
      Named.__init__(self, 'Brian')
  assert_factories_consistent(John, Brian)

########NEW FILE########
__FILENAME__ = test_lockable
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import threading
from twitter.common.lang import Lockable

def test_basic_mutual_exclusion():
  class Foo(Lockable):
    def __init__(self):
      self.counter = 0
      self.start_event = threading.Event()
      self.finish_event = threading.Event()
      Lockable.__init__(self)

    @Lockable.sync
    def pooping(self):
      self.counter += 1
      self.start_event.set()
      self.finish_event.wait()

  f = Foo()

  class FooSetter(threading.Thread):
    def run(self):
      f.pooping()

  fs1 = FooSetter()
  fs2 = FooSetter()
  fs1.start()
  fs2.start()

  # yield threads
  f.start_event.wait(timeout=1.0)
  assert f.start_event.is_set()

  # assert mutual exclusion
  assert f.counter == 1

  # unblock ==> other wakes up
  f.start_event.clear()
  f.finish_event.set()

  f.start_event.wait(timeout=1.0)
  assert f.start_event.is_set()
  assert f.counter == 2

########NEW FILE########
__FILENAME__ = test_reader
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from twitter.common.lang import Compatibility
from twitter.common.log.parsers import GlogLine
from twitter.common.log.reader import (
  Buffer,
  Stream,
  StreamMuxer)


TEST_GLOG_LINES = """
Log file created at: 2012/11/01 18:39:49
Running on machine: smf1-aes-07-sr2.prod.twitter.com
[DIWEF]mmdd hh:mm:ss.uuuuuu pid file:line] msg
Command line: ./thermos_executor.pex
I1101 18:39:49.557605 14209 executor_base.py:43] Executor [None]: registered() called with:
I1101 18:39:49.558563 14209 executor_base.py:43] Executor [None]:    ExecutorInfo:  executor_id {
  value: "thermos-1351795185681-wickman-hello_world_crush-2-601439aa-6f17-4e78-90a0-ef40de4a36db"
}
resources {
  name: "cpus"
  type: SCALAR
  scalar {
    value: 0.25
  }
}
I1101 18:39:50.000000 14209 executor_base.py:43] Executor [None]:    Yet another
  set of
  executor
  lines
"""

TEST_GLOG_LINES_LENGTH = len(TEST_GLOG_LINES.split('\n'))


def sio():
  return Compatibility.StringIO(TEST_GLOG_LINES)


def read_all(buf, terminator=None):
  lines = []
  while True:
    line = buf.next()
    if line is terminator:
      break
    lines.append(line)
  return lines


def write_and_rewind(sio, buf):
  sio.write(buf)
  sio.seek(-len(buf), os.SEEK_CUR)


def test_buffer_tail():
  writer = Compatibility.StringIO()
  buf = Buffer(writer, infinite=False)  # infinite ==> eof is end of line
  assert buf.next() is None
  write_and_rewind(writer, '1234')
  assert buf.next() == '1234'

  writer = Compatibility.StringIO()
  buf = Buffer(writer, infinite=True)  # infinite ==> eof is end of line
  assert buf.next() is None
  write_and_rewind(writer, '1234')
  assert buf.next() is None
  write_and_rewind(writer, '\n')
  assert buf.next() == '1234'


def test_buffer_chunksizes():
  for bufsize in (1, 2, 3, 10, 32, 512, 1024, 4096):
    class TestBuffer(Buffer):
      CHUNKSIZE = bufsize
    buf = TestBuffer(sio())
    all_lines = read_all(buf)
    assert all_lines == TEST_GLOG_LINES.split('\n')
    assert len(all_lines) == TEST_GLOG_LINES_LENGTH
    assert '\n'.join(all_lines) == '\n'.join(TEST_GLOG_LINES.split('\n'))


def test_stream():
  stream = Stream(sio(), (GlogLine,))
  lines = read_all(stream, terminator=Stream.EOF)
  assert len(lines) == 3
  last_line = lines[-1]
  # does assembly of trailing non-GlogLines work properly?
  assert last_line.raw.startswith('I1101')
  assert TEST_GLOG_LINES[-len(last_line.raw):] == last_line.raw

  # test tailed logs
  writer = Compatibility.StringIO()
  stream = Stream(writer, (GlogLine,), infinite=True)
  assert stream.next() is None
  write_and_rewind(writer, lines[0].raw)
  assert stream.next() is None
  write_and_rewind(writer, '\n')

  # this is somewhat counterintuitive behavior -- we need to see two log lines in order
  # to print one, simply because otherwise we don't know if the current line is finished.
  # you could imagine a scenario, however, when you'd want (after a certain duration)
  # to print out whatever is in the buffers regardless.  this should probably be the
  # default behavior in infinite=True, but it will add a lot of complexity to the
  # implementation.
  assert stream.next() is None
  write_and_rewind(writer, lines[1].raw)
  assert stream.next() == lines[0]

  assert stream.next() is None
  write_and_rewind(writer, '\n')
  assert stream.next() == None
  write_and_rewind(writer, lines[2].raw)
  assert stream.next() == lines[1]


########NEW FILE########
__FILENAME__ = test_scribe_handler
#!/usr/bin/python

__author__ = "Chris Chen (cchen@twitter.com)"


import mox
import logging
import unittest2 as unittest

from twitter.common.log.handlers import ScribeHandler

try:
  from scribe import scribe
  from thrift.protocol import TBinaryProtocol
  from thrift.transport import TTransport, TSocket
  _SCRIBE_PRESENT = True
except ImportError:
  _SCRIBE_PRESENT = False


_CATEGORY = "python_default"
_HOST = "localhost"
_PORT = 1463
_TEST_MSG = ("For years, the war-crimes fugitive known as 'The Terminator' was so supremely "
             "confident that he played tennis at a luxury hotel near the Congo-Rwanda border, "
             "flaunting his freedom while United Nations peacekeepers drove past.")

if _SCRIBE_PRESENT:
  _MESSAGES = [scribe.LogEntry(category=_CATEGORY, message=_TEST_MSG)]


class TestHandler(unittest.TestCase):
  def setUp(self):
    self.mox = mox.Mox()
    self.handler = None
    self.logger = logging.getLogger()
    self.logger.setLevel(logging.DEBUG)
    if _SCRIBE_PRESENT:
      self.handler_drop = ScribeHandler(buffer=False,
                                        category=_CATEGORY,
                                        host=_HOST,
                                        port=_PORT)
      self.handler_buffer = ScribeHandler(buffer=True,
                                          category=_CATEGORY,
                                          host=_HOST,
                                          port=_PORT)

  def tearDown(self):
    if self.handler:
      self.logger.removeHandler(self.handler)
      self.handler.close()
    self.mox.UnsetStubs()
    self.mox.VerifyAll()

  def set_handler(self, handler):
    self.handler = handler
    self.logger.addHandler(handler)

  def setup_mock_client(self):
    self.mox.StubOutClassWithMocks(TBinaryProtocol, "TBinaryProtocol")
    self.mock_protocol = TBinaryProtocol.TBinaryProtocol(trans=self.mock_transport,
                                                         strictRead=False,
                                                         strictWrite=False)
    self.mox.StubOutClassWithMocks(scribe, "Client")
    self.mock_client = scribe.Client(iprot=self.mock_protocol, oprot=self.mock_protocol)

  def setup_mock_transport(self):
    self.mox.StubOutClassWithMocks(TSocket, "TSocket")
    self.mock_tsocket = TSocket.TSocket(host=_HOST, port=_PORT)
    self.mox.StubOutClassWithMocks(TTransport, "TFramedTransport")
    self.mock_transport = TTransport.TFramedTransport(self.mock_tsocket)

  @unittest.skipIf(_SCRIBE_PRESENT, "Scribe Modules Present")
  def test_drop_no_scribe(self):
    with self.assertRaises(ScribeHandler.ScribeHandlerException):
      self.handler_drop = ScribeHandler(buffer=False,
                                        category=_CATEGORY,
                                        host=_HOST,
                                        port=_PORT)
      logging.debug(_TEST_MSG)

  @unittest.skipIf(_SCRIBE_PRESENT, "Scribe Modules Present")
  def test_buffer_no_scribe(self):
    with self.assertRaises(ScribeHandler.ScribeHandlerException):
      self.handler_buffer = ScribeHandler(buffer=True,
                                          category=_CATEGORY,
                                          host=_HOST,
                                          port=_PORT)
      logging.debug(_TEST_MSG)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_drop_ok(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_drop)
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.OK)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.error(_TEST_MSG)
    self.assertFalse(self.handler.messages_pending)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_drop_client_fail(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_drop)
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.TRY_LATER)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertFalse(self.handler.messages_pending)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_drop_connect_fail(self):
    self.setup_mock_transport()
    self.set_handler(self.handler_drop)
    self.mock_transport.open().AndRaise(TTransport.TTransportException)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertFalse(self.handler.messages_pending)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_ok(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.OK)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.error(_TEST_MSG)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_client_fail(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.TRY_LATER)
    self.mock_transport.close()
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.OK)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_client_fail_close_client_fail(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.TRY_LATER)
    self.mock_transport.close()
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.TRY_LATER)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_client_fail_close_connect_fail(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.TRY_LATER)
    self.mock_transport.close()
    self.mock_transport.open().AndRaise(TTransport.TTransportException)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_connect_fail(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open().AndRaise(TTransport.TTransportException)
    self.mock_transport.close()
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.OK)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_connect_fail_close_client_fail(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open().AndRaise(TTransport.TTransportException)
    self.mock_transport.close()
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.TRY_LATER)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_connect_fail_close_connect_fail(self):
    self.setup_mock_transport()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open().AndRaise(TTransport.TTransportException)
    self.mock_transport.close()
    self.mock_transport.open().AndRaise(TTransport.TTransportException)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_client_recover(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES).AndReturn(scribe.ResultCode.TRY_LATER)
    self.mock_transport.close()
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES + _MESSAGES).AndReturn(scribe.ResultCode.OK)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)
    logging.debug(_TEST_MSG)
    self.assertFalse(self.handler.messages_pending)

  @unittest.skipUnless(_SCRIBE_PRESENT, "Scribe Modules Not Present")
  def test_buffer_connect_recover(self):
    self.setup_mock_transport()
    self.setup_mock_client()
    self.set_handler(self.handler_buffer)
    self.mock_transport.open().AndRaise(TTransport.TTransportException)
    self.mock_transport.close()
    self.mock_transport.open()
    self.mock_client.Log(_MESSAGES + _MESSAGES).AndReturn(scribe.ResultCode.OK)
    self.mock_transport.close()
    self.mox.ReplayAll()
    logging.debug(_TEST_MSG)
    self.assertEquals(self.handler._log_buffer, _MESSAGES)
    logging.debug(_TEST_MSG)
    self.assertFalse(self.handler.messages_pending)

if __name__ == "__main__":
  unittest.main()

########NEW FILE########
__FILENAME__ = test_tracer
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.lang import Compatibility
from twitter.common.log.tracer import Tracer, Trace
from twitter.common.testing.clock import ThreadedClock


def test_tracing_timed():
  sio = Compatibility.StringIO()
  clock = ThreadedClock()
  final_trace = []

  class PrintTraceInterceptor(Tracer):
    def print_trace(self, *args, **kw):
      final_trace.append(self._local.parent)

  tracer = PrintTraceInterceptor(output=sio, clock=clock, predicate=lambda v: False)
  assert not hasattr(tracer._local, 'parent')

  with tracer.timed('hello'):
    clock.tick(1.0)
    with tracer.timed('world 1'):
      clock.tick(1.0)
    with tracer.timed('world 2'):
      clock.tick(1.0)

  assert len(final_trace) == 1
  final_trace = final_trace[0]
  assert final_trace._start == 0
  assert final_trace._stop == 3
  assert final_trace.duration() == 3
  assert final_trace.msg == 'hello'
  assert len(final_trace.children) == 2
  child = final_trace.children[0]
  assert child._start == 1
  assert child._stop == 2
  assert child.parent is final_trace
  assert child.msg == 'world 1'
  child = final_trace.children[1]
  assert child._start == 2
  assert child._stop == 3
  assert child.parent is final_trace
  assert child.msg == 'world 2'

  # should not log if verbosity low
  assert sio.getvalue() == ''


def test_tracing_filter():
  sio = Compatibility.StringIO()
  tracer = Tracer(output=sio)
  tracer.log('hello world')
  assert sio.getvalue() == 'hello world\n'

  sio = Compatibility.StringIO()
  tracer = Tracer(output=sio, predicate=lambda v: v >= 1)
  tracer.log('hello world')
  assert sio.getvalue() == ''
  tracer.log('hello world', V=1)
  assert sio.getvalue() == 'hello world\n'
  tracer.log('ehrmagherd', V=2)
  assert sio.getvalue() == 'hello world\nehrmagherd\n'

  sio = Compatibility.StringIO()
  tracer = Tracer(output=sio, predicate=lambda v: (v % 2 == 0))
  tracer.log('hello world', V=0)
  assert sio.getvalue() == 'hello world\n'
  tracer.log('morf gorf', V=1)
  assert sio.getvalue() == 'hello world\n'
  tracer.log('ehrmagherd', V=2)
  assert sio.getvalue() == 'hello world\nehrmagherd\n'

########NEW FILE########
__FILENAME__ = test_gauges
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest

from twitter.common.quantity import Amount, Time, Data
from twitter.common.metrics import (
  Label,
  AtomicGauge,
  MutatorGauge,

  gaugelike,
  namable,
  namablegauge,
)

def test_basic_immutable_gauge():
  lb = Label('a', 'b')
  assert lb.name() == 'a', 'name should be properly initialized'
  assert lb.read() == 'b', 'label should be properly set'

  # should handle a variety of types
  lb = Label('a', None)
  assert lb.read() == None

  lb = Label('a', [1,2,3])
  assert lb.read() == [1,2,3]

  lb = Label('b', {})
  assert lb.read() == {}

def test_basic_mutable_gauge():
  mg = MutatorGauge('a')
  assert mg.name() == 'a'
  assert mg.read() == None
  mg = MutatorGauge('a', 'b')
  assert mg.name() == 'a'
  assert mg.read() == 'b'
  mg.write('c')
  assert mg.name() == 'a'
  assert mg.read() == 'c'
  mg.write(None)
  assert mg.read() == None

def test_atomic_gauge():
  ag = AtomicGauge('a')
  assert ag.name() == 'a'
  assert ag.read() == 0
  assert ag.add(-2) == -2
  ag = AtomicGauge('a')
  assert ag.decrement() == -1

def test_atomic_gauge_types():
  with pytest.raises(TypeError):
    ag = AtomicGauge('a', None)
  with pytest.raises(TypeError):
    ag = AtomicGauge('a', 'hello')
  ag = AtomicGauge('a', 23)
  with pytest.raises(TypeError):
    ag.add(None)
  with pytest.raises(TypeError):
    ag.add('hello')

def test_named_gauge_types():
  with pytest.raises(TypeError):
    ag = AtomicGauge(0)
  with pytest.raises(TypeError):
    ag = AtomicGauge(None)
  with pytest.raises(TypeError):
    lb = Label(None, 3)
  with pytest.raises(TypeError):
    mg = MutatorGauge({})

def test_duck_typing():
  class Anonymous(object):
    pass
  A = Anonymous()
  A.name = lambda: 'anonymous'
  assert namable(A), 'any object with a name method should be considered namable'
  A = Anonymous()
  A.read = lambda: 5
  assert gaugelike(A), 'any object with a read method should be considered gaugelike'
  A.name = lambda: 'anonymous'
  assert namablegauge(A), 'any object with read/name method shoudl be considered namablegauge'

  A.name = 'anonymous'
  assert not namablegauge(A)
  assert not namable(A)
  assert gaugelike(A)

  A.read = '5'
  assert not gaugelike(A)

########NEW FILE########
__FILENAME__ = test_metrics
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest

from twitter.common.quantity import Amount, Time, Data
from twitter.common.metrics import Label, MutatorGauge
from twitter.common.metrics import (
    CompoundMetrics,
    Observable,
    RootMetrics)
from twitter.common.metrics.metrics import Metrics


def test_root_metrics_singleton():
  rm = RootMetrics()
  rm2 = RootMetrics()
  assert id(rm) == id(rm2)


def test_basic_registration_and_clear():
  lb = Label('ping', 'pong')
  rm = RootMetrics()
  rm.register(lb)
  assert rm.sample() == {'ping': 'pong'}
  rm.clear()
  assert rm.sample() == {}


def test_nontrivial_gauges():
  for label_value in ['a', 0, 2.5, [1,2,"3"], {'a': 'b'}, {'c': None}, False]:
    lb = Label('ping', label_value)
    rm = RootMetrics()
    rm.register(lb)
    assert rm.sample() == {'ping': label_value}
    rm.clear()
    assert rm.sample() == {}


def test_basic_scoping():
  lb = Label('ping', 'pong')
  rm = RootMetrics()
  rm.register(lb)
  rm.scope('bing').register(lb)
  assert rm.sample() == { 'ping': 'pong', 'bing.ping': 'pong' }
  rm.clear()


def test_scoped_registration_uses_references():
  mg = MutatorGauge('name', 'brian')
  rm = RootMetrics()
  rm.scope('earth').register(mg)
  rm.scope('pluto').register(mg)
  assert rm.sample() == { 'earth.name': 'brian', 'pluto.name': 'brian' }
  mg.write('zargon')
  assert rm.sample() == { 'earth.name': 'zargon', 'pluto.name': 'zargon' }
  rm.clear()


def test_register_string():
  rm = RootMetrics()
  hello_gauge = rm.register('hello')
  assert rm.sample() == { 'hello': None }
  hello_gauge.write('poop')
  assert rm.sample() == { 'hello': 'poop' }
  rm.clear()


def test_nested_scopes():
  rm = RootMetrics()
  mg = rm.scope('a').scope('b').scope('c').register('123')
  mg.write(Amount(1, Time.MILLISECONDS))
  assert rm.sample() == {'a.b.c.123': '1 ms'}
  rm.clear()


def test_bad_scope_names():
  rm = RootMetrics()
  my_scope = rm.scope('my_scope')
  with pytest.raises(TypeError):
    my_scope.scope(None)
  with pytest.raises(TypeError):
    my_scope.scope({})
  with pytest.raises(TypeError):
    my_scope.scope(123)
  with pytest.raises(TypeError):
    my_scope.scope(RootMetrics)


def test_compound_metrics():
  metrics1 = Metrics()
  metrics2 = Metrics()

  metrics1.register(Label('value', 'first'))
  metrics2.register(Label('value', 'second'))
  assert CompoundMetrics(metrics1, metrics2).sample() == {'value': 'second'}

  metrics1.register(Label('other', 'third'))
  assert CompoundMetrics(metrics1, metrics2).sample() == {
      'value': 'second', 'other': 'third'}


def test_observable():
  class Derp(Observable):
    def __init__(self):
      self.metrics.register(Label('value', 'derp value'))
  metrics = Metrics()
  metrics.register_observable('derpspace', Derp())
  assert metrics.sample() == {'derpspace.value': 'derp value'}

########NEW FILE########
__FILENAME__ = test_rate
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
import unittest

from twitter.common.quantity import Amount, Time, Data
from twitter.common.metrics import (
  AtomicGauge,
  MutatorGauge,
  NamedGauge,
  Rate
)

class FakeGauge(NamedGauge):
  def __init__(self, name):
    self._supplies = []
    NamedGauge.__init__(self, name)

  def supplies(self, values):
    self._supplies.extend(values)
    return self

  def read(self):
    return self._supplies.pop(0)

class TestClock(object):
  def __init__(self):
    self._time = 0

  def advance(self, ticks):
    self._time += ticks

  def time(self):
    return self._time

class TestRate(unittest.TestCase):
  def test_empty(self):
    clock = TestClock()
    gauge = FakeGauge('test').supplies([100000])
    rate = Rate("foo", gauge, window = Amount(30, Time.SECONDS), clock=clock)
    assert rate.read() == 0

  def test_windowing(self):
    TEN_SECONDS = Amount(10, Time.SECONDS).as_(Time.SECONDS)

    clock = TestClock()
    gauge = FakeGauge('test').supplies([100, 0, 50, 100, 150, 100, 50])
    rate = Rate("foo", gauge, window = Amount(30, Time.SECONDS), clock=clock)

    assert rate.read() == 0

    clock.advance(TEN_SECONDS)
    assert -100.0 / 10 == rate.read()

    clock.advance(TEN_SECONDS)
    assert -50.0 / 20 == rate.read()

    clock.advance(TEN_SECONDS)
    assert 0 == rate.read()

    clock.advance(TEN_SECONDS)
    assert 150.0 / 30 == rate.read()

    clock.advance(TEN_SECONDS)
    assert 50.0 / 30 == rate.read()

    clock.advance(TEN_SECONDS)
    assert -50.0 / 30 == rate.read()

  def test_static_constructor(self):
    gauge = FakeGauge('test').supplies([100000])
    rate = Rate.of(gauge)
    assert rate.name() == 'test_per_1secs'
    rate = Rate.of(gauge, window = Amount(5, Time.MINUTES))
    assert rate.name() == 'test_per_5mins'
    rate = Rate.of(gauge, name = 'holyguacamole')
    assert rate.name() == 'holyguacamole_per_1secs'
    rate = Rate.of(gauge, name = 'holyguacamole', window = Amount(3, Time.HOURS))
    assert rate.name() == 'holyguacamole_per_3hrs'

########NEW FILE########
__FILENAME__ = test_sampling
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pytest

from twitter.common.contextutil import temporary_file
from twitter.common.metrics import Label
from twitter.common.metrics.metrics import Metrics
from twitter.common.metrics.sampler import (
    MetricSampler,
    SamplerBase,
    DiskMetricWriter,
    DiskMetricReader)
from twitter.common.quantity import Amount, Time
from twitter.common.testing.clock import ThreadedClock


def test_sampler_base():
  class TestSampler(SamplerBase):
    def __init__(self, period, clock):
      self.count = 0
      SamplerBase.__init__(self, period, clock)

    def iterate(self):
      self.count += 1

  test_clock = ThreadedClock()
  sampler = TestSampler(Amount(1, Time.SECONDS), clock=test_clock)
  sampler.start()

  test_clock.tick(0.5)
  assert sampler.count == 0
  test_clock.tick(0.5)
  assert sampler.count == 1
  test_clock.tick(5)
  assert sampler.count == 6
  assert not sampler.is_stopped()
  sampler.stop()
  # make sure that stopping the sampler short circuits any sampling
  test_clock.tick(5)
  assert sampler.count == 6


def test_metric_read_write():
  metrics = Metrics()

  with temporary_file() as fp:
    os.unlink(fp.name)

    writer = DiskMetricWriter(metrics, fp.name)
    reader = DiskMetricReader(fp.name)

    assert reader.sample() == {}
    reader.iterate()
    assert reader.sample() == {}

    writer.iterate()
    assert reader.sample() == {}
    reader.iterate()
    assert reader.sample() == {}

    metrics.register(Label('herp', 'derp'))
    writer.iterate()
    assert reader.sample() == {}
    reader.iterate()
    assert reader.sample() == {'herp': 'derp'}


def test_metric_sample():
  metrics = Metrics()
  sampler = MetricSampler(metrics)
  assert sampler.sample() == {}
  sampler.iterate()
  assert sampler.sample() == {}
  metrics.register(Label('herp', 'derp'))
  assert sampler.sample() == {}
  sampler.iterate()
  assert sampler.sample() == {'herp': 'derp'}

########NEW FILE########
__FILENAME__ = test_socks
from twitter.common.net.socks import (
    SocksiPyConnection,
    SocksiPyHandler,
    urllib_opener
)

import mox
import socks


def make_handler(*args, **kw):
  opener = urllib_opener(*args, **kw)
  our_handler = None
  for handler in opener.handlers:
    if isinstance(handler, SocksiPyHandler):
      our_handler = handler
      break
  assert our_handler is not None
  return our_handler


def test_constructor_unwrapping():
  handler = make_handler('foo', 1234, password='bork')
  connection = handler.build_connection('www.google.com', port=80)
  assert connection._proxyargs == (
      socks.PROXY_TYPE_SOCKS5, 'foo', 1234, True, None, 'bork')


def test_mock_connect():
  m = mox.Mox()

  class MockSocket(object):
    def setproxy(self, *args):
      self.args = args
    def settimeout(self, value):
      self.timeout = value
    def connect(self, tup):
      self.host, self.port = tup

  mock_socket = MockSocket()

  m.StubOutWithMock(socks, 'socksocket')
  socks.socksocket().AndReturn(mock_socket)
  m.ReplayAll()

  handler = make_handler('foo', 1234)
  conn = handler.build_connection('www.google.com', timeout=1)
  conn.connect()
  assert mock_socket.args == conn._proxyargs
  assert mock_socket.timeout == 1
  assert mock_socket.host == 'www.google.com'
  assert mock_socket.port == 80

  m.UnsetStubs()
  m.VerifyAll()

########NEW FILE########
__FILENAME__ = options_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common import options

import pytest
import unittest

class TestOptions(unittest.TestCase):
  def test_parser_defaults(self):
    parser = options.parser()
    assert parser.interspersed_arguments() is False
    assert parser.usage() == ""

  def test_mutation_creates_new_parser(self):
    parser1 = options.parser()
    parser2 = parser1.interspersed_arguments(True)
    parser3 = parser2.usage("foo")
    assert parser1 is not parser2
    assert parser2 is not parser3
    assert parser1 is not parser3
    assert parser1.interspersed_arguments() is False
    assert parser1.usage() == ""
    assert parser2.interspersed_arguments() is True
    assert parser2.usage() == ""
    assert parser3.interspersed_arguments() is True
    assert parser3.usage() == "foo"

  def test_basic_parsing(self):
    option = options.Option('-m', '--my_option', dest='my_option')

    # w/o option
    values, leftovers = options.parser().options([option]).parse([])
    assert not hasattr(values, 'my_option')
    assert leftovers == []

    # w/ option
    values, leftovers = options.parser().options([option]).parse(['-m', 'poop'])
    assert values.my_option == 'poop'
    assert leftovers == []

    # w/ long option
    values, leftovers = options.parser().options([option]).parse(['--my_option', 'plork'])
    assert values.my_option == 'plork'
    assert leftovers == []

    # w/ option and leftover
    values, leftovers = options.parser().options([option]).parse(['--my_option', 'plork', 'hork'])
    assert values.my_option == 'plork'
    assert leftovers == ['hork']

  def test_default_parsing(self):
    option = options.Option('-m', '--my_option', default="specified", dest='my_option')
    values, leftovers = options.parser().options([option]).parse([])
    assert hasattr(values, 'my_option')
    assert leftovers == []
    assert values.my_option == 'specified'

  def test_value_inheritance(self):
    option_list = [
      options.Option('-a', dest='a'),
      options.Option('-b', dest='b')
    ]

    values, leftovers = options.parser().options(option_list).parse([])
    assert not hasattr(values, 'a')
    assert not hasattr(values, 'b')

    # w/ option
    values, leftovers = options.parser().options(option_list).parse(['-a', 'value_a'])
    assert hasattr(values, 'a')
    assert values.a == 'value_a'
    assert not hasattr(values, 'b')

    # w/ inherited option
    values, leftovers = options.parser().values(values).options(option_list).parse(['-b', 'value_b'])
    assert values.a == 'value_a'
    assert values.b == 'value_b'

    # w/ inherits w/o parsing any new args
    values, leftovers = options.parser().values(values).options(option_list).parse([])
    assert values.a == 'value_a'
    assert values.b == 'value_b'

    # w/ overwrites despite inheriting
    values, leftovers = options.parser().values(values).options(option_list).parse(['-a', 'new_value_a'])
    assert values.a == 'new_value_a'
    assert values.b == 'value_b'

  def test_multiple_value_inheritance(self):
    option_list = [
      options.Option('-a', dest='a'),
      options.Option('-b', dest='b')
    ]

    values_with_a, _ = options.parser().options(option_list).parse(['-a', 'value_a'])
    values_with_b, _ = options.parser().options(option_list).parse(['-b', 'value_b'])
    values, leftovers = (options.parser()
                                .options(option_list)
                                .values(values_with_a)
                                .values(values_with_b)).parse([])
    assert values.a == 'value_a'
    assert values.b == 'value_b'

    # and parsed values overwrite
    values, leftovers = (options.parser()
                                .options(option_list)
                                .values(values_with_a)
                                .values(values_with_b)).parse(['-a', 'new_value_a'])
    assert values.a == 'new_value_a'
    assert values.b == 'value_b'

  def test_multiple_option_inheritance(self):
    option_a = options.Option('-a', dest='a')
    option_b = options.Option('-b', dest='b')
    values, leftovers = (options.parser()
                                .options([option_a])
                                .options([option_b])).parse(['-a', 'value_a', '-b', 'value_b'])
    assert values.a == 'value_a'
    assert values.b == 'value_b'

  def test_groups(self):
    option_a = options.Option('-a', dest='a')
    option_b = options.Option('-b', dest='b')
    option_group_a = options.group('a')
    option_group_b = options.group('b')
    option_group_a.add_option(options.Option('--a1', dest='a1'), options.Option('--a2', dest='a2'))
    option_group_b.add_option(options.Option('--b1', dest='b1'), options.Option('--b2', dest='b2'))

    partial_parser = (options.parser()
                             .interspersed_arguments(True)
                             .groups([option_group_a, option_group_b]))
    full_parser = partial_parser.options([option_a, option_b])

    parameters = ['--a1', 'value_a1', '--a2', 'value_a2',
                  '--b1', 'value_b1', '--b2', 'value_b2']
    full_parameters = parameters + ['-a', 'value_a', '-b', 'value_b']

    values, leftovers = partial_parser.parse(parameters)
    assert values.a1 == 'value_a1'
    assert values.a2 == 'value_a2'
    assert values.b1 == 'value_b1'
    assert values.b2 == 'value_b2'
    assert leftovers == []

    values, leftovers = full_parser.parse(full_parameters)
    assert values.a1 == 'value_a1'
    assert values.a2 == 'value_a2'
    assert values.b1 == 'value_b1'
    assert values.b2 == 'value_b2'
    assert values.a == 'value_a'
    assert values.b == 'value_b'
    assert leftovers == []

########NEW FILE########
__FILENAME__ = test_handle_parser
import unittest
import pytest
from twitter.common.process.process_handle import ProcessHandleParser

class TestHandleParser(unittest.TestCase):
  @classmethod
  def setup_class(cls):
    cls.attrs = "first last year_of_birth day_of_birth".split()
    cls.type_map = {
      'first': '%s',
      'last': '%s',
      'year_of_birth': '%d',
      'day_of_birth': '%d'
    }
    cls.days_of_week = ['m', 't', 'w', 'th', 'f', 's', 'su']
    cls.handlers = {
      'first': lambda k, v: v.upper(),
      'last': lambda k, v: v.lower(),
      'day_of_birth': lambda k, v: cls.days_of_week[v % len(cls.days_of_week)]
    }
    cls.ph = ProcessHandleParser(cls.attrs, cls.type_map, cls.handlers)

  def test_basic(self):
    test_strings = [
      'john doe 1900 23',
      'JoHn DoE 1900 23',
      'JoHn\tDoE\t1900\t23',
      '   JoHn \t DoE \t1900\t 23 ',
    ]
    for test_string in test_strings:
      attrs = self.ph.parse(test_string)
      assert attrs['first'] == 'JOHN'
      assert attrs['last'] == 'doe'
      assert attrs['year_of_birth'] == 1900
      assert attrs['day_of_birth'] == self.days_of_week[23 % len(self.days_of_week)]

########NEW FILE########
__FILENAME__ = test_crawler
import contextlib
import os

from twitter.common.contextutil import temporary_dir
from twitter.common.python.http.crawler import Crawler, PageParser
from twitter.common.python.testing import create_layout


def lpp(page):
  links = PageParser.links(page)
  rels = PageParser.rel_links(page)
  return list(links), list(rels)


def test_page_parser_empty():
  links, rels = lpp("")
  assert links == []
  assert rels == []


def test_page_parser_basic():
  for target in ('href', 'href =', 'href =""'): #, 'href= ""'):
    assert lpp(target.lower()) == ([], [])
    assert lpp(target.upper()) == ([], [])
  for target in ('a href=', 'a href=""'):
    assert lpp(target.lower()) == ([''], [])
    assert lpp(target.upper()) == ([''], [])
  assert lpp('a href=11') == (['11'], [])
  assert lpp('a href=12') == (['12'], [])
  for href in ('pooping', '{};a[32[32{#@'):
    for start, end in ( ('', ''), ('"', '"'), ("'", "'") ):
      target = '%s%s%s' % (start, href, end)
      assert lpp('<a href=%s>' % target) == ([href], [])
      assert lpp("<a href=%s>" % target) == ([href], [])
      assert lpp('anything <a href=%s> anything' % target) == ([href], [])
      assert lpp("<a href=%s> <a href='stuff'>" % target) == ([href, 'stuff'], [])
      assert lpp("<a href='stuff'> <a href=%s>" % target) == (['stuff', href], [])


def test_page_parser_rels():
  VALID_RELS = tuple(PageParser.REL_TYPES)
  for rel in VALID_RELS + ('', ' ', 'blah'):
    for start, end in ( ('', ''), ('"', '"'), ("'", "'") ):
      target = 'rel=%s%s%s' % (start, rel, end)
      links, rels = lpp("<a href='things' %s> <a href='stuff'>" % target)
      assert links == ['things', 'stuff']
      if rel in VALID_RELS:
        assert rels == ['things']
      else:
        assert rels == []
      links, rels = lpp("<a href='stuff' %s> <a href='things'>" % target)
      assert links == ['stuff', 'things']
      if rel in VALID_RELS:
        assert rels == ['stuff']
      else:
        assert rels == []

def test_page_parser_skips_data_rels():
  for ext in PageParser.REL_SKIP_EXTENSIONS:
    things = 'things%s' % ext
    assert lpp("<a href='%s' rel=download>" % things) == ([things], [])
  for ext in ('.html', '.xml', '', '.txt', '.tar.gz.txt'):
    things = 'things%s' % ext
    assert lpp("<a href='%s' rel=download>" % things) == ([things], [things])


def test_crawler_local():
  FL = ('a.txt', 'b.txt', 'c.txt')
  with temporary_dir() as td:
    for fn in FL:
      with open(os.path.join(td, fn), 'w') as fp:
        pass
    for dn in (1, 2):
      os.mkdir(os.path.join(td, 'dir%d' % dn))
      for fn in FL:
        with open(os.path.join(td, 'dir%d' % dn, fn), 'w') as fp:
          pass

    # basic file / dir rel splitting
    links, rels = Crawler(enable_cache=False).execute(td)
    assert set(links) == set(os.path.join(td, fn) for fn in FL)
    assert set(rels) == set(os.path.join(td, 'dir%d' % n) for n in (1, 2))

    # recursive crawling, single vs multi-threaded
    for caching in (False, True):
      for threads in (1, 2, 3):
        links = Crawler(enable_cache=caching, threads=threads).crawl([td])
        expect_links = (set(os.path.join(td, fn) for fn in FL) |
                        set(os.path.join(td, 'dir1', fn) for fn in FL) |
                        set(os.path.join(td, 'dir2', fn) for fn in FL))
        assert set(links) == expect_links


def test_crawler_unknown_scheme():
  # skips unknown url schemes
  Crawler(enable_cache=False).execute('ftp://ftp.cdrom.com') == (set(), set())


# TODO(wickman)
#   test remote http crawling via mock
#   test page decoding via mock

########NEW FILE########
__FILENAME__ = test_http
import contextlib
import errno
import os
import socket
import threading

from twitter.common.lang import Compatibility
from twitter.common.python.common import safe_mkdtemp
from twitter.common.python.http import CachedWeb, Web, FetchError
from twitter.common.testing.clock import ThreadedClock

import pytest


if Compatibility.PY3:
  from unittest import mock
  import urllib.error as urllib_error
  import urllib.parse as urlparse
  import urllib.request as urllib_request
  URLLIB_REQUEST = 'urllib.request'
else:
  import mock
  import urllib2 as urllib_request
  import urllib2 as urllib_error
  import urlparse
  URLLIB_REQUEST = 'urllib2'


# TODO(wickman) things that still need testing: encoding of code + headers into .headers file


@mock.patch('socket.gethostbyname')
def test_open_resolve_failure(gethostbyname_mock):
  gethostbyname_mock.side_effect = socket.gaierror(errno.EADDRNOTAVAIL, 'Could not resolve host.')
  with pytest.raises(FetchError):
    Web().open('http://www.google.com')


def test_resolve_timeout():
  event = threading.Event()
  class FakeWeb(Web):
    NS_TIMEOUT_SECS = 0.001
    def _resolves(self, fullurl):
      event.wait()
    def _reachable(self, fullurl):
      return True
  with pytest.raises(FetchError):
    FakeWeb().open('http://www.google.com')
  # unblock anonymous thread
  event.set()


@mock.patch('socket.gethostbyname')
@mock.patch('socket.create_connection')
def test_unreachable_error(create_connection_mock, gethostbyname_mock):
  gethostbyname_mock.return_value = '1.2.3.4'
  create_connection_mock.side_effect = socket.error(errno.ENETUNREACH,
      'Could not reach network.')
  with pytest.raises(FetchError):
    Web().open('http://www.google.com')
  gethostbyname_mock.assert_called_once_with('www.google.com')


@mock.patch('%s.urlopen' % URLLIB_REQUEST)
def test_local_open(urlopen_mock):
  urlopen_mock.return_value = b'data'
  assert Web().open('/local/filename') == b'data'


def test_local_cached_open():
  cache = safe_mkdtemp()
  web = CachedWeb(cache=cache)

  source_dir = safe_mkdtemp()
  source = os.path.join(source_dir, 'filename')
  with open(source, 'wb') as fp:
    fp.write(b'data')

  with contextlib.closing(web.open(source)) as cached_fp1:
    assert b'data' == cached_fp1.read()
  with contextlib.closing(web.open(source)) as cached_fp2:
    assert b'data' == cached_fp2.read()


def test_maybe_local():
  maybe_local = Web().maybe_local_url
  assert maybe_local('http://www.google.com') == 'http://www.google.com'
  assert maybe_local('https://www.google.com/whatever') == 'https://www.google.com/whatever'
  assert maybe_local('tmp/poop.txt') == 'file://' + os.path.realpath('tmp/poop.txt')
  assert maybe_local('/tmp/poop.txt') == 'file://' + os.path.realpath('/tmp/poop.txt')
  assert maybe_local('www.google.com') == 'file://' + os.path.realpath('www.google.com')


class MockOpener(object):
  DEFAULT_DATA = b'Blah blah blahhhhh'

  def __init__(self, rv=DEFAULT_DATA, code=200):
    self.rv = rv
    self.code = code
    self.opened = threading.Event()
    self.error = None

  def clear(self):
    self.opened.clear()

  def open(self, url, conn_timeout=None):
    if conn_timeout == 0:
      raise urllib_error.URLError('Could not reach %s within deadline.' % url)
    if url.startswith('http'):
      self.opened.set()
    if self.error:
      raise urllib_error.HTTPError(url, self.error, None, None, Compatibility.BytesIO(b'glhglhg'))
    return urllib_request.addinfourl(Compatibility.BytesIO(self.rv), url, None, self.code)


def test_connect_timeout_using_open():
  URL = 'http://www.google.com'
  DATA = b'This is google.com!'

  clock = ThreadedClock()
  opener = MockOpener(DATA)
  web = CachedWeb(clock=clock, opener=opener)
  assert not os.path.exists(web.translate_url(URL))
  with pytest.raises(FetchError):
    with contextlib.closing(web.open(URL, conn_timeout=0)):
      pass
  with contextlib.closing(web.open(URL, conn_timeout=0.01)) as fp:
    assert fp.read() == DATA


@mock.patch('os.path.getmtime')
def test_caching(getmtime_mock):
  URL = 'http://www.google.com'
  DATA = b'This is google.com!'
  clock = ThreadedClock()
  getmtime_mock.return_value = 0

  opener = MockOpener(DATA)
  web = CachedWeb(clock=clock, opener=opener)
  assert not os.path.exists(web.translate_url(URL))
  with contextlib.closing(web.open(URL)) as fp:
    assert fp.read() == DATA
  assert os.path.exists(web.translate_url(URL))
  assert opener.opened.is_set()
  opener.clear()

  assert web.expired(URL, ttl=0.5) is False
  clock.tick(1)
  assert web.expired(URL, ttl=0.5)

  with contextlib.closing(web.open(URL)) as fp:
    assert fp.read() == DATA
  assert not opener.opened.is_set()

  with contextlib.closing(web.open(URL, ttl=0.5)) as fp:
    assert fp.read() == DATA
  assert opener.opened.is_set(), 'expect expired url to cause http get'

########NEW FILE########
__FILENAME__ = test_environment
from contextlib import closing, contextmanager
import os
import zipfile

from twitter.common.contextutil import temporary_dir, temporary_file
from twitter.common.python.compatibility import nested
from twitter.common.python.environment import PEXEnvironment
from twitter.common.python.pex import PEX
from twitter.common.python.pex_builder import PEXBuilder
from twitter.common.python.pex_info import PexInfo
from twitter.common.python.testing import make_distribution


@contextmanager
def yield_pex_builder(zip_safe=True):
  with nested(temporary_dir(), make_distribution('p1', zipped=True, zip_safe=zip_safe)) as (td, p1):
    pb = PEXBuilder(path=td)
    pb.add_egg(p1.location)
    yield pb


def test_force_local():
  with nested(yield_pex_builder(), temporary_dir(), temporary_file()) as (pb, pex_root, pex_file):
    pex = pb.path()
    pb.info.pex_root = pex_root
    pb.build(pex_file.name)

    code_cache = PEXEnvironment.force_local(pex_file.name, pb.info)
    assert os.path.exists(pb.info.zip_unsafe_cache)
    assert len(os.listdir(pb.info.zip_unsafe_cache)) == 1
    assert [os.path.basename(code_cache)] == os.listdir(pb.info.zip_unsafe_cache)
    assert set(os.listdir(code_cache)) == set([PexInfo.PATH, '__main__.py'])

    # idempotence
    assert PEXEnvironment.force_local(pex_file.name, pb.info) == code_cache


def normalize(path):
  return os.path.normpath(os.path.realpath(path))


def test_write_zipped_internal_cache():
  # zip_safe pex will not be written to install cache unless always_write_cache
  with nested(yield_pex_builder(zip_safe=True), temporary_dir(), temporary_file()) as (
      pb, pex_root, pex_file):

    pex = pb.path()
    pb.info.pex_root = pex_root
    pb.build(pex_file.name)

    dists = PEXEnvironment.write_zipped_internal_cache(pex_file.name, pb.info)
    assert len(dists) == 1
    assert normalize(dists[0].location).startswith(
        normalize(os.path.join(pex_file.name, pb.info.internal_cache))), (
        'loc: %s, cache: %s' % (
            normalize(dists[0].location),
            normalize(os.path.join(pex_file.name, pb.info.internal_cache))))

    pb.info.always_write_cache = True
    dists = PEXEnvironment.write_zipped_internal_cache(pex_file.name, pb.info)
    assert len(dists) == 1
    assert normalize(dists[0].location).startswith(normalize(pb.info.install_cache))

  # zip_safe pex will not be written to install cache unless always_write_cache
  with nested(yield_pex_builder(zip_safe=False), temporary_dir(), temporary_file()) as (
      pb, pex_root, pex_file):

    pex = pb.path()
    pb.info.pex_root = pex_root
    pb.build(pex_file.name)

    dists = PEXEnvironment.write_zipped_internal_cache(pex_file.name, pb.info)
    assert len(dists) == 1
    assert normalize(dists[0].location).startswith(normalize(pb.info.install_cache))
    original_location = normalize(dists[0].location)

    # do the second time to validate idempotence of caching
    dists = PEXEnvironment.write_zipped_internal_cache(pex_file.name, pb.info)
    assert len(dists) == 1
    assert normalize(dists[0].location) == original_location


def test_load_internal_cache_unzipped():
  # zip_safe pex will not be written to install cache unless always_write_cache
  with nested(yield_pex_builder(zip_safe=True), temporary_dir()) as (pb, pex_root):
    pex = pb.path()
    pb.info.pex_root = pex_root
    pb.freeze()

    dists = list(PEXEnvironment.load_internal_cache(pb.path(), pb.info))
    assert len(dists) == 1
    assert normalize(dists[0].location).startswith(
        normalize(os.path.join(pb.path(), pb.info.internal_cache)))

########NEW FILE########
__FILENAME__ = test_finders
from twitter.common.python.finders import (
    ChainedFinder,
    _add_finder as add_finder,
    _remove_finder as remove_finder,
)

try:
  import mock
except ImportError:
  from unittest import mock

import pkg_resources


def test_chained_finder():
  def finder1(importer, path_item, only=False):
    for foo in ('foo', 'bar'):
      yield foo

  def finder2(importer, path_item, only=False):
    yield 'baz'

  cf = ChainedFinder([finder1])
  assert list(cf(None, None)) == ['foo', 'bar']

  cf = ChainedFinder([finder1, finder2])
  assert list(cf(None, None)) == ['foo', 'bar', 'baz']


GET_FINDER = 'twitter.common.python.finders._get_finder'
REGISTER_FINDER = 'twitter.common.python.finders.pkg_resources.register_finder'


def test_add_new_finder():
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      mock_get_finder.return_value = None
      add_finder('foo', 'bar')
      mock_register_finder.assert_called_with('foo', 'bar')


def test_append_finder():
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      mock_get_finder.return_value = 'bar'
      add_finder('foo', 'baz')
      mock_register_finder.assert_called_with('foo', ChainedFinder(['bar', 'baz']))

  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      mock_get_finder.return_value = ChainedFinder(['bar'])
      add_finder('foo', 'baz')
      mock_register_finder.assert_called_with('foo', ChainedFinder(['bar', 'baz']))


def test_remove_finder():
  # wasn't registered
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      mock_get_finder.return_value = None
      remove_finder('foo', 'baz')
      assert not mock_register_finder.called

  # was registered but we're asking for the wrong one
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      mock_get_finder.return_value = ChainedFinder(['bar'])
      remove_finder('foo', 'baz')
      assert not mock_register_finder.called

  # was registered but we're asking for the wrong one
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      cf = ChainedFinder(['bar', 'baz', 'bak'])
      mock_get_finder.return_value = cf
      remove_finder('foo', 'baz')
      assert cf.finders == ['bar', 'bak']
      assert not mock_register_finder.called

  # was registered but we're asking for the wrong one
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      cf = ChainedFinder(['bar', 'baz'])
      mock_get_finder.return_value = cf
      remove_finder('foo', 'baz')
      mock_register_finder.assert_called_with('foo', 'bar')

  # was registered but we're asking for the wrong one
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      mock_get_finder.return_value = 'bar'
      remove_finder('foo', 'bar')
      mock_register_finder.assert_called_with('foo', pkg_resources.find_nothing)

  # was registered but we're asking for the wrong one
  with mock.patch(GET_FINDER) as mock_get_finder:
    with mock.patch(REGISTER_FINDER) as mock_register_finder:
      mock_get_finder.return_value = ChainedFinder(['bar'])
      remove_finder('foo', 'bar')
      mock_register_finder.assert_called_with('foo', pkg_resources.find_nothing)

########NEW FILE########
__FILENAME__ = test_interpreter
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import sys

from twitter.common.python import interpreter

from mock import patch
import pytest


class TestPythonInterpreter(object):

  @pytest.mark.skipif('sys.version_info >= (3,0)')
  def test_all_does_not_raise_with_empty_path_envvar(self):
    """ additionally, tests that the module does not raise at import """
    with patch.dict(os.environ, clear=True):
      reload(interpreter)
      interpreter.PythonInterpreter.all()

########NEW FILE########
__FILENAME__ = test_obtainer
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.python.fetcher import Fetcher
from twitter.common.python.interpreter import PythonInterpreter
from twitter.common.python.obtainer import Obtainer
from twitter.common.python.package import EggPackage, SourcePackage, WheelPackage

from pkg_resources import Requirement, get_build_platform


def test_package_precedence():
  source = SourcePackage('psutil-0.6.1.tar.gz')
  egg = EggPackage('psutil-0.6.1-py2.6.egg')
  whl = WheelPackage('psutil-0.6.1-cp26-none-macosx_10_4_x86_64.whl')

  # default precedence
  assert Obtainer.package_precedence(whl) > Obtainer.package_precedence(egg)
  assert Obtainer.package_precedence(egg) > Obtainer.package_precedence(source)
  assert Obtainer.package_precedence(whl) > Obtainer.package_precedence(source)

  # overridden precedence
  PRECEDENCE = (EggPackage, WheelPackage)
  assert Obtainer.package_precedence(source, PRECEDENCE) == (source.version, -1)  # unknown rank
  assert Obtainer.package_precedence(whl, PRECEDENCE) > Obtainer.package_precedence(
      source, PRECEDENCE)
  assert Obtainer.package_precedence(egg, PRECEDENCE) > Obtainer.package_precedence(
      whl, PRECEDENCE)


class FakeCrawler(object):
  def __init__(self, hrefs):
    self._hrefs = hrefs
    self.opener = None

  def crawl(self, *args, **kw):
    return self._hrefs


class FakeObtainer(Obtainer):
  def __init__(self, links):
    self.__links = list(links)
    super(FakeObtainer, self).__init__(FakeCrawler([]), [], [])

  def _iter_unordered(self, req):
    return iter(self.__links)


def test_iter_ordering():
  pi = PythonInterpreter.get()
  tgz = SourcePackage('psutil-0.6.1.tar.gz')
  egg = EggPackage('psutil-0.6.1-py%s-%s.egg' % (pi.python, get_build_platform()))
  whl = WheelPackage('psutil-0.6.1-cp%s-none-%s.whl' % (
      pi.python.replace('.', ''),
      get_build_platform().replace('-', '_').replace('.', '_').lower()))
  req = Requirement.parse('psutil')

  assert list(FakeObtainer([tgz, egg, whl]).iter(req)) == [whl, egg, tgz]
  assert list(FakeObtainer([egg, tgz, whl]).iter(req)) == [whl, egg, tgz]


def test_href_translation():
  VERSIONS = ['0.4.0', '0.4.1', '0.5.0', '0.6.0']

  def fake_link(version):
    return 'http://www.example.com/foo/bar/psutil-%s.tar.gz' % version

  fc = FakeCrawler([fake_link(v) for v in VERSIONS])
  ob = Obtainer(fc, [], [])

  for v in VERSIONS:
    pkgs = list(ob.iter(Requirement.parse('psutil==%s' % v)))
    assert len(pkgs) == 1, 'Version: %s' % v
    assert pkgs[0] == SourcePackage(fake_link(v))

  assert list(ob.iter(Requirement.parse('psutil>=0.5.0'))) == [
    SourcePackage(fake_link('0.6.0')),
    SourcePackage(fake_link('0.5.0'))]

  assert list(ob.iter(Requirement.parse('psutil'))) == [
      SourcePackage(fake_link(v)) for v in reversed(VERSIONS)]

########NEW FILE########
__FILENAME__ = test_package
import contextlib
import os

from zipfile import ZipFile

from twitter.common.contextutil import temporary_dir
from twitter.common.python.http import Web
from twitter.common.python.package import (
    EggPackage,
    SourcePackage,
)
from twitter.common.python.testing import create_layout

from pkg_resources import Requirement, parse_version
import pytest


def test_source_packages():
  for ext in ('.tar.gz', '.tar', '.tgz', '.zip', '.tar.bz2'):
    sl = SourcePackage('a_p_r-3.1.3' + ext)
    assert sl._name == 'a_p_r'
    assert sl.name == 'a-p-r'
    assert sl.raw_version == '3.1.3'
    assert sl.version == parse_version(sl.raw_version)
    for req in ('a_p_r', 'a_p_r>2', 'a_p_r>3', 'a_p_r>=3.1.3', 'a_p_r==3.1.3', 'a_p_r>3,<3.5'):
      assert sl.satisfies(req)
      assert sl.satisfies(Requirement.parse(req))
    for req in ('foo', 'a_p_r==4.0.0', 'a_p_r>4.0.0', 'a_p_r>3.0.0,<3.0.3', 'a==3.1.3'):
      assert not sl.satisfies(req)
  sl = SourcePackage('python-dateutil-1.5.tar.gz')
  assert sl.name == 'python-dateutil'
  assert sl.raw_version == '1.5'

  with temporary_dir() as td:
    dateutil_base = 'python-dateutil-1.5'
    dateutil = '%s.zip' % dateutil_base
    with contextlib.closing(ZipFile(os.path.join(td, dateutil), 'w')) as zf:
      zf.writestr(os.path.join(dateutil_base, 'file1.txt'), 'junk1')
      zf.writestr(os.path.join(dateutil_base, 'file2.txt'), 'junk2')
    sl = SourcePackage('file://' + os.path.join(td, dateutil), opener=Web())
    with temporary_dir() as td2:
      sl.fetch(location=td2)
      print(os.listdir(td2))
      assert set(os.listdir(os.path.join(td2, dateutil_base))) == set(['file1.txt', 'file2.txt'])


def test_egg_packages():
  el = EggPackage('psutil-0.4.1-py2.6-macosx-10.7-intel.egg')
  assert el.name == 'psutil'
  assert el.raw_version == '0.4.1'
  assert el.py_version == '2.6'
  assert el.platform == 'macosx-10.7-intel'
  for req in ('psutil', 'psutil>0.4', 'psutil==0.4.1', 'psutil>0.4.0,<0.4.2'):
    assert el.satisfies(req)
  for req in ('foo', 'bar==0.4.1'):
    assert not el.satisfies(req)

  el = EggPackage('pytz-2012b-py2.6.egg')
  assert el.name == 'pytz'
  assert el.raw_version == '2012b'
  assert el.py_version == '2.6'
  assert el.platform is None

  # Eggs must have their own version and a python version.
  with pytest.raises(EggPackage.InvalidLink):
    EggPackage('bar.egg')

  with pytest.raises(EggPackage.InvalidLink):
    EggPackage('bar-1.egg')

  with pytest.raises(EggPackage.InvalidLink):
    EggPackage('bar-py2.6.egg')

  dateutil = 'python_dateutil-1.5-py2.6.egg'
  with create_layout([dateutil]) as td:
    el = EggPackage('file://' + os.path.join(td, dateutil), opener=Web())

    with temporary_dir() as td2:
      # local file fetch w/o location will always remain same
      loc1 = el.fetch()
      assert loc1 == os.path.join(td, dateutil)

      el.fetch(location=td2)
      assert os.listdir(td2) == [dateutil]

########NEW FILE########
__FILENAME__ = test_pep425
from twitter.common.python.pep425 import PEP425, PEP425Extras
from twitter.common.python.interpreter import PythonIdentity

import pytest


def test_platform_iterator():
  # non macosx
  assert list(PEP425Extras.platform_iterator('blah')) == ['blah']
  assert list(PEP425Extras.platform_iterator('linux_x86_64')) == ['linux_x86_64']

  # macosx
  assert set(PEP425Extras.platform_iterator('macosx_10_4_x86_64')) == set([
      'macosx_10_4_x86_64',
      'macosx_10_3_x86_64',
      'macosx_10_2_x86_64',
      'macosx_10_1_x86_64',
      'macosx_10_0_x86_64',
  ])
  assert set(PEP425Extras.platform_iterator('macosx_10_0_universal')) == set([
      'macosx_10_0_i386',
      'macosx_10_0_ppc',
      'macosx_10_0_ppc64',
      'macosx_10_0_x86_64',
      'macosx_10_0_universal',
  ])

  with pytest.raises(ValueError):
    list(PEP425Extras.platform_iterator('macosx_10'))

  with pytest.raises(ValueError):
    list(PEP425Extras.platform_iterator('macosx_10_0'))

  with pytest.raises(ValueError):
    list(PEP425Extras.platform_iterator('macosx_9_x86_64'))


def test_iter_supported_tags():
  identity = PythonIdentity('CPython', 2, 6, 5)
  platform = 'linux-x86_64'

  def iter_solutions():
    for interp in ('cp', 'py'):
      for interp_suffix in ('2', '20', '21', '22', '23', '24', '25', '26'):
        for platform in ('linux_x86_64', 'any'):
          yield (interp + interp_suffix, 'none', platform)

  assert set(PEP425.iter_supported_tags(identity, platform)) == set(iter_solutions())

########NEW FILE########
__FILENAME__ = test_pex
import os
import subprocess
import textwrap

from twitter.common.contextutil import temporary_dir
from twitter.common.python.compatibility import nested
from twitter.common.python.pex_builder import PEXBuilder


def write_pex(td, exe_contents):
  with open(os.path.join(td, 'exe.py'), 'w') as fp:
    fp.write(exe_contents)

  pb = PEXBuilder(path=td)
  pb.set_executable(os.path.join(td, 'exe.py'))
  pb.freeze()

  return pb


def run_pex(pex, env=None):
  po = subprocess.Popen(pex, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env)
  po.wait()
  return po.stdout.read(), po.returncode


def run_test(body, env=None):
  with nested(temporary_dir(), temporary_dir()) as (td1, td2):
    pb = write_pex(td1, body)
    pex = os.path.join(td2, 'app.pex')
    pb.build(pex)
    return run_pex(pex, env=env)


def test_pex_uncaught_exceptions():
  body = "raise Exception('This is an exception')"
  so, rc = run_test(body)
  assert b'This is an exception' in so
  assert rc == 1


def test_pex_sys_exit_does_not_raise():
  body = "import sys; sys.exit(2)"
  so, rc = run_test(body)
  assert so == b'', 'Should not print SystemExit exception.'
  assert rc == 2


def test_pex_atexit_swallowing():
  body = textwrap.dedent("""
  import atexit

  def raise_on_exit():
    raise Exception('This is an exception')

  atexit.register(raise_on_exit)
  """)

  so, rc = run_test(body)
  assert so == b''
  assert rc == 0

  env_copy = os.environ.copy()
  env_copy.update(PEX_TEARDOWN_VERBOSE='1')
  so, rc = run_test(body, env=env_copy)
  assert b'This is an exception' in so
  assert rc == 0

########NEW FILE########
__FILENAME__ = test_pex_builder
from contextlib import closing, contextmanager
import os
import zipfile

from twitter.common.contextutil import temporary_dir
from twitter.common.dirutil import safe_mkdir
from twitter.common.python.compatibility import nested
from twitter.common.python.pex import PEX
from twitter.common.python.pex_builder import PEXBuilder
from twitter.common.python.testing import make_distribution
from twitter.common.python.util import DistributionHelper


exe_main = """
import sys
from my_package.my_module import do_something
do_something()

with open(sys.argv[1], 'w') as fp:
  fp.write('success')
"""


def write_pex(td, exe_contents, dists=None):
  dists = dists or []

  with open(os.path.join(td, 'exe.py'), 'w') as fp:
    fp.write(exe_contents)

  pb = PEXBuilder(path=td)
  for dist in dists:
    pb.add_egg(dist.location)
  pb.set_executable(os.path.join(td, 'exe.py'))
  pb.freeze()

  return pb


def test_pex_builder():
  # test w/ and w/o zipfile dists
  with nested(temporary_dir(), make_distribution('p1', zipped=True)) as (td, p1):
    write_pex(td, exe_main, dists=[p1])

    success_txt = os.path.join(td, 'success.txt')
    PEX(td).run(args=[success_txt])
    assert os.path.exists(success_txt)
    with open(success_txt) as fp:
      assert fp.read() == 'success'

  # test w/ and w/o zipfile dists
  with nested(temporary_dir(), temporary_dir(), make_distribution('p1', zipped=True)) as (
      td1, td2, p1):
    target_egg_dir = os.path.join(td2, os.path.basename(p1.location))
    safe_mkdir(target_egg_dir)
    with closing(zipfile.ZipFile(p1.location, 'r')) as zf:
      zf.extractall(target_egg_dir)
    p1 = DistributionHelper.distribution_from_path(target_egg_dir)

    write_pex(td1, exe_main, dists=[p1])

    success_txt = os.path.join(td1, 'success.txt')
    PEX(td1).run(args=[success_txt])
    assert os.path.exists(success_txt)
    with open(success_txt) as fp:
      assert fp.read() == 'success'

########NEW FILE########
__FILENAME__ = test_platform
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
from twitter.common.python.platforms import Platform

class TestPlatform(object):
  def test_pure_python(self):
    assert Platform.compatible(None, None)
    assert Platform.compatible(None, 'i386')
    assert Platform.compatible(None, 'universal')

  def test_unknown(self):
    with pytest.raises(Platform.UnknownPlatformError):
      Platform.compatible('macosx-10.0-morfgorf', 'macosx-10.1-morfgorf')
    with pytest.raises(Platform.UnknownPlatformError):
      Platform.compatible('macosx-10.0-x86_64', 'macosx-10.1-morfgorf')
    with pytest.raises(Platform.UnknownPlatformError):
      Platform.compatible('macosx-10.0-morfgorf', 'macosx-10.1-x86_64')

  def test_versioning(self):
    # Major versions incompatible
    assert not Platform.compatible('macosx-9.1-x86_64', 'macosx-10.0-x86_64')
    assert not Platform.compatible('macosx-10.0-x86_64', 'macosx-9.1-x86_64')

    # Platforms equal
    assert Platform.compatible('macosx-10.0-x86_64', 'macosx-10.0-x86_64')

    # Minor versions less than
    assert Platform.compatible('macosx-10.0-x86_64', 'macosx-10.1-x86_64')
    assert not Platform.compatible('macosx-10.1-x86_64', 'macosx-10.0-x86_64')
    assert Platform.compatible('macosx-10.9-x86_64', 'macosx-10.10-x86_64')
    assert not Platform.compatible('macosx-10.10-x86_64', 'macosx-10.9-x86_64')

  def test_platform_subsets(self):
    # Pure platform subset
    assert Platform.compatible('macosx-10.0-i386', 'macosx-10.0-intel')

    # Version and platform subset
    assert Platform.compatible('macosx-10.0-i386', 'macosx-10.1-intel')
    assert Platform.compatible('macosx-10.0-x86_64', 'macosx-10.1-intel')

    # Intersecting sets of platform but not pure subset
    assert Platform.compatible('macosx-10.0-fat', 'macosx-10.1-intel')

    # Non-intersecting sets of platform
    assert not Platform.compatible('macosx-10.0-ppc', 'macosx-10.1-intel')

    # Test our common case
    assert Platform.compatible('macosx-10.4-x86_64', 'macosx-10.7-intel')

  def test_cross_platform(self):
    assert not Platform.compatible('linux-x86_64', 'macosx-10.0-x86_64')

    # TODO(wickman): Should we do extended platform support beyond OS X?
    assert not Platform.compatible('linux-i386', 'linux-x86_64')

########NEW FILE########
__FILENAME__ = test_util
import contextlib
import functools
from hashlib import sha1
import os
import random
from textwrap import dedent
import zipfile

from twitter.common.contextutil import temporary_file, temporary_dir
from twitter.common.dirutil import safe_mkdir, safe_mkdtemp
from twitter.common.python.installer import Installer, EggInstaller, WheelInstaller
from twitter.common.python.testing import (
    make_distribution,
    temporary_content,
    write_zipfile,
)
from twitter.common.python.util import CacheHelper, DistributionHelper


def test_hash():
  empty_hash = sha1().hexdigest()

  with temporary_file() as fp:
    fp.flush()
    assert empty_hash == CacheHelper.hash(fp.name)

  with temporary_file() as fp:
    string = b'asdf' * 1024 * sha1().block_size + b'extra padding'
    fp.write(string)
    fp.flush()
    assert sha1(string).hexdigest() == CacheHelper.hash(fp.name)

  with temporary_file() as fp:
    empty_hash = sha1()
    fp.write(b'asdf')
    fp.flush()
    hash_output = CacheHelper.hash(fp.name, digest=empty_hash)
    assert hash_output == empty_hash.hexdigest()


CONTENT = {
  '__main__.py': 200,
  '.deps/morfgorf': 10000,
  'twitter/__init__.py': 0,
  'twitter/common/python/foo.py': 4000,
  'twitter/common/python/bar.py': 8000,
  'twitter/common/python/bar.pyc': 6000,
}


def test_hash_consistency():
  for reverse in (False, True):
    with temporary_content(CONTENT) as td:
      dir_hash = CacheHelper.dir_hash(td)
      with temporary_file() as tf:
        zipped = write_zipfile(td, tf.name, reverse=reverse)
        with contextlib.closing(zipfile.ZipFile(tf.name, 'r')) as zf:
          zip_hash = CacheHelper.zip_hash(zf)
          assert zip_hash == dir_hash
          assert zip_hash != sha1().hexdigest()  # make sure it's not an empty hash


def test_zipsafe():
  make_egg = functools.partial(make_distribution, installer_impl=EggInstaller)
  make_whl = functools.partial(make_distribution, installer_impl=WheelInstaller)

  for zipped in (False, True):
    for zip_safe in (False, True):
      # Eggs can be zip safe
      with make_egg(zipped=zipped, zip_safe=zip_safe) as dist:
        assert DistributionHelper.zipsafe(dist) is zip_safe

      # Wheels cannot be zip safe
      with make_whl(zipped=zipped, zip_safe=zip_safe) as dist:
        assert not DistributionHelper.zipsafe(dist)

  for zipped in (False, True):
    for zip_safe in (False, True):
      with make_egg(zipped=zipped, zip_safe=zip_safe) as dist:
        assert DistributionHelper.zipsafe(dist) is zip_safe

########NEW FILE########
__FILENAME__ = test_parsing
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
from twitter.common.quantity import Time, Amount
from twitter.common.quantity.parse_simple import parse_time, InvalidTime

def test_basic():
  assert parse_time('') == Amount(0, Time.SECONDS)
  assert parse_time('1s') == Amount(1, Time.SECONDS)
  assert parse_time('2m60s') == Amount(3, Time.MINUTES)
  assert parse_time('1d') == Amount(1, Time.DAYS)
  assert parse_time('1d1H3600s') == Amount(26, Time.HOURS)
  assert parse_time('1d-1s') == Amount(86399, Time.SECONDS)

def test_bad():
  bad_strings = ['foo', 'dhms', '1s30d', 'a b c d', '  ', '1s2s3s']
  for bad_string in bad_strings:
    with pytest.raises(InvalidTime):
      parse_time(bad_string)

  bad_strings = [123, type]
  for bad_string in bad_strings:
    with pytest.raises(TypeError):
      parse_time(bad_string)

########NEW FILE########
__FILENAME__ = test_quantity
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
from twitter.common.quantity import Amount, Time, Data

def test_equals():
  assert Amount(1, Time.DAYS) == Amount(1, Time.DAYS), \
    "identical amounts should be equal"

  assert Amount(1, Time.DAYS) == Amount(24, Time.HOURS), \
    "expected equality to be calculated from amounts converted to a common unit"

  assert Amount(25, Time.HOURS) != Amount(1, Time.DAYS), \
    "expected unit conversions to not lose precision"

  assert Amount(1, Time.DAYS) != Amount(25, Time.HOURS), \
    "expected unit conversions to not lose precision"

  with pytest.raises(TypeError):
    Amount(1, Time.NANOSECONDS) == Amount(1, Data.BYTES)


def test_comparison_mixed_units():
  assert Amount(1, Time.MINUTES) > Amount(59, Time.SECONDS)
  assert Amount(1, Time.MINUTES) == Amount(60, Time.SECONDS)
  assert Amount(1, Time.MINUTES) < Amount(61, Time.SECONDS)

  assert Amount(59, Time.SECONDS) < Amount(1, Time.MINUTES)
  assert Amount(60, Time.SECONDS) == Amount(1, Time.MINUTES)
  assert Amount(61, Time.SECONDS) > Amount(1, Time.MINUTES)


def test_sorting():
  elements = [1, 2, 3, 4]
  elements_unsorted = [2, 4, 1, 3]

  def map_to_amount(amtlist):
    return [Amount(x, Time.MILLISECONDS) for x in amtlist]

  assert map_to_amount(elements) == sorted(map_to_amount(elements_unsorted))


def test_reduction():
  minute = Amount(60, Time.SECONDS)
  assert minute._amount == 1 and minute._unit == Time.MINUTES


def test_add():
  kb = Amount(512, Data.BYTES) + Amount(1536, Data.BYTES)
  assert kb._amount == 2 and kb._unit == Data.KB
  kb = kb + Amount(1, Data.BYTES)
  assert kb._amount == 2049 and kb._unit == Data.BYTES

  # disparate units
  value = Amount(1, Data.KB) + Amount(1, Data.MB)
  assert value._amount == 1025
  assert value._unit == Data.KB

def test_mul():
  assert 5 * Amount(12, Time.SECONDS) == Amount(12, Time.SECONDS) * 5
  amount = 5 * Amount(12, Time.SECONDS)
  assert amount._amount == 1 and amount._unit == Time.MINUTES

########NEW FILE########
__FILENAME__ = recordio_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

try:
  from cStringIO import StringIO
except ImportError:
  from StringIO import StringIO

from contextlib import contextmanager
import mox
import os
import struct
import tempfile

from twitter.common.recordio import RecordIO, RecordWriter, RecordReader, StringCodec
from twitter.common.recordio.filelike import FileLike, StringIOFileLike

import pytest

from recordio_test_harness import (
    DurableFile as DurableFileBase,
    EphemeralFile as EphemeralFileBase
)


class RecordioTestBase(mox.MoxTestBase):
  @classmethod
  @contextmanager
  def DurableFile(cls, mode):
    with DurableFileBase(mode) as fp:
      yield fp

  @classmethod
  @contextmanager
  def EphemeralFile(cls, mode):
    with EphemeralFileBase(mode) as fp:
      yield fp

  def test_append_fails_on_nonexistent_file(self):
    fn = tempfile.mktemp()
    assert RecordWriter.append(fn, 'hello world!') == False

  def test_append_fails_on_inaccessible_file(self):
    with RecordioTestBase.EphemeralFile('w') as fp:
      os.fchmod(fp.fileno(), 000)
      with pytest.raises(IOError):
        RecordWriter.append(fp.name, 'hello world!')

  def test_append_raises_on_bad_codec(self):
    fn = tempfile.mktemp()
    with pytest.raises(RecordIO.InvalidCodec):
      RecordIO.Writer.append(fn, 'hello world!', 'not a codec!')

  def test_append_fails_on_errors(self):
    record = 'hello'
    self.mox.StubOutWithMock(RecordIO.Writer, 'do_write')
    RecordIO.Writer.do_write(mox.IsA(file), record, mox.IsA(StringCodec)).AndRaise(IOError)
    RecordIO.Writer.do_write(mox.IsA(file), record, mox.IsA(StringCodec)).AndRaise(OSError)

    self.mox.ReplayAll()

    with RecordioTestBase.EphemeralFile('r+') as fp:
      assert RecordIO.Writer.append(fp.name, record, StringCodec()) == False
      assert RecordIO.Writer.append(fp.name, record, StringCodec()) == False

  def test_basic_recordwriter_write(self):
    test_string = "hello world"
    with self.EphemeralFile('r+') as fp:
      rw = RecordWriter(fp)
      rw.write(test_string)
      fp.seek(0)
      rr = RecordReader(fp)
      assert rr.read() == test_string

  def test_basic_recordwriter_write_synced(self):
    test_string = "hello world"
    with self.EphemeralFile('r+') as fp:
      RecordWriter.do_write(fp, test_string, StringCodec(), sync=True)
      fp.seek(0)
      rr = RecordReader(fp)
      assert rr.read() == test_string

  def test_basic_recordwriter_write_fail(self):
    test_string = "hello"
    header = struct.pack('>L', len(test_string))
    fp = self.mox.CreateMock(file)
    fp.write(header).AndRaise(IOError)
    fp.write(header).AndRaise(OSError)

    self.mox.ReplayAll()

    assert RecordWriter.do_write(fp, test_string, StringCodec()) == False
    assert RecordWriter.do_write(fp, test_string, StringCodec()) == False

  def test_basic_recordreader_iterator(self):
    test_strings = ["hello", "world", "etc"]
    with self.EphemeralFile('r+') as fp:
      for string in test_strings:
        RecordWriter.do_write(fp, string, StringCodec(), sync=True)
      fp.seek(0)
      rr = RecordReader(fp)
      assert list(rr) == test_strings

  def test_basic_recordreader_iter_failure(self):
    self.mox.StubOutWithMock(RecordIO.Reader, 'do_read')
    fp = self.mox.CreateMock(FileLike)
    fp.mode = 'r+'
    fp.dup().AndReturn(fp)
    RecordIO.Reader.do_read(fp, mox.IsA(StringCodec)).AndRaise(RecordIO.Error)
    fp.close()

    self.mox.ReplayAll()

    with pytest.raises(RecordIO.Error):
      list(RecordReader(fp))

  def test_basic_recordreader_dup_failure(self):
    fp = self.mox.CreateMock(FileLike)
    fp.mode = 'r+'
    fp.Error = FileLike.Error
    fp.dup().AndRaise(FileLike.Error)

    self.mox.ReplayAll()

    rr = RecordReader(fp)
    assert list(rr) == []
    self.mox.VerifyAll()

  def test_bad_header_size(self):
    with self.EphemeralFile('r+') as fp:
      fpw = FileLike.get(fp)
      fpw.write(struct.pack('>L', RecordIO.MAXIMUM_RECORD_SIZE))
      fpw._fp.truncate(RecordIO.RECORD_HEADER_SIZE - 1)
      fpw.flush()
      fpw.seek(0)

      rr = RecordReader(fp)
      with pytest.raises(RecordIO.PrematureEndOfStream):
        rr.read()
      assert fpw.tell() != 0
      fpw.seek(0)
      assert rr.try_read() is None
      assert fpw.tell() == 0

  def test_record_too_large(self):
    with self.EphemeralFile('r+') as fp:
      fpw = FileLike.get(fp)
      fpw.write(struct.pack('>L', RecordIO.MAXIMUM_RECORD_SIZE+1))
      fpw.write('a')
      fpw.flush()
      fpw.seek(0)

      rr = RecordReader(fp)
      with pytest.raises(RecordIO.RecordSizeExceeded):
        rr.read()

  def test_raises_if_initialized_with_nil_filehandle(self):
    with pytest.raises(RecordIO.InvalidFileHandle):
      RecordWriter(None)
    with pytest.raises(RecordIO.InvalidFileHandle):
      RecordReader(None)

  def test_raises_if_initialized_with_bad_codec(self):
    with self.EphemeralFile('r+') as fp:
      with pytest.raises(RecordIO.InvalidCodec):
        RecordIO.Writer(fp, "not_a_codec")

  def test_premature_end_of_stream(self):
    with self.EphemeralFile('r+') as fp:
      fpr = FileLike.get(fp)
      fpr = fp
      fpr.write(struct.pack('>L', 1))
      fpr.seek(0)
      rr = RecordReader(fpr)
      with pytest.raises(RecordIO.PrematureEndOfStream):
        rr.read()

  def test_premature_end_of_stream_mid_message(self):
    with self.EphemeralFile('r+') as fp:
      fpr = FileLike.get(fp)
      fpr = fp
      fpr.write(struct.pack('>L', 2))
      fpr.write('a')
      fpr.seek(0)
      rr = RecordReader(fpr)
      with pytest.raises(RecordIO.PrematureEndOfStream):
        rr.read()

  def test_filelike_dup_raises(self):
    self.mox.StubOutWithMock(os, 'fdopen')
    self.mox.StubOutWithMock(os, 'close')
    os.fdopen(mox.IsA(int), mox.IsA(str)).AndRaise(OSError)
    os.close(mox.IsA(int)).AndRaise(OSError)

    self.mox.ReplayAll()

    with RecordioTestBase.EphemeralFile('r+') as fp:
      fl = FileLike(fp)
      with pytest.raises(FileLike.Error):
        fl.dup()

  def test_basic_recordwriter_write_synced_raises(self):
    test_string = "hello world"
    self.mox.StubOutWithMock(os, 'fsync')
    with RecordioTestBase.EphemeralFile('r+') as fp:
      os.fsync(fp.fileno()).AndRaise(OSError)

      self.mox.ReplayAll()

      rw = RecordWriter(FileLike(fp))
      rw.set_sync(True)
      rw.write(test_string)
      fp.seek(0)
      rr = RecordReader(fp)
      assert rr.read() == test_string


class TestRecordioBuiltin(RecordioTestBase):
  def test_recordwriter_framing(self):
    test_string_1 = "hello world"
    test_string_2 = "ahoy ahoy, bonjour"

    with self.EphemeralFile('w') as fp:
      fn = fp.name
      rw = RecordWriter(fp)
      rw.write(test_string_1)
      rw.close()

      with open(fn, 'a') as fpa:
        rw = RecordWriter(fpa)
        rw.write(test_string_2)

      with open(fn) as fpr:
        rr = RecordReader(fpr)
        assert rr.read() == test_string_1
        assert rr.read() == test_string_2

  def test_paranoid_append_framing(self):
    with self.DurableFile('w') as fp:
      fn = fp.name

    test_string_1 = "hello world"
    test_string_2 = "ahoy ahoy, bonjour"

    RecordWriter.append(fn, test_string_1)
    RecordWriter.append(fn, test_string_2)

    with open(fn) as fpr:
      rr = RecordReader(fpr)
      assert rr.read() == test_string_1
      assert rr.read() == test_string_2

    os.remove(fn)

  def test_recordwriter_raises_on_readonly_file(self):
    with self.EphemeralFile('r') as fp:
      with pytest.raises(RecordIO.InvalidFileHandle):
        RecordWriter(fp)

  def test_recordwriter_initializing(self):
    for mode in ('a', 'r+', 'w'):
      with self.EphemeralFile(mode) as fp:
        try:
          RecordWriter(fp)
        except Exception as e:
          assert False, (
              "Failed to initialize RecordWriter in '%s' mode (exception: %s)" % (mode, e))

  def test_recordreader_works_with_plus(self):
    for mode in ('a+', 'w+'):
      with self.EphemeralFile(mode) as fp:
        try:
          RecordReader(fp)
        except Exception as e:
          assert False, (
              "Failed to initialize RecordReader in '%s' mode (exception: %s)" % (mode, e))

  def test_recordreader_fails_with_writeonly(self):
    for mode in ('a', 'w'):
      with self.EphemeralFile(mode) as fp:
        with pytest.raises(RecordIO.InvalidFileHandle):
          RecordReader(fp)

  def test_basic_recordreader_try_read(self):
    test_string = "hello world"
    with self.EphemeralFile('r') as fp:
      fn = fp.name

      rr = RecordReader(fp)
      assert rr.try_read() is None
      rr.close()

      with open(fn, 'w') as fpw:
        rw = RecordWriter(fpw)
        rw.write(test_string)

      with open(fn) as fpr:
        rr = RecordReader(fpr)
        assert rr.try_read() == test_string

  def test_basic_recordreader_read(self):
    test_string = "hello world"
    with self.EphemeralFile('r') as fp:
      fn = fp.name

      rr = RecordReader(fp)
      assert rr.read() is None
      rr.close()

      with open(fn, 'w') as fpw:
        rw = RecordWriter(fpw)
        rw.write(test_string)

      with open(fn) as fpr:
        rr = RecordReader(fpr)
        assert rr.read() == test_string


class TestRecordioStringIO(RecordioTestBase):
  @classmethod
  @contextmanager
  def EphemeralFile(cls, mode):
    yield StringIO()

  def test_string_codec(self):
    for bad_value in (None, 1234, object):
      with pytest.raises(RecordIO.InvalidTypeException):
        assert StringCodec().encode(bad_value)
      with pytest.raises(RecordIO.InvalidTypeException):
        assert StringCodec().decode(bad_value)

########NEW FILE########
__FILENAME__ = recordio_test_harness
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import tempfile
from contextlib import contextmanager

try:
  from cStringIO import StringIO
except ImportError:
  from StringIO import StringIO


@contextmanager
def DurableFile(mode):
  fn = tempfile.mktemp()
  with open(fn, 'w'):
    pass
  with open(fn, mode) as fp:
    yield fp


@contextmanager
def EphemeralFile(mode):
  with DurableFile(mode) as fp:
    fn = fp.name
    yield fp
  os.remove(fn)

########NEW FILE########
__FILENAME__ = thrift_recordio_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import struct

import pytest

from twitter.common.recordio import RecordIO
from twitter.common.recordio import ThriftRecordWriter, ThriftRecordReader
from twitter.common.recordio.thrift_recordio import ThriftRecordIO
from twitter_test.thrift.ttypes import IntType, StringType, BinaryType

from recordio_test_harness import EphemeralFile


def test_basic_thriftrecordwriter_write():
  test_string = StringType("hello world")

  with EphemeralFile('w') as fp:
    fn = fp.name

    rw = ThriftRecordWriter(fp)
    rw.write(test_string)
    rw.close()

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, StringType)
      assert rr.read() == test_string


def test_thriftrecordwriter_framing():
  test_string_1 = StringType("hello world")
  test_string_2 = StringType("ahoy ahoy, bonjour")

  with EphemeralFile('w') as fp:
    fn = fp.name

    rw = ThriftRecordWriter(fp)
    rw.write(test_string_1)
    rw.close()

    with open(fn, 'a') as fpa:
      rw = ThriftRecordWriter(fpa)
      rw.write(test_string_2)

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, StringType)
      assert rr.read() == test_string_1
      assert rr.read() == test_string_2


def test_thriftrecordreader_iteration():
  test_string_1 = StringType("hello world")
  test_string_2 = StringType("ahoy ahoy, bonjour")

  with EphemeralFile('w') as fp:
    fn = fp.name

    rw = ThriftRecordWriter(fp)
    rw.write(test_string_1)
    rw.write(test_string_2)
    rw.close()

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, StringType)
      records = []
      for record in rr:
        records.append(record)
      assert records == [test_string_1, test_string_2]


def test_thriftrecordreader_nested_iteration():
  test_string_1 = StringType("hello world")
  test_string_2 = StringType("ahoy ahoy, bonjour")

  with EphemeralFile('w') as fp:
    fn = fp.name

    rw = ThriftRecordWriter(fp)
    rw.write(test_string_1)
    rw.write(test_string_2)
    rw.close()

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, StringType)
      records = []
      for record in rr:
        records.append(record)
        for record2 in rr:
          records.append(record2)
      assert records == [
        test_string_1,
        test_string_1, test_string_2,
        test_string_2,
        test_string_1, test_string_2]


def test_paranoid_thrift_append_framing():
  test_string_1 = StringType("hello world")
  test_string_2 = StringType("ahoy ahoy, bonjour")

  with EphemeralFile('w') as fp:
    fn = fp.name

    ThriftRecordWriter.append(fn, test_string_1)
    ThriftRecordWriter.append(fn, test_string_2)

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, StringType)
      assert rr.read() == test_string_1
      assert rr.read() == test_string_2


def test_thrift_recordwriter_type_mismatch():
  test_string = StringType("hello world")
  with EphemeralFile('w') as fp:
    fn = fp.name

    rw = ThriftRecordWriter(fp)
    rw.write(test_string)
    rw.close()

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, IntType)
      # This is a peculiar behavior of Thrift in that it just returns
      # ThriftType() with no serialization applied
      assert rr.read() == IntType()


def test_premature_end_of_stream_mid_message_thrift():
  with EphemeralFile('w') as fp:
    fn = fp.name

    fp.write(struct.pack('>L', 2))
    fp.write('a')
    fp.close()

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, StringType)
      with pytest.raises(RecordIO.PrematureEndOfStream):
        rr.read()


def test_thrift_garbage():
  with EphemeralFile('w') as fp:
    fn = fp.name

    fp.write(struct.pack('>L', 2))
    fp.write('ab')
    fp.close()

    with open(fn) as fpr:
      rr = ThriftRecordReader(fpr, StringType)
      with pytest.raises(RecordIO.PrematureEndOfStream):
        rr.read()


def test_thrift_invalid_codec_with_nonclass():
  with EphemeralFile('w') as fp:
    with pytest.raises(ThriftRecordIO.InvalidThriftException):
      ThriftRecordReader(fp, 5)


def test_thrift_invalid_codec_with_object_instead_of_class():
  with EphemeralFile('w') as fp:
    with pytest.raises(ThriftRecordIO.InvalidThriftException):
      ThriftRecordReader(fp, StringType())

########NEW FILE########
__FILENAME__ = resourcepool_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import gc
import pytest
import time
from collections import namedtuple
try:
  from Queue import Empty
except ImportError:
  from queue import Empty
from twitter.common.resourcepool import ResourcePool
from twitter.common.quantity import Amount, Time


MyResource = namedtuple('MyResource', 'id')


class TestResourcePool(object):
  def setup_method(self, method):
    self.pool = ResourcePool([MyResource(i) for i in range(10)])

  def test_consume_resources(self):
    consumed = [self.pool.acquire() for _ in range(5)]
    assert self.pool._resources.qsize() == 5
    consumed.extend(self.pool.acquire() for _ in range(5))
    assert self.pool.empty()

  def test_consume_too_many_resources(self):
    _ = [self.pool.acquire() for _ in range(10)]
    with pytest.raises(Empty):
      self.pool.acquire(0.1)

  def test_context_manager(self):
    with self.pool.acquire() as resource:
      assert self.pool._resources.qsize() == 9
      assert resource.id == 0
      with self.pool.acquire() as r2:
        assert self.pool._resources.qsize() == 8
        assert r2.id == 1
    assert self.pool._resources.qsize() == 10

  def test_cleanup(self):
    def acquire():
      resource = self.pool.acquire()
      assert self.pool._resources.qsize() == 9

    acquire()
    # Make extra sure that resource has been freed
    gc.collect()
    assert self.pool._resources.qsize() == 10

  def test_wait_with_amount(self):
    pool = ResourcePool([])
    now = time.time()
    with pytest.raises(Empty):
      # TODO(wickman) We should also be able to round-down for non-integral Amount types.
      with pool.acquire(timeout=Amount(1, Time.SECONDS) +
                                Amount(10, Time.MILLISECONDS)) as resource:
        pass
    elapsed = time.time() - now
    assert elapsed >= 1.0


########NEW FILE########
__FILENAME__ = test_span
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest

from twitter.common.rpc.finagle.trace import SpanId


def test_span_from_value():
  # hex regex works
  with pytest.raises(SpanId.InvalidSpanId):
    SpanId.from_value('1234')
  assert SpanId.from_value('0000000000001234').value == int('1234', 16)
  assert SpanId.from_value(1234).value == 1234
  assert SpanId.from_value(SpanId(1234)).value == 1234
  assert SpanId.from_value(None).value is None

########NEW FILE########
__FILENAME__ = test_address
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
from twitter.common.rpc import Address

def test_from_string():
  addr = Address.from_string('localhost:1234')
  assert addr.host == 'localhost'
  assert addr.port == 1234
  addr = Address.from_string('localhost: 1234')
  assert addr.host == 'localhost'
  assert addr.port == 1234

  with pytest.raises(Address.InvalidFormat):
    Address.from_string('localhost:')

  with pytest.raises(Address.InvalidFormat):
    Address.from_string('localhost:-1')

  with pytest.raises(Address.InvalidFormat):
    Address.from_string('localhost:beeblebrox')

  with pytest.raises(Address.InvalidFormat):
    Address.from_string('localhost:beeblebrox', host='localhost')


def test_from_pair():
  addr = Address.from_pair('localhost', 1234)
  assert addr.host == 'localhost'
  assert addr.port == 1234

  with pytest.raises(Address.InvalidFormat):
    addr = Address.from_pair(('localhost', 1234))

  with pytest.raises(Address.InvalidFormat):
    addr = Address.from_pair('localhost')

  with pytest.raises(Address.InvalidFormat):
    addr = Address.from_pair('localhost', 1, 2, 3, 4)

def test_from_tuple():
  addr = Address.from_tuple(('localhost', 1234))
  assert addr.host == 'localhost'
  assert addr.port == 1234

  with pytest.raises(Address.InvalidFormat):
    addr = Address.from_tuple('localhost', 1234)

  with pytest.raises(Address.InvalidFormat):
    addr = Address.from_tuple('localhost')

  with pytest.raises(Address.InvalidFormat):
    addr = Address.from_tuple(('localhost', 1), 2, 3, 4)


def test_from_address():
  addr = Address.from_tuple(('localhost', 1234))
  addr = Address.from_address(addr)
  assert addr.host == 'localhost'
  assert addr.port == 1234


def test_parse():
  def assert_kosher(addr):
    assert addr.host == 'localhost'
    assert addr.port == 1234

  addr = Address.parse('localhost:1234')
  assert_kosher(addr)
  addr = Address.parse('localhost', 1234)
  assert_kosher(addr)
  addr = Address.parse(('localhost', 1234))
  assert_kosher(addr)
  addr = Address.from_address(addr)
  assert_kosher(addr)

########NEW FILE########
__FILENAME__ = test_basic_scanf
import pytest
import unittest
from twitter.common.string.scanf import ScanfParser

def almost_equal(a, b, digits=7):
  return abs(a-b) < 0.1**digits

def basic_scanf(fmt, string, extra=False):
  formatter = ScanfParser(fmt)
  result = formatter.parse(string, extra)
  assert len(result.ungrouped()) == 1
  return result.ungrouped()[0]

def test_bad_input():
  conversions = ScanfParser.CONVERSIONS.keys()
  bad_stuff = [
    " a", " 1", " +",
    "a ", "1 ", "+ ",
  ]
  garbage_stuff = [
    0, 1, None, dir, [], {}, (), type
  ]

  for c in conversions:
    for b in bad_stuff:
      with pytest.raises(ScanfParser.ParseError):
        basic_scanf(c, b)
    for b in garbage_stuff:
      with pytest.raises(TypeError):
        basic_scanf(c, b)

def test_no_matches():
  match = ScanfParser("%%").parse("%")
  assert len(match.groups()) == 0
  assert len(match.ungrouped()) == 0

  test_strings = ["a", " ", "hello hello", "1.0 hello nothing to see here move along", ""]
  for t_s in test_strings:
    match = ScanfParser(t_s).parse(t_s)
    assert len(match.groups()) == 0
    assert len(match.ungrouped()) == 0

def test_garbage_formats():
  garbage_input = [0, 1, None, dir, [], {}, (), type]
  for garbage in garbage_input:
    with pytest.raises(TypeError):
      ScanfParser(garbage)

def test_special_characters():
  special_stuffs = [
    (')', '('),
    ('(', ')'),  ('[', ']'),     ('{', '}'),
    ('(', ')+'),
    ('(|', ')'),
    ('{,', '}'),
    ('$', '^'), ('^', '$'),
    (' ', '+'), (' ', '*'), (' ', '?')
  ]
  for before, after in special_stuffs:
    assert basic_scanf(before+'%c'+after, before+'a'+after) == 'a'
    assert basic_scanf(before+'%c'+after, before+u'a'+after) == 'a'
    assert basic_scanf(before+'%c'+after, before+' '+after) == ' '

def test_character_conversion():
  assert basic_scanf('%c', 'a') == 'a'
  assert basic_scanf('%c', u'a') == 'a'
  assert basic_scanf('%c', ' ') == ' '

def test_integer_conversion():
  for conversion in ('%d', '%ld', '%lld'):
    assert basic_scanf(conversion, '1') == 1
    assert basic_scanf(conversion, '01') == 1
    assert basic_scanf(conversion, '+01') == 1
    assert basic_scanf(conversion, '-01') == -1

def test_failing_integer_conversion():
  with pytest.raises(ScanfParser.ParseError):
    basic_scanf('%d', "\x90")
  with pytest.raises(ScanfParser.ParseError):
    basic_scanf('%d', "x")
  with pytest.raises(ScanfParser.ParseError):
    basic_scanf('%d', "hello")

def test_long_conversion():
  for conversion in ('%u', '%lu', '%llu'):
    assert basic_scanf(conversion, '1') == 1
    assert basic_scanf(conversion, '01') == 1

def test_float_conversion():
  factor_tests = {
    '': 1.0,
    'e-0': 1.0,
    'e-1': 0.1,
    'e+1': 10.0,
    'e1': 10.0,
    'e0': 1.0,
    'e5': 1.e5,
  }
  for exponent, xfactor in factor_tests.items():
    assert almost_equal(basic_scanf('%f', '0' + exponent), 0 * xfactor)
    assert almost_equal(basic_scanf('%f', '.1' + exponent), .1 * xfactor)
    assert almost_equal(basic_scanf('%f', '2.' + exponent), 2 * xfactor)
    assert almost_equal(basic_scanf('%f', '3.4' + exponent), 3.4 * xfactor)
    assert almost_equal(basic_scanf('%f', '-.5' + exponent), -0.5 * xfactor)

def test_string_conversion():
  for st in ('a', u'a', '123', u'123', 'a\x12\x23'):
    assert basic_scanf('%s', st) == st
  assert basic_scanf('%s', '\x00') == ''

def test_extra_stuff():
  extra_stuff = [ ' ', ' a', ' a b', ' $']
  for extra in extra_stuff:
    for st in ('a', u'a', '123', u'123', 'a\x12\x23'):
      assert basic_scanf('%s', st+extra, extra=True) == st

########NEW FILE########
__FILENAME__ = test_structured_scanf
import pytest
import unittest
from twitter.common.string.scanf import ScanfParser

def almost_equal(a, b, digits=7):
  return abs(a-b) < 0.1**digits

def scanf(fmt, string):
  formatter = ScanfParser(fmt)
  result = formatter.parse(string)
  return result.groups(), result.ungrouped()

def test_parsing_names():
  with pytest.raises(ScanfParser.ParseError):
    ScanfParser("%()s")
  with pytest.raises(ScanfParser.ParseError):
    ScanfParser("%(s")
  with pytest.raises(ScanfParser.ParseError):
    ScanfParser("%)s")
  ScanfParser("%")  # This is valid but could cause string overflow if parser not careful
  with pytest.raises(ScanfParser.ParseError):
    ScanfParser("% c")

def test_multi():
  # Regexes are powerful beasts.
  pairs = {
    "%f %c": "1.0 h",
    " %f %c": " 1.0 h",
    "%f %c ": "1.0 h ",
    " %f %c ": " 1.0 h ",
    "1%f %c": "11.0 h",
    " %f3%c": " 1.03h",
    "%f %c5": "1.0 h5",
    " %f %c '": " 1.0 h '",
  }

  for pattern, value in pairs.items():
    _, vals = scanf(pattern, value)
    assert len(vals) == 2, "%s has two values" % pattern
    assert almost_equal(vals[0], 1.0)
    assert vals[1] == 'h'

def test_mixed_multi_named_then_unnamed():
  pairs = {
    " %(val1)f %c": " 1.0 h",
    "1%(val1)f %c": "11.0 h",
    " %(val1)f3%c": " 1.03h",
  }

  for pattern, value in pairs.items():
    d, l = scanf(pattern, value)
    assert len(d) == 1
    assert len(l) == 1
    assert almost_equal(d['val1'], 1.0)
    assert l[0] == 'h'

def test_mixed_multi_unnamed_then_named():
  pairs = {
    "%f %(val1)c": "1.0 h",
    " %f3%(val1)c": " 1.03h",
    "%f %(val1)c5": "1.0 h5",
  }

  for pattern, value in pairs.items():
    d, l = scanf(pattern, value)
    assert len(d) == 1
    assert len(l) == 1
    assert almost_equal(l[0], 1.0)
    assert d['val1'] == 'h'

def test_mixed_ignored():
  pairs = {
    "%*f %(val1)c": "1.0 h",
    " %*f %(val1)c": " 1.0 h",
    "1%*f %(val1)c": "11.0 h",
    " %*f3%(val1)c": " 1.03h",
  }

  for pattern, value in pairs.items():
    d, l = scanf(pattern, value)
    assert len(d) == 1
    assert len(l) == 0
    assert d['val1'] == 'h'

def test_many():
  # named
  d, l = scanf("%(a)c %(b)c %(c)c %(d)c", "a b c d")
  assert len(l) == 0
  assert len(d) == 4
  for ch in 'abcd':
    assert d[ch] == ch

  # unnamed
  d, l = scanf("%c%c%c %c", "abc d")
  assert len(l) == 4
  assert len(d) == 0
  for val, idx in zip('abcd', range(4)):
    assert l[idx] == val


def test_weird_names():
  weird_names = ['{', '}', '{}', '(', '*', '%c', '%%', ' ', 'w u hhhht', '123', '^', ',']
  # named
  for weird_name in weird_names:
    d, l = scanf("%(" + weird_name + ")s", "whee!")
  assert len(l) == 0
  assert len(d) == 1
  assert weird_name in d
  assert d[weird_name] == 'whee!'

########NEW FILE########
__FILENAME__ = command_util_test
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Tejal Desai'

import logging
import os
import re
import subprocess
import tempfile
import unittest

from twitter.common.util.command_util import CommandUtil


class CommandUtilTest(unittest.TestCase):
  def test_execute_internal(self):
    temp_filename = tempfile.mktemp()
    handler = logging.FileHandler(temp_filename)
    logging.getLogger().addHandler(handler)
    logging.getLogger().setLevel(logging.INFO)
    ret = CommandUtil._execute_internal(['ls' , '-z' ], True, True, True)
    self.assertNotEqual(ret, 0)
    logging.getLogger().removeHandler(handler)
    with open(temp_filename, "r") as file1:
      str1 = file1.read()
    self.assertTrue(bool(re.search(".*Executing: ls -z.*", str1))) #command logged
    self.assertTrue(bool(re.search(".*illegal option.*", str1)) or bool(re.search(".*invalid option.*", str1))) #Error logged

  def test_execute(self):
    temp_filename = tempfile.mktemp()
    ret = CommandUtil.execute(['echo' , 'test'], True, temp_filename)
    self.assertEqual(ret, 0)
    with open(temp_filename, "r") as file1:
      str1 = file1.read()
    self.assertEqual("test\n", str1) #output stored in the input file
    os.remove(temp_filename)

  def test_execute_suppress_stdout(self):
    temp_filename = tempfile.mktemp()
    handler = logging.FileHandler(temp_filename)
    logging.getLogger().addHandler(handler)
    logging.getLogger().setLevel(logging.INFO)
    ret = CommandUtil.execute_suppress_stdout(['echo' , 'test'], True)
    self.assertEqual(ret, 0)
    with open(temp_filename, "r") as file1:
      str1 = file1.read()
    self.assertTrue(bool(re.search("Executing: echo", str1))) #command Logged
    self.assertFalse(bool(re.search("\ntest", str1)))   #Output not logged

  def test_execute_suppress_out_err(self):
    temp_filename = tempfile.mktemp()
    handler = logging.FileHandler(temp_filename)
    logging.getLogger().addHandler(handler)
    logging.getLogger().setLevel(logging.INFO)
    ret = CommandUtil.execute_suppress_stdout_stderr(['echo' , 'test'], True)
    self.assertEqual(ret, 0)
    with open(temp_filename, "r") as file1:
      str1 = file1.read()
    self.assertTrue(bool(re.search("Executing: echo test", str1)))  #command logged
    self.assertFalse(bool(re.search("\ntest", str1)))   #Output not logged

    #Test case 2
    ret = CommandUtil.execute_suppress_stdout_stderr(['ls' , '-z'], True)
    self.assertNotEqual(ret, 0)
    with open(temp_filename, "r") as file1:
      str1 = file1.read()

    self.assertTrue(bool(re.search("Executing: ls -z", str1)))  #command logged
    self.assertFalse(bool(re.search("illegal option", str1)))  #error not logged
    logging.getLogger().removeHandler(handler)

  def test_check_call(self):
    self.assertRaises(subprocess.CalledProcessError, CommandUtil.check_call, ['ls' , '-z'])
    ret = CommandUtil.check_call(['ls' , '-v'])
    self.assertEqual(ret, 0)

  def test_execute_and_get_output(self):
    (ret, output) = CommandUtil.execute_and_get_output(['echo', 'test'])
    self.assertEqual(ret, 0)
    self.assertEqual(output, 'test\n')

    (ret, output) = CommandUtil.execute_and_get_output(['echo1', 'test'])
    self.assertEqual(ret, 1)
    self.assertEqual(output, None)

########NEW FILE########
__FILENAME__ = topological_sort_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest
import pytest
from twitter.common.util import (
  DependencyCycle,
  UnderspecifiedDependencies,
  topological_sort
)

class TopologicalSortTest(unittest.TestCase):
  def test_empty(self):
    assert list(topological_sort([])) == []
    assert list(topological_sort({})) == []

  def test_types(self):
    with pytest.raises(TypeError):
      list(topological_sort([1,2,3]))
    with pytest.raises(TypeError):
      list(topological_sort(None))
    with pytest.raises(TypeError):
      list(topological_sort((1,2), (2,3)))

  def test_basic_ordering(self):
    def run_asserts(output):
      assert output.pop(0) == set([1])
      assert output.pop(0) == set([2])
      assert output.pop(0) == set([3])

    output = list(topological_sort([(1,2), (2,3)]))
    run_asserts(output)
    output = list(topological_sort({2:1, 3:2}))
    run_asserts(output)

  def test_mixed_dict_sets(self):
    def run_asserts(output):
      assert output.pop(0) == set([1])
      assert output.pop(0) == set([2])
      assert output.pop(0) == set([3])
      assert output.pop(0) == set([4])
    output = list(topological_sort([(1,2), (2,3), (2,4), (3,4)]))
    run_asserts(output)
    output = list(topological_sort({2: 1, 3: 2, 4: set([2, 3])}))
    run_asserts(output)

  def test_mixed_types(self):
    deps = {
      1: "bob",
      2: "frank",
      "frank": "esther",
      "esther": set(["brian", 3]),
    }
    iter = topological_sort(deps)
    assert next(iter) == set(["bob", "brian", 3])
    assert next(iter) == set([1, "esther"])
    assert next(iter) == set(["frank"])
    assert next(iter) == set([2])

  def test_filtering(self):
    output = list(topological_sort([(1,1), (1,2)]))
    assert output.pop(0) == set([1])
    assert output.pop(0) == set([2])
    output = list(topological_sort({1: 1, 2: set([2,1])}))
    assert output.pop(0) == set([1])
    assert output.pop(0) == set([2])

  def test_cycles(self):
    with pytest.raises(DependencyCycle):
      list(topological_sort([(1,2), (2,1)]))
    with pytest.raises(DependencyCycle):
      list(topological_sort([(1,2), (2,3), (3,1)]))
    with pytest.raises(DependencyCycle):
      list(topological_sort({1: 2, 2: 3, 3: 1}))

  def test_unspecified_dependencies(self):
    with pytest.raises(UnderspecifiedDependencies):
      list(topological_sort([(1,2)], require_fully_specified=True))
    with pytest.raises(UnderspecifiedDependencies):
      list(topological_sort({2:1}, require_fully_specified=True))
    assert list(topological_sort({1:None, 2:1}, require_fully_specified=True)) == [
      set([1]), set([2])]

########NEW FILE########
__FILENAME__ = client_test
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pytest
import socket
import threading
import time
import zookeeper

from twitter.common import log
from twitter.common.log.options import LogOptions

from twitter.common.zookeeper.client import ZooKeeper, ZooDefs
from twitter.common.zookeeper.test_server import ZookeeperServer

import mox

MAX_EVENT_WAIT_SECS = 30.0
MAX_EXPIRE_WAIT_SECS = 60.0
CONNECT_TIMEOUT_SECS = 10.0
CONNECT_RETRIES = 6


if os.getenv('ZOOKEEPER_TEST_DEBUG'):
  LogOptions.set_stderr_log_level('NONE')
  LogOptions.set_disk_log_level('DEBUG')
  LogOptions.set_log_dir('/tmp')
  log.init('client_test')


def make_zk(server, **kw):
  return ZooKeeper('localhost:%d' % server.zookeeper_port,
                   timeout_secs=CONNECT_TIMEOUT_SECS,
                   max_reconnects=CONNECT_RETRIES,
                   **kw)


def test_client_connect():
  with ZookeeperServer() as server:
    zk = make_zk(server)
    assert zk.get_children('/') == ['zookeeper']


def sha_password_digest(username, password):
  import base64, hashlib
  return base64.b64encode(hashlib.sha1(username + ':' + password).digest())


def test_client_connect_with_auth():
  with ZookeeperServer() as server:
    zk = make_zk(server, authentication=('digest', 'username:password'))
    finish_event = threading.Event()

    def run_create_tests():
      zk.create('/unprotected_znode', 'unprotected content', ZooDefs.Acls.OPEN_ACL_UNSAFE)
      _, unprotected_acl = zk.get_acl('/unprotected_znode')
      zk.create('/protected_znode', 'protected content', ZooDefs.Acls.CREATOR_ALL_ACL)
      _, protected_acl = zk.get_acl('/protected_znode')
      assert unprotected_acl == ZooDefs.Acls.OPEN_ACL_UNSAFE
      assert len(protected_acl) == 1
      assert protected_acl[0]['perms'] == ZooDefs.Acls.CREATOR_ALL_ACL[0]['perms']
      assert protected_acl[0]['scheme'] == 'digest'
      assert protected_acl[0]['id'] == 'username:%s' % sha_password_digest('username', 'password')
      content, _ = zk.get('/unprotected_znode')
      assert content == 'unprotected content'
      content, _ = zk.get('/protected_znode')
      assert content == 'protected content'
      zk.delete('/unprotected_znode')
      zk.delete('/protected_znode')
      finish_event.set()

    # run normally
    run_create_tests()
    finish_event.wait()

    # run after connection loss
    assert server.shutdown()
    finish_event.clear()
    class BackgroundTester(threading.Thread):
      def run(self):
        run_create_tests()
    BackgroundTester().start()
    server.start()
    finish_event.wait()

    # run after session loss
    session_id = zk.session_id
    assert server.shutdown()
    finish_event.clear()
    BackgroundTester().start()
    server.expire(session_id)
    server.start()
    finish_event.wait()


def test_client_connect_times_out():
  sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
  sock.bind(('localhost', 0))
  _, port = sock.getsockname()
  sock.close()
  with pytest.raises(ZooKeeper.ConnectionTimeout):
    ZooKeeper('localhost:%d' % port, timeout_secs=1.0)


def test_client_reconnect():
  with ZookeeperServer() as server:
    zk = make_zk(server)
    zk.get_children('/')
    assert server.shutdown()

    # zk.get_children should block until reconnected
    children = []
    class GetThread(threading.Thread):
      def run(self):
        children.extend(zk.get_children('/'))
    gt = GetThread()
    gt.start()

    assert server.start()
    gt.join()

    assert children == ['zookeeper']


def test_expand_ensemble():
  m = mox.Mox()
  m.StubOutWithMock(socket, 'gethostbyname_ex')
  socket.gethostbyname_ex('localhost').AndReturn(('localhost', [], ['foo']))
  socket.gethostbyname_ex('localhost').AndReturn(('localhost', [], ['bar']))
  socket.gethostbyname_ex('localhost').AndReturn(('localhost', [], ['baz', 'bak']))
  m.ReplayAll()

  assert ZooKeeper.expand_ensemble('localhost:1234') == 'foo:1234'
  assert ZooKeeper.expand_ensemble('localhost:1234,localhost') == 'bar:1234,baz:2181,bak:2181'

  m.UnsetStubs()
  m.VerifyAll()


def test_bad_ensemble():
  with pytest.raises(ZooKeeper.InvalidEnsemble):
     ZooKeeper.expand_ensemble('localhost:')

  with pytest.raises(ZooKeeper.InvalidEnsemble):
     ZooKeeper.expand_ensemble('localhost:sheeps')

  m = mox.Mox()
  m.StubOutWithMock(socket, 'gethostbyname_ex')
  socket.gethostbyname_ex('zookeeper.twitter.com').AndRaise(
      socket.gaierror(8, 'nodename nor servname provided, or not known'))
  m.ReplayAll()

  with pytest.raises(ZooKeeper.InvalidEnsemble):
    ZooKeeper.expand_ensemble('zookeeper.twitter.com:2181')

  m.UnsetStubs()
  m.VerifyAll()


def test_async_while_headless():
  server = ZookeeperServer()

  disconnected = threading.Event()
  def on_event(zk, event, state, _):
    if zk._live.is_set() and state != zookeeper.CONNECTED_STATE:
      disconnected.set()

  zk = make_zk(server, watch=on_event)

  children = []
  completion_event = threading.Event()
  def children_completion(_, rc, results):
    children.extend(results)
    completion_event.set()

  assert server.shutdown()
  disconnected.wait(timeout=MAX_EVENT_WAIT_SECS)
  assert disconnected.is_set()

  zk.aget_children('/', None, children_completion)

  assert server.start()
  completion_event.wait(timeout=MAX_EVENT_WAIT_SECS)
  assert completion_event.is_set()

  assert children == ['zookeeper']

  server.stop()


def test_stopped():
  with ZookeeperServer() as server:
    zk = ZooKeeper('localhost:%d' % server.zookeeper_port)
    assert zk.get_children('/') == ['zookeeper']
    zk.stop()
    with pytest.raises(ZooKeeper.Stopped):
      zk.get_children('/')


def test_client_stops_propagate_through_completions():
  with ZookeeperServer() as server:
    zk = ZooKeeper('localhost:%d' % server.zookeeper_port)
    server.shutdown()

    # zk.get_children should block until reconnected
    stopped_event = threading.Event()
    class GetThread(threading.Thread):
      def run(self):
        try:
          zk.get_children('/')
        except ZooKeeper.Stopped:
          stopped_event.set()

    gt = GetThread()
    gt.start()
    time.sleep(0.1)  # guarantee an interpreter thread yield

    zk.stop()
    stopped_event.wait(timeout=MAX_EVENT_WAIT_SECS)
    assert stopped_event.is_set()


def test_session_event():
  with ZookeeperServer() as server:
    disconnected = threading.Event()
    def on_event(zk, event, state, _):
      if zk._live.is_set() and state != zookeeper.CONNECTED_STATE:
        disconnected.set()

    zk = ZooKeeper(server.ensemble, watch=on_event)
    session_id = zk.session_id

    children = []
    completion_event = threading.Event()
    def children_completion(_, rc, results):
      children.extend(results)
      completion_event.set()

    server.shutdown()
    disconnected.wait(timeout=MAX_EVENT_WAIT_SECS)
    assert disconnected.is_set()

    zk.aget_children('/', None, children_completion)

    # expire session
    server.expire(session_id)
    server.start()

    completion_event.wait(timeout=MAX_EVENT_WAIT_SECS)
    assert completion_event.is_set()
    assert children == ['zookeeper']


def test_safe_operations():
  with ZookeeperServer() as server:
    zk = ZooKeeper(server.ensemble)
    assert zk.safe_create('/a/b/c/d') == '/a/b/c/d'
    session_id = zk.session_id

    finish_event = threading.Event()
    class CreateThread(threading.Thread):
      def run(self):
        zk.safe_create('/foo/bar/baz/bak')
        finish_event.set()

    server.shutdown()
    server.expire(session_id)

    ct = CreateThread()
    ct.start()
    server.start()
    finish_event.wait(timeout=MAX_EXPIRE_WAIT_SECS)
    assert finish_event.is_set()
    assert zk.exists('/a/b/c/d')
    assert zk.exists('/foo/bar/baz/bak')

    session_id = zk.session_id

    assert zk.safe_delete('/a')

    delete_event = threading.Event()
    class DeleteThread(threading.Thread):
      def run(self):
        zk.safe_delete('/foo')
        delete_event.set()

    server.shutdown()
    server.expire(session_id)

    dt = DeleteThread()
    dt.start()
    server.start()

    delete_event.wait(timeout=MAX_EXPIRE_WAIT_SECS)
    assert delete_event.is_set()
    assert not zk.exists('/a')
    assert not zk.exists('/foo')


def test_safe_create():
  with ZookeeperServer() as server:
    zk_auth = ZooKeeper(server.ensemble, authentication=('digest', 'jack:jill'))

    zk_auth.safe_create('/a', acl=ZooDefs.Acls.EVERYONE_READ_CREATOR_ALL)
    assert zk_auth.exists('/a')

    zk_noauth = ZooKeeper(server.ensemble)
    with pytest.raises(zookeeper.NoAuthException):
      zk_noauth.safe_create('/a/b')
    assert not zk_auth.exists('/a/b')

    zk_auth.safe_create('/a/b', acl=ZooDefs.Acls.OPEN_ACL_UNSAFE)
    assert zk_noauth.exists('/a/b')

    zk_noauth.safe_create('/a/b/c')
    assert zk_noauth.exists('/a/b/c')


def test_metrics():
  with ZookeeperServer() as server:
    event = threading.Event()
    def watch_set(*args):
      event.set()
    zk = ZooKeeper(server.ensemble, watch=watch_set)
    zk._live.wait(timeout=MAX_EVENT_WAIT_SECS)
    sample = zk.metrics.sample()
    assert sample['live'] == 1
    assert sample['session_id'] == zk.session_id
    assert sample['session_expirations'] == 0
    assert sample['connection_losses'] == 0
    old_session_id = zk.session_id

    event.clear()
    server.expire(zk.session_id)
    event.wait(timeout=MAX_EXPIRE_WAIT_SECS)
    zk._live.wait(timeout=MAX_EVENT_WAIT_SECS)

    sample = zk.metrics.sample()
    assert sample['live'] == 1
    assert sample['session_id'] == zk.session_id
    assert old_session_id != zk.session_id
    assert sample['session_expirations'] == 1

########NEW FILE########
__FILENAME__ = test_active_kazoo_group
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.common.zookeeper.group.kazoo_group import ActiveKazooGroup, Membership
from twitter.common.zookeeper.group.test_kazoo_group import TestKazooGroup

from kazoo.client import KazooClient
from kazoo.exceptions import NoNodeError, SessionExpiredError
from kazoo.protocol.states import EventType, KeeperState

from mock import ANY, Mock


DEFAULT_PATH = '/some/path/to/group'


def _mock_zk(state=KeeperState.CONNECTED):
  mock_zk = Mock(name='zk', spec=KazooClient)
  mock_zk.state = state

  return mock_zk


def _extract_callback(mock):
  return mock.return_value.rawlink.call_args[0][0]


def _happy_path(group, mock_zk, num_members):
  completion_callback = _extract_callback(mock_zk.get_children_async)

  members = []
  for member_id in range(834, 834 + num_members):
    znode_member_id = ActiveKazooGroup.id_to_znode(member_id)
    mock_get_children_result = Mock(name='mock get children async result')
    mock_get_children_result.get = Mock(return_value=[znode_member_id])

    completion_callback(mock_get_children_result)

    mock_zk.get_async.assert_called_with(DEFAULT_PATH + '/' + znode_member_id)

    info_callback = _extract_callback(mock_zk.get_async)

    member_data = 'some data for member %s' % member_id

    mock_get_result = Mock(name='mock get async result')
    mock_get_result.get = Mock(return_value=(member_data, Mock(name='znode stat')))
    info_callback(mock_get_result)

    member = Membership(member_id)
    members.append(member)
    assert group._members[member].result() == member_data

  return (frozenset(members), completion_callback)


def _unhappy_path(mock_zk, side_effect):
  mock_async_result = Mock()
  mock_async_result.get.side_effect = side_effect

  completion_callback = _extract_callback(mock_zk.get_children_async)

  completion_callback(mock_async_result)

class TestActiveKazooGroup(TestKazooGroup, unittest.TestCase):
  GroupImpl = ActiveKazooGroup

  def test_should_watch_group_path_on_init(self):
    mock_zk = _mock_zk()

    mock_async_result = Mock()
    mock_zk.get_children_async.return_value = mock_async_result

    # we're asserting on the side effects of creating the group
    ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    mock_zk.get_children_async.assert_called_with(DEFAULT_PATH, ANY)
    mock_async_result.rawlink.assert_called_with(ANY)

  def test_updates_internal_state_when_children_are_added(self):
    mock_zk = _mock_zk()

    group = ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    _happy_path(group, mock_zk, 2)

  def test_updates_internal_state_when_children_are_removed(self):
    mock_zk = _mock_zk()

    group = ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    members, completion_callback = _happy_path(group, mock_zk, 2)

    member_remaining, member_removed = list(members)
    znode_member_id = ActiveKazooGroup.id_to_znode(member_remaining.id)
    mock_get_children_result = Mock(name='mock get children async result')
    mock_get_children_result.get = Mock(return_value=[znode_member_id])

    completion_callback(mock_get_children_result)

    assert member_removed not in group._members

  def test_notifies_watchers_when_children_are_added(self):
    mock_zk = _mock_zk()

    mock_callback = Mock(name='monitor callback')

    def first_callback(*args, **kwargs):
      group.monitor(frozenset([Membership(1)]), mock_callback)

    group = ActiveKazooGroup(mock_zk, DEFAULT_PATH)
    group.monitor(callback=first_callback)

    members, _ = _happy_path(group, mock_zk, 1)

    mock_callback.assert_called_with(members)

  def test_notifies_watchers_when_children_are_removed(self):
    mock_zk = _mock_zk()

    mock_callback = Mock(name='monitor callback')

    def devnull(*args, **kwargs): pass

    group = ActiveKazooGroup(mock_zk, DEFAULT_PATH)
    group.monitor(frozenset([]), mock_callback)

    members, completion_callback = _happy_path(group, mock_zk, 2)

    member_remaining, member_removed = list(members)
    znode_member_id = ActiveKazooGroup.id_to_znode(member_remaining.id)
    mock_get_children_result = Mock(name='mock get children async result')
    mock_get_children_result.get = Mock(return_value=[znode_member_id])

    completion_callback(mock_get_children_result)

    assert member_removed not in group._members

    mock_callback.assert_called_with(set([member_remaining]))

  def test_waits_for_nodes_to_be_created_if_they_dont_exist(self):
    mock_zk = _mock_zk()

    # we're asserting on the side effects of creating the group
    ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    _unhappy_path(mock_zk, NoNodeError)

    mock_zk.exists_async.assert_called_with(DEFAULT_PATH, ANY)
    mock_zk.exists_async.return_value.rawlink.assert_called_with(ANY)

  def test_sets_a_state_listener_if_disconnected(self):
    mock_zk = _mock_zk(state=KeeperState.EXPIRED_SESSION)

    group = ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    assert len(group._KazooGroup__listener_queue) == 0

    _unhappy_path(mock_zk, SessionExpiredError)

    assert len(group._KazooGroup__listener_queue) == 1
    state, _ = group._KazooGroup__listener_queue[0]
    assert state == KeeperState.CONNECTED

  def test_znode_watch_triggered_for_child_events_causes_reprocess(self):
    mock_zk = _mock_zk()

    # we're asserting on the side effects of creating the group
    ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    assert mock_zk.get_children_async.call_count == 1

    _, watch_callback = mock_zk.get_children_async.call_args[0]

    mock_watch_event = Mock()
    mock_watch_event.state = KeeperState.CONNECTED
    mock_watch_event.type = EventType.CHILD

    watch_callback(mock_watch_event)

    assert mock_zk.get_children_async.call_count == 2

  def test_znode_watch_triggered_for_deleted_znode_causes_wait_for_it_to_exist(self):
    mock_zk = _mock_zk()

    # we're asserting on the side effects of creating the group
    ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    assert mock_zk.exists_async.call_count == 0

    _, watch_callback = mock_zk.get_children_async.call_args[0]

    mock_watch_event = Mock()
    mock_watch_event.state = KeeperState.CONNECTED
    mock_watch_event.type = EventType.DELETED

    watch_callback(mock_watch_event)

    assert mock_zk.exists_async.call_count == 1

  def test_sets_a_state_listener_if_disconnected_in_exists_completion(self):
    mock_zk = _mock_zk(state=KeeperState.EXPIRED_SESSION)

    group = ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    _unhappy_path(mock_zk, NoNodeError)

    exists_completion = _extract_callback(mock_zk.exists_async)

    mock_async_result = Mock()
    mock_async_result.get.side_effect = SessionExpiredError

    exists_completion(mock_async_result)

    assert len(group._KazooGroup__listener_queue) == 1
    state, _ = group._KazooGroup__listener_queue[0]
    assert state == KeeperState.CONNECTED

  def test_watches_again_if_no_node_raised_in_exists_completion(self):
    mock_zk = _mock_zk(state=KeeperState.EXPIRED_SESSION)

    ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    _unhappy_path(mock_zk, NoNodeError)

    assert mock_zk.exists_async.call_count == 1

    exists_completion = _extract_callback(mock_zk.exists_async)

    mock_async_result = Mock()
    mock_async_result.get.side_effect = NoNodeError

    exists_completion(mock_async_result)

    assert mock_zk.exists_async.call_count == 2

  def test_monitors_when_watched_node_is_created(self):
    mock_zk = _mock_zk(state=KeeperState.EXPIRED_SESSION)

    ActiveKazooGroup(mock_zk, DEFAULT_PATH)

    _unhappy_path(mock_zk, NoNodeError)

    assert mock_zk.get_children_async.call_count == 1

    exists_completion = _extract_callback(mock_zk.exists_async)

    mock_async_result = Mock()

    exists_completion(mock_async_result)

    assert mock_zk.get_children_async.call_count == 2
########NEW FILE########
__FILENAME__ = test_base
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import threading
import time

from twitter.common.zookeeper.test_server import ZookeeperServer
from twitter.common.zookeeper.group.group_base import Membership

import pytest


class GroupTestBase(object):
  # REQUIRE
  #
  # GroupImpl, AlternateGroupImpl, ACLS

  MAX_EVENT_WAIT_SECS = 30.0
  CONNECT_TIMEOUT_SECS = 10.0
  CONNECT_RETRIES = 6
  SERVER = None
  CHROOT_PREFIX = 0

  @classmethod
  def make_zk(cls, ensemble, **kw):
    raise NotImplementedError

  @classmethod
  def session_id(cls, zk):
    raise NotImplementedError
  
  def setUp(self):
    if GroupTestBase.SERVER is None:
      GroupTestBase.SERVER = ZookeeperServer()
    self._server = GroupTestBase.SERVER
    self._server.restart()
    self._zk = self.make_zk(self._server.ensemble)

  def tearDown(self):
    self._zk.stop()
    GroupTestBase.CHROOT_PREFIX += 1

  def test_sync_join(self):
    zkg = self.GroupImpl(self._zk, '/test')
    membership = zkg.join('hello world')
    assert isinstance(membership, Membership)
    assert membership != Membership.error()
    assert zkg.info(membership) == 'hello world'

  def test_join_through_expiration(self):
    self._zk.live.wait()
    session_id = self.session_id(self._zk)
    zkg = self.GroupImpl(self._zk, '/test')
    self._server.shutdown()
    join_event = threading.Event()
    join_membership = []
    def on_join(membership):
      join_membership[:] = [membership]
      join_event.set()
    cancel_event = threading.Event()
    cancel_membership = []
    def on_cancel():
      cancel_event.set()
    zkg.join('hello world', on_join, on_cancel)
    self._server.expire(session_id)
    self._server.start()
    join_event.wait(self.MAX_EVENT_WAIT_SECS)
    assert join_event.is_set()
    assert not cancel_event.is_set()
    assert zkg.info(join_membership[0]) == 'hello world'
    self._server.expire(self.session_id(self._zk))
    cancel_event.wait(self.MAX_EVENT_WAIT_SECS)
    assert cancel_event.is_set()

  def test_sync_cancel(self):
    # N.B. This test can be nondeterministic.  It is possible for the cancellation
    # to be called prior to zkg.monitor being called, in which case zkg.monitor
    # will return immediately.  If the Python thread scheduler happens to call
    # zkg.monitor first, then an aget_children with watch + completion will be
    # be dispatched and the intended code path will execute.  In both cases,
    # the test should succeed.  This is why we have artificial sleeps, in order
    # to attempt to exercise both cases, but it is not guaranteed.
    zkg = self.GroupImpl(self._zk, '/test')
    class BackgroundMonitor(threading.Thread):
      def __init__(self, *memberships):
        self.memberships = set(memberships)
        self.new_memberships = None
        super(BackgroundMonitor, self).__init__()
      def run(self):
        self.new_memberships = zkg.monitor(membership=self.memberships)

    # immediate
    membership = zkg.join('hello world')
    bm = BackgroundMonitor(membership)
    assert zkg.cancel(membership)
    bm.start()
    bm.join()
    assert bm.new_memberships == set()

    # potentially not immediate
    membership = zkg.join('hello world')
    bm = BackgroundMonitor(membership)
    bm.start()
    time.sleep(0.1)  # > 100ms sleep guaranteed thread yield
    assert zkg.cancel(membership)
    bm.join()
    assert bm.new_memberships == set()

    # multiple
    membership1 = zkg.join('hello world')
    membership2 = zkg.join('herpes derpes')
    bm = BackgroundMonitor(membership1, membership2)
    bm.start()
    assert zkg.cancel(membership1)
    bm.join()
    assert bm.new_memberships == set([membership2])

  def test_cancel_through_expiration(self):
    zkg = self.GroupImpl(self._zk, '/test')
    membership = zkg.join('hello world')

    session_id = self.session_id(self._zk)
    self._server.shutdown()

    cancel_event = threading.Event()
    cancel = []
    def on_cancel(success):
      cancel[:] = [success]
      cancel_event.set()

    zkg.cancel(membership, on_cancel)

    # expire session & restart server
    self._server.expire(session_id)
    self._server.start()

    cancel_event.wait()
    assert cancel_event.is_set()
    assert cancel == [True]

    # TODO(wickman) test a case where on_cancel is provided with false:
    # pretty much the only situation in which this can occur is if the
    # membership is created with a particular ACL and the cancelling Group
    # does not provide one.

  def test_info(self):
    zkg1 = self.GroupImpl(self._zk, '/test')
    zkg2 = self.GroupImpl(self._zk, '/test')
    membership = zkg1.join('hello world')
    assert zkg2.info(membership) == 'hello world'

  def test_authentication(self):
    secure_zk = self.make_zk(self._server.ensemble,
        authentication=('digest', 'username:password'))

    # auth => unauth
    zkg = self.GroupImpl(self._zk, '/test')
    secure_zkg = self.GroupImpl(secure_zk, '/test', acl=self.ACLS['EVERYONE_READ_CREATOR_ALL'])
    membership = zkg.join('hello world')
    assert secure_zkg.info(membership) == 'hello world'
    membership = secure_zkg.join('secure hello world')
    assert zkg.info(membership) == 'secure hello world'

    # unauth => auth
    zkg = self.GroupImpl(self._zk, '/secure-test')
    secure_zkg = self.GroupImpl(secure_zk, '/secure-test',
        acl=self.ACLS['EVERYONE_READ_CREATOR_ALL'])
    membership = secure_zkg.join('hello world')
    assert zkg.info(membership) == 'hello world'
    assert zkg.join('unsecure hello world') == Membership.error()

    # unauth => auth monitor
    zkg = self.GroupImpl(self._zk, '/secure-test2')
    secure_zkg = self.GroupImpl(secure_zk, '/secure-test2',
         acl=self.ACLS['EVERYONE_READ_CREATOR_ALL'])
    membership_event = threading.Event()
    members = set()
    def new_membership(m):
      members.update(m)
      membership_event.set()
    zkg.monitor(callback=new_membership)
    membership = secure_zkg.join('hello world')
    membership_event.wait(timeout=1.0)
    assert membership_event.is_set()
    assert members == set([membership])

  def test_monitor_through_parent_death(self):
    zkg = self.GroupImpl(self._zk, '/test')

    membership_event = threading.Event()
    members = set()
    def new_membership(m):
      members.update(m)
      membership_event.set()
    zkg.monitor(callback=new_membership)

    membership = zkg.join('hello world')
    assert membership != Membership.error()

    membership_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert membership_event.is_set()
    assert members == set([membership])

    membership_event.clear()
    members.clear()
    zkg.monitor(set([membership]), callback=new_membership)
    zkg.cancel(membership)

    membership_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert membership_event.is_set()
    assert members == set()

    membership_event.clear()
    members.clear()
    zkg.monitor(callback=new_membership)

    self._zk.delete('/test')

    membership = zkg.join('hello world 2')
    assert membership != Membership.error()

    membership_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert membership_event.is_set()
    assert members == set([membership])

  def test_info_after_expiration(self):
    zkg = self.GroupImpl(self._zk, '/test')
    membership = zkg.join('hello world')
    assert zkg.info(membership) == 'hello world'
    membership_event = threading.Event()
    members = [membership]
    def on_membership(new_membership):
      members[:] = new_membership
      membership_event.set()
    zkg.monitor(members, on_membership)
    self._server.expire(self.session_id(self._zk))
    membership_event.wait()
    assert members == []
    assert zkg.info(membership) == Membership.error()
    membership = zkg.join('herp derp')
    assert zkg.info(membership) == 'herp derp'

  def test_sync_join_with_cancel(self):
    zkg1 = self.GroupImpl(self._zk, '/test')
    zkg2 = self.GroupImpl(self._zk, '/test')
    cancel_event = threading.Event()
    def on_cancel():
      cancel_event.set()
    membership = zkg1.join('hello world', expire_callback=on_cancel)
    assert zkg2.cancel(membership)
    cancel_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert cancel_event.is_set()

  def test_async_join(self):
    zkg = self.GroupImpl(self._zk, '/test')
    event = threading.Event()
    memberships = []
    def on_join(membership):
      memberships.append(membership)
      event.set()
    zkg.join('hello world', callback=on_join)
    event.wait()
    assert len(memberships) == 1 and memberships[0] != Membership.error()
    zkg.cancel(memberships[0])

  def test_async_join_with_cancel(self):
    zkg1 = self.GroupImpl(self._zk, '/test')
    zkg2 = self.GroupImpl(self._zk, '/test')
    event = threading.Event()
    cancel_event = threading.Event()
    memberships = []
    def on_join(membership):
      memberships.append(membership)
      event.set()
    def on_cancel():
      cancel_event.set()

    # sync
    zkg1.join('hello world', callback=on_join, expire_callback=on_cancel)
    event.wait()
    zkg2.cancel(memberships[0])
    cancel_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert cancel_event.is_set()

    # clear
    event.clear()
    cancel_event.clear()
    memberships = []

    # async
    zkg1.join('hello world', callback=on_join, expire_callback=on_cancel)
    event.wait()

    client_cancel = threading.Event()
    successes = []
    def on_client_side_cancel(troof):
      successes.append(troof)
      client_cancel.set()
    zkg2.cancel(memberships[0], callback=on_client_side_cancel)

    client_cancel.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    cancel_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert client_cancel.is_set()
    assert len(successes) == 1 and successes[0] == True
    assert cancel_event.is_set()

  def test_async_monitor(self):
    zkg1 = self.GroupImpl(self._zk, '/test')
    zkg2 = self.GroupImpl(self._zk, '/test')

    membership_event = threading.Event()
    members = set()
    def new_membership(m):
      members.update(m)
      membership_event.set()
    zkg1.monitor(callback=new_membership)
    membership = zkg2.join('hello world')
    membership_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert membership_event.is_set()
    assert members == set([membership])

  def test_monitor_then_info(self):
    zkg1 = self.GroupImpl(self._zk, '/test')
    zkg2 = self.GroupImpl(self._zk, '/test')
    zkg2.join('hello 1')
    zkg2.join('hello 2')
    zkg2.join('hello 3')
    members = zkg1.monitor()
    for member in members:
      assert zkg1.info(member) is not None
      assert zkg1.info(member).startswith('hello')

  def test_against_alternate_groups(self):
    zkg1 = self.GroupImpl(self._zk, '/test')
    zkg2 = self.AlternateGroupImpl(self._zk, '/test')
    assert zkg1.list() == []
    assert zkg2.list() == []
    m1 = zkg1.join('morf gorf')
    assert len(zkg1.list()) == 1
    assert len(zkg2.list()) == 0
    m2 = zkg2.join('herp derp')
    assert len(zkg1.list()) == 1
    assert len(zkg2.list()) == 1
    assert zkg1.info(m1) == 'morf gorf'
    assert zkg1.info(m2) == Membership.error()
    assert zkg2.info(m1) == Membership.error()
    assert zkg2.info(m2) == 'herp derp'

  def test_hard_root_acl(self):
    secure_zk = self.make_zk(self._server.ensemble, authentication=('digest', 'username:password'))
    secure_zk.live.wait()
    secure_zk.create('/test', '', self.ACLS['EVERYONE_READ_CREATOR_ALL'])
    secure_zk.set_acl('/', 0, self.ACLS['READ_ACL_UNSAFE'])
    secure_zkg = self.GroupImpl(secure_zk, '/test', acl=self.ACLS['EVERYONE_READ_CREATOR_ALL'])
    membership = secure_zkg.join('secure hello world')
    assert membership != Membership.error()
    assert secure_zkg.info(membership) == 'secure hello world'

########NEW FILE########
__FILENAME__ = test_group
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import time
import threading
import unittest

from twitter.common.zookeeper.client import ZooKeeper, ZooDefs
from twitter.common.zookeeper.group.group import ActiveGroup, Group
from twitter.common.zookeeper.group.test_base import GroupTestBase

import zookeeper


class AlternateGroup(Group):
  MEMBER_PREFIX = 'herpderp_'


class TestGroup(GroupTestBase, unittest.TestCase):
  GroupImpl = Group
  AlternateGroupImpl = AlternateGroup
  ACLS = dict(
    OPEN_ACL_UNSAFE=ZooDefs.Acls.OPEN_ACL_UNSAFE,
    CREATOR_ALL_ACL=ZooDefs.Acls.CREATOR_ALL_ACL,
    READ_ACL_UNSAFE=ZooDefs.Acls.READ_ACL_UNSAFE,
    EVERYONE_READ_CREATOR_ALL=ZooDefs.Acls.EVERYONE_READ_CREATOR_ALL,
  )

  @classmethod
  def make_zk(cls, ensemble, **kw):
    return ZooKeeper(ensemble,
                     timeout_secs=cls.CONNECT_TIMEOUT_SECS,
                     max_reconnects=cls.CONNECT_RETRIES,
                     **kw)

  @classmethod
  def session_id(cls, zk):
    return zk.session_id

  def test_children_filtering(self):
    zk = self.make_zk(self._server.ensemble)
    zk.create('/test', '', self.ACLS['OPEN_ACL_UNSAFE'])
    zk.create('/test/alt_member_', '',  self.ACLS['OPEN_ACL_UNSAFE'],
        zookeeper.SEQUENCE | zookeeper.EPHEMERAL)
    zk.create('/test/candidate_', '',  self.ACLS['OPEN_ACL_UNSAFE'],
        zookeeper.SEQUENCE | zookeeper.EPHEMERAL)
    zkg = self.GroupImpl(self._zk, '/test')
    assert list(zkg) == []
    assert zkg.monitor(membership=set(['frank', 'larry'])) == set()

  def test_monitor_through_expiration(self):
    session_expired = threading.Event()
    def on_watch(_, event, state, path):
      if event == zookeeper.SESSION_EVENT and state == zookeeper.EXPIRED_SESSION_STATE:
        session_expired.set()

    zk1 = self.make_zk(self._server.ensemble, watch=on_watch)
    zkg1 = self.GroupImpl(self._zk, '/test')
    session_id1 = self.session_id(zk1)

    zk2 = self.make_zk(self._server.ensemble)
    zkg2 = self.GroupImpl(zk2, '/test')
    member1 = zkg2.join('hello 1')
    new_members = zkg1.monitor([]) # wait until the first group acknowledges the join
    assert new_members == set([member1])

    membership_event = threading.Event()
    membership = []
    def on_membership(new_members):
      membership[:] = new_members
      membership_event.set()

    zkg1.monitor([member1], on_membership)
    self._server.expire(session_id1)
    session_expired.wait(self.MAX_EVENT_WAIT_SECS)
    assert not membership_event.is_set()

    member2 = zkg2.join('hello 2')
    membership_event.wait()
    assert membership_event.is_set()
    assert membership == [member1, member2]

    for member in membership:
      assert zkg1.info(member) is not None
      assert zkg1.info(member).startswith('hello')


class TestActiveGroup(TestGroup):
  GroupImpl = ActiveGroup

  # These tests do use time.sleep but mostly to simulate real eventual consistency
  # in the behavior of iter and getitem.  Because we don't have more intimate control
  # over the underlying store (Zookeeper), this will have to do.
  def test_container_idioms(self):
    zkg1 = self.GroupImpl(self._zk, '/test')
    zkg2 = self.GroupImpl(self._zk, '/test')

    def devnull(*args, **kw):
      pass

    def cancel_by_value(group, cancel_group, value):
      for member in group:
        if group[member] == value:
          cancel_group.cancel(member, callback=devnull)
          break

    def assert_iter_equals(membership, max_wait=self.MAX_EVENT_WAIT_SECS):
      total_wait = 0.0
      while total_wait < max_wait:
        members = list(zkg1)
        if len(members) == len(membership):
          break
        time.sleep(0.1)
        total_wait += 0.1
      for member in members:
        assert zkg1[member] in membership

    zkg2.join('hello 1', callback=devnull)
    zkg2.join('hello 2', callback=devnull)
    zkg2.join('hello 3', callback=devnull)
    assert_iter_equals(['hello 1', 'hello 2', 'hello 3'])

    cancel_by_value(zkg1, zkg2, 'hello 2')
    assert_iter_equals(['hello 1', 'hello 3'])

    # cancel on same group
    cancel_by_value(zkg1, zkg1, 'hello 3')
    assert_iter_equals(['hello 1'])

    # join on same group
    zkg1.join('hello 4', callback=devnull)
    assert_iter_equals(['hello 1', 'hello 4'])

    # join on same group
    zkg2.join('hello 5', callback=devnull)
    assert_iter_equals(['hello 1', 'hello 4', 'hello 5'])

########NEW FILE########
__FILENAME__ = test_kazoo_group
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import threading
import unittest

from twitter.common.zookeeper.group.group_base import Membership
from twitter.common.zookeeper.group.kazoo_group import KazooGroup
from twitter.common.zookeeper.group.test_base import GroupTestBase
from twitter.common.zookeeper.kazoo_client import TwitterKazooClient

from kazoo.protocol.states import KazooState
import kazoo.security as ksec


class AlternateKazooGroup(KazooGroup):
  MEMBER_PREFIX = 'herpderp_'


class TestKazooGroup(GroupTestBase, unittest.TestCase):
  GroupImpl = KazooGroup
  AlternateGroupImpl = AlternateKazooGroup
  ACLS = dict(
    OPEN_ACL_UNSAFE=ksec.OPEN_ACL_UNSAFE,
    CREATOR_ACLL_ACL=ksec.CREATOR_ALL_ACL,
    READ_ACL_UNSAFE=ksec.READ_ACL_UNSAFE,
    EVERYONE_READ_CREATOR_ALL=[
        dict(scheme='auth', id='', perms=ksec.Permissions.ALL),
        dict(scheme='world', id='anyone', perms=ksec.Permissions.READ),
    ]
  )

  @classmethod
  def make_zk(cls, ensemble, **kw):
    if 'authentication' in kw:
      kw.update(auth_data = [kw.pop('authentication')])
    tzk = TwitterKazooClient.make(
        ensemble + ('/%08d' % GroupTestBase.CHROOT_PREFIX),
        timeout=cls.CONNECT_TIMEOUT_SECS,
        max_retries=cls.CONNECT_RETRIES, **kw)
    started = threading.Event()
    def listen(state):
      if state == KazooState.CONNECTED:
        started.set()
      return True
    tzk.add_listener(listen)
    started.wait()
    tzk.ensure_path('/')
    return tzk

  @classmethod
  def session_id(cls, zk):
    return zk._session_id

  def test_children_filtering(self):
    zk = self.make_zk(self._server.ensemble)
    zk.live.wait()
    zk.create('/test', '', acl=ksec.OPEN_ACL_UNSAFE)
    zk.create('/test/alt_member_', '',  acl=ksec.OPEN_ACL_UNSAFE, sequence=True, ephemeral=True)
    zk.create('/test/candidate_', '',  acl=ksec.OPEN_ACL_UNSAFE, sequence=True, ephemeral=True)
    zkg = self.GroupImpl(self._zk, '/test')
    assert list(zkg) == []
    assert zkg.monitor(membership=set(['frank', 'larry'])) == set()

  def test_hard_root_acl(self):
    secure_zk = self.make_zk(self._server.ensemble,
        authentication=('digest', 'username:password'))
    secure_zk.live.wait()
    secure_zk.create('/test', '', acl=ksec.CREATOR_ALL_ACL + ksec.READ_ACL_UNSAFE)
    secure_zk.set_acls('/', self.ACLS['READ_ACL_UNSAFE'])
    secure_zkg = self.GroupImpl(secure_zk, '/test', acl=self.ACLS['EVERYONE_READ_CREATOR_ALL'])
    membership = secure_zkg.join('secure hello world')
    assert membership != Membership.error()
    assert secure_zkg.info(membership) == 'secure hello world'

  def test_monitor_expire_then_create(self):
    session_expired = threading.Event()
    membership_event = threading.Event()

    def listener(state):
      if state == KazooState.LOST:
        session_expired.set()

    def on_membership(new_members):
      membership_event.set()

    zk1 = self.make_zk(self._server.ensemble)
    zk1.add_listener(listener)
    zk1.live.wait()

    zkg1 = self.GroupImpl(self._zk, '/test')

    zk2 = self.make_zk(self._server.ensemble)
    zkg2 = self.GroupImpl(zk2, '/test')

    zkg1.monitor([], on_membership)

    self._server.expire(self.session_id(zk1))
    session_expired.wait(self.MAX_EVENT_WAIT_SECS)

    member1 = zkg2.join('hello 1')
    membership_event.wait(timeout=self.MAX_EVENT_WAIT_SECS)
    assert membership_event.is_set()

  def test_monitor_through_expiration(self):
    session_expired = threading.Event()

    def listener(state):
      if state == KazooState.LOST:
        session_expired.set()

    zk1 = self.make_zk(self._server.ensemble)
    zk1.add_listener(listener)
    zkg1 = self.GroupImpl(self._zk, '/test')

    zk2 = self.make_zk(self._server.ensemble)
    zkg2 = self.GroupImpl(zk2, '/test')

    member1 = zkg2.join('hello 1')
    new_members = zkg1.monitor([]) # wait until the first group acknowledges the join
    assert new_members == set([member1])

    membership_event = threading.Event()
    membership = []
    def on_membership(new_members):
      membership[:] = new_members
      membership_event.set()

    zkg1.monitor([member1], on_membership)
    self._server.expire(self.session_id(zk1))
    session_expired.wait(self.MAX_EVENT_WAIT_SECS)
    assert session_expired.is_set()
    assert not membership_event.is_set()

    member2 = zkg2.join('hello 2')
    membership_event.wait()
    assert membership_event.is_set()
    assert membership == [member1, member2]

    for member in membership:
      assert zkg1.info(member) is not None
      assert zkg1.info(member).startswith('hello')

########NEW FILE########
__FILENAME__ = kazoo_client_test
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import threading
import time

from twitter.common.zookeeper.kazoo_client import TwitterKazooClient
from twitter.common.zookeeper.test_server import ZookeeperServer


MAX_EVENT_WAIT_SECS = 30.0


def test_metrics():
  with ZookeeperServer() as server:
    event = threading.Event()
    def state_change(state):
      event.set()
      return True

    zk = TwitterKazooClient('localhost:%d' % server.zookeeper_port)
    zk.start()
    zk.live.wait(timeout=MAX_EVENT_WAIT_SECS)
    zk.add_listener(state_change)
    sample = zk.metrics.sample()
    assert sample['session_id'] == zk._session_id
    assert sample['session_expirations'] == 0
    assert sample['connection_losses'] == 0
    old_session_id = zk._session_id

    server.expire(zk._session_id)
    event.wait(timeout=MAX_EVENT_WAIT_SECS)

    zk.live.wait(timeout=MAX_EVENT_WAIT_SECS)

    sample = zk.metrics.sample()
    assert sample['session_id'] == zk._session_id
    assert old_session_id != zk._session_id
    assert sample['session_expirations'] == 1
    assert sample['connection_losses'] > 0

########NEW FILE########
__FILENAME__ = test_base
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import threading
import time

from twitter.common.zookeeper.group.group_base import Membership
from twitter.common.zookeeper.serverset import (
    Endpoint,
    ServerSet,
    ServiceInstance)
from twitter.common.zookeeper.test_server import ZookeeperServer


class ServerSetTestBase(object):
  SERVICE_PATH = '/twitter/service/test'
  INSTANCE1 = Endpoint(host='127.0.0.1', port=1234)
  INSTANCE2 = Endpoint(host='127.0.0.1', port=1235)
  ADDITIONAL1 = {'http': Endpoint(host='127.0.0.1', port=8080)}
  ADDITIONAL2 = {'thrift': Endpoint(host='127.0.0.1', port=8081)}

  @classmethod
  def make_zk(cls, ensemble):
    raise NotImplementedError

  @classmethod
  def session_id(cls, client):
    raise NotImplementedError

  def setUp(self):
    self._server = ZookeeperServer()

  def tearDown(self):
    self._server.stop()

  def test_client_iteration(self):
    ss = ServerSet(self.make_zk(self._server.ensemble), self.SERVICE_PATH)
    assert list(ss) == []
    ss.join(self.INSTANCE1)
    assert list(ss) == [ServiceInstance(self.INSTANCE1)]
    ss.join(self.INSTANCE2)
    assert list(ss) == [ServiceInstance(self.INSTANCE1), ServiceInstance(self.INSTANCE2)]

  def test_async_client_iteration(self):
    ss1 = ServerSet(self.make_zk(self._server.ensemble), self.SERVICE_PATH)
    ss2 = ServerSet(self.make_zk(self._server.ensemble), self.SERVICE_PATH)
    ss1.join(self.INSTANCE1)
    ss2.join(self.INSTANCE2)
    assert list(ss1) == [ServiceInstance(self.INSTANCE1), ServiceInstance(self.INSTANCE2)]
    assert list(ss2) == [ServiceInstance(self.INSTANCE1), ServiceInstance(self.INSTANCE2)]

  def test_shard_id_registers(self):
    ss1 = ServerSet(self.make_zk(self._server.ensemble), self.SERVICE_PATH)
    ss2 = ServerSet(self.make_zk(self._server.ensemble), self.SERVICE_PATH)
    ss1.join(self.INSTANCE1, shard=0)
    ss2.join(self.INSTANCE2, shard=1)
    assert list(ss1) == [ServiceInstance(self.INSTANCE1, shard=0), ServiceInstance(self.INSTANCE2, shard=1)]
    assert list(ss2) == [ServiceInstance(self.INSTANCE1, shard=0), ServiceInstance(self.INSTANCE2, shard=1)]

  def test_canceled_join_long_time(self):
    zk = self.make_zk(self._server.ensemble)
    zk.live.wait()
    session_id = self.session_id(zk)
    ss = ServerSet(zk, self.SERVICE_PATH)
    join_signal = threading.Event()
    memberships = []

    def on_expire():
      pass

    def do_join():
      memberships.append(ss.join(self.INSTANCE1, expire_callback=on_expire))

    class JoinThread(threading.Thread):
      def run(_):
        while True:
          join_signal.wait()
          join_signal.clear()
          do_join()

    joiner = JoinThread()
    joiner.daemon = True
    joiner.start()

    do_join()
    assert len(memberships) == 1 and memberships[0] is not Membership.error()
    self._server.expire(session_id)
    self._server.shutdown()
    join_signal.set()
    self._server.start()
    while len(memberships) == 1:
      time.sleep(0.1)
    assert len(memberships) == 2 and memberships[1] is not Membership.error()

  def test_client_watcher(self):
    canceled_endpoints = []
    canceled = threading.Event()
    joined_endpoints = []
    joined = threading.Event()

    def on_join(endpoint):
      joined_endpoints[:] = [endpoint]
      joined.set()

    def on_leave(endpoint):
      canceled_endpoints[:] = [endpoint]
      canceled.set()

    service1 = ServerSet(self.make_zk(self._server.ensemble),
        self.SERVICE_PATH, on_join=on_join, on_leave=on_leave)
    service2 = ServerSet(self.make_zk(self._server.ensemble),
        self.SERVICE_PATH)

    member1 = service2.join(self.INSTANCE1)
    joined.wait(2.0)
    assert joined.is_set()
    assert not canceled.is_set()
    assert joined_endpoints == [ServiceInstance(self.INSTANCE1)]
    joined.clear()

    service2.join(self.INSTANCE2)
    joined.wait(2.0)
    assert joined.is_set()
    assert not canceled.is_set()
    assert joined_endpoints == [ServiceInstance(self.INSTANCE2)]
    joined.clear()

    service2.cancel(member1)
    canceled.wait(2.0)
    assert canceled.is_set()
    assert not joined.is_set()
    assert canceled_endpoints == [ServiceInstance(self.INSTANCE1)]
    canceled.clear()

########NEW FILE########
__FILENAME__ = test_endpoint
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.zookeeper.serverset.endpoint import Endpoint, ServiceInstance, Status


def _service_instance(vals):
  json = '''{
    "additionalEndpoints": {
        "aurora": {
            "host": "smfd-akb-%d-sr1.devel.twitter.com",
            "port": 31181
        },
        "health": {
            "host": "smfd-akb-%d-sr1.devel.twitter.com",
            "port": 31181
        }
    },
    "serviceEndpoint": {
        "host": "smfd-akb-%d-sr1.devel.twitter.com",
        "port": 31181
    },
    "shard": %d,
    "status": "ALIVE"
}''' % vals

  return ServiceInstance.unpack(json)


def test_endpoint_equality():
  assert Endpoint('host', 8340) == Endpoint('host', 8340)


def test_endpoint_hash_equality():
  assert Endpoint('host', 8340).__hash__() == Endpoint('host', 8340).__hash__()


def test_endpoint_inequality():
  assert Endpoint('host', 8340) != Endpoint('xhost', 8341)


def test_endpoint_hash_inequality():
  assert Endpoint('host', 8340).__hash__() != Endpoint('xhost', 8341).__hash__()


def test_status_equality():
  assert Status.from_string('DEAD') == Status.from_string('DEAD')


def test_status_hash_equality():
  assert Status.from_string('DEAD').__hash__() == Status.from_string('DEAD').__hash__()


def test_status_inequality():
  assert Status.from_string('DEAD') != Status.from_string('STARTING')


def test_status_hash_inequality():
  assert Status.from_string('DEAD').__hash__() != Status.from_string('STARTING').__hash__()


def test_service_instance_equality():
  vals = (1, 2, 3, 4)
  assert _service_instance(vals) == _service_instance(vals)


def test_service_instance_hash_equality():
  vals = (1, 2, 3, 4)
  assert _service_instance(vals).__hash__() == _service_instance(vals).__hash__()


def test_service_instance_inequality():
  vals = (1, 2, 3, 4)
  vals2 = (5, 6, 7, 8)
  assert _service_instance(vals) != _service_instance(vals2)


def test_service_instance_hash_inequality():
  vals = (1, 2, 3, 4)
  vals2 = (5, 6, 7, 8)
  assert _service_instance(vals).__hash__() != _service_instance(vals2).__hash__()

########NEW FILE########
__FILENAME__ = test_kazoo_serverset
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.common.zookeeper.kazoo_client import TwitterKazooClient
from twitter.common.zookeeper.serverset.test_base import ServerSetTestBase


class TestKazooServerSet(ServerSetTestBase, unittest.TestCase):
  @classmethod
  def make_zk(cls, ensemble):
    return TwitterKazooClient.make(ensemble)

  @classmethod
  def session_id(cls, client):
    return client._session_id

########NEW FILE########
__FILENAME__ = test_serverset
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.common.zookeeper.client import ZooKeeper
from twitter.common.zookeeper.serverset.test_base import ServerSetTestBase


class TestServerSet(ServerSetTestBase, unittest.TestCase):
  @classmethod
  def make_zk(cls, ensemble):
    return ZooKeeper(ensemble)

  @classmethod
  def session_id(cls, client):
    return client.session_id

########NEW FILE########
__FILENAME__ = test_serverset_unit
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.common.zookeeper.serverset.endpoint import ServiceInstance
from twitter.common.zookeeper.serverset.serverset import ServerSet
from twitter.common.zookeeper.group.group_base import GroupInterface, Membership

from twitter.common.zookeeper.group.kazoo_group import ActiveKazooGroup

from kazoo.client import KazooClient

import mock


SERVICE_INSTANCE_JSON = '''{
    "additionalEndpoints": {
        "aurora": {
            "host": "smfd-aki-15-sr1.devel.twitter.com",
            "port": 31510
        },
        "health": {
            "host": "smfd-aki-15-sr1.devel.twitter.com",
            "port": 31510
        }
    },
    "serviceEndpoint": {
        "host": "smfd-aki-15-sr1.devel.twitter.com",
        "port": 31510
    },
    "shard": 0,
    "status": "ALIVE"
}'''


@mock.patch('twitter.common.zookeeper.serverset.serverset.ActiveKazooGroup')
@mock.patch('twitter.common.zookeeper.serverset.serverset.validate_group_implementation')
def test_internal_monitor(mock_group_impl_validator, MockActiveKazooGroup):
  mock_zk = mock.Mock(spec=KazooClient)
  mock_group = mock.MagicMock(spec=GroupInterface)
  MockActiveKazooGroup.mock_add_spec(ActiveKazooGroup)
  MockActiveKazooGroup.return_value = mock_group

  # by default it tries to assert that the group impl is a subclass of GroupInterface
  # since the group impl will be a mock, it doesn't pass that check, so we mock the validator
  # as well.
  mock_group_impl_validator.return_value = True

  def devnull(*args, **kwargs): pass

  serverset = ServerSet(
      mock_zk,
      '/some/path/to/group',
      on_join=devnull,
      on_leave=devnull)

  members = [Membership(id) for id in range(2)]

  print("Members are: %s" % members)
  serverset._internal_monitor(frozenset(members))

  for call in mock_group.info.mock_calls:
    _, (_, callback), _ = call
    callback(ServiceInstance.unpack(SERVICE_INSTANCE_JSON))

  assert len(serverset._members) == 2

########NEW FILE########
__FILENAME__ = context_utils
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import io

from twitter.common.collections import maybe_list
from twitter.common.dirutil import safe_mkdtemp
from twitter.common.lang import Compatibility

from twitter.pants.base.config import Config
from twitter.pants.base.target import Target
from twitter.pants.goal import Context, RunTracker
from twitter.pants.reporting.report import Report


def create_options(options_hash=None):
  """Creates an options object populated with no options at all by default.

  :param dict options_hash: An optional dict of option values.
  """
  opts = options_hash or {}
  if not isinstance(opts, dict):
    raise ValueError('The given options_hash must be a dict, got: %s' % options_hash)

  class Options(object):
    def __init__(self):
      self.__dict__ = opts
  return Options()


def create_config(sample_ini='', defaults=None):
  """Creates a ``Config`` from the ``sample_ini`` file contents.

  :param string sample_ini: The contents of the ini file containing the config values.
  :param dict defaults: An optional dict of global default ini values to seed.
  """
  if not isinstance(sample_ini, Compatibility.string):
    raise ValueError('The sample_ini supplied must be a string, given: %s' % sample_ini)

  parser = Config.create_parser(defaults)
  with io.BytesIO(sample_ini) as ini:
    parser.readfp(ini)
  return Config(parser)


def create_run_tracker(info_dir=None):
  """Creates a ``RunTracker`` and starts it.

  :param string info_dir: An optional director for the run tracker to store state; defaults to a
    new temp dir that will be be cleaned up on interpreter exit.
  """
  # TODO(John Sirois): Rework uses around a context manager for cleanup of the info_dir in a more
  # disciplined manner
  info_dir = info_dir or safe_mkdtemp()
  run_tracker = RunTracker(info_dir)
  report = Report()
  run_tracker.start(report)
  return run_tracker


def create_context(config='', options=None, target_roots=None, **kwargs):
  """Creates a ``Context`` with no config values, options, or targets by default.

  :param config: Either a ``Context`` object or else a string representing the contents of the
    pants.ini to parse the config from.
  :param options: An optional dict of of option values.
  :param target_roots: An optional list of target roots to seed the context target graph from.
  :param ``**kwargs``: Any additional keyword arguments to pass through to the Context constructor.
  """
  config = config if isinstance(config, Config) else create_config(config)
  run_tracker = create_run_tracker()
  target_roots = maybe_list(target_roots, Target) if target_roots else []
  return Context(config, create_options(options or {}), run_tracker, target_roots, **kwargs)

########NEW FILE########
__FILENAME__ = test_abbreviate_target_ids
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.abbreviate_target_ids import abbreviate_target_ids

class AbbreviateTargetIdsTest(unittest.TestCase):
  def _test(self, expected, *actual):
    self.assertEqual(expected, abbreviate_target_ids(actual))

  def test_empty(self):
    self._test({})

  def test_single(self):
    self._test({'a': 'a'}, 'a')
    self._test({'a.b.c': 'c'}, 'a.b.c')

  def test_simple(self):
    self._test({'a': 'a',
                'b': 'b',
                'c': 'c'},
               'a', 'b', 'c')
    self._test({'x.a': 'a',
                'y.b': 'b',
                'z.c': 'c'},
               'x.a', 'y.b', 'z.c')

  def test_complex(self):
    self._test({'x.a': 'a',
                'x.b': 'b',
                'x.c': 'c'},
               'x.a', 'x.b', 'x.c')
    self._test({'x.a': 'x.a',
                'y.a': 'y.a',
                'z.b': 'b'},
               'x.a', 'y.a', 'z.b')
    self._test({'x.a': 'a',
                'x.y.a': 'y.a',
                'z.b': 'b'},
               'x.a', 'x.y.a', 'z.b')

########NEW FILE########
__FILENAME__ = test_address
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pytest
import unittest

from contextlib import contextmanager

from twitter.common.contextutil import temporary_dir, pushd
from twitter.common.dirutil import touch

from twitter.pants.base.address import Address
from twitter.pants.base.build_environment import set_buildroot


class AddressTest(unittest.TestCase):
  @contextmanager
  def workspace(self, *buildfiles):
    with temporary_dir() as root_dir:
      set_buildroot(root_dir)
      with pushd(root_dir):
        for buildfile in buildfiles:
          touch(os.path.join(root_dir, buildfile))
        yield os.path.realpath(root_dir)

  def assertAddress(self, root_dir, path, name, address):
    self.assertEqual(root_dir, address.buildfile.root_dir)
    self.assertEqual(path, address.buildfile.relpath)
    self.assertEqual(name, address.target_name)

  def test_full_forms(self):
    with self.workspace('a/BUILD') as root_dir:
      self.assertAddress(root_dir, 'a/BUILD', 'b', Address.parse(root_dir, 'a:b'))
      self.assertAddress(root_dir, 'a/BUILD', 'b', Address.parse(root_dir, 'a/:b'))
      self.assertAddress(root_dir, 'a/BUILD', 'b', Address.parse(root_dir, 'a/BUILD:b'))
      self.assertAddress(root_dir, 'a/BUILD', 'b', Address.parse(root_dir, 'a/BUILD/:b'))

  def test_default_form(self):
    with self.workspace('a/BUILD') as root_dir:
      self.assertAddress(root_dir, 'a/BUILD', 'a', Address.parse(root_dir, 'a'))
      self.assertAddress(root_dir, 'a/BUILD', 'a', Address.parse(root_dir, 'a/BUILD'))
      self.assertAddress(root_dir, 'a/BUILD', 'a', Address.parse(root_dir, 'a/BUILD/'))

  def test_top_level(self):
    with self.workspace('BUILD') as root_dir:
      self.assertAddress(root_dir, 'BUILD', 'c', Address.parse(root_dir, ':c'))
      self.assertAddress(root_dir, 'BUILD', 'c', Address.parse(root_dir, '.:c'))
      self.assertAddress(root_dir, 'BUILD', 'c', Address.parse(root_dir, './:c'))
      self.assertAddress(root_dir, 'BUILD', 'c', Address.parse(root_dir, './BUILD:c'))
      self.assertAddress(root_dir, 'BUILD', 'c', Address.parse(root_dir, 'BUILD:c'))

  def test_parse_from_root_dir(self):
    with self.workspace('a/b/c/BUILD') as root_dir:
      self.assertAddress(root_dir, 'a/b/c/BUILD', 'c',
                         Address.parse(root_dir, 'a/b/c', is_relative=False))
      self.assertAddress(root_dir, 'a/b/c/BUILD', 'c',
                         Address.parse(root_dir, 'a/b/c', is_relative=True))

  def test_parse_from_sub_dir(self):
    with self.workspace('a/b/c/BUILD') as root_dir:
      with pushd(os.path.join(root_dir, 'a')):
        self.assertAddress(root_dir, 'a/b/c/BUILD', 'c',
                           Address.parse(root_dir, 'b/c', is_relative=True))

        with pytest.raises(IOError):
          Address.parse(root_dir, 'b/c', is_relative=False)

########NEW FILE########
__FILENAME__ = test_build_file
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'John Sirois'

from twitter.common.collections import OrderedSet
from twitter.common.dirutil import touch, safe_mkdir
from twitter.pants.base.build_file import BuildFile

import os
import shutil
import tempfile
import unittest

class BuildFileTest(unittest.TestCase):

  @classmethod
  def makedirs(cls, path):
    safe_mkdir(os.path.join(BuildFileTest.root_dir, path))

  @classmethod
  def touch(cls, path):
    touch(os.path.join(BuildFileTest.root_dir, path))

  @classmethod
  def buildfile(cls, path):
    return BuildFile(BuildFileTest.root_dir, path)

  @classmethod
  def setUpClass(cls):
    BuildFileTest.base_dir = tempfile.mkdtemp()

    # Seed a BUILD outside the build root that should not be detected
    touch(os.path.join(BuildFileTest.base_dir, 'BUILD'))

    BuildFileTest.root_dir = os.path.join(BuildFileTest.base_dir, 'root')

    BuildFileTest.touch('grandparent/parent/BUILD')
    BuildFileTest.touch('grandparent/parent/BUILD.twitter')
    BuildFileTest.makedirs('grandparent/parent/BUILD.dir')
    BuildFileTest.makedirs('grandparent/BUILD')
    BuildFileTest.touch('BUILD')
    BuildFileTest.touch('BUILD.twitter')
    BuildFileTest.touch('grandparent/parent/child1/BUILD')
    BuildFileTest.touch('grandparent/parent/child1/BUILD.twitter')
    BuildFileTest.touch('grandparent/parent/child2/child3/BUILD')
    BuildFileTest.makedirs('grandparent/parent/child2/BUILD')
    BuildFileTest.makedirs('grandparent/parent/child4')

  @classmethod
  def tearDownClass(cls):
    shutil.rmtree(BuildFileTest.root_dir)

  def setUp(self):
    self.buildfile = BuildFileTest.buildfile('grandparent/parent/BUILD')

  def testSiblings(self):
    buildfile = BuildFileTest.buildfile('grandparent/parent/BUILD.twitter')
    self.assertEquals(OrderedSet([buildfile]), OrderedSet(self.buildfile.siblings()))
    self.assertEquals(OrderedSet([self.buildfile]), OrderedSet(buildfile.siblings()))

    buildfile = BuildFileTest.buildfile('grandparent/parent/child2/child3/BUILD')
    self.assertEquals(OrderedSet(), OrderedSet(buildfile.siblings()))

  def testFamily(self):
    self.assertEquals(OrderedSet([
        BuildFileTest.buildfile('grandparent/parent/BUILD'),
        BuildFileTest.buildfile('grandparent/parent/BUILD.twitter'),
    ]), self.buildfile.family())

    buildfile = BuildFileTest.buildfile('grandparent/parent/child2/child3/BUILD')
    self.assertEquals(OrderedSet([buildfile]), buildfile.family())

  def testAncestors(self):
    self.assertEquals(OrderedSet([
        BuildFileTest.buildfile('BUILD'),
        BuildFileTest.buildfile('BUILD.twitter'),
    ]), self.buildfile.ancestors())

  def testDescendants(self):
    self.assertEquals(OrderedSet([
        BuildFileTest.buildfile('grandparent/parent/child1/BUILD'),
        BuildFileTest.buildfile('grandparent/parent/child1/BUILD.twitter'),
        BuildFileTest.buildfile('grandparent/parent/child2/child3/BUILD'),
    ]), self.buildfile.descendants())

########NEW FILE########
__FILENAME__ = test_build_invalidator
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import hashlib
import tempfile

from contextlib import contextmanager

from twitter.common.contextutil import temporary_dir
from twitter.pants.base.build_invalidator import BuildInvalidator, CacheKeyGenerator


TEST_CONTENT = 'muppet'


def expected_hash(tf):
  return hashlib.sha1(os.path.basename(tf.name) + TEST_CONTENT).hexdigest()


@contextmanager
def test_env(content=TEST_CONTENT):
  with temporary_dir() as d:
    with tempfile.NamedTemporaryFile() as f:
      f.write(content)
      f.flush()
      yield f, CacheKeyGenerator(), BuildInvalidator(d)


def test_cache_key_hash():
  with test_env() as (f, keygen, cache):
    key = keygen.key_for('test', [f.name])
    assert key.hash == expected_hash(f)


def test_needs_update_missing_key():
  with test_env() as (f, keygen, cache):
    key = keygen.key_for('test', [f.name])
    assert cache.needs_update(key)


def test_needs_update_after_change():
  with test_env() as (f, keygen, cache):
    key = keygen.key_for('test', [f.name])
    assert cache.needs_update(key)
    cache.update(key)
    assert not cache.needs_update(key)
    f.truncate()
    f.write('elmo')
    f.flush()
    key = keygen.key_for('test', [f.name])
    assert cache.needs_update(key)
    cache.update(key)
    assert not cache.needs_update(key)

########NEW FILE########
__FILENAME__ = test_build_root
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

import unittest

from twitter.common.contextutil import environment_as, pushd, temporary_dir
from twitter.common.dirutil import safe_mkdir, safe_mkdtemp, safe_rmtree, touch

from twitter.pants.base.build_root import BuildRoot


class BuildRootTest(unittest.TestCase):

  def setUp(self):
    self.original_root = BuildRoot().path
    self.new_root = os.path.realpath(safe_mkdtemp())
    BuildRoot().reset()

  def tearDown(self):
    BuildRoot().reset()
    safe_rmtree(self.new_root)

  def test_via_env(self):
    with environment_as(PANTS_BUILD_ROOT=self.new_root):
      self.assertEqual(self.new_root, BuildRoot().path)

  def test_via_set(self):
    BuildRoot().path = self.new_root
    self.assertEqual(self.new_root, BuildRoot().path)

  def test_reset(self):
    BuildRoot().path = self.new_root
    BuildRoot().reset()
    self.assertEqual(self.original_root, BuildRoot().path)

  def test_via_pantsini(self):
    with temporary_dir() as root:
      root = os.path.realpath(root)
      touch(os.path.join(root, 'pants.ini'))
      with pushd(root):
        self.assertEqual(root, BuildRoot().path)

      BuildRoot().reset()
      child = os.path.join(root, 'one', 'two')
      safe_mkdir(child)
      with pushd(child):
        self.assertEqual(root, BuildRoot().path)

  def test_temporary(self):
    with BuildRoot().temporary(self.new_root):
      self.assertEqual(self.new_root, BuildRoot().path)
    self.assertEqual(self.original_root, BuildRoot().path)

  def test_singleton(self):
    self.assertEqual(BuildRoot().path, BuildRoot().path)
    BuildRoot().path = self.new_root
    self.assertEqual(BuildRoot().path, BuildRoot().path)

########NEW FILE########
__FILENAME__ = test_double_dag
from twitter.pants.base.double_dag import DoubleDag
from twitter.pants.reporting.report import Report
from twitter.pants.testutils import MockLogger, MockTarget
from twitter.pants.testutils.base_mock_target_test import BaseMockTargetTest


def make_dag(nodes):
  return DoubleDag(nodes, lambda t: t.dependencies, MockLogger(Report.INFO))


class DoubleDagTest(BaseMockTargetTest):

  def check_dag_node(self, dag, data, children, parents):
    node = dag.lookup(data)

    self.assertEquals(node.data, data)
    self.assertEquals(node.children, set(map(dag.lookup, children)))
    self.assertEquals(node.parents, set(map(dag.lookup, parents)))

  def test_simple_dag(self):
    a = MockTarget('a')
    b = MockTarget('b', [a])
    c = MockTarget('c', [b])
    d = MockTarget('d', [c, a])
    e = MockTarget('e', [d])

    def test_dag(dag):
      self.assertEquals(dag._roots, set([dag.lookup(e)]))
      self.assertEquals(dag.leaves, set([dag.lookup(a)]))

      self.check_dag_node(dag, e, [d], [])
      self.check_dag_node(dag, d, [a, c], [e])
      self.check_dag_node(dag, c, [b], [d])
      self.check_dag_node(dag, b, [a], [c])
      self.check_dag_node(dag, a, [], [b, d])

    test_dag(make_dag([e, d, c, b, a]))
    test_dag(make_dag([a, b, c, d, e]))
    test_dag(make_dag([a, b, e, d, c]))
    test_dag(make_dag([d, a, c, e, b]))

  def test_binary_search_dag(self):

    rrr = MockTarget('rrr')
    rrl = MockTarget('rrl')
    rlr = MockTarget('rlr')
    rll = MockTarget('rll')
    lrr = MockTarget('lrr')
    lrl = MockTarget('lrl')
    llr = MockTarget('llr')
    lll = MockTarget('lll')

    rr = MockTarget('rr', [rrr, rrl])
    rl = MockTarget('rl', [rlr, rll])
    lr = MockTarget('lr', [lrr, lrl])
    ll = MockTarget('ll', [llr, lll])

    r = MockTarget('r', [rr, rl])
    l = MockTarget('l', [lr, ll])

    root = MockTarget('root', [r, l])

    def test_dag(dag):

      def t(n):
        return dag.lookup(n)

      self.assertEquals(dag._roots, set([t(root)]))
      self.assertEquals(dag.leaves, set(map(t, [rrr, rrl, rlr, rll, lrr, lrl, llr, lll])))

      self.check_dag_node(dag, root, [r, l], [])

      self.check_dag_node(dag, r, [rl, rr], [root])
      self.check_dag_node(dag, l, [ll, lr], [root])

      self.check_dag_node(dag, rr, [rrl, rrr], [r])
      self.check_dag_node(dag, rl, [rll, rlr], [r])
      self.check_dag_node(dag, lr, [lrl, lrr], [l])
      self.check_dag_node(dag, ll, [lll, llr], [l])

      self.check_dag_node(dag, rrr, [], [rr])
      self.check_dag_node(dag, rrl, [], [rr])
      self.check_dag_node(dag, rlr, [], [rl])
      self.check_dag_node(dag, rll, [], [rl])
      self.check_dag_node(dag, lrr, [], [lr])
      self.check_dag_node(dag, lrl, [], [lr])
      self.check_dag_node(dag, llr, [], [ll])
      self.check_dag_node(dag, lll, [], [ll])

    # Test in order
    test_dag(make_dag([root, r, l, rr, rl, lr, ll, rrr, rrl, rlr, rll, lrr, lrl, llr, lll]))

    # Test a couple of randomly chosen orders
    test_dag(make_dag([lrl, r, root, rl, rrr, rll, lr, lrr, ll, lll, l, rr, rrl, rlr, llr]))
    test_dag(make_dag([ll, rrl, lrl, rl, rlr, lr, root, rrr, rll, r, llr, rr, lrr, l, lll]))
    test_dag(make_dag([rr, rlr, rl, rrr, rrl, l, root, lr, lrr, llr, r, rll, lrl, ll, lll]))
    test_dag(make_dag([l, lll, rrr, rll, ll, lrl, llr, rl, root, r, lr, rlr, rr, lrr, rrl]))

  def test_diamond_in_different_orders(self):
    a = MockTarget('a')
    b = MockTarget('b', [a])
    c = MockTarget('c', [a])
    d = MockTarget('d', [c, b])

    def test_diamond_dag(dag):
      self.assertEquals(dag._roots, set([dag.lookup(d)]))
      self.assertEquals(dag.leaves, set([dag.lookup(a)]))
      self.check_dag_node(dag, d, [b, c], [])
      self.check_dag_node(dag, c, [a], [d])
      self.check_dag_node(dag, b, [a], [d])
      self.check_dag_node(dag, a, [], [b, c])

    test_diamond_dag(make_dag([a, b, c, d]))
    test_diamond_dag(make_dag([d, c, b, a]))
    test_diamond_dag(make_dag([b, d, a, c]))

  def test_find_children_across_unused_target(self):
    a = MockTarget('a')
    b = MockTarget('b', [a])
    c = MockTarget('c', [b])
    d = MockTarget('d', [c, a])
    e = MockTarget('e', [d])


########NEW FILE########
__FILENAME__ = test_generator
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.generator import TemplateData

class TemplateDataTest(unittest.TestCase):
  def setUp(self):
    self.data = TemplateData(foo = 'bar', baz = 42)

  def test_member_access(self):
    try:
      self.data.bip
      self.fail("Access to undefined template data slots should raise")
    except AttributeError:
      # expected
      pass

  def test_member_mutation(self):
    try:
      self.data.baz = 1 / 137
      self.fail("Mutation of a template data's slots should not be allowed")
    except AttributeError:
      # expected
      pass

  def test_extend(self):
    self.assertEqual(self.data.extend(jake = 0.3), TemplateData(baz = 42, foo = 'bar', jake = 0.3))

  def test_equals(self):
    self.assertEqual(self.data, TemplateData(baz = 42).extend(foo = 'bar'))

########NEW FILE########
__FILENAME__ = test_hash_utils
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import mox

from twitter.common.contextutil import temporary_file

from twitter.pants.base.hash_utils import hash_all, hash_file


class TestHashUtils(mox.MoxTestBase):

  def setUp(self):
    super(TestHashUtils, self).setUp()
    self.digest = self.mox.CreateMockAnything()

  def test_hash_all(self):
    self.digest.update('jake')
    self.digest.update('jones')
    self.digest.hexdigest().AndReturn('42')
    self.mox.ReplayAll()

    self.assertEqual('42', hash_all(['jake', 'jones'], digest=self.digest))

  def test_hash_file(self):
    self.digest.update('jake jones')
    self.digest.hexdigest().AndReturn('1137')
    self.mox.ReplayAll()

    with temporary_file() as fd:
      fd.write('jake jones')
      fd.close()

      self.assertEqual('1137', hash_file(fd.name, digest=self.digest))


########NEW FILE########
__FILENAME__ = test_parse_context
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pytest

from textwrap import dedent

from twitter.common.contextutil import temporary_dir
from twitter.common.dirutil import safe_mkdir

from twitter.pants.base.address import Address
from twitter.pants.base.build_file import BuildFile
from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target


def create_buildfile(root_dir, relpath, name='BUILD', content=''):
  path = os.path.join(root_dir, relpath)
  safe_mkdir(path)
  buildfile = os.path.join(path, name)
  with open(buildfile, 'a') as f:
    f.write(content)
  return BuildFile(root_dir, relpath)


class ParseContextTest(BaseBuildRootTest):
  def test_locate(self):
    with pytest.raises(ParseContext.ContextError):
      ParseContext.locate()

    with temporary_dir() as root_dir:
      a_context = ParseContext(create_buildfile(root_dir, 'a'))
      b_context = ParseContext(create_buildfile(root_dir, 'b'))

      def test_in_a():
        self.assertEquals(a_context, ParseContext.locate())
        return b_context.do_in_context(lambda: ParseContext.locate())

      self.assertEquals(b_context, a_context.do_in_context(test_in_a))

  def test_parse(self):
    with temporary_dir() as root_dir:
      buildfile = create_buildfile(root_dir, 'a',
        content=dedent("""
          with open('%s/a/b', 'w') as b:
            b.write('jack spratt')
        """ % root_dir).strip()
      )
      b_file = os.path.join(root_dir, 'a', 'b')
      self.assertFalse(os.path.exists(b_file))
      ParseContext(buildfile).parse()
      with open(b_file, 'r') as b:
        self.assertEquals('jack spratt', b.read())

  def test_on_context_exit(self):
    with temporary_dir() as root_dir:
      parse_context = ParseContext(create_buildfile(root_dir, 'a'))
      with pytest.raises(parse_context.ContextError):
        parse_context.on_context_exit(lambda: 37)

    with temporary_dir() as root_dir:
      buildfile = create_buildfile(root_dir, 'a',
        content=dedent("""
          import os
          from twitter.pants.base.parse_context import ParseContext
          def leave_a_trail(file, contents=''):
            with open(file, 'w') as b:
              b.write(contents)
          b_file = os.path.join(os.path.dirname(__file__), 'b')
          ParseContext.locate().on_context_exit(leave_a_trail, b_file, contents='42')
          assert not os.path.exists(b_file), 'Expected context exit action to be delayed.'
        """).strip()
      )
      b_file = os.path.join(root_dir, 'a', 'b')
      self.assertFalse(os.path.exists(b_file))
      ParseContext(buildfile).parse()
      with open(b_file, 'r') as b:
        self.assertEquals('42', b.read())

  def test_sibling_references(self):
    with temporary_dir() as root_dir:
      buildfile = create_buildfile(root_dir, 'a', name='BUILD',
        content=dedent("""
          dependencies(name='util',
            dependencies=[
              jar(org='com.twitter', name='util', rev='0.0.1')
            ]
          )
        """).strip()
      )
      sibling = create_buildfile(root_dir, 'a', name='BUILD.sibling',
        content=dedent("""
          dependencies(name='util-ex',
            dependencies=[
              pants(':util'),
              jar(org='com.twitter', name='util-ex', rev='0.0.1')
            ]
          )
        """).strip()
      )
      ParseContext(buildfile).parse()

      utilex = Target.get(Address.parse(root_dir, 'a:util-ex', is_relative=False))
      utilex_deps = set(utilex.resolve())

      util = Target.get(Address.parse(root_dir, 'a:util', is_relative=False))
      util_deps = set(util.resolve())

      self.assertEquals(util_deps, util_deps.intersection(utilex_deps))

########NEW FILE########
__FILENAME__ = test_revision
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
import unittest

from twitter.pants.base.revision import Revision


class RevisionTest(unittest.TestCase):
  def assertComponents(self, revision, *expected):
    self.assertEqual(list(expected), revision.components)


class SemverTest(RevisionTest):
  def test_bad(self):
    for bad_rev in ('a.b.c', '1.b.c', '1.2.c', '1.2.3;4', '1.2.3;4+5'):
      with pytest.raises(Revision.BadRevision):
        Revision.semver(bad_rev)

  def test_simple(self):
    self.assertEqual(Revision.semver('1.2.3'), Revision.semver('1.2.3'))
    self.assertComponents(Revision.semver('1.2.3'), 1, 2, 3, None, None)

    self.assertTrue(Revision.semver('1.2.3') > Revision.semver('1.2.2'))
    self.assertTrue(Revision.semver('1.3.0') > Revision.semver('1.2.2'))
    self.assertTrue(Revision.semver('1.3.10') > Revision.semver('1.3.2'))
    self.assertTrue(Revision.semver('2.0.0') > Revision.semver('1.3.2'))

  def test_pre_release(self):
    self.assertEqual(Revision.semver('1.2.3-pre1.release.1'),
                     Revision.semver('1.2.3-pre1.release.1'))
    self.assertComponents(Revision.semver('1.2.3-pre1.release.1'),
                          1, 2, 3, 'pre1', 'release', 1, None)

    self.assertTrue(
      Revision.semver('1.2.3-pre1.release.1') < Revision.semver('1.2.3-pre2.release.1'))
    self.assertTrue(
      Revision.semver('1.2.3-pre1.release.2') < Revision.semver('1.2.3-pre1.release.10'))

    self.assertTrue(Revision.semver('1.2.3') < Revision.semver('1.2.3-pre2.release.1'))

  def test_build(self):
    self.assertEqual(Revision.semver('1.2.3+pre1.release.1'),
                     Revision.semver('1.2.3+pre1.release.1'))
    self.assertComponents(Revision.semver('1.2.3+pre1.release.1'),
                          1, 2, 3, None, 'pre1', 'release', 1)

    self.assertTrue(
      Revision.semver('1.2.3+pre1.release.1') < Revision.semver('1.2.3+pre2.release.1'))
    self.assertTrue(
      Revision.semver('1.2.3+pre1.release.2') < Revision.semver('1.2.3+pre1.release.10'))

    self.assertTrue(Revision.semver('1.2.3') < Revision.semver('1.2.3+pre2.release.1'))
    self.assertTrue(
      Revision.semver('1.2.3+pre1.release.2') < Revision.semver('1.2.3-pre1.release.2'))

  def test_pre_release_build(self):
    self.assertEqual(Revision.semver('1.2.3-pre1.release.1+1'),
                     Revision.semver('1.2.3-pre1.release.1+1'))
    self.assertComponents(Revision.semver('1.2.3-pre1.release.1+1'),
                          1, 2, 3, 'pre1', 'release', 1, 1)

    self.assertTrue(
      Revision.semver('1.2.3-pre1.release.1') < Revision.semver('1.2.3-pre2.release.1+1'))
    self.assertTrue(
      Revision.semver('1.2.3-pre1.release.2') > Revision.semver('1.2.3-pre1.release.1+1'))

    self.assertTrue(Revision.semver('1.2.3') < Revision.semver('1.2.3-pre2.release.2+1.foo'))
    self.assertTrue(
      Revision.semver('1.2.3-pre1.release.2+1') < Revision.semver('1.2.3-pre1.release.2+1.foo'))
    self.assertTrue(
      Revision.semver('1.2.3-pre1.release.2+1') < Revision.semver('1.2.3-pre1.release.2+2'))


class LenientTest(RevisionTest):
  def test(self):
    self.assertComponents(Revision.lenient('1.2.3'), 1, 2, 3)
    self.assertComponents(Revision.lenient('1.2.3-SNAPSHOT-eabc'), 1, 2, 3, 'SNAPSHOT', 'eabc')
    self.assertComponents(Revision.lenient('1.2.3-SNAPSHOT4'), 1, 2, 3, 'SNAPSHOT', 4)

    self.assertTrue(Revision.lenient('a') < Revision.lenient('b'))
    self.assertTrue(Revision.lenient('1') < Revision.lenient('2'))
    self.assertTrue(Revision.lenient('1') < Revision.lenient('a'))

    self.assertEqual(Revision.lenient('1.2.3'), Revision.lenient('1.2.3'))
    self.assertTrue(Revision.lenient('1.2.3') < Revision.lenient('1.2.3-SNAPSHOT'))
    self.assertTrue(Revision.lenient('1.2.3-SNAPSHOT') < Revision.lenient('1.2.3-SNAPSHOT-abc'))
    self.assertTrue(Revision.lenient('1.2.3-SNAPSHOT-abc') < Revision.lenient('1.2.3-SNAPSHOT-bcd'))
    self.assertTrue(
      Revision.lenient('1.2.3-SNAPSHOT-abc6') < Revision.lenient('1.2.3-SNAPSHOT-abc10'))

########NEW FILE########
__FILENAME__ = test_run_info
import unittest2 as unittest

from twitter.common.contextutil import temporary_file_path
from twitter.pants.base.run_info import RunInfo


class RunInfoTest(unittest.TestCase):
  def test_run_info_read(self):
    with temporary_file_path() as tmppath:
      with open(tmppath, 'w') as tmpfile:
        tmpfile.write('foo:bar\n baz :qux quux')
      ri = RunInfo(tmppath)
      self.assertEquals(ri.path(), tmppath)

      # Test get_info access.
      self.assertEquals(ri.get_info('foo'), 'bar')
      self.assertEquals(ri.get_info('baz'), 'qux quux')
      self.assertIsNone(ri.get_info('nonexistent'))

      # Test dict-like access.
      self.assertEquals(ri['foo'], 'bar')
      self.assertEquals(ri['baz'], 'qux quux')

  def test_write_run_info(self):
    with temporary_file_path() as tmppath:
      ri = RunInfo(tmppath)
      ri.add_info('key1', 'val1')
      ri.add_infos(('key2', ' val2'), (' key3 ', 'val3 '))
      self.assertEquals({'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}, ri.get_as_dict())

      with open(tmppath, 'r') as tmpfile:
        contents = tmpfile.read()
      self.assertEquals('key1: val1\nkey2: val2\nkey3: val3\n', contents)

########NEW FILE########
__FILENAME__ = base_build_root_test
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import unittest

from tempfile import mkdtemp

from twitter.common.dirutil import safe_open, safe_rmtree, safe_mkdir

from twitter.pants.base.build_root import BuildRoot
from twitter.pants.base.address import Address
from twitter.pants.base.target import Target
from twitter.pants.targets.sources import SourceRoot


class BaseBuildRootTest(unittest.TestCase):
  """A baseclass useful for tests requiring a temporary buildroot."""

  build_root = None

  @classmethod
  def build_path(cls, relpath):
    """Returns the canonical BUILD file path for the given relative build path."""
    if os.path.basename(relpath).startswith('BUILD'):
      return relpath
    else:
      return os.path.join(relpath, 'BUILD')

  @classmethod
  def create_dir(cls, relpath):
    """Creates a directory under the buildroot.

    relpath: The relative path to the directory from the build root.
    """
    safe_mkdir(os.path.join(cls.build_root, relpath))

  @classmethod
  def create_file(cls, relpath, contents='', mode='w'):
    """Writes to a file under the buildroot.

    relpath:  The relative path to the file from the build root.
    contents: A string containing the contents of the file - '' by default..
    mode:     The mode to write to the file in - over-write by default.
    """
    with safe_open(os.path.join(cls.build_root, relpath), mode=mode) as fp:
      fp.write(contents)

  @classmethod
  def create_target(cls, relpath, target):
    """Adds the given target specification to the BUILD file at relpath.

    relpath: The relative path to the BUILD file from the build root.
    target:  A string containing the target definition as it would appear in a BUILD file.
    """
    cls.create_file(cls.build_path(relpath), target, mode='a')

  @classmethod
  def setUpClass(cls):
    cls.build_root = mkdtemp(suffix='_BUILD_ROOT')
    BuildRoot().path = cls.build_root
    cls.create_file('pants.ini')
    Target._clear_all_addresses()

  @classmethod
  def tearDownClass(cls):
    BuildRoot().reset()
    SourceRoot.reset()
    safe_rmtree(cls.build_root)

  @classmethod
  def target(cls, address):
    """Resolves the given target address to a Target object.

    address: The BUILD target address to resolve.

    Returns the corresponding Target or else None if the address does not point to a defined Target.
    """
    return Target.get(Address.parse(cls.build_root, address, is_relative=False))

########NEW FILE########
__FILENAME__ = test_artifact_cache
import SimpleHTTPServer
import SocketServer
import os
from threading import Thread
import unittest

from twitter.common.contextutil import pushd, temporary_dir, temporary_file
from twitter.common.dirutil import safe_mkdir
from twitter.pants.base.build_invalidator import CacheKey
from twitter.pants.cache.cache_setup import create_artifact_cache, select_best_url
from twitter.pants.cache.combined_artifact_cache import CombinedArtifactCache
from twitter.pants.cache.local_artifact_cache import LocalArtifactCache
from twitter.pants.cache.restful_artifact_cache import RESTfulArtifactCache
from twitter.pants.testutils import MockLogger


class MockPinger(object):
  def __init__(self, hosts_to_times):
    self._hosts_to_times = hosts_to_times
  # Returns a fake ping time such that the last host is always the 'fastest'.
  def pings(self, hosts):
    return map(lambda host: (host, self._hosts_to_times.get(host, 9999)), hosts)


# A very trivial server that serves files under the cwd.
class SimpleRESTHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):
  def __init__(self, request, client_address, server):
    # The base class implements GET and HEAD.
    SimpleHTTPServer.SimpleHTTPRequestHandler.__init__(self, request, client_address, server)

  def do_HEAD(self):
    return SimpleHTTPServer.SimpleHTTPRequestHandler.do_HEAD(self)

  def do_PUT(self):
    path = self.translate_path(self.path)
    content_length = int(self.headers.getheader('content-length'))
    content = self.rfile.read(content_length)
    safe_mkdir(os.path.dirname(path))
    with open(path, 'wb') as outfile:
      outfile.write(content)
    self.send_response(200)
    self.end_headers()

  def do_DELETE(self):
    path = self.translate_path(self.path)
    if os.path.exists(path):
      os.unlink(path)
      self.send_response(200)
    else:
      self.send_error(404, 'File not found')
    self.end_headers()


TEST_CONTENT1 = 'muppet'
TEST_CONTENT2 = 'kermit'


class TestArtifactCache(unittest.TestCase):
  def test_select_best_url(self):
    spec = 'http://host1|https://host2:666/path/to|http://host3/path/'
    best = select_best_url(spec, MockPinger({'host1':  5, 'host2:666': 3, 'host3': 7}), MockLogger())
    self.assertEquals('https://host2:666/path/to', best)

  def test_cache_spec_parsing(self):
    artifact_root = '/bogus/artifact/root'

    def check(expected_type, spec):
      cache = create_artifact_cache(MockLogger(), artifact_root, spec, 'TestTask', 'testing')
      self.assertTrue(isinstance(cache, expected_type))
      self.assertEquals(cache.artifact_root, artifact_root)

    with temporary_dir() as tmpdir:
      cachedir = os.path.join(tmpdir, 'cachedir')  # Must be a real path, so we can safe_mkdir it.
      check(LocalArtifactCache, cachedir)
      check(RESTfulArtifactCache, 'http://localhost/bar')
      check(CombinedArtifactCache, [cachedir, 'http://localhost/bar'])


  def test_local_cache(self):
    with temporary_dir() as artifact_root:
      with temporary_dir() as cache_root:
        artifact_cache = LocalArtifactCache(None, artifact_root, cache_root)
        self.do_test_artifact_cache(artifact_cache)


  def test_restful_cache(self):
    httpd = None
    httpd_thread = None
    try:
      with temporary_dir() as cache_root:
        with pushd(cache_root):  # SimpleRESTHandler serves from the cwd.
          httpd = SocketServer.TCPServer(('localhost', 0), SimpleRESTHandler)
          port = httpd.server_address[1]
          httpd_thread = Thread(target=httpd.serve_forever)
          httpd_thread.start()
          with temporary_dir() as artifact_root:
            artifact_cache = RESTfulArtifactCache(MockLogger(), artifact_root,
                                                  'http://localhost:%d' % port)
            self.do_test_artifact_cache(artifact_cache)
    finally:
      if httpd:
        httpd.shutdown()
      if httpd_thread:
        httpd_thread.join()


  def do_test_artifact_cache(self, artifact_cache):
    key = CacheKey('muppet_key', 'fake_hash', 42, [])
    with temporary_file(artifact_cache.artifact_root) as f:
      # Write the file.
      f.write(TEST_CONTENT1)
      path = f.name
      f.close()

      # Cache it.
      self.assertFalse(artifact_cache.has(key))
      self.assertFalse(bool(artifact_cache.use_cached_files(key)))
      artifact_cache.insert(key, [path])
      self.assertTrue(artifact_cache.has(key))

      # Stomp it.
      with open(path, 'w') as outfile:
        outfile.write(TEST_CONTENT2)

      # Recover it from the cache.
      self.assertTrue(bool(artifact_cache.use_cached_files(key)))

      # Check that it was recovered correctly.
      with open(path, 'r') as infile:
        content = infile.read()
      self.assertEquals(content, TEST_CONTENT1)

      # Delete it.
      artifact_cache.delete(key)
      self.assertFalse(artifact_cache.has(key))

########NEW FILE########
__FILENAME__ = test_goal
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.commands.goal import Goal, GoalError

class GoalTest(unittest.TestCase):
  def test_parse_args(self):
    def assert_result(goals, specs, args):
      g, s = Goal.parse_args(args)
      self.assertEquals((goals, specs), (list(g), list(s)))

    assert_result(goals=[], specs=[], args=[])
    assert_result(goals=[], specs=[], args=['--'])
    assert_result(goals=[], specs=[], args=['-v', '--help'])

    assert_result(goals=['compile'], specs=[], args=['compile', '--log'])
    assert_result(goals=['compile', 'test'], specs=[], args=['compile', 'test'])
    assert_result(goals=['compile', 'test'], specs=[], args=['compile', '-v', 'test'])

    assert_result(goals=[], specs=['resolve'], args=['--', 'resolve', '--ivy-open'])
    assert_result(goals=['test'], specs=['resolve'], args=['test', '--', 'resolve', '--ivy-open'])

    try:
      Goal.parse_args(['test', 'lib:all', '--', 'resolve'])
      self.fail('Expected mixed specs and goals to the left of an explicit '
                'multi-goal sep (--) to be rejected.')
    except GoalError:
      pass # expected

    try:
      Goal.parse_args(['resolve', 'lib/all', 'test', '--'])
      self.fail('Expected mixed specs and goals to the left of an explicit '
                'multi-goal sep (--) to be rejected.')
    except GoalError:
      pass # expected

    assert_result(goals=['test'], specs=['lib:all'], args=['lib:all', '-v', 'test'])
    assert_result(goals=['test'], specs=['lib/'], args=['-v', 'test', 'lib/'])
    assert_result(goals=['test'], specs=['lib/io:sound'], args=['test', '-v', 'lib/io:sound'])
    assert_result(goals=['test'], specs=['lib:all'], args=['-h', 'test', '-v', 'lib:all', '-x'])


########NEW FILE########
__FILENAME__ = base_engine_test
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.goal import Goal, Phase


class EngineTestBase(unittest.TestCase):

  @classmethod
  def _namespace(cls, identifier):
    return '__%s.%s__%s__' % (cls.__module__, cls.__name__, identifier)

  @classmethod
  def as_phase(cls, phase_name):
    """Returns a ``Phase`` object of the given name"""
    return Phase(cls._namespace(phase_name))

  @classmethod
  def as_phases(cls, *phase_names):
    """Converts the given phase names to a list of ``Phase`` objects."""
    return map(cls.as_phase, phase_names)

  @classmethod
  def installed_goal(cls, name, action=None, group=None, dependencies=None, phase=None):
    """Creates and installs a goal with the given name.

    :param string name: The goal name.
    :param action: The goal's action.
    :param group: The goal's group if it belongs to one.
    :param list dependencies: The list of phase names the goal depends on, if any.
    :param string phase: The name of the phase to install the goal in if different from the goal
      name.
    :returns The installed ``Goal`` object.
    """
    goal = Goal(cls._namespace(name),
                action=action or (lambda: None),
                group=group,
                dependencies=map(cls._namespace, dependencies or []))
    goal.install(cls._namespace(phase) if phase is not None else None)
    return goal

########NEW FILE########
__FILENAME__ = test_engine
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import mox

from twitter.pants.engine.engine import Engine, Timer
from twitter.pants.tasks import TaskError

from ..base.context_utils import create_context
from .base_engine_test import EngineTestBase


class TimerTest(mox.MoxTestBase, EngineTestBase):
  def setUp(self):
    super(TimerTest, self).setUp()
    self.ticker = self.mox.CreateMockAnything()

  def test_begin(self):
    self.ticker().AndReturn(0)  # start timer
    self.ticker().AndReturn(11)  # start timed goal_succeed #1
    self.ticker().AndReturn(13)  # finish timed goal_succeed #1
    self.ticker().AndReturn(17)  # start timed goal_succeed #2
    self.ticker().AndReturn(23)  # finish timed goal_succeed #2
    self.ticker().AndReturn(29)  # start timed goal_fail #1
    self.ticker().AndReturn(42)  # finish timed goal_fail #1
    self.ticker().AndReturn(42)  # start timed goal_muddle #1
    self.ticker().AndReturn(42)  # finish timed goal_muddle #1
    self.ticker().AndReturn(42)  # finish timer
    self.mox.ReplayAll()

    goal_succeed = self.installed_goal('succeed', phase='first')
    goal_fail = self.installed_goal('fail', phase='first')
    goal_muddle = self.installed_goal('muddle', phase='second')

    with Timer.begin(self.ticker) as timer:
      with timer.timed(goal_succeed):
        pass
      with timer.timed(goal_succeed):
        pass
      with timer.timed(goal_fail):
        pass
      with timer.timed(goal_muddle):
        pass

    self.assertEqual(42, timer.elapsed)

    first_timings = timer.timings.pop(self.as_phase('first'))
    second_timings = timer.timings.pop(self.as_phase('second'))
    self.assertEqual(0, len(timer.timings))

    goal_succeed_timings = first_timings.pop(goal_succeed)
    goal_fail_timings = first_timings.pop(goal_fail)
    self.assertEqual(0, len(first_timings))
    self.assertEqual([2, 6], goal_succeed_timings)
    self.assertEqual([13], goal_fail_timings)

    goal_muddle_timings = second_timings.pop(goal_muddle)
    self.assertEqual(0, len(second_timings))
    self.assertEqual([0], goal_muddle_timings)


class ExecutionOrderTest(EngineTestBase):
  def test_execution_order(self):
    self.installed_goal('invalidate')
    self.installed_goal('clean-all', dependencies=['invalidate'])

    self.installed_goal('resolve')
    self.installed_goal('javac', dependencies=['resolve'], phase='compile')
    self.installed_goal('scalac', dependencies=['resolve'], phase='compile')
    self.installed_goal('junit', dependencies=['compile'], phase='test')

    self.assertEqual(self.as_phases('invalidate', 'clean-all', 'resolve', 'compile', 'test'),
                     list(Engine.execution_order(self.as_phases('clean-all', 'test'))))

    self.assertEqual(self.as_phases('resolve', 'compile', 'test', 'invalidate', 'clean-all'),
                     list(Engine.execution_order(self.as_phases('test', 'clean-all'))))


class EngineTest(EngineTestBase):
  class RecordingEngine(Engine):
    def __init__(self, action=None):
      super(EngineTest.RecordingEngine, self).__init__(print_timing=False)
      self._action = action
      self._attempts = []

    @property
    def attempts(self):
      return self._attempts

    def attempt(self, timer, context, phases):
      self._attempts.append((timer, context, phases))
      if self._action:
        self._action()

  def setUp(self):
    self.context = create_context()

  def assert_attempt(self, engine, *phase_names):
    self.assertEqual(1, len(engine.attempts))

    timer, context, phases = engine.attempts[0]
    self.assertTrue(timer.elapsed >= 0, 'Expected timer to be finished.')
    self.assertEqual(self.context, context)
    self.assertEqual(self.as_phases(*phase_names), phases)

  def test_execute_success(self):
    engine = self.RecordingEngine()
    result = engine.execute(self.context, self.as_phases('one', 'two'))
    self.assertEqual(0, result)
    self.assert_attempt(engine, 'one', 'two')

  def _throw(self, error):
    def throw():
      raise error
    return throw

  def test_execute_raise(self):
    engine = self.RecordingEngine(action=self._throw(TaskError()))
    result = engine.execute(self.context, self.as_phases('three'))
    self.assertEqual(1, result)
    self.assert_attempt(engine, 'three')

  def test_execute_code(self):
    engine = self.RecordingEngine(action=self._throw(TaskError(exit_code=42)))
    result = engine.execute(self.context, self.as_phases('four', 'five', 'six'))
    self.assertEqual(42, result)
    self.assert_attempt(engine, 'four', 'five', 'six')

########NEW FILE########
__FILENAME__ = test_group_engine
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
import unittest

from textwrap import dedent

from twitter.pants.engine.group_engine import GroupEngine, GroupIterator, GroupMember
from twitter.pants.goal import Goal, Group
from twitter.pants.tasks import Task
from twitter.pants.tasks.check_exclusives import ExclusivesMapping

from ..base.context_utils import create_context
from ..base_build_root_test import BaseBuildRootTest
from .base_engine_test import EngineTestBase


class GroupMemberTest(unittest.TestCase):
  def test_from_goal_valid(self):
    def predicate(tgt):
      return tgt == 42

    goal = Goal('fred', action=lambda: None, group=Group('heathers', predicate))
    self.assertEqual(GroupMember('heathers', 'fred', predicate), GroupMember.from_goal(goal))

  def test_from_goal_invalid(self):
    with pytest.raises(ValueError):
      GroupMember.from_goal(Goal('fred', action=lambda: None))


class JvmTargetTest(BaseBuildRootTest):
  @classmethod
  def java_library(cls, path, name, deps=None):
    cls._library(path, 'java_library', name, deps)

  @classmethod
  def python_library(cls, path, name, deps=None):
    cls._library(path, 'python_library', name, deps)

  @classmethod
  def scala_library(cls, path, name, deps=None):
    cls._library(path, 'scala_library', name, deps)

  @classmethod
  def _library(cls, path, target_type, name, deps=None):
    cls.create_target(path, dedent('''
      %(target_type)s(name='%(name)s',
        dependencies=[%(deps)s],
        sources=[],
      )
    ''' % dict(target_type=target_type,
               name=name,
               deps=','.join('pants("%s")' % d for d in (deps or [])))))

  @classmethod
  def targets(cls, *addresses):
    return map(cls.target, addresses)


class GroupIteratorTestBase(JvmTargetTest):
  def setUp(self):
    super(GroupIteratorTestBase, self).setUp()

    self.red = GroupMember('colors', 'red', lambda tgt: 'red' in tgt.name)
    self.green = GroupMember('colors', 'green', lambda tgt: 'green' in tgt.name)
    self.blue = GroupMember('colors', 'blue', lambda tgt: 'blue' in tgt.name)

  def iterate(self, *addresses):
    return list(GroupIterator(self.targets(*addresses), [self.red, self.green, self.blue]))


class GroupIteratorSingleTest(GroupIteratorTestBase):
  def test(self):
    self.java_library('root', 'colorless', deps=[])
    self.java_library('root', 'a_red', deps=['root:colorless'])
    self.java_library('root', 'b_red', deps=['root:a_red'])
    self.java_library('root', 'c_red', deps=['root:a_red', 'root:colorless'])
    self.java_library('root', 'd_red', deps=['root:b_red', 'root:c_red'])

    chunks = self.iterate('root:d_red')
    self.assertEqual(1, len(chunks))

    group_member, targets = chunks[0]
    self.assertEqual(self.red, group_member)
    self.assertEqual(set(self.targets('root:d_red', 'root:b_red', 'root:c_red', 'root:a_red')),
                     set(targets))


class GroupIteratorMultipleTest(GroupIteratorTestBase):
  def test(self):
    self.java_library('root', 'colorless', deps=[])
    self.java_library('root', 'a_red', deps=['root:colorless'])
    self.java_library('root', 'a_blue', deps=['root:a_red'])
    self.java_library('root', 'a_green', deps=['root:a_blue', 'root:colorless'])
    self.java_library('root', 'b_red', deps=['root:a_blue'])
    self.java_library('root', 'c_red', deps=['root:b_red'])

    chunks = self.iterate('root:c_red', 'root:a_green')
    self.assertEqual(4, len(chunks))

    group_member, targets = chunks[0]
    self.assertEqual(self.red, group_member)
    self.assertEqual(set(self.targets('root:a_red')), set(targets))

    group_member, targets = chunks[1]
    self.assertEqual(self.blue, group_member)
    self.assertEqual(set(self.targets('root:a_blue')), set(targets))

    group_member, targets = chunks[2]
    self.assertEqual(self.green, group_member)
    self.assertEqual(set(self.targets('root:a_green')), set(targets))

    group_member, targets = chunks[3]
    self.assertEqual(self.red, group_member)
    self.assertEqual(set(self.targets('root:b_red', 'root:c_red')), set(targets))


class GroupIteratorTargetsTest(GroupIteratorTestBase):
  """Test that GroupIterator raises an exception when given non-internal targets."""

  def test_internal_targets(self):
    self.java_library('root', 'colorless', deps=[])
    self.iterate('root:colorless')

  def test_non_internal_targets(self):
    self.python_library('root2', 'colorless', deps=[])
    with pytest.raises(ValueError):
      self.iterate('root2:colorless')


class GroupEngineTest(EngineTestBase, JvmTargetTest):
  @classmethod
  def setUpClass(cls):
    super(GroupEngineTest, cls).setUpClass()

    cls.java_library('src/java', 'a')
    cls.scala_library('src/scala', 'b', deps=['src/java:a'])
    cls.java_library('src/java', 'c', deps=['src/scala:b'])
    cls.scala_library('src/scala', 'd', deps=['src/java:c'])
    cls.java_library('src/java', 'e', deps=['src/scala:d'])
    cls.python_library('src/python', 'f')

  def setUp(self):
    super(GroupEngineTest, self).setUp()

    self.context = create_context(options=dict(explain=False),
                                  target_roots=self.targets('src/java:e', 'src/python:f'))
    self.assertTrue(self.context.is_unlocked())

    # TODO(John Sirois): disentangle GroupEngine from relying upon the CheckExclusives task being
    # run.  It should either arrange this directly or else the requirement should be in a different
    # layer.
    exclusives_mapping = ExclusivesMapping(self.context)
    exclusives_mapping._populate_target_maps(self.context.targets())
    self.context.products.safe_create_data('exclusives_groups', lambda: exclusives_mapping)

    self.engine = GroupEngine(print_timing=False)
    self.recorded_actions = []

  def tearDown(self):
    self.assertTrue(self.context.is_unlocked())

  def construct_action(self, tag):
    return 'construct', tag, self.context

  def execute_action(self, tag, targets=None):
    return 'execute', tag, (targets or self.context.targets())

  def record(self, tag):
    class RecordingTask(Task):
      def __init__(me, context):
        super(RecordingTask, me).__init__(context)
        self.recorded_actions.append(self.construct_action(tag))

      def execute(me, targets):
        self.recorded_actions.append(self.execute_action(tag, targets=targets))

    return RecordingTask

  def install_goal(self, name, group=None, dependencies=None, phase=None):
    return self.installed_goal(name,
                               action=self.record(name),
                               group=group,
                               dependencies=dependencies,
                               phase=phase)

  def test_no_groups(self):
    self.install_goal('resolve')
    self.install_goal('javac', dependencies=['resolve'], phase='compile')
    self.install_goal('checkstyle', phase='compile')
    self.install_goal('resources')
    self.install_goal('test', dependencies=['compile', 'resources'])

    result = self.engine.execute(self.context, self.as_phases('test'))
    self.assertEqual(0, result)

    expected = [self.construct_action('test'),
                self.construct_action('resources'),
                self.construct_action('checkstyle'),
                self.construct_action('javac'),
                self.construct_action('resolve'),
                self.execute_action('resolve'),
                self.execute_action('javac'),
                self.execute_action('checkstyle'),
                self.execute_action('resources'),
                self.execute_action('test')]
    self.assertEqual(expected, self.recorded_actions)

  def test_groups(self):
    self.install_goal('resolve')
    self.install_goal('javac',
                      group=Group('jvm', lambda t: t.is_java),
                      dependencies=['resolve'],
                      phase='compile')
    self.install_goal('scalac',
                      group=Group('jvm', lambda t: t.is_scala),
                      dependencies=['resolve'],
                      phase='compile')
    self.install_goal('checkstyle', phase='compile')

    result = self.engine.execute(self.context, self.as_phases('compile'))
    self.assertEqual(0, result)

    expected = [self.construct_action('checkstyle'),
                self.construct_action('scalac'),
                self.construct_action('javac'),
                self.construct_action('resolve'),
                self.execute_action('resolve'),
                self.execute_action('javac', targets=self.targets('src/java:a')),
                self.execute_action('scalac', targets=self.targets('src/scala:b')),
                self.execute_action('javac', targets=self.targets('src/java:c')),
                self.execute_action('scalac', targets=self.targets('src/scala:d')),
                self.execute_action('javac', targets=self.targets('src/java:e')),
                self.execute_action('checkstyle')]
    self.assertEqual(expected, self.recorded_actions)


########NEW FILE########
__FILENAME__ = test_archive
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import unittest

from twitter.common.contextutil import temporary_dir
from twitter.common.dirutil import safe_mkdir, touch

from twitter.pants.fs.archive import TAR, TGZ, TBZ2, ZIP

class ArchiveTest(unittest.TestCase):
  def round_trip(self, archiver, empty_dirs):
    def listtree(root):
      listing = set()
      for path, dirs, files in os.walk(root):
        relpath = os.path.normpath(os.path.relpath(path, root))
        if empty_dirs:
          listing.update(os.path.normpath(os.path.join(relpath, d)) for d in dirs)
        listing.update(os.path.normpath(os.path.join(relpath, f)) for f in files)
      return listing

    def test_round_trip(prefix=None):
      with temporary_dir() as fromdir:
        safe_mkdir(os.path.join(fromdir, 'a/b/c'))
        touch(os.path.join(fromdir, 'a/b/d/e.txt'))
        with temporary_dir() as archivedir:
          archive = archiver.create(fromdir, archivedir, 'archive', prefix=prefix)
          with temporary_dir() as todir:
            archiver.extract(archive, todir)
            fromlisting = listtree(fromdir)
            if prefix:
              fromlisting = set(os.path.join(prefix, x) for x in fromlisting)
              if empty_dirs:
                fromlisting.add(prefix)
            self.assertEqual(fromlisting, listtree(todir))

    test_round_trip()
    test_round_trip(prefix='jake')

  def test_tar(self):
    self.round_trip(TAR, empty_dirs=True)
    self.round_trip(TGZ, empty_dirs=True)
    self.round_trip(TBZ2, empty_dirs=True)

  def test_zip(self):
    self.round_trip(ZIP, empty_dirs=False)

########NEW FILE########
__FILENAME__ = test_safe_filename
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pytest
import unittest

from twitter.pants.fs.fs import safe_filename


class SafeFilenameTest(unittest.TestCase):
  class FixedDigest(object):
    def __init__(self, size):
      self._size = size

    def update(self, value):
      pass

    def hexdigest(self):
      return self._size * '*'

  def test_bad_name(self):
    with pytest.raises(ValueError):
      safe_filename(os.path.join('more', 'than', 'a', 'name.game'))

  def test_noop(self):
    self.assertEqual('jack.jill', safe_filename('jack', '.jill', max_length=9))
    self.assertEqual('jack.jill', safe_filename('jack', '.jill', max_length=100))

  def test_shorten(self):
    self.assertEqual('**.jill',
                     safe_filename('jack', '.jill', digest=self.FixedDigest(2), max_length=8))

  def test_shorten_fail(self):
    with pytest.raises(ValueError):
      safe_filename('jack', '.beanstalk', digest=self.FixedDigest(3), max_length=12)

########NEW FILE########
__FILENAME__ = test_distribution
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from collections import namedtuple
from contextlib import contextmanager

import os
import subprocess
import textwrap
import unittest

import pytest

from twitter.common.collections import maybe_list
from twitter.common.contextutil import environment_as, temporary_dir
from twitter.common.dirutil import chmod_plus_x, safe_open, touch

from twitter.pants.base.revision import Revision
from twitter.pants.java.distribution import Distribution


class MockDistributionTest(unittest.TestCase):
  EXE = namedtuple('Exe', ['name', 'contents'])

  @classmethod
  def exe(cls, name, version=None):
    contents = None if not version else textwrap.dedent('''
        #!/bin/sh
        if [ $# -ne 3 ]; then
          # Sanity check a classpath switch with a value plus the classname for main
          echo "Expected 3 arguments, got $#: $@" >&2
          exit 1
        fi
        echo "java.version=%s"
      ''' % version).strip()
    return cls.EXE(name, contents=contents)

  @contextmanager
  def distribution(self, files=None, executables=None):
    with temporary_dir() as jdk:
      for f in maybe_list(files or ()):
        touch(os.path.join(jdk, f))
      for exe in maybe_list(executables or (), expected_type=self.EXE):
        path = os.path.join(jdk, exe.name)
        with safe_open(path, 'w') as fp:
          fp.write(exe.contents or '')
        chmod_plus_x(path)
      yield jdk

  def test_validate_basic(self):
    with pytest.raises(Distribution.Error):
      with self.distribution() as jdk:
        Distribution(bin_path=jdk).validate()

    with pytest.raises(Distribution.Error):
      with self.distribution(files='java') as jdk:
        Distribution(bin_path=jdk).validate()

    with self.distribution(executables=self.exe('java')) as jdk:
      Distribution(bin_path=jdk).validate()

  def test_validate_jdk(self):
    with pytest.raises(Distribution.Error):
      with self.distribution(executables=self.exe('java')) as jdk:
        Distribution(bin_path=jdk, jdk=True).validate()

    with self.distribution(executables=[self.exe('java'), self.exe('javac')]) as jdk:
      Distribution(bin_path=jdk, jdk=True).validate()

  def test_validate_version(self):
    with pytest.raises(Distribution.Error):
      with self.distribution(executables=self.exe('java', '1.7.0_25')) as jdk:
        Distribution(bin_path=jdk, minimum_version='1.7.0_45').validate()

    with self.distribution(executables=self.exe('java', '1.7.0_25')) as jdk:
      Distribution(bin_path=jdk, minimum_version='1.7.0_25').validate()
      Distribution(bin_path=jdk, minimum_version=Revision.semver('1.6.0')).validate()

  def test_validated_binary(self):
    with pytest.raises(Distribution.Error):
      with self.distribution(files='jar', executables=self.exe('java')) as jdk:
        Distribution(bin_path=jdk).binary('jar')

    with self.distribution(executables=[self.exe('java'), self.exe('jar')]) as jdk:
      Distribution(bin_path=jdk).binary('jar')

  def test_locate(self):
    @contextmanager
    def env(**kwargs):
      environment = dict(JDK_HOME=None, JAVA_HOME=None, PATH=None)
      environment.update(**kwargs)
      with environment_as(**environment):
        yield

    with pytest.raises(Distribution.Error):
      with env():
        Distribution.locate()

    with pytest.raises(Distribution.Error):
      with self.distribution(files='java') as jdk:
        with env(PATH=jdk):
          Distribution.locate()

    with pytest.raises(Distribution.Error):
      with self.distribution(executables=self.exe('java')) as jdk:
        with env(PATH=jdk):
          Distribution.locate(jdk=True)

    with pytest.raises(Distribution.Error):
      with self.distribution(executables=self.exe('java', '1.6.0')) as jdk:
        with env(PATH=jdk):
          Distribution.locate(minimum_version='1.7.0')

    with pytest.raises(Distribution.Error):
      with self.distribution(executables=self.exe('java')) as jdk:
        with env(JDK_HOME=jdk):
          Distribution.locate()

    with pytest.raises(Distribution.Error):
      with self.distribution(executables=self.exe('java')) as jdk:
        with env(JAVA_HOME=jdk):
          Distribution.locate()

    with self.distribution(executables=self.exe('java')) as jdk:
      with env(PATH=jdk):
        Distribution.locate()

    with self.distribution(executables=[self.exe('java'), self.exe('javac')]) as jdk:
      with env(PATH=jdk):
        Distribution.locate(jdk=True)

    with self.distribution(executables=self.exe('java', '1.7.0')) as jdk:
      with env(PATH=jdk):
        Distribution.locate(minimum_version='1.6.0')

    with self.distribution(executables=self.exe('bin/java')) as jdk:
      with env(JDK_HOME=jdk):
        Distribution.locate()

    with self.distribution(executables=self.exe('bin/java')) as jdk:
      with env(JAVA_HOME=jdk):
        Distribution.locate()


def exe_path(name):
  process = subprocess.Popen(['which', name], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
  stdout, _ = process.communicate()
  if process.returncode != 0:
    return None
  path = stdout.strip()
  return path if os.path.exists(path) and os.access(path, os.X_OK) else None


class LiveDistributionTest(unittest.TestCase):
  JAVA = exe_path('java')
  JAVAC = exe_path('javac')

  @pytest.mark.skipif('not LiveDistributionTest.JAVA', reason='No java executable on the PATH.')
  def test_validate_live(self):
    with pytest.raises(Distribution.Error):
      Distribution(bin_path=os.path.dirname(self.JAVA), minimum_version='999.9.9').validate()

    Distribution(bin_path=os.path.dirname(self.JAVA)).validate()
    Distribution(bin_path=os.path.dirname(self.JAVA), minimum_version='1.3.1').validate()
    Distribution.locate(jdk=False)

  @pytest.mark.skipif('not LiveDistributionTest.JAVAC', reason='No javac executable on the PATH.')
  def test_validate_live_jdk(self):
    Distribution(bin_path=os.path.dirname(self.JAVAC), jdk=True).validate()
    Distribution(bin_path=os.path.dirname(self.JAVAC), jdk=True).binary('javap')
    Distribution.locate(jdk=True)

########NEW FILE########
__FILENAME__ = test_open_jar
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import errno
import os
import pytest
import sys
import unittest

from contextlib import contextmanager

from twitter.common.contextutil import temporary_dir, temporary_file
from twitter.common.dirutil import safe_mkdir
from twitter.common.lang import Compatibility

from twitter.pants.java.jar import open_jar


class OpenJarTest(unittest.TestCase):

  @contextmanager
  def jarfile(self):
    with temporary_file() as fd:
      fd.close()
      yield fd.name

  def test_mkdirs(self):
    def assert_mkdirs(path, *entries):
      with self.jarfile() as jarfile:
        with open_jar(jarfile, 'w') as jar:
          jar.mkdirs(path)
        with open_jar(jarfile) as jar:
          self.assertEquals(list(entries), jar.namelist())

    if Compatibility.PY2 and sys.version_info[1] <= 6:
      # Empty zip files in python 2.6 or lower cannot be read normally.
      # Although BadZipFile should be raised, Apple's python 2.6.1 is sloppy and lets an IOError
      # bubble, so we check for that case explicitly.
      from zipfile import BadZipfile
      with pytest.raises(Exception) as raised_info:
        assert_mkdirs('')
      raised = raised_info.value
      self.assertTrue(isinstance(raised, (BadZipfile, IOError)))
      if isinstance(raised, IOError):
        self.assertEqual(errno.EINVAL, raised.errno)
    else:
      assert_mkdirs('')

    assert_mkdirs('a', 'a/')
    assert_mkdirs('a/b/c', 'a/', 'a/b/', 'a/b/c/')

  def test_write_dir(self):
    with temporary_dir() as chroot:
      dir = os.path.join(chroot, 'a/b/c')
      safe_mkdir(dir)
      with self.jarfile() as jarfile:
        with open_jar(jarfile, 'w') as jar:
          jar.write(dir, 'd/e')
        with open_jar(jarfile) as jar:
          self.assertEquals(['d/', 'd/e/'], jar.namelist())

  def test_write_file(self):
    with temporary_dir() as chroot:
      dir = os.path.join(chroot, 'a/b/c')
      safe_mkdir(dir)
      data_file = os.path.join(dir, 'd.txt')
      with open(data_file, 'w') as fd:
        fd.write('e')
      with self.jarfile() as jarfile:
        with open_jar(jarfile, 'w') as jar:
          jar.write(data_file, 'f/g/h')
        with open_jar(jarfile) as jar:
          self.assertEquals(['f/', 'f/g/', 'f/g/h'], jar.namelist())
          self.assertEquals('e', jar.read('f/g/h'))

  def test_writestr(self):
    def assert_writestr(path, contents, *entries):
      with self.jarfile() as jarfile:
        with open_jar(jarfile, 'w') as jar:
          jar.writestr(path, contents)
        with open_jar(jarfile) as jar:
          self.assertEquals(list(entries), jar.namelist())
          self.assertEquals(contents, jar.read(path))

    assert_writestr('a.txt', 'b', 'a.txt')
    assert_writestr('a/b/c.txt', 'd', 'a/', 'a/b/', 'a/b/c.txt')

########NEW FILE########
__FILENAME__ = test_fetcher
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from contextlib import closing

import os

import mox
import pytest
import requests

from twitter.common.contextutil import temporary_file
from twitter.common.lang import Compatibility
from twitter.common.quantity import Amount, Data, Time

from twitter.pants.net.http.fetcher import Fetcher


class FetcherTest(mox.MoxTestBase):
  def setUp(self):
    super(FetcherTest, self).setUp()

    self.requests = self.mox.CreateMockAnything()
    self.response = self.mox.CreateMock(requests.Response)
    self.fetcher = Fetcher(requests_api=self.requests)
    self.listener = self.mox.CreateMock(Fetcher.Listener)

  def expect_get(self, url, chunk_size_bytes, timeout_secs, listener=True):
    self.requests.get(url, stream=True, timeout=timeout_secs).AndReturn(self.response)
    self.response.status_code = 200
    self.response.headers = {'content-length': '11'}
    if listener:
      self.listener.status(200, content_length=11)

    chunks = ['0123456789', 'a']
    self.response.iter_content(chunk_size=chunk_size_bytes).AndReturn(chunks)
    return chunks

  def test_get(self):
    for chunk in self.expect_get('http://bar', chunk_size_bytes=1024, timeout_secs=60):
      self.listener.recv_chunk(chunk)
    self.listener.finished()
    self.response.close()

    self.mox.ReplayAll()

    self.fetcher.fetch('http://bar',
                       self.listener,
                       chunk_size=Amount(1, Data.KB),
                       timeout=Amount(1, Time.MINUTES))

  def test_checksum_listener(self):
    digest = self.mox.CreateMockAnything()
    for chunk in self.expect_get('http://baz', chunk_size_bytes=1, timeout_secs=37):
      self.listener.recv_chunk(chunk)
      digest.update(chunk)

    self.listener.finished()
    digest.hexdigest().AndReturn('42')

    self.response.close()

    self.mox.ReplayAll()

    checksum_listener = Fetcher.ChecksumListener(digest=digest)
    self.fetcher.fetch('http://baz',
                       checksum_listener.wrap(self.listener),
                       chunk_size=Amount(1, Data.BYTES),
                       timeout=Amount(37, Time.SECONDS))
    self.assertEqual('42', checksum_listener.checksum)

  def test_download_listener(self):
    downloaded = ''
    for chunk in self.expect_get('http://foo', chunk_size_bytes=1048576, timeout_secs=3600):
      self.listener.recv_chunk(chunk)
      downloaded += chunk

    self.listener.finished()
    self.response.close()

    self.mox.ReplayAll()

    with closing(Compatibility.StringIO()) as fp:
      self.fetcher.fetch('http://foo',
                         Fetcher.DownloadListener(fp).wrap(self.listener),
                         chunk_size=Amount(1, Data.MB),
                         timeout=Amount(1, Time.HOURS))
      self.assertEqual(downloaded, fp.getvalue())

  def test_size_mismatch(self):
    self.requests.get('http://foo', stream=True, timeout=60).AndReturn(self.response)
    self.response.status_code = 200
    self.response.headers = {'content-length': '11'}
    self.listener.status(200, content_length=11)

    self.response.iter_content(chunk_size=1024).AndReturn(['a', 'b'])
    self.listener.recv_chunk('a')
    self.listener.recv_chunk('b')

    self.response.close()

    self.mox.ReplayAll()

    with pytest.raises(self.fetcher.Error):
      self.fetcher.fetch('http://foo',
                         self.listener,
                         chunk_size=Amount(1, Data.KB),
                         timeout=Amount(1, Time.MINUTES))

  def test_get_error_transient(self):
    self.requests.get('http://foo', stream=True, timeout=60).AndRaise(requests.ConnectionError)

    self.mox.ReplayAll()

    with pytest.raises(self.fetcher.TransientError):
      self.fetcher.fetch('http://foo',
                         self.listener,
                         chunk_size=Amount(1, Data.KB),
                         timeout=Amount(1, Time.MINUTES))

  def test_get_error_permanent(self):
    self.requests.get('http://foo', stream=True, timeout=60).AndRaise(requests.TooManyRedirects)

    self.mox.ReplayAll()

    with pytest.raises(self.fetcher.PermanentError) as e:
      self.fetcher.fetch('http://foo',
                         self.listener,
                         chunk_size=Amount(1, Data.KB),
                         timeout=Amount(1, Time.MINUTES))
    self.assertTrue(e.value.response_code is None)

  def test_http_error(self):
    self.requests.get('http://foo', stream=True, timeout=60).AndReturn(self.response)
    self.response.status_code = 404
    self.listener.status(404)

    self.response.close()

    self.mox.ReplayAll()

    with pytest.raises(self.fetcher.PermanentError) as e:
      self.fetcher.fetch('http://foo',
                         self.listener,
                         chunk_size=Amount(1, Data.KB),
                         timeout=Amount(1, Time.MINUTES))
    self.assertEqual(404, e.value.response_code)

  def test_iter_content_error(self):
    self.requests.get('http://foo', stream=True, timeout=60).AndReturn(self.response)
    self.response.status_code = 200
    self.response.headers = {}
    self.listener.status(200, content_length=None)

    self.response.iter_content(chunk_size=1024).AndRaise(requests.Timeout)
    self.response.close()

    self.mox.ReplayAll()

    with pytest.raises(self.fetcher.TransientError):
      self.fetcher.fetch('http://foo',
                         self.listener,
                         chunk_size=Amount(1, Data.KB),
                         timeout=Amount(1, Time.MINUTES))

  def expect_download(self, path_or_fd=None):
    downloaded = ''
    for chunk in self.expect_get('http://1', chunk_size_bytes=13, timeout_secs=13, listener=False):
      downloaded += chunk
    self.response.close()

    self.mox.ReplayAll()

    path = self.fetcher.download('http://1',
                                 path_or_fd=path_or_fd,
                                 chunk_size=Amount(13, Data.BYTES),
                                 timeout=Amount(13, Time.SECONDS))
    return downloaded, path

  def test_download(self):
    downloaded, path = self.expect_download()
    try:
      with open(path) as fp:
        self.assertEqual(downloaded, fp.read())
    finally:
      os.unlink(path)

  def test_download_fd(self):
    with temporary_file() as fd:
      downloaded, path = self.expect_download(path_or_fd=fd)
      self.assertEqual(path, fd.name)
      fd.close()
      with open(path) as fp:
        self.assertEqual(downloaded, fp.read())

  def test_download_path(self):
    with temporary_file() as fd:
      fd.close()
      downloaded, path = self.expect_download(path_or_fd=fd.name)
      self.assertEqual(path, fd.name)
      with open(path) as fp:
        self.assertEqual(downloaded, fp.read())

########NEW FILE########
__FILENAME__ = test_xargs
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import errno
import os

import mox
import pytest

from twitter.pants.process.xargs import Xargs


class XargsTest(mox.MoxTestBase):
  def setUp(self):
    super(XargsTest, self).setUp()
    self.call = self.mox.CreateMockAnything()
    self.xargs = Xargs(self.call)

  def test_execute_nosplit_success(self):
    self.call(['one', 'two', 'three', 'four']).AndReturn(0)
    self.mox.ReplayAll()

    self.assertEqual(0, self.xargs.execute(['one', 'two', 'three', 'four']))

  def test_execute_nosplit_raise(self):
    exception = Exception()

    self.call(['one', 'two', 'three', 'four']).AndRaise(exception)
    self.mox.ReplayAll()

    with pytest.raises(Exception) as raised:
      self.xargs.execute(['one', 'two', 'three', 'four'])
    self.assertTrue(exception is raised.value)

  def test_execute_nosplit_fail(self):
    self.call(['one', 'two', 'three', 'four']).AndReturn(42)
    self.mox.ReplayAll()

    self.assertEqual(42, self.xargs.execute(['one', 'two', 'three', 'four']))

  TOO_BIG = OSError(errno.E2BIG, os.strerror(errno.E2BIG))

  def test_execute_split(self):
    self.call(['one', 'two', 'three', 'four']).AndRaise(self.TOO_BIG)
    self.call(['one', 'two']).AndReturn(0)
    self.call(['three', 'four']).AndReturn(0)
    self.mox.ReplayAll()

    self.assertEqual(0, self.xargs.execute(['one', 'two', 'three', 'four']))

  def test_execute_uneven(self):
    self.call(['one', 'two', 'three']).AndRaise(self.TOO_BIG)
    # TODO(John Sirois): We really don't care if the 1st call gets 1 argument or 2, we just
    # care that all arguments get passed just once via exactly 2 rounds of call - consider making
    # this test less brittle to changes in the chunking logic.
    self.call(['one']).AndReturn(0)
    self.call(['two', 'three']).AndReturn(0)
    self.mox.ReplayAll()

    self.assertEqual(0, self.xargs.execute(['one', 'two', 'three']))

  def test_execute_split_multirecurse(self):
    self.call(['one', 'two', 'three', 'four']).AndRaise(self.TOO_BIG)
    self.call(['one', 'two']).AndRaise(self.TOO_BIG)
    self.call(['one']).AndReturn(0)
    self.call(['two']).AndReturn(0)
    self.call(['three', 'four']).AndReturn(0)
    self.mox.ReplayAll()

    self.assertEqual(0, self.xargs.execute(['one', 'two', 'three', 'four']))

  def test_execute_split_fail_fast(self):
    self.call(['one', 'two', 'three', 'four']).AndRaise(self.TOO_BIG)
    self.call(['one', 'two']).AndReturn(42)
    self.mox.ReplayAll()

    self.assertEqual(42, self.xargs.execute(['one', 'two', 'three', 'four']))

  def test_execute_split_fail_slow(self):
    self.call(['one', 'two', 'three', 'four']).AndRaise(self.TOO_BIG)
    self.call(['one', 'two']).AndReturn(0)
    self.call(['three', 'four']).AndReturn(42)
    self.mox.ReplayAll()

    self.assertEqual(42, self.xargs.execute(['one', 'two', 'three', 'four']))

########NEW FILE########
__FILENAME__ = test_antlr_builder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

import antlr3
import antlr3.tree

from twitter.common.python.test.ExprLexer import ExprLexer
from twitter.common.python.test.ExprParser import ExprParser
from twitter.common.python.test.Eval import Eval

# We import this gratuitously, just to test that namespace packages work correctly in the
# generated ANTLR code. This module shares a namespace prefix with the generated
# ANTLR code, and so will be masked by it if namespace packages are broken.
from twitter.common.python.test2.csvLexer import csvLexer

class AntlrBuilderTest(unittest.TestCase):
  def test_generated_parser(self):
    """The 'test' here is the very fact that we can successfully import the generated antlr code.
    However there's no harm in also exercising it. This code is modified from the canonical example
    at http://www.antlr.org/wiki/display/ANTLR3/Example ."""
    char_stream = antlr3.ANTLRStringStream('4 + 5\n')
    lexer = ExprLexer(char_stream)
    tokens = antlr3.CommonTokenStream(lexer)
    parser = ExprParser(tokens)
    r = parser.prog()

    # this is the root of the AST
    root = r.tree

    nodes = antlr3.tree.CommonTreeNodeStream(root)
    nodes.setTokenStream(tokens)
    eval = Eval(nodes)
    eval.prog()

if __name__ == '__main__':
  unittest.main()

########NEW FILE########
__FILENAME__ = test_resolver
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.common.contextutil import temporary_file
from twitter.common.python.platforms import Platform

from twitter.pants.base.config import Config
from twitter.pants.python.resolver import get_platforms


class ResolverTest(unittest.TestCase):
  def setUp(self):
    with temporary_file() as ini:
      ini.write(
'''
[python-setup]
platforms: [
  'current',
  'linux-x86_64']
''')
      ini.close()
      self.config = Config.load(configpath=ini.name)

  def test_get_current_platform(self):
    expected_platforms = [Platform.current(), 'linux-x86_64']
    self.assertEqual(set(expected_platforms),
                     set(get_platforms(self.config.getlist('python-setup', 'platforms'))))


########NEW FILE########
__FILENAME__ = test_thrift_builder
#==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from textwrap import dedent

from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.base.context_utils import create_config
from twitter.pants.python.thrift_builder import PythonThriftBuilder
from twitter.pants.targets.python_thrift_library import PythonThriftLibrary
from twitter.pants.targets.sources import SourceRoot

from mock import call, MagicMock, mock_open, patch


sample_ini_test = """
[DEFAULT]
pants_workdir: %(buildroot)s
thrift_workdir: %(pants_workdir)s/thrift
"""


class TestPythonThriftBuilder(BaseBuildRootTest):

  @classmethod
  def setUpClass(self):
    super(TestPythonThriftBuilder, self).setUpClass()
    SourceRoot.register(os.path.realpath(os.path.join(self.build_root, 'test_thrift_replacement')),
                        PythonThriftLibrary)
    self.create_target('test_thrift_replacement', dedent('''
      python_thrift_library(name='one',
        sources=['thrift/keyword.thrift'],
        dependencies=None
      )
    '''))

  def test_keyword_replacement(self):
    m = mock_open(read_data='')
    with patch('__builtin__.open', m, create=True):
      with patch('shutil.copyfile'):
        builder = PythonThriftBuilder(target=self.target('test_thrift_replacement:one'),
                                    root_dir=self.build_root,
                                    config=create_config(sample_ini=sample_ini_test))

        builder._modify_thrift = MagicMock()
        builder._run_thrift = MagicMock()
        builder.run_thrifts()

        builder._modify_thrift.assert_called_once_with(os.path.realpath('%s/thrift/py-thrift/%s'
                                                                      % (self.build_root,
                                                                        'thrift/keyword.thrift')))

  def test_keyword_replaced(self):
    thrift_contents = dedent('''
      namespace py gen.twitter.tweetypie.tweet
      struct UrlEntity {
        1: i16 from
      }
    ''')
    expected_replaced_contents = dedent('''
      namespace py gen.twitter.tweetypie.tweet
      struct UrlEntity {
        1: i16 from_
      }
    ''')
    builder = PythonThriftBuilder(target=self.target('test_thrift_replacement:one'),
                                  root_dir=self.build_root,
                                  config=create_config(sample_ini=sample_ini_test))
    m = mock_open(read_data=thrift_contents)
    with patch('__builtin__.open', m, create=True):
      builder = PythonThriftBuilder(target=self.target('test_thrift_replacement:one'),
                                  root_dir=self.build_root,
                                  config=create_config(sample_ini=sample_ini_test))
      builder._modify_thrift('thrift_dummmy.thrift')
      expected_open_call_list = [call('thrift_dummmy.thrift'), call('thrift_dummmy.thrift', 'w')]
      m.call_args_list == expected_open_call_list
      mock_file_handle = m()
      mock_file_handle.write.assert_called_once_with(expected_replaced_contents)

  def test_non_keyword_file(self):
    thrift_contents = dedent('''
      namespace py gen.twitter.tweetypie.tweet
      struct UrlEntity {
        1: i16 no_keyword
        2: i16 from_
        3: i16 _fromdsd
        4: i16 FROM
        5: i16 fromsuffix
      }
    ''')
    builder = PythonThriftBuilder(target=self.target('test_thrift_replacement:one'),
                                  root_dir=self.build_root,
                                  config=create_config(sample_ini=sample_ini_test))
    m = mock_open(read_data=thrift_contents)
    with patch('__builtin__.open', m, create=True):
      builder = PythonThriftBuilder(target=self.target('test_thrift_replacement:one'),
                                  root_dir=self.build_root,
                                  config=create_config(sample_ini=sample_ini_test))
      builder._modify_thrift('thrift_dummmy.thrift')
      expected_open_call_list = [call('thrift_dummmy.thrift'), call('thrift_dummmy.thrift', 'w')]
      m.call_args_list == expected_open_call_list
      mock_file_handle = m()
      mock_file_handle.write.assert_called_once_with(thrift_contents)

########NEW FILE########
__FILENAME__ = test_thrift_namespace_packages
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.birds.duck.ttypes import Duck
from twitter.birds.goose.ttypes import Goose

class ThritNamespacePackagesTest(unittest.TestCase):
  def test_thrift_namespaces(self):
    """The 'test' here is the very fact that we can successfully import the generated thrift code
    with a shared package prefix (twitter.birds) from two different eggs.
    However there's no harm in also exercising the thrift objects, just to be sure we can."""
    myDuck = Duck()
    myDuck.quack = 'QUACKQUACKQUACK'
    myGoose = Goose()
    myGoose.laysGoldenEggs = True

########NEW FILE########
__FILENAME__ = test_linkify
import os
import shutil
import tempfile
import unittest

from twitter.pants.reporting.linkify import linkify


def ensure_file_exists(path):
  os.makedirs(os.path.dirname(path))
  open(path, 'a').close()

class RunInfoTest(unittest.TestCase):
  def setUp(self):
    self._buildroot = tempfile.mkdtemp(prefix='test_html_reporter')

  def tearDown(self):
    if os.path.exists(self._buildroot):
      shutil.rmtree(self._buildroot, ignore_errors=True)

  def _do_test_linkify(self, expected_link, url):
    s = 'foo %s bar' % url
    expected = 'foo <a target="_blank" href="%s">%s</a> bar' % (expected_link, url)
    linkified = linkify(self._buildroot, s)
    self.assertEqual(expected, linkified)

  def test_linkify_absolute_paths(self):
    relpath = 'underscore_and.dot/and-dash/baz'
    path = os.path.join(self._buildroot, relpath)
    ensure_file_exists(path)
    self._do_test_linkify('/browse/%s' % relpath, path)

  def test_linkify_relative_paths(self):
    relpath = 'underscore_and.dot/and-dash/baz'
    path = os.path.join(self._buildroot, relpath)
    ensure_file_exists(path)
    self._do_test_linkify('/browse/%s' % relpath, relpath)

  def test_linkify_http(self):
    url = 'http://foobar.com/baz/qux'
    self._do_test_linkify(url, url)

    url = 'http://localhost:666/baz/qux'
    self._do_test_linkify(url, url)

  def test_linkify_https(self):
    url = 'https://foobar.com/baz/qux'
    self._do_test_linkify(url, url)

  def test_linkify_target(self):
    ensure_file_exists(os.path.join(self._buildroot, 'foo/bar/BUILD'))
    self._do_test_linkify('/browse/foo/bar/BUILD', 'foo/bar')
    self._do_test_linkify('/browse/foo/bar/BUILD', 'foo/bar:target')


########NEW FILE########
__FILENAME__ = test_git
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import subprocess
import re
import unittest

from itertools import izip_longest

import pytest

from twitter.common.contextutil import environment_as, pushd, temporary_dir
from twitter.common.dirutil import safe_open, safe_mkdtemp, safe_rmtree, touch
from twitter.pants.scm import Scm
from twitter.pants.scm.git import Git


class Version(object):
  def __init__(self, text):
    self._components = map(int, text.split('.'))

  def __cmp__(self, other):
    for ours, theirs in izip_longest(self._components, other._components, fillvalue=0):
      difference = cmp(ours, theirs)
      if difference != 0:
        return difference
    return 0


class VersionTest(unittest.TestCase):
  def test_equal(self):
    self.assertEqual(Version('1'), Version('1.0.0.0'))
    self.assertEqual(Version('1.0'), Version('1.0.0.0'))
    self.assertEqual(Version('1.0.0'), Version('1.0.0.0'))
    self.assertEqual(Version('1.0.0.0'), Version('1.0.0.0'))

  def test_less(self):
    self.assertTrue(Version('1.6') < Version('2'))
    self.assertTrue(Version('1.6') < Version('1.6.1'))
    self.assertTrue(Version('1.6') < Version('1.10'))

  def test_greater(self):
    self.assertTrue(Version('1.6.22') > Version('1'))
    self.assertTrue(Version('1.6.22') > Version('1.6'))
    self.assertTrue(Version('1.6.22') > Version('1.6.2'))
    self.assertTrue(Version('1.6.22') > Version('1.6.21'))
    self.assertTrue(Version('1.6.22') > Version('1.6.21.3'))


def git_version():
  process = subprocess.Popen(['git', '--version'], stdout=subprocess.PIPE)
  (stdout, stderr) = process.communicate()
  assert process.returncode == 0, "Failed to determine git version."
  matches = re.search('(.*)\s(\d+.*\d+)\s(.*)', stdout)
  return Version(matches.group(2))


@pytest.mark.skipif("git_version() < Version('1.7.10')")
class GitTest(unittest.TestCase):
  @staticmethod
  def init_repo(remote_name, remote):
    subprocess.check_call(['git', 'init'])
    subprocess.check_call(['git', 'config', 'user.email', 'you@example.com'])
    subprocess.check_call(['git', 'config', 'user.name', 'Your Name'])
    subprocess.check_call(['git', 'remote', 'add', remote_name, remote])

  @classmethod
  def setUpClass(cls):
    cls.origin = safe_mkdtemp()
    with pushd(cls.origin):
      subprocess.check_call(['git', 'init', '--bare'])

    cls.gitdir = safe_mkdtemp()
    cls.worktree = safe_mkdtemp()

    cls.readme_file = os.path.join(cls.worktree, 'README')

    with environment_as(GIT_DIR=cls.gitdir, GIT_WORK_TREE=cls.worktree):
      cls.init_repo('depot', cls.origin)

      touch(cls.readme_file)
      subprocess.check_call(['git', 'add', 'README'])
      subprocess.check_call(['git', 'commit', '-am', 'initial commit.'])
      subprocess.check_call(['git', 'tag', 'first'])
      subprocess.check_call(['git', 'push', '--tags', 'depot', 'master'])
      subprocess.check_call(['git', 'branch', '--set-upstream', 'master', 'depot/master'])

      with safe_open(cls.readme_file, 'w') as readme:
        readme.write('Hello World.')
      subprocess.check_call(['git', 'commit', '-am', 'Update README.'])

    cls.clone2 = safe_mkdtemp()
    with pushd(cls.clone2):
      cls.init_repo('origin', cls.origin)
      subprocess.check_call(['git', 'pull', '--tags', 'origin', 'master:master'])

      with safe_open(os.path.realpath('README'), 'a') as readme:
        readme.write('--')
      subprocess.check_call(['git', 'commit', '-am', 'Update README 2.'])
      subprocess.check_call(['git', 'push', '--tags', 'origin', 'master'])

    cls.git = Git(gitdir=cls.gitdir, worktree=cls.worktree)

  @classmethod
  def tearDownClass(cls):
    safe_rmtree(cls.origin)
    safe_rmtree(cls.gitdir)
    safe_rmtree(cls.worktree)
    safe_rmtree(cls.clone2)

  def test(self):
    self.assertEqual(set(), self.git.changed_files())
    self.assertEqual(set(['README']), self.git.changed_files(from_commit='HEAD^'))

    tip_sha = self.git.commit_id
    self.assertTrue(tip_sha)

    self.assertTrue(tip_sha in self.git.changelog())

    self.assertTrue(self.git.tag_name.startswith('first-'), msg='un-annotated tags should be found')
    self.assertEqual('master', self.git.branch_name)

    def edit_readme():
      with open(self.readme_file, 'a') as readme:
        readme.write('More data.')

    edit_readme()
    with open(os.path.join(self.worktree, 'INSTALL'), 'w') as untracked:
      untracked.write('make install')
    self.assertEqual(set(['README']), self.git.changed_files())
    self.assertEqual(set(['README', 'INSTALL']), self.git.changed_files(include_untracked=True))

    try:
      # These changes should be rejected because our branch point from origin is 1 commit behind
      # the changes pushed there in clone 2.
      self.git.commit('API Changes.')
    except Scm.RemoteException:
      with environment_as(GIT_DIR=self.gitdir, GIT_WORK_TREE=self.worktree):
        subprocess.check_call(['git', 'reset', '--hard', 'depot/master'])
      self.git.refresh()
      edit_readme()

    self.git.commit('''API '"' " Changes.''')
    self.git.tag('second', message='''Tagged ' " Changes''')

    with temporary_dir() as clone:
      with pushd(clone):
        self.init_repo('origin', self.origin)
        subprocess.check_call(['git', 'pull', '--tags', 'origin', 'master:master'])

        with open(os.path.realpath('README')) as readme:
          self.assertEqual('--More data.', readme.read())

        git = Git()

        # Check that we can pick up committed and uncommitted changes.
        with safe_open(os.path.realpath('CHANGES'), 'w') as changes:
          changes.write('none')
        subprocess.check_call(['git', 'add', 'CHANGES'])
        self.assertEqual(set(['README', 'CHANGES']), git.changed_files(from_commit='first'))

        self.assertEqual('master', git.branch_name)
        self.assertEqual('second', git.tag_name, msg='annotated tags should be found')

########NEW FILE########
__FILENAME__ = test_artifact
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.targets.artifact import Artifact
from twitter.pants.targets.repository import Repository


class ArtifactTest(unittest.TestCase):

  def test_validation(self):
    with ParseContext.temp():
      repo = Repository(name="myRepo", url="myUrl", push_db="myPushDb")
      Artifact(org="testOrg", name="testName", repo=repo, description="Test")
      self.assertRaises(ValueError, Artifact,
                        org=1, name="testName", repo=repo, description="Test")
      self.assertRaises(ValueError, Artifact,
                        org="testOrg", name=1, repo=repo, description="Test")
      self.assertRaises(ValueError, Artifact,
                        org="testOrg", name="testName", repo=1, description="Test")
      self.assertRaises(ValueError, Artifact,
                        org="testOrg", name="testName", repo=repo, description=1)

########NEW FILE########
__FILENAME__ = test_bundle
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from textwrap import dedent

from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.targets.jvm_binary import Bundle

class BundleTest(BaseBuildRootTest):

  def test_bundle_filemap_dest_bypath(self):
    self.create_dir('src/java/org/archimedes/buoyancy/config')
    self.create_file('src/java/org/archimedes/buoyancy/config/densities.xml')
    self.create_target('src/java/org/archimedes/buoyancy/BUILD', dedent('''
      jvm_app(name='buoyancy',
        binary=jvm_binary(name='unused'),
        bundles=bundle().add('config/densities.xml'))
    '''))
    app = self.target('src/java/org/archimedes/buoyancy')
    # after one big refactor, ../../../../../ snuck into this path:
    self.assertEquals(app.bundles[0].filemap.values()[0],
                      'config/densities.xml')

  def test_bundle_filemap_dest_byglobs(self):
    self.create_dir('src/java/org/archimedes/tub/config')
    self.create_file('src/java/org/archimedes/tub/config/one.xml')
    self.create_file('src/java/org/archimedes/tub/config/two.xml')
    self.create_target('src/java/org/archimedes/tub/BUILD', dedent('''
      jvm_app(name='tub',
        binary=jvm_binary(name='unused'),
        bundles=bundle().add(globs('config/*.xml')))
    '''))
    app = self.target('src/java/org/archimedes/tub')
    for k in app.bundles[0].filemap.keys():
      if k.endswith('archimedes/tub/config/one.xml'):
        onexml_key = k
    self.assertEquals(app.bundles[0].filemap[onexml_key],
                      'config/one.xml')

  def test_bundle_filemap_dest_relative(self):
    self.create_dir('src/java/org/archimedes/crown/gold/config')
    self.create_file('src/java/org/archimedes/crown/gold/config/five.xml')
    self.create_target('src/java/org/archimedes/crown/BUILD', dedent('''
      jvm_app(name='crown',
        binary=jvm_binary(name='unused'),
        bundles=bundle(relative_to='gold').add('gold/config/five.xml'))
    '''))
    app = self.target('src/java/org/archimedes/crown')
    for k in app.bundles[0].filemap.keys():
      if k.endswith('archimedes/crown/gold/config/five.xml'):
        fivexml_key = k
    self.assertEquals(app.bundles[0].filemap.values()[0],
                      'config/five.xml')

  def test_bundle_add_add(self):
    self.create_dir('src/java/org/archimedes/volume/config/stone')
    self.create_file('src/java/org/archimedes/volume/config/stone/dense.xml')
    self.create_dir('src/java/org/archimedes/volume/config')
    self.create_file('src/java/org/archimedes/volume/config/metal/dense.xml')
    self.create_target('src/java/org/archimedes/volume/BUILD', dedent('''
      jvm_app(name='volume',
        binary=jvm_binary(name='unused'),
        bundles=bundle(relative_to='config')
          .add('config/stone/dense.xml')
          .add('config/metal/dense.xml'))
    '''))
    app = self.target('src/java/org/archimedes/volume')
    for k in app.bundles[0].filemap.keys():
      if k.endswith('archimedes/volume/config/stone/dense.xml'):
        stonexml_key = k
    self.assertEquals(app.bundles[0].filemap[stonexml_key],
                      'stone/dense.xml')

########NEW FILE########
__FILENAME__ = test_exclusive
from twitter.pants.testutils import MockTarget
from twitter.pants.base.config import Config
from twitter.pants.goal import Context
from twitter.pants.tasks.check_exclusives import CheckExclusives
from twitter.pants.testutils.base_mock_target_test import BaseMockTargetTest


class ExclusivesTargetTest(BaseMockTargetTest):
  """Test exclusives propagation in the dependency graph"""

  @classmethod
  def setUpClass(cls):
     cls.config = Config.load()

  def setupTargets(self):
    a = MockTarget('a', exclusives={'a': '1', 'b': '1'})
    b = MockTarget('b', exclusives={'a': '1'})
    c = MockTarget('c', exclusives = {'a': '2'})
    d = MockTarget('d', dependencies=[a, b])
    e = MockTarget('e', dependencies=[a, c], exclusives={'c': '1'})
    return a, b, c, d, e

  def testPropagation(self):
    a, b, c, d, e = self.setupTargets()
    d_excl = d.get_all_exclusives()
    self.assertEquals(d_excl['a'], set(['1']))
    e_excl = e.get_all_exclusives()
    self.assertEquals(e_excl['a'], set(['1', '2']))

  def testPartitioning(self):
    # Target e has conflicts; in this test, we want to check that partitioning
    # of valid targets works to prevent conflicts in chunks, so we only use a-d.
    a, b, c, d, _ = self.setupTargets()
    context = Context(ExclusivesTargetTest.config, options={}, run_tracker=None, target_roots=[a, b, c, d])
    context.products.require_data('exclusives_groups')
    check_exclusives_task = CheckExclusives(context, signal_error=True)
    check_exclusives_task.execute([a, b, c, d])
    egroups = context.products.get_data('exclusives_groups')
    self.assertEquals(egroups.get_targets_for_group_key("a=1"), set([a, b, d]))
    self.assertEquals(egroups.get_targets_for_group_key("a=2"), set([c]))





########NEW FILE########
__FILENAME__ = test_internal
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target, TargetDefinitionException
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.testutils import MockTarget
from twitter.pants.testutils.base_mock_target_test import BaseMockTargetTest


class InternalTargetTest(BaseMockTargetTest):

  def test_validation(self):
    with ParseContext.temp('InternalTargetTest/test_validation'):
      InternalTarget(name="valid", dependencies=None)
      self.assertRaises(TargetDefinitionException, InternalTarget,
                        name=1, dependencies=None)

      InternalTarget(name="valid2", dependencies=Target(name='mybird'))
      self.assertRaises(TargetDefinitionException, InternalTarget,
                        name='valid3', dependencies=1)

  def test_detect_cycle_direct(self):
    a = MockTarget('a')

    # no cycles yet
    InternalTarget.sort_targets([a])
    a.update_dependencies([a])
    try:
      InternalTarget.sort_targets([a])
      self.fail("Expected a cycle to be detected")
    except InternalTarget.CycleException:
      # expected
      pass

  def test_detect_cycle_indirect(self):
    c = MockTarget('c')
    b = MockTarget('b', [c])
    a = MockTarget('a', [c, b])

    # no cycles yet
    InternalTarget.sort_targets([a])

    c.update_dependencies([a])
    try:
      InternalTarget.sort_targets([a])
      self.fail("Expected a cycle to be detected")
    except InternalTarget.CycleException:
      # expected
      pass

  def testSort(self):
    a = MockTarget('a', [])
    b = MockTarget('b', [a])
    c = MockTarget('c', [b])
    d = MockTarget('d', [c, a])
    e = MockTarget('e', [d])

    self.assertEquals(InternalTarget.sort_targets([a,b,c,d,e]), [e,d,c,b,a])
    self.assertEquals(InternalTarget.sort_targets([b,d,a,e,c]), [e,d,c,b,a])
    self.assertEquals(InternalTarget.sort_targets([e,d,c,b,a]), [e,d,c,b,a])

########NEW FILE########
__FILENAME__ = test_jar_library
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target, TargetDefinitionException
from twitter.pants.targets.jar_library import JarLibrary


class JarLibraryTest(unittest.TestCase):

  def test_validation(self):
    with ParseContext.temp('JarLibraryTest/test_validation'):
      target = Target(name='mybird')
      JarLibrary(name="test", dependencies=target)
      self.assertRaises(TargetDefinitionException, JarLibrary,
                        name="test1", dependencies=None)

########NEW FILE########
__FILENAME__ = test_pants_target
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import TargetDefinitionException
from twitter.pants.targets.pants_target import Pants


class PantsTargetTest(unittest.TestCase):

  def test_validation(self):
    basedir = 'PantsTargetTest/test_validation'
    with ParseContext.temp(basedir):
      self.assertRaises(TargetDefinitionException, Pants, spec='fake')
      self.assertRaises(TargetDefinitionException, Pants, spec='%s:fake' % basedir)

########NEW FILE########
__FILENAME__ = test_python_binary
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target, TargetDefinitionException
from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.targets.python_binary import PythonBinary


class TestPythonBinary(BaseBuildRootTest):
  def tearDown(self):
    Target._clear_all_addresses()

  def test_python_binary_must_have_some_entry_point(self):
    with ParseContext.temp('src'):
      with pytest.raises(TargetDefinitionException):
        PythonBinary(name = 'binary')

  def test_python_binary_with_entry_point_no_source(self):
    with ParseContext.temp('src'):
      assert PythonBinary(name = 'binary', entry_point = 'blork').entry_point == 'blork'

  def test_python_binary_with_source_no_entry_point(self):
    with ParseContext.temp('src'):
      assert PythonBinary(name = 'binary1', source = 'blork.py').entry_point == 'blork'
      assert PythonBinary(name = 'binary2', source = 'bin/blork.py').entry_point == 'bin.blork'

  def test_python_binary_with_entry_point_and_source(self):
    with ParseContext.temp('src'):
      assert 'blork' == PythonBinary(
          name = 'binary1', entry_point = 'blork', source='blork.py').entry_point
      assert 'blork:main' == PythonBinary(
          name = 'binary2', entry_point = 'blork:main', source='blork.py').entry_point
      assert 'bin.blork:main' == PythonBinary(
          name = 'binary3', entry_point = 'bin.blork:main', source='bin/blork.py').entry_point

  def test_python_binary_with_entry_point_and_source_mismatch(self):
    with ParseContext.temp('src'):
      with pytest.raises(TargetDefinitionException):
        PythonBinary(name = 'binary1', entry_point = 'blork', source='hork.py')
      with pytest.raises(TargetDefinitionException):
        PythonBinary(name = 'binary2', entry_point = 'blork:main', source='hork.py')
      with pytest.raises(TargetDefinitionException):
        PythonBinary(name = 'binary3', entry_point = 'bin.blork', source='blork.py')
      with pytest.raises(TargetDefinitionException):
        PythonBinary(name = 'binary4', entry_point = 'bin.blork', source='bin.py')

########NEW FILE########
__FILENAME__ = test_python_target
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import unittest

from textwrap import dedent

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import TargetDefinitionException
from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.targets.artifact import Artifact
from twitter.pants.targets.python_target import PythonTarget
from twitter.pants.targets.python_artifact import PythonArtifact
from twitter.pants.targets.repository import Repository
from twitter.pants.targets.sources import SourceRoot


class PythonTargetTest(BaseBuildRootTest):

  @classmethod
  def setUpClass(self):
    super(PythonTargetTest, self).setUpClass()
    SourceRoot.register(os.path.realpath(os.path.join(self.build_root, 'test_python_target')),
                        PythonTarget)

    self.create_target('test_thrift_replacement', dedent('''
      python_thrift_library(name='one',
        sources=['thrift/keyword.thrift'],
        dependencies=None
      )
    '''))

  def test_validation(self):
    with ParseContext.temp('PythonTargetTest/test_validation'):

      # Adding a JVM Artifact as a provides on a PythonTarget doesn't make a lot of sense. This test
      # sets up that very scenario, and verifies that pants throws a TargetDefinitionException.
      self.assertRaises(TargetDefinitionException, PythonTarget, name="one", sources=[],
        provides=Artifact(org='com.twitter', name='one-jar',
        repo=Repository(name='internal', url=None, push_db=None, exclusives=None)))

      name = "test-with-PythonArtifact"
      pa = PythonArtifact(name='foo', version='1.0', description='foo')

      # This test verifies that adding a 'setup_py' provides to a PythonTarget is okay.
      self.assertEquals(PythonTarget(name=name, provides=pa, sources=[]).name, name)
      name = "test-with-none"

      # This test verifies that having no provides is okay.
      self.assertEquals(PythonTarget(name=name, provides=None, sources=[]).name, name)

########NEW FILE########
__FILENAME__ = test_target
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import Target, TargetDefinitionException


class TargetTest(unittest.TestCase):

  def test_validation(self):
    with ParseContext.temp('TargetTest/test_validation'):
      self.assertRaises(TargetDefinitionException, Target, name=None)
      name = "test"
      self.assertEquals(Target(name=name).name, name)

########NEW FILE########
__FILENAME__ = test_util
# ==================================================================================================
# Copyright 2013 Foursquare Labs, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'Ryan Williams'

import unittest

class MockPantsTarget(object):
  def __init__(self, spec):
    self.foo = spec

  def __eq__(self, other):
    if not isinstance(other, MockPantsTarget):
      return False
    return self.foo == other.foo

  def __repr__(self):
    return "MockPantsTarget(%s)" % str(self.foo)


from twitter.pants.targets.util import resolve

class ResolveTest(unittest.TestCase):

  def testString(self):
    self.assertEquals(resolve("asdf", clazz=MockPantsTarget).foo, "asdf")

  def testUnicodeString(self):
    self.assertEquals(resolve(u"asdf", clazz=MockPantsTarget).foo, u"asdf")

  def testNone(self):
    self.assertEquals(resolve(None, clazz=MockPantsTarget), None)

  def testPantsTarget(self):
    self.assertEquals(resolve(MockPantsTarget("asdf"), clazz=MockPantsTarget).foo, "asdf")

  def testMixedList(self):
    self.assertEquals(
      resolve([MockPantsTarget("1"), "2", MockPantsTarget("3"), "4", "5"], clazz=MockPantsTarget),
      [MockPantsTarget("1"),
       MockPantsTarget("2"),
       MockPantsTarget("3"),
       MockPantsTarget("4"),
       MockPantsTarget("5")])

  def testNonTarget(self):
    self.assertEquals(
      resolve([MockPantsTarget(1), [4, 'asdf'], "qwer",], clazz=MockPantsTarget),
      [MockPantsTarget(1), [4, MockPantsTarget('asdf')], MockPantsTarget('qwer')])

########NEW FILE########
__FILENAME__ = test_base
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest

from contextlib import closing
from optparse import OptionGroup, OptionParser
from StringIO import StringIO

from twitter.common.collections import maybe_list

from twitter.pants.base.context_utils import create_context, create_config, create_run_tracker
from twitter.pants.base.target import Target
from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.commands.goal import SpecParser
from twitter.pants.goal import Mkflag, Context
from twitter.pants.tasks import Task
from twitter.pants.tasks.console_task import ConsoleTask


def prepare_task(task_type, config=None, args=None, targets=None, **kwargs):
  """Prepares a Task for execution.

  task_type: The class of the Task to create.
  config: An optional string representing the contents of a pants.ini config.
  args: optional list of command line flags, these should be prefixed with '--test-'.
  targets: optional list of Target objects passed on the command line.
  **kwargs: Any additional args the Task subclass constructor takes beyond the required context.

  Returns a new Task ready to execute.
  """

  assert issubclass(task_type, Task), 'task_type must be a Task subclass, got %s' % task_type

  config = create_config(config or '')

  parser = OptionParser()
  option_group = OptionGroup(parser, 'test')
  mkflag = Mkflag('test')
  task_type.setup_parser(option_group, args, mkflag)
  options, _ = parser.parse_args(args or [])

  run_tracker = create_run_tracker()

  context = Context(config, options, run_tracker, targets or [])
  return task_type(context, **kwargs)


class TaskTest(BaseBuildRootTest):
  """A baseclass useful for testing Tasks."""

  @classmethod
  def targets(cls, spec):
    """Resolves a target spec to one or more Target objects.

    spec: Either BUILD target address or else a target glob using the siblings ':' or
          descendants '::' suffixes.

    Returns the set of all Targets found.
    """
    return set(target for target, _ in SpecParser(cls.build_root).parse(spec) if target)

  def assertDeps(self, target, expected_deps=None):
    """Check that actual and expected dependencies of the given target match.

    :param target: :class:`twitter.pants.base.target.Target` to check
      dependencies of.
    :param expected_deps: :class:`twitter.pants.base.target.Target` or list of
      ``Target`` instances that are expected dependencies of ``target``.
    """
    expected_deps_list = maybe_list(expected_deps or [], expected_type=Target)
    self.assertEquals(set(expected_deps_list), set(target.dependencies))


class ConsoleTaskTest(TaskTest):
  """A baseclass useful for testing ConsoleTasks."""

  @classmethod
  def setUpClass(cls):
    super(ConsoleTaskTest, cls).setUpClass()

    task_type = cls.task_type()
    assert issubclass(task_type, ConsoleTask), \
        'task_type() must return a ConsoleTask subclass, got %s' % task_type

  @classmethod
  def task_type(cls):
    """Subclasses must return the type of the ConsoleTask subclass under test."""
    raise NotImplementedError()

  def execute_task(self, config=None, args=None, targets=None, extra_targets=None):
    """Creates a new task and executes it with the given config, command line args and targets.

    config:        an optional string representing the contents of a pants.ini config.
    args:          optional list of command line flags, these should be prefixed with '--test-'.
    targets:       optional list of Target objects passed on the command line.
    extra_targets: optional list of extra targets in the context in addition to those passed on the
                   command line
    Returns the text output of the task.
    """
    with closing(StringIO()) as output:
      task = prepare_task(self.task_type(), config=config, args=args, targets=targets,
                          outstream=output)
      task.execute(list(targets or ()) + list(extra_targets or ()))
      return output.getvalue()

  def execute_console_task(self, config=None, args=None, targets=None, extra_targets=None,
                           **kwargs):
    """Creates a new task and executes it with the given config, command line args and targets.

    config:        an optional string representing the contents of a pants.ini config.
    args:          optional list of command line flags, these should be prefixed with '--test-'.
    targets:       optional list of Target objects passed on the command line.
    extra_targets: optional list of extra targets in the context in addition to those passed on the
                   command line
    **kwargs: additional kwargs are passed to the task constructor.

    Returns the list of items returned from invoking the console task's console_output method.
    """
    task = prepare_task(self.task_type(), config=config, args=args, targets=targets, **kwargs)
    return list(task.console_output(list(targets or ()) + list(extra_targets or ())))

  def assert_entries(self, sep, *output, **kwargs):
    """Verifies the expected output text is flushed by the console task under test.

    NB: order of entries is not tested, just presence.

    sep:      the expected output separator.
    *output:  the output entries expected between the separators
    **kwargs: additional kwargs are passed to the task constructor except for config args, targets
              and extra_targets which are passed to execute_task.
    """
    # We expect each output line to be suffixed with the separator, so for , and [1,2,3] we expect:
    # '1,2,3,' - splitting this by the separator we should get ['1', '2', '3', ''] - always an extra
    # empty string if the separator is properly always a suffix and not applied just between
    # entries.
    self.assertEqual(sorted(list(output) + ['']), sorted((self.execute_task(**kwargs)).split(sep)))

  def assert_console_output(self, *output, **kwargs):
    """Verifies the expected output entries are emitted by the console task under test.

    NB: order of entries is not tested, just presence.

    *output:  the expected output entries
    **kwargs: additional kwargs are passed to the task constructor except for config args, targets
              and extra_targets which are passed to execute_console_task.
    """
    self.assertEqual(sorted(output), sorted(self.execute_console_task(**kwargs)))

  def assert_console_raises(self, exception, **kwargs):
    """Verifies the expected exception is raised by the console task under test.

    **kwargs: additional kwargs are passed to the task constructor except for config args, targets
              and extra_targets which are passed to execute_console_task.
    """
    with pytest.raises(exception):
      self.execute_console_task(**kwargs)

########NEW FILE########
__FILENAME__ = test_binary_create
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.context_utils import create_context
from twitter.pants.tasks.binary_create import BinaryCreate


sample_ini_test_1 = """
[DEFAULT]
pants_distdir = /tmp/dist
"""


class BinaryCreateTest(unittest.TestCase):

  def test_binary_create_init(self):
    options = {'jvm_binary_create_outdir': None,
               'binary_create_compressed': None,
               'binary_create_zip64': None,
               'jvm_binary_create_deployjar': None}
    binary_create = BinaryCreate(create_context(config=sample_ini_test_1, options=options))
    self.assertEquals(binary_create.outdir, '/tmp/dist')


########NEW FILE########
__FILENAME__ = test_builddict
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from contextlib import closing
from StringIO import StringIO

from twitter.pants.tasks.builddictionary import assemble, BuildBuildDictionary
from twitter.pants.tasks.test_base import prepare_task, TaskTest


OUTDIR = "/tmp/dist"

sample_ini_test_1 = """
[DEFAULT]
outdir: %s
""" % OUTDIR


class BaseBuildBuildDictionaryTest(TaskTest):

  def execute_task(self, config=sample_ini_test_1):
    with closing(StringIO()) as output:
      task = prepare_task(BuildBuildDictionary, config=config)
      task.execute(())
      return output.getvalue()


class BuildBuildDictionaryTestEmpty(BaseBuildBuildDictionaryTest):

  def test_builddict_empty(self):
    """Execution should be silent."""
    # We don't care _that_ much that execution be silent. Nice if at least
    # one test executes the task and doesn't explode, tho.
    self.assertEqual('', self.execute_task())


class ExtractedContentSanityTests(BaseBuildBuildDictionaryTest):
  def test_usual_syms(self):
    usual_syms = assemble()
    usual_names = usual_syms.keys()
    self.assertTrue(len(usual_names) > 20, "Strangely few symbols found")
    for expected in ['jvm_binary', 'python_binary']:
      self.assertTrue(expected in usual_names, "Didn't find %s" % expected)
    for unexpected in ['__builtins__', 'Target']:
      self.assertTrue(unexpected not in usual_names, "Found %s" % unexpected)

########NEW FILE########
__FILENAME__ = test_bundle_create
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base.context_utils import create_context
from twitter.pants.tasks.bundle_create import BundleCreate


sample_ini_test_1 = """
[DEFAULT]
pants_distdir = /tmp/dist
"""


class BundleCreateTest(unittest.TestCase):

  def test_bundle_create_init(self):
    options = {
               'jvm_binary_create_outdir': None,
               'binary_create_compressed': None,
               'binary_create_zip64': None,
               'jvm_binary_create_deployjar': None,
               'bundle_create_prefix': None,
               'bundle_create_archive': None
               }
    bundle_create = BundleCreate(create_context(config=sample_ini_test_1, options=options))
    self.assertEquals(bundle_create.outdir, '/tmp/dist')


########NEW FILE########
__FILENAME__ = test_cache_manager
import shutil
import tempfile

from twitter.pants.base.build_invalidator import CacheKey, CacheKeyGenerator
from twitter.pants.tasks.cache_manager import CacheManager, InvalidationCheck, VersionedTarget
from twitter.pants.testutils import MockTarget
from twitter.pants.testutils.base_mock_target_test import BaseMockTargetTest


class AppendingCacheKeyGenerator(CacheKeyGenerator):
  """Generates cache keys for versions of target sets."""

  @staticmethod
  def combine_cache_keys(cache_keys):
    if len(cache_keys) == 1:
      return cache_keys[0]
    else:
      sorted_cache_keys = sorted(cache_keys)  # For commutativity.
      combined_id = ','.join([cache_key.id for cache_key in sorted_cache_keys])
      combined_hash = ','.join([cache_key.hash for cache_key in sorted_cache_keys])
      combined_num_sources = reduce(lambda x, y: x + y, [cache_key.num_sources for cache_key in sorted_cache_keys], 0)
      return CacheKey(combined_id, combined_hash, combined_num_sources, [])

  def key_for_target(self, target, sources=None, fingerprint_extra=None):
    return CacheKey(target.id, target.id, target.num_sources, [])

  def key_for(self, tid, sources):
    return CacheKey(tid, tid, len(sources), [])


def print_vt(vt):
  print('%d (%s) %s: [ %s ]' % (len(vt.targets), vt.cache_key, vt.valid, ', '.join(['%s(%s)' % (v.id, v.cache_key) for v in vt.versioned_targets])))


class CacheManagerTest(BaseMockTargetTest):
  class TestCacheManager(CacheManager):
    def __init__(self, tmpdir):
      CacheManager.__init__(self, AppendingCacheKeyGenerator(), tmpdir, True, None, False)

  def setUp(self):
    self._dir = tempfile.mkdtemp()
    self.cache_manager = CacheManagerTest.TestCacheManager(self._dir)

  def tearDown(self):
    shutil.rmtree(self._dir, ignore_errors=True)

  def make_vts(self, target):
    return VersionedTarget(self.cache_manager, target, target.id)

  def test_partition(self):
    a = MockTarget('a', [], 1)
    b = MockTarget('b', [a], 1)
    c = MockTarget('c', [b], 1)
    d = MockTarget('d', [c, a], 1)
    e = MockTarget('e', [d], 1)

    targets = [a, b, c, d, e]

    all_vts = self.cache_manager._sort_and_validate_targets(targets)

    [ print_vt(vt) for vt in all_vts ]
    print ''

    invalid_vts = filter(lambda vt: not vt.valid, all_vts)
    self.assertEquals(5, len(invalid_vts))

    self.assertEquals(5, len(all_vts))

    vts_targets = [vt.targets[0] for vt in all_vts]
    self.assertEquals(set(targets), set(vts_targets))

    ic = InvalidationCheck(all_vts, [], 3)
    partitioned = ic.all_vts_partitioned

    [ print_vt(vt) for vt in partitioned ]

    # NOTE(ryan): several correct partitionings are possible, but in all cases 4 1-source targets will be added to the
    # first partition before it exceeds the limit of 3, and the final target will be in a partition by itself.
    self.assertEquals(2, len(partitioned))
    self.assertEquals(4, len(partitioned[0].targets))
    self.assertEquals(1, len(partitioned[1].targets))

########NEW FILE########
__FILENAME__ = test_check_exclusives
from twitter.pants.base.config import Config
from twitter.pants.goal import Context
from twitter.pants.testutils import MockTarget
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.check_exclusives import CheckExclusives
from twitter.pants.testutils.base_mock_target_test import BaseMockTargetTest


class CheckExclusivesTest(BaseMockTargetTest):
  """Test of the CheckExclusives task."""

  @classmethod
  def setUpClass(cls):
     cls.config = Config.load()

  def test_check_exclusives(self):
    a = MockTarget('a', exclusives={'a': '1', 'b': '1'})
    b = MockTarget('b', exclusives={'a': '1'})
    c = MockTarget('c', exclusives={'a': '2'})
    d = MockTarget('d', dependencies=[a, b])
    e = MockTarget('e', dependencies=[a, c], exclusives={'c': '1'})

    context = Context(CheckExclusivesTest.config, options={}, run_tracker=None, target_roots=[d, e])
    check_exclusives_task = CheckExclusives(context, signal_error=True)
    try:
      check_exclusives_task.execute([d, e])
      self.fail("Expected a conflicting exclusives exception to be thrown.")
    except TaskError:
      pass

  def test_classpath_compatibility(self):
    # test the compatibility checks for different exclusive groups.
    a = MockTarget('a', exclusives={'a': '1', 'b': '1'})
    b = MockTarget('b', exclusives={'a': '1', 'b': '<none>'})
    c = MockTarget('c', exclusives={'a': '2', 'b': '2'})
    d = MockTarget('d')

    context = Context(CheckExclusivesTest.config, options={}, run_tracker=None,
                      target_roots=[a, b, c, d])
    context.products.require_data('exclusives_groups')
    check_exclusives_task = CheckExclusives(context, signal_error=True)
    check_exclusives_task.execute([a, b, c, d])
    egroups = context.products.get_data('exclusives_groups')
    # Expected compatibility:
    # a is compatible with nothing but itself.
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[a], egroups.target_to_key[a]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[a], egroups.target_to_key[b]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[a], egroups.target_to_key[d]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[a], egroups.target_to_key[c]))

    # b is compatible with itself and a.
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[b], egroups.target_to_key[a]))
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[b], egroups.target_to_key[b]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[b], egroups.target_to_key[c]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[b], egroups.target_to_key[d]))

    # c is compatible with nothing but itself
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[c], egroups.target_to_key[c]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[c], egroups.target_to_key[a]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[c], egroups.target_to_key[b]))
    self.assertFalse(egroups._is_compatible(egroups.target_to_key[c], egroups.target_to_key[d]))

    # d is compatible with everything.
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[d], egroups.target_to_key[a]))
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[d], egroups.target_to_key[b]))
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[d], egroups.target_to_key[c]))
    self.assertTrue(egroups._is_compatible(egroups.target_to_key[d], egroups.target_to_key[d]))

  def test_classpath_updates(self):
    # Check that exclusive groups classpaths accumulate properly.
    a = MockTarget('a', exclusives={'a': '1', 'b': '1'})
    b = MockTarget('b', exclusives={'a': '1', 'b': '<none>'})
    c = MockTarget('c', exclusives={'a': '2', 'b': '2'})
    d = MockTarget('d')

    context = Context(CheckExclusivesTest.config, options={}, run_tracker=None,
                      target_roots=[a, b, c, d])
    context.products.require_data('exclusives_groups')
    check_exclusives_task = CheckExclusives(context, signal_error=True)
    check_exclusives_task.execute([a, b, c, d])
    egroups = context.products.get_data('exclusives_groups')

    egroups.set_base_classpath_for_group("a=1,b=1", ["a1", "b1"])
    egroups.set_base_classpath_for_group("a=1,b=<none>", ["a1"])
    egroups.set_base_classpath_for_group("a=2,b=2", ["a2", "b2"])
    egroups.set_base_classpath_for_group("a=<none>,b=<none>", ["none"])
    egroups.update_compatible_classpaths(None, ["update_without_group"])
    egroups.update_compatible_classpaths("a=<none>,b=<none>", ["update_all"])
    egroups.update_compatible_classpaths("a=1,b=<none>", ["update_a1bn"])
    egroups.update_compatible_classpaths("a=2,b=2", ["update_only_a2b2"])
    self.assertEquals(egroups.get_classpath_for_group("a=2,b=2"),
                      ["update_only_a2b2", "update_all", "update_without_group", "a2", "b2"])
    self.assertEquals(egroups.get_classpath_for_group("a=1,b=1"),
                      ["update_a1bn", "update_all", "update_without_group", "a1", "b1"])
    self.assertEquals(egroups.get_classpath_for_group("a=1,b=<none>"),
                      ["update_a1bn", "update_all", "update_without_group", "a1"])
    self.assertEquals(egroups.get_classpath_for_group("a=<none>,b=<none>"),
                      ["update_all", "update_without_group", "none"])

    # make sure repeated additions of the same thing are idempotent.
    egroups.update_compatible_classpaths("a=1,b=1", ["a1", "b1", "xxx"])
    self.assertEquals(egroups.get_classpath_for_group("a=1,b=1"),
                      ["xxx", "update_a1bn", "update_all", "update_without_group", "a1", "b1"])







########NEW FILE########
__FILENAME__ = test_check_published_deps
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.tasks.check_published_deps import CheckPublishedDeps
from twitter.pants.tasks.test_base import ConsoleTaskTest


class CheckPublishedDepsTest(ConsoleTaskTest):

  @classmethod
  def task_type(cls):
    return CheckPublishedDeps

  @classmethod
  def setUpClass(cls):
    super(CheckPublishedDepsTest, cls).setUpClass()

    cls.create_file('repo/pushdb/publish.properties', dedent('''
        revision.major.org.name%lib1=2
        revision.minor.org.name%lib1=0
        revision.patch.org.name%lib1=0
        revision.sha.org.name%lib1=12345
        revision.major.org.name%lib2=2
        revision.minor.org.name%lib2=0
        revision.patch.org.name%lib2=0
        revision.sha.org.name%lib2=12345
        '''))
    cls.create_target('repo/BUILD', dedent('''
        import os
        repo(name='repo',
             url='http://www.www.com',
             push_db=os.path.join(os.path.dirname(__file__), 'pushdb', 'publish.properties'))
        '''))

    cls.create_target('provider/BUILD', dedent('''
        java_library(name='lib1',
          provides=artifact(
            org='org.name',
            name='lib1',
            repo=pants('repo')),
          sources=[])
        java_library(name='lib2',
          provides=artifact(
            org='org.name',
            name='lib2',
            repo=pants('repo')),
          sources=[])
        '''))
    cls.create_target('outdated/BUILD', dedent('''
        jar_library(name='outdated',
          dependencies=[jar(org='org.name', name='lib1', rev='1.0.0')]
        )
        '''))
    cls.create_target('uptodate/BUILD', dedent('''
        jar_library(name='uptodate',
          dependencies=[jar(org='org.name', name='lib2', rev='2.0.0')]
        )
        '''))
    cls.create_target('both/BUILD', dedent('''
        dependencies(name='both',
          dependencies=[
            pants('outdated'),
            pants('uptodate'),
          ]
        )
        '''))

  def test_all_up_to_date(self):
    self.assert_console_output(
      targets=[self.target('uptodate')]
    )

  def test_print_up_to_date_and_outdated(self):
    self.assert_console_output(
      'outdated org.name#lib1 1.0.0 latest 2.0.0',
      'up-to-date org.name#lib2 2.0.0',
      targets=[self.target('both')],
      args=['--test-print-uptodate']
    )

  def test_outdated(self):
    self.assert_console_output(
      'outdated org.name#lib1 1.0.0 latest 2.0.0',
      targets=[self.target('outdated')]
    )

########NEW FILE########
__FILENAME__ = test_config
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

__author__ = 'jsirois'

import unittest

from twitter.common.contextutil import temporary_file
from twitter.pants.base.config import Config

class ConfigTest(unittest.TestCase):

  def setUp(self):
    with temporary_file() as ini:
      ini.write(
'''
[DEFAULT]
answer: 42
scale: 1.2
path: /a/b/%(answer)s
embed: %(path)s::foo
disclaimer:
  Let it be known
  that.

[a]
fast: True
list: [1, 2, 3, %(answer)s]

[b]
preempt: False
dict: {
    'a': 1,
    'b': %(answer)s,
    'c': ['%(answer)s', %(answer)s]
  }
''')
      ini.close()
      self.config = Config.load(configpath=ini.name)


  def test_getstring(self):
    self.assertEquals('/a/b/42', self.config.get('a', 'path'))
    self.assertEquals('/a/b/42::foo', self.config.get('a', 'embed'))
    self.assertEquals(
      '''
Let it be known
that.''',
      self.config.get('b', 'disclaimer'))

    self.checkDefaults(self.config.get, '')
    self.checkDefaults(self.config.get, '42')


  def test_getint(self):
    self.assertEquals(42, self.config.getint('a', 'answer'))
    self.checkDefaults(self.config.get, 42)


  def test_getfloat(self):
    self.assertEquals(1.2, self.config.getfloat('b', 'scale'))
    self.checkDefaults(self.config.get, 42.0)


  def test_getbool(self):
    self.assertTrue(self.config.getbool('a', 'fast'))
    self.assertFalse(self.config.getbool('b', 'preempt'))
    self.checkDefaults(self.config.get, True)


  def test_getlist(self):
    self.assertEquals([1, 2, 3, 42], self.config.getlist('a', 'list'))
    self.checkDefaults(self.config.get, [])
    self.checkDefaults(self.config.get, [42])


  def test_getmap(self):
    self.assertEquals(dict(a=1, b=42, c=['42', 42]), self.config.getdict('b', 'dict'))
    self.checkDefaults(self.config.get, {})
    self.checkDefaults(self.config.get, dict(a=42))


  def checkDefaults(self, accessor, default):
    self.assertEquals(None, accessor('c', 'fast'))
    self.assertEquals(None, accessor('c', 'preempt', None))
    self.assertEquals(default, accessor('c', 'jake', default=default))

########NEW FILE########
__FILENAME__ = test_console_task
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import sys
import threading
import unittest

import pytest

from Queue import Empty, Queue

from twitter.pants.tasks.test_base import prepare_task
from twitter.pants.tasks.console_task import ConsoleTask


class ConsoleTaskTest(unittest.TestCase):
  class Infinite(ConsoleTask):
    def __init__(self, context, outstream=sys.stdout):
      super(ConsoleTaskTest.Infinite, self).__init__(context, outstream)
      self.halt = threading.Event()

    def console_output(self, _):
      while not self.halt.isSet():
        yield 'jake'

    def stop(self):
      self.halt.set()

  def test_sigpipe(self):
    r, w = os.pipe()
    task = prepare_task(self.Infinite, outstream=os.fdopen(w, 'w'))

    raised = Queue(maxsize=1)

    def execute():
      try:
        task.execute([])
      except IOError as e:
        raised.put(e)

    execution = threading.Thread(target=execute, name='ConsoleTaskTest_sigpipe')
    execution.setDaemon(True)
    execution.start()
    try:
      data = os.read(r, 5)
      self.assertEqual('jake\n', data)
      os.close(r)
    finally:
      task.stop()
      execution.join()

    with pytest.raises(Empty):
      e = raised.get_nowait()

      # Instead of taking the generic pytest.raises message, provide a more detailed failure
      # message that shows exactly what untrapped error was on the queue.
      self.fail('task raised %s' % e)

########NEW FILE########
__FILENAME__ = test_context
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base.config import Config
from twitter.pants.goal import Context
from twitter.pants.testutils import MockTarget
from twitter.pants.testutils.base_mock_target_test import BaseMockTargetTest


class ContextTest(BaseMockTargetTest):
  @classmethod
  def setUpClass(cls):
    cls.config = Config.load()

  @classmethod
  def create_context(cls, **kwargs):
    return Context(cls.config, run_tracker=None, **kwargs)

  def test_dependents_empty(self):
    context = self.create_context(options={}, target_roots=[])
    dependees = context.dependents()
    self.assertEquals(0, len(dependees))

  def test_dependents_direct(self):
    a = MockTarget('a')
    b = MockTarget('b', [a])
    c = MockTarget('c', [b])
    d = MockTarget('d', [c, a])
    e = MockTarget('e', [d])
    context = self.create_context(options={}, target_roots=[a, b, c, d, e])
    dependees = context.dependents(lambda t: t in set([e, c]))
    self.assertEquals(set([c]), dependees.pop(d))
    self.assertEquals(0, len(dependees))

########NEW FILE########
__FILENAME__ = test_dependees
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.targets.python_tests import PythonTests, PythonTestSuite
from twitter.pants.targets.sources import SourceRoot
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.dependees import ReverseDepmap
from twitter.pants.tasks.test_base import ConsoleTaskTest

import mox


class BaseReverseDepmapTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return ReverseDepmap


class ReverseDepmapEmptyTest(BaseReverseDepmapTest):
  def test(self):
    self.assert_console_output(targets=[])


class ReverseDepmapTest(BaseReverseDepmapTest, mox.MoxTestBase):
  @classmethod
  def setUpClass(cls):
    super(ReverseDepmapTest, cls).setUpClass()

    def create_target(path, name, alias=False, deps=()):
      cls.create_target(path, dedent('''
          %(type)s(name='%(name)s',
            dependencies=[%(deps)s]
          )
          ''' % dict(
        type='dependencies' if alias else 'python_library',
        name=name,
        deps=','.join("pants('%s')" % dep for dep in list(deps)))
      ))

    create_target('common/a', 'a', deps=['common/d'])
    create_target('common/b', 'b')
    create_target('common/c', 'c')
    create_target('common/d', 'd')
    create_target('tests/d', 'd', deps=['common/d'])
    create_target('overlaps', 'one', deps=['common/a', 'common/b'])
    create_target('overlaps', 'two', deps=['common/a', 'common/c'])
    create_target('overlaps', 'three', deps=['common/a', 'overlaps:one'])
    create_target('overlaps', 'four', alias=True, deps=['common/b'])
    create_target('overlaps', 'five', deps=['overlaps:four'])

    cls.create_target('resources/a', dedent('''
      resources(
        name='a_resources',
        sources=['a.resource']
      )
    '''))

    cls.create_target('src/java/a', dedent('''
      java_library(
        name='a_java',
        resources=[pants('resources/a:a_resources')]
      )
    '''))

    #Compile idl tests
    cls.create_target('src/thrift/example', dedent('''
      java_thrift_library(
        name='mybird',
        compiler='scrooge',
        language='scala',
        sources=['1.thrift']
      )
      '''))

    cls.create_target('src/thrift/example', dedent('''
      jar_library(
        name='compiled_scala',
        dependencies=[
          pants(':mybird')
        ]
      )
      '''))

    cls.create_target('src/thrift/example', dedent('''
      scala_library(
        name='compiled_scala_user',
        dependencies=[
          pants(':compiled_scala')
        ],
        sources=['1.scala'],
      )
      '''))

    create_target('src/thrift/dependent', 'my-example', deps=['src/thrift/example:mybird'])

    #External Dependency tests
    cls.create_target('src/java/example', dedent('''
      java_library(
        name='mybird',
        dependencies=[
          jar(org='com', name='twitter')
        ],
        sources=['1.java'],
      )
      '''))

    cls.create_target('src/java/example', dedent('''
      java_library(
        name='example2',
        dependencies=[
          pants(':mybird')
        ],
        sources=['2.java']
      )
      '''))

  def test_roots(self):
    self.assert_console_output(
      'overlaps/BUILD:two',
      targets=[self.target('common/c')],
      extra_targets=[self.target('common/a')]
    )

  def test_normal(self):
    self.assert_console_output(
      'overlaps/BUILD:two',
      targets=[self.target('common/c')]
    )

  def test_closed(self):
    self.assert_console_output(
      'overlaps/BUILD:two',
      'common/c/BUILD:c',
      args=['--test-closed'],
      targets=[self.target('common/c')]
    )

  def test_transitive(self):
    self.assert_console_output(
      'overlaps/BUILD:one',
      'overlaps/BUILD:three',
      'overlaps/BUILD:four',
      'overlaps/BUILD:five',
      args=['--test-transitive'],
      targets=[self.target('common/b')]
    )

  def test_nodups_dependees(self):
    self.assert_console_output(
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      targets=[
        self.target('common/a'),
        self.target('overlaps:one')
      ],
    )

  def test_nodups_roots(self):
    targets = [self.target('common/c')] * 2
    self.assertEqual(2, len(targets))
    self.assert_console_output(
      'overlaps/BUILD:two',
      'common/c/BUILD:c',
      args=['--test-closed'],
      targets=targets
    )

  def test_aliasing(self):
    self.assert_console_output(
      'overlaps/BUILD:five',
      targets=[self.target('overlaps:four')]
    )

  def test_depeendees_type(self):
    self._set_up_mocks(PythonTests, ["%s/tests" % get_buildroot()])
    self.assert_console_output(
      'tests/d/BUILD:d',
      args=['--test-type=python_tests'],
      targets=[self.target('common/d')]
    )

  def test_empty_depeendees_type(self):
    self._set_up_mocks(PythonTestSuite, [])
    self.assert_console_raises(
      TaskError,
      args=['--test-type=python_test_suite'],
      targets=[self.target('common/d')]
    )

  def test_compile_idls(self):
    self.assert_console_output(
      'src/thrift/dependent/BUILD:my-example',
      'src/thrift/example/BUILD:compiled_scala',
      'src/thrift/example/BUILD:compiled_scala_user',
      targets=[
        self.target('src/thrift/example:mybird'),
      ],
    )

  def test_external_dependency(self):
    self.assert_console_output(
      'src/java/example/BUILD:example2',
       targets=[self.target('src/java/example/BUILD:mybird')]
    )

  def test_resources_dependees(self):
    self.assert_console_output(
      'src/java/a/BUILD:a_java',
       targets=[self.target('resources/a:a_resources')]
    )

  def _set_up_mocks(self, class_type, src_roots):
    self.mox.StubOutWithMock(SourceRoot, 'roots')
    SourceRoot.roots(class_type).AndReturn(src_roots)
    self.mox.ReplayAll()

########NEW FILE########
__FILENAME__ = test_dependencies
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.tasks.dependencies import Dependencies
from twitter.pants.tasks.task_error import TaskError
from twitter.pants.tasks.test_base import ConsoleTaskTest

import pytest


# some helper methods to be able to setup the state in a cleaner way
def pants(path):
  return "pants('%s')" % path

def jar(org, name, rev):
  return "jar('%s', '%s', '%s')" % (org, name, rev)

def python_requirement(name):
  return "python_requirement('%s')" % name


class BaseDependenciesTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return Dependencies

  @classmethod
  def define_target(cls, path, name, ttype='python_library', deps=()):
    cls.create_target(path, dedent('''
        %(type)s(name='%(name)s',
          dependencies=[%(deps)s]
        )
        ''' % dict(
      type=ttype,
      name=name,
      deps=','.join(deps))
    ))

  @classmethod
  def scala_library(cls, path, name, deps=()):
    cls.create_target(path, dedent('''
      scala_library(name='%(name)s',
        dependencies=[%(deps)s],
        sources=[],
      )
    ''' % dict(
      name=name,
      deps=','.join(deps))
    ))


class DependenciesEmptyTest(BaseDependenciesTest):
  def test_no_targets(self):
    self.assert_console_output(targets=[])

class NonPythonDependenciesTest(BaseDependenciesTest):
  @classmethod
  def setUpClass(cls):
    super(NonPythonDependenciesTest, cls).setUpClass()

    cls.scala_library('dependencies', 'third')
    cls.scala_library('dependencies', 'first',
      deps=[pants('dependencies:third')])

    cls.scala_library('dependencies', 'second',
      deps=[
        jar('org.apache', 'apache-jar', '12.12.2012')]);

    cls.scala_library('project', 'project',
      deps=[
        pants('dependencies:first'),
        pants('dependencies:second')])

  def test_without_dependencies(self):
    self.assert_console_output(
      'dependencies/BUILD:third',
      targets=[self.target('dependencies:third')]
    )

  def test_all_dependencies(self):
    self.assert_console_output(
      'dependencies/BUILD:third',
      'dependencies/BUILD:first',
      'dependencies/BUILD:second',
      'project/BUILD:project',
      'org.apache:apache-jar:12.12.2012',
      targets=[self.target('project:project')]
    )

  def test_internal_dependencies(self):
    self.assert_console_output(
      'dependencies/BUILD:third',
      'dependencies/BUILD:first',
      'dependencies/BUILD:second',
      'project/BUILD:project',
      args=['--test-internal-only'],
      targets=[self.target('project:project')]
    )

  def test_external_dependencies(self):
    self.assert_console_output(
      'org.apache:apache-jar:12.12.2012',
      args=['--test-external-only'],
      targets=[self.target('project:project')]
    )


class PythonDependenciesTests(BaseDependenciesTest):
  @classmethod
  def setUpClass(cls):
    super(PythonDependenciesTests, cls).setUpClass()

    cls.define_target('dependencies', 'python_leaf')

    cls.define_target('dependencies', 'python_inner',
      deps=[
        pants('dependencies:python_leaf')
      ])

    cls.define_target('dependencies', 'python_inner_with_external',
      deps=[
        python_requirement("antlr_python_runtime==3.1.3")
      ])

    cls.define_target('dependencies', 'python_root',
      deps=[
        pants('dependencies:python_inner'),
        pants('dependencies:python_inner_with_external')
      ])

  def test_normal(self):
    self.assert_console_output(
      'antlr-python-runtime==3.1.3',
      'dependencies/BUILD:python_inner',
      'dependencies/BUILD:python_inner_with_external',
      'dependencies/BUILD:python_leaf',
      'dependencies/BUILD:python_root',
      targets=[self.target('dependencies:python_root')]
    )

  def test_internal_dependencies(self):
    with pytest.raises(TaskError):
      self.assert_console_output(
        args=['--test-internal-only'],
        targets=[self.target('dependencies:python_root')]
      )

  def test_external_dependencies(self):
    with pytest.raises(TaskError):
      self.assert_console_output(
        args=['--test-external-only'],
        targets=[self.target('dependencies:python_root')]
      )

########NEW FILE########
__FILENAME__ = test_depmap
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.tasks.depmap import Depmap
from twitter.pants.tasks.task_error import TaskError
from twitter.pants.tasks.test_base import ConsoleTaskTest


class BaseDepmapTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return Depmap


class DepmapTest(BaseDepmapTest):
  @classmethod
  def setUpClass(cls):
    super(DepmapTest, cls).setUpClass()

    def create_target(path, name, type, deps=(), **kwargs):
      cls.create_target(path, dedent('''
          %(type)s(name='%(name)s',
            dependencies=[%(deps)s],
            %(extra)s
          )
          ''' % dict(
        type=type,
        name=name,
        deps=','.join("pants('%s')" % dep for dep in list(deps)),
        extra=('' if not kwargs else ', '.join('%s=%r' % (k, v) for k, v in kwargs.items()))
      )))

    def create_python_binary_target(path, name, entry_point, type, deps=()):
      cls.create_target(path, dedent('''
          %(type)s(name='%(name)s',
            entry_point='%(entry_point)s',
            dependencies=[%(deps)s]
          )
          ''' % dict(
        type=type,
        entry_point=entry_point,
        name=name,
        deps=','.join("pants('%s')" % dep for dep in list(deps)))
      ))

    def create_jvm_app(path, name, type, binary, deps=()):
      cls.create_target(path, dedent('''
          %(type)s(name='%(name)s',
            binary=pants('%(binary)s'),
            bundles=%(deps)s
          )
          ''' % dict(
        type=type,
        name=name,
        binary=binary,
        deps=deps)
      ))

    create_target('common/a', 'a', 'dependencies')
    create_target('common/b', 'b', 'jar_library')
    cls.create_target('common/c', dedent('''
      scala_library(name='c',
        sources=[],
      )
    '''))
    create_target('common/d', 'd', 'python_library')
    create_python_binary_target('common/e', 'e', 'common.e.entry', 'python_binary')
    create_target('common/f', 'f', 'jvm_binary')
    create_target('common/g', 'g', 'jvm_binary', deps=['common/f:f'])
    cls.create_dir('common/h')
    cls.create_file('common/h/common.f')
    create_jvm_app('common/h', 'h', 'jvm_app', 'common/f:f', "bundle().add('common.f')")
    cls.create_dir('common/i')
    cls.create_file('common/i/common.g')
    create_jvm_app('common/i', 'i', 'jvm_app', 'common/g:g', "bundle().add('common.g')")
    create_target('overlaps', 'one', 'jvm_binary', deps=['common/h', 'common/i'])
    cls.create_target('overlaps', dedent('''
      scala_library(name='two',
        dependencies=[pants('overlaps:one')],
        sources=[],
      )
    '''))
    cls.create_target('resources/a', dedent('''
      resources(
        name='a_resources',
        sources=['a.resource']
      )
    '''))

    cls.create_target('src/java/a', dedent('''
      java_library(
        name='a_java',
        resources=[pants('resources/a:a_resources')]
      )
    '''))

  def test_empty(self):
    self.assert_console_raises(
      TaskError,
      targets=[self.target('common/a')]
    )

  def test_jar_library(self):
    self.assert_console_raises(
      TaskError,
      targets=[self.target('common/b')],
    )

  def test_scala_library(self):
    self.assert_console_output(
      'internal-common.c.c',
      targets=[self.target('common/c')]
    )

  def test_python_library(self):
    self.assert_console_raises(
      TaskError,
      targets=[self.target('common/d')]
    )

  def test_python_binary(self):
    self.assert_console_raises(
      TaskError,
      targets=[self.target('common/e')]
    )

  def test_jvm_binary1(self):
    self.assert_console_output(
      'internal-common.f.f',
      targets=[self.target('common/f')]
    )

  def test_jvm_binary2(self):
    self.assert_console_output(
      'internal-common.g.g',
      '  internal-common.f.f',
      targets=[self.target('common/g')]
    )

  def test_jvm_app1(self):
    self.assert_console_output(
      'internal-common.h.h',
      '  internal-common.f.f',
      targets=[self.target('common/h')]
    )

  def test_jvm_app2(self):
    self.assert_console_output(
      'internal-common.i.i',
      '  internal-common.g.g',
      '    internal-common.f.f',
      targets=[self.target('common/i')]
    )

  def test_overlaps_one(self):
    self.assert_console_output(
      'internal-overlaps.one',
      '  internal-common.h.h',
      '    internal-common.f.f',
      '  internal-common.i.i',
      '    internal-common.g.g',
      '      *internal-common.f.f',
      targets=[self.target('overlaps:one')]
    )

  def test_overlaps_two(self):
    self.assert_console_output(
      'internal-overlaps.two',
      '  internal-overlaps.one',
      '    internal-common.h.h',
      '      internal-common.f.f',
      '    internal-common.i.i',
      '      internal-common.g.g',
      '        *internal-common.f.f',
      targets=[self.target('overlaps:two')]
    )

  def test_overlaps_two_minimal(self):
    self.assert_console_output(
      'internal-overlaps.two',
      '  internal-overlaps.one',
      '    internal-common.h.h',
      '      internal-common.f.f',
      '    internal-common.i.i',
      '      internal-common.g.g',
      targets=[self.target('overlaps:two')],
      args=['--test-minimal']
    )

  def test_multi(self):
    self.assert_console_output(
      'internal-common.g.g',
      '  internal-common.f.f',
      'internal-common.h.h',
      '  internal-common.f.f',
      'internal-common.i.i',
      '  internal-common.g.g',
      '    internal-common.f.f',
      targets=[self.target('common/g'), self.target('common/h'), self.target('common/i')]
    )

  def test_resources(self):
    self.assert_console_output(
      'internal-src.java.a.a_java',
      '  internal-resources.a.a_resources',
      targets=[self.target('src/java/a:a_java')]
    )

########NEW FILE########
__FILENAME__ = test_detect_duplicates
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import pytest
import tempfile

from contextlib import closing, contextmanager
from zipfile import ZipFile

from twitter.common.dirutil import safe_rmtree, touch

from twitter.pants.base.context_utils import create_context
from twitter.pants.tasks.detect_duplicates import DuplicateDetector
from twitter.pants.tasks.task_error import TaskError
from twitter.pants.tasks.test_base import TaskTest


class DuplicateDetectorTest(TaskTest):
  def setUp(self):
    self.base_dir = tempfile.mkdtemp()

    def generate_path(name):
      return os.path.join(self.base_dir, name)

    test_class_path = generate_path('com/twitter/Test.class')
    duplicate_class_path = generate_path('com/twitter/commons/Duplicate.class')
    unique_class_path = generate_path('org/apache/Unique.class')

    touch(test_class_path)
    touch(duplicate_class_path)
    touch(unique_class_path)

    def generate_jar(path, *class_name):
      with closing(ZipFile(generate_path(path), 'w')) as zipfile:
        for clazz in class_name:
          zipfile.write(clazz)
        return zipfile.filename

    @contextmanager
    def jars():
      test_jar = generate_jar('test.jar', test_class_path, duplicate_class_path)
      jar_with_duplicates = generate_jar('dups.jar', duplicate_class_path, unique_class_path)
      jar_without_duplicates = generate_jar('no_dups.jar', unique_class_path)

      jars = []
      jars.append(test_jar)
      jars.append(jar_with_duplicates)
      jars.append(jar_without_duplicates)
      yield jars

    with jars() as jars:
      self.path_with_duplicates = [jars[0], jars[1]]
      self.path_without_duplicates = [jars[0], jars[2]]

  def tearDown(self):
    safe_rmtree(self.base_dir)

  def test_duplicate_found(self):
    options = {'fail_fast': False}
    task = DuplicateDetector(create_context(options=options))
    self.assertTrue(task._is_conflicts(self.path_with_duplicates, None))

  def test_duplicate_not_found(self):
    options = {'fail_fast': False}
    task = DuplicateDetector(create_context(options=options))
    self.assertFalse(task._is_conflicts(self.path_without_duplicates, None))

  def test_fail_fast_error_raised(self):
    options = {'fail_fast': True}
    task = DuplicateDetector(create_context(options=options))
    with pytest.raises(TaskError):
      task._is_conflicts(self.path_with_duplicates, None)

########NEW FILE########
__FILENAME__ = test_filemap
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
from textwrap import dedent

from twitter.pants.tasks.filemap import Filemap
from twitter.pants.tasks.test_base import ConsoleTaskTest


class FilemapTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return Filemap

  @classmethod
  def setUpClass(cls):
    super(FilemapTest, cls).setUpClass()

    def create_target(path, name, *files):
      for f in files:
        cls.create_file(os.path.join(path, f), '')

      cls.create_target(path, dedent('''
          python_library(name='%s',
            sources=[%s]
          )
          ''' % (name, ','.join(repr(f) for f in files))))

    cls.create_target('common', 'source_root.here(python_library)')
    create_target('common/a', 'a', 'one.py')
    create_target('common/b', 'b', 'two.py', 'three.py')
    create_target('common/c', 'c', 'four.py')

  def test_all(self):
    self.assert_console_output(
      'common/a/one.py common/a/BUILD:a',
      'common/b/two.py common/b/BUILD:b',
      'common/b/three.py common/b/BUILD:b',
      'common/c/four.py common/c/BUILD:c',
    )

  def test_one(self):
    self.assert_console_output(
      'common/b/two.py common/b/BUILD:b',
      'common/b/three.py common/b/BUILD:b',
      targets=[self.target('common/b')]
    )

  def test_dup(self):
    self.assert_console_output(
      'common/a/one.py common/a/BUILD:a',
      'common/c/four.py common/c/BUILD:c',
      targets=[self.target('common/a'), self.target('common/c'), self.target('common/a')]
    )


########NEW FILE########
__FILENAME__ = test_filter
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.tasks.filter import Filter
from twitter.pants.tasks.test_base import ConsoleTaskTest


class BaseFilterTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return Filter


class FilterEmptyTargetsTest(BaseFilterTest):
  def test_no_filters(self):
    self.assert_console_output()

  def test_type(self):
    self.assert_console_output(args=['--test-type=page'])
    self.assert_console_output(args=['--test-type=-java_library'])

  def test_regex(self):
    self.assert_console_output(args=['--test-regex=^common'])
    self.assert_console_output(args=['--test-regex=-^common'])


class FilterTest(BaseFilterTest):
  @classmethod
  def setUpClass(cls):
    super(FilterTest, cls).setUpClass()

    requirement_injected = set()

    def create_target(path, name, *deps):
      if path not in requirement_injected:
        cls.create_target(path, "python_requirement('foo')")
        requirement_injected.add(path)
      all_deps = ["pants('%s')" % dep for dep in deps] + ["pants(':foo')"]
      cls.create_target(path, dedent('''
          python_library(name='%s',
            dependencies=[%s]
          )
          ''' % (name, ','.join(all_deps))))

    create_target('common/a', 'a')
    create_target('common/b', 'b')
    create_target('common/c', 'c')
    create_target('overlaps', 'one', 'common/a', 'common/b')
    create_target('overlaps', 'two', 'common/a', 'common/c')
    create_target('overlaps', 'three', 'common/a', 'overlaps:one')

  def test_roots(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      'common/a/BUILD:foo',
      'common/b/BUILD:b',
      'common/b/BUILD:foo',
      'common/c/BUILD:c',
      'common/c/BUILD:foo',
      targets=self.targets('common/::'),
      extra_targets=self.targets('overlaps/::')
    )

  def test_nodups(self):
    targets = [self.target('common/b')] * 2
    self.assertEqual(2, len(targets))
    self.assert_console_output(
      'common/b/BUILD:b',
      targets=targets
    )

  def test_no_filters(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      'common/a/BUILD:foo',
      'common/b/BUILD:b',
      'common/b/BUILD:foo',
      'common/c/BUILD:c',
      'common/c/BUILD:foo',
      'overlaps/BUILD:one',
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      'overlaps/BUILD:foo',
      targets=self.targets('::')
    )

  def test_filter_type(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      'common/b/BUILD:b',
      'common/c/BUILD:c',
      'overlaps/BUILD:one',
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      args=['--test-type=python_library'],
      targets=self.targets('::')
    )

    self.assert_console_output(
      'common/a/BUILD:foo',
      'common/b/BUILD:foo',
      'common/c/BUILD:foo',
      'overlaps/BUILD:foo',
      args=['--test-type=-python_library'],
      targets=self.targets('::')
    )

    self.assert_console_output(
      'common/a/BUILD:a',
      'common/a/BUILD:foo',
      'common/b/BUILD:b',
      'common/b/BUILD:foo',
      'common/c/BUILD:c',
      'common/c/BUILD:foo',
      'overlaps/BUILD:one',
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      'overlaps/BUILD:foo',
      args=['--test-type=PythonRequirement,twitter.pants.targets.python_library.PythonLibrary'],
      targets=self.targets('::')
    )

  def test_filter_target(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      'overlaps/BUILD:foo',
      args=['--test-target=common/a,overlaps/:foo'],
      targets=self.targets('::')
    )

    self.assert_console_output(
      'common/a/BUILD:foo',
      'common/b/BUILD:b',
      'common/b/BUILD:foo',
      'common/c/BUILD:c',
      'common/c/BUILD:foo',
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      args=['--test-target=-common/a/BUILD:a,overlaps:one,overlaps:foo'],
      targets=self.targets('::')
    )

  def test_filter_ancestor(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      'common/a/BUILD:foo',
      'common/b/BUILD:b',
      'common/b/BUILD:foo',
      'overlaps/BUILD:one',
      'overlaps/BUILD:foo',
      args=['--test-ancestor=overlaps:one,overlaps:foo'],
      targets=self.targets('::')
    )

    self.assert_console_output(
      'common/c/BUILD:c',
      'common/c/BUILD:foo',
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      args=['--test-ancestor=-overlaps:one,overlaps:foo'],
      targets=self.targets('::')
    )

  def test_filter_regex(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      'common/a/BUILD:foo',
      'common/b/BUILD:b',
      'common/b/BUILD:foo',
      'common/c/BUILD:c',
      'common/c/BUILD:foo',
      args=['--test-regex=^common'],
      targets=self.targets('::')
    )

    self.assert_console_output(
      'common/a/BUILD:foo',
      'common/b/BUILD:foo',
      'common/c/BUILD:foo',
      'overlaps/BUILD:one',
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      'overlaps/BUILD:foo',
      args=['--test-regex=+foo,^overlaps'],
      targets=self.targets('::')
    )

    self.assert_console_output(
      'overlaps/BUILD:one',
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      args=['--test-regex=-^common,foo$'],
      targets=self.targets('::')
    )

########NEW FILE########
__FILENAME__ = test_jar_create
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os
import tempfile

from collections import defaultdict
from contextlib import contextmanager, closing
from textwrap import dedent

from twitter.common.contextutil import temporary_dir
from twitter.common.dirutil import safe_open, safe_rmtree

from twitter.pants.base.context_utils import create_context
from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.goal.products import MultipleRootedProducts
from twitter.pants.java.jar import open_jar
from twitter.pants.targets.java_library import JavaLibrary
from twitter.pants.targets.java_thrift_library import JavaThriftLibrary
from twitter.pants.targets.resources import Resources
from twitter.pants.targets.scala_library import ScalaLibrary
from twitter.pants.targets.sources import SourceRoot
from twitter.pants.tasks.jar_create import is_jvm_library, JarCreate


class JarCreateTestBase(BaseBuildRootTest):
  @staticmethod
  def create_options(**kwargs):
    options = dict(jar_create_outdir=None,
                   jar_create_transitive=None,
                   jar_create_compressed=None,
                   jar_create_classes=None,
                   jar_create_sources=None,
                   jar_create_idl=None,
                   jar_create_javadoc=None)
    options.update(**kwargs)
    return options

  @classmethod
  def create_files(cls, path, files):
    for f in files:
      cls.create_file(os.path.join(path, f), contents=f)

  @classmethod
  def library(cls, path, target_type, name, sources):
    cls.create_files(path, sources)

    cls.create_target(path, dedent('''
      %(target_type)s(name='%(name)s',
        sources=[%(sources)s],
      )
    ''' % dict(target_type=target_type, name=name, sources=repr(sources or []))))

    return cls.target('%s:%s' % (path, name))

  @classmethod
  def resources(cls, path, name, *sources):
    return cls.library(path, 'resources', name, sources)


class JarCreateMiscTest(JarCreateTestBase):
  def test_jar_create_init(self):
    ini = dedent("""
          [DEFAULT]
          pants_workdir: /tmp/pants.d
          pants_supportdir: /tmp/build-support
          """).strip()

    jar_create = JarCreate(create_context(config=ini, options=self.create_options()))
    self.assertEquals(jar_create._output_dir, '/tmp/pants.d/jars')
    self.assertEquals(jar_create.confs, ['default'])

  def test_resources_with_scala_java_files(self):
    for ftype in ('java', 'scala'):
      target = self.resources(os.path.join('project', ftype),
                              'target_%s' % ftype,
                              'hello.%s' % ftype)
      self.assertFalse(is_jvm_library(target))


class JarCreateExecuteTest(JarCreateTestBase):
  @classmethod
  def library_with_resources(cls, path, target_type, name, sources, resources=None):
    cls.create_files(path, sources)

    cls.create_target(path, dedent('''
      %(target_type)s(name='%(name)s',
        sources=[%(sources)s],
        %(resources)s
      )
    ''' % dict(target_type=target_type,
               name=name,
               sources=repr(sources or []),
               resources=('resources=pants("%s")' % resources if resources else ''))))

    return cls.target('%s:%s' % (path, name))

  @classmethod
  def java_library(cls, path, name, sources, resources=None):
    return cls.library_with_resources(path, 'java_library', name, sources, resources=resources)

  @classmethod
  def scala_library(cls, path, name, sources, resources=None):
    return cls.library_with_resources(path, 'scala_library', name, sources, resources=resources)

  @classmethod
  def java_thrift_library(cls, path, name, *sources):
    return cls.library(path, 'java_thrift_library', name, sources)

  @classmethod
  def setUpClass(cls):
    super(JarCreateExecuteTest, cls).setUpClass()

    def get_source_root_fs_path(path):
        return os.path.realpath(os.path.join(cls.build_root, path))

    SourceRoot.register(get_source_root_fs_path('src/resources'), Resources)
    SourceRoot.register(get_source_root_fs_path('src/java'), JavaLibrary)
    SourceRoot.register(get_source_root_fs_path('src/scala'), ScalaLibrary)
    SourceRoot.register(get_source_root_fs_path('src/thrift'), JavaThriftLibrary)

    cls.res = cls.resources('src/resources/com/twitter', 'spam', 'r.txt')
    cls.jl = cls.java_library('src/java/com/twitter', 'foo', ['a.java'],
                              resources='src/resources/com/twitter:spam')
    cls.sl = cls.scala_library('src/scala/com/twitter', 'bar', ['c.scala'])
    cls.jtl = cls.java_thrift_library('src/thrift/com/twitter', 'baz', 'd.thrift')

  def setUp(self):
    super(JarCreateExecuteTest, self).setUp()
    self.jar_outdir = tempfile.mkdtemp()

  def tearDown(self):
    super(JarCreateExecuteTest, self).tearDown()
    safe_rmtree(self.jar_outdir)

  def context(self, config='', **options):
    opts = dict(jar_create_outdir=self.jar_outdir)
    opts.update(**options)
    return create_context(config=config, options=self.create_options(**opts),
                          target_roots=[self.jl, self.sl, self.jtl])

  @contextmanager
  def add_products(self, context, product_type, target, *products):
    product_mapping = context.products.get(product_type)
    with temporary_dir() as outdir:
      def create_product(product):
        with safe_open(os.path.join(outdir, product), mode='w') as fp:
          fp.write(product)
        return product
      product_mapping.add(target, outdir, map(create_product, products))
      yield temporary_dir

  @contextmanager
  def add_data(self, context, data_type, target, *products):
    make_products = lambda: defaultdict(MultipleRootedProducts)
    data_by_target = context.products.get_data(data_type, make_products)
    with temporary_dir() as outdir:
      def create_product(product):
        abspath = os.path.join(outdir, product)
        with safe_open(abspath, mode='w') as fp:
          fp.write(product)
        return abspath
      data_by_target[target].add_abs_paths(outdir, map(create_product, products))
      yield temporary_dir

  def assert_jar_contents(self, context, product_type, target, *contents):
    jar_mapping = context.products.get(product_type).get(target)
    self.assertEqual(1, len(jar_mapping))
    for basedir, jars in jar_mapping.items():
      self.assertEqual(1, len(jars))
      with open_jar(os.path.join(basedir, jars[0])) as jar:
        self.assertEqual(list(contents), jar.namelist())
        for content in contents:
          if not content.endswith('/'):
            with closing(jar.open(content)) as fp:
              self.assertEqual(os.path.basename(content), fp.read())

  def assert_classfile_jar_contents(self, context, empty=False):
    with self.add_data(context, 'classes_by_target', self.jl, 'a.class', 'b.class'):
      with self.add_data(context, 'classes_by_target', self.sl, 'c.class'):
        with self.add_data(context, 'resources_by_target', self.res, 'r.txt.transformed'):
          JarCreate(context).execute(context.targets())
          if empty:
            self.assertTrue(context.products.get('jars').empty())
          else:
            self.assert_jar_contents(context, 'jars', self.jl,
                                     'a.class', 'b.class', 'r.txt.transformed')
            self.assert_jar_contents(context, 'jars', self.sl, 'c.class')

  def test_classfile_jar_required(self):
    context = self.context()
    context.products.require('jars')
    self.assert_classfile_jar_contents(context)

  def test_classfile_jar_flagged(self):
    self.assert_classfile_jar_contents(self.context(jar_create_classes=True))

  def test_classfile_jar_not_required(self):
    self.assert_classfile_jar_contents(self.context(), empty=True)

  def assert_source_jar_contents(self, context, empty=False):
    JarCreate(context).execute(context.targets())

    if empty:
      self.assertTrue(context.products.get('source_jars').empty())
    else:
      self.assert_jar_contents(context, 'source_jars', self.jl,
                               'com/', 'com/twitter/', 'com/twitter/a.java', 'com/twitter/r.txt')
      self.assert_jar_contents(context, 'source_jars', self.sl,
                               'com/', 'com/twitter/', 'com/twitter/c.scala')

  def test_source_jar_required(self):
    context = self.context()
    context.products.require('source_jars')
    self.assert_source_jar_contents(context)

  def test_source_jar_flagged(self):
    self.assert_source_jar_contents(self.context(jar_create_sources=True))

  def test_source_jar_not_required(self):
    self.assert_source_jar_contents(self.context(), empty=True)

  def assert_javadoc_jar_contents(self, context, empty=False, **kwargs):
    with self.add_products(context, 'javadoc', self.jl, 'a.html', 'b.html'):
      with self.add_products(context, 'scaladoc', self.sl, 'c.html'):
        JarCreate(context, **kwargs).execute(context.targets())

        if empty:
          self.assertTrue(context.products.get('javadoc_jars').empty())
        else:
          self.assert_jar_contents(context, 'javadoc_jars', self.jl, 'a.html', 'b.html')
          self.assert_jar_contents(context, 'javadoc_jars', self.sl, 'c.html')

  def test_javadoc_jar_required(self):
    context = self.context()
    context.products.require('javadoc_jars')
    self.assert_javadoc_jar_contents(context)

  def test_javadoc_jar_flagged(self):
    self.assert_javadoc_jar_contents(self.context(jar_create_javadoc=True))

  def test_javadoc_jar_constructor_required(self):
    self.assert_javadoc_jar_contents(self.context(), jar_javadoc=True)

  def test_javadoc_jar_not_required(self):
    self.assert_javadoc_jar_contents(self.context(), empty=True, jar_javadoc=False)


########NEW FILE########
__FILENAME__ = test_jar_library_with_empty_dependencies
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest
import unittest

from twitter.pants.base.parse_context import ParseContext
from twitter.pants.base.target import TargetDefinitionException
from twitter.pants.targets.jar_library import JarLibrary


class JarLibraryWithEmptyDependenciesTest(unittest.TestCase):

  def test_empty_dependencies(self):
    with ParseContext.temp():
      JarLibrary("test-jar-library-with-empty-dependencies", [])

  def test_no_dependencies(self):
    with pytest.raises(TargetDefinitionException):
      with ParseContext.temp():
        JarLibrary("test-jar-library-with-empty-dependencies", None)

########NEW FILE########
__FILENAME__ = test_jar_library_with_overrides
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import unittest

from twitter.pants.base import ParseContext
from twitter.pants.targets.exclude import Exclude
from twitter.pants.targets.jar_library import JarLibrary
from twitter.pants.targets.jar_dependency import JarDependency
from twitter.pants.targets.pants_target import Pants


class JarLibraryWithOverrides(unittest.TestCase):

  def test_jar_dependency(self):
    with ParseContext.temp():
      org, name = "org", "name"
      # thing to override
      nay = JarDependency(org, name, "0.0.1")
      yea = JarDependency(org, name, "0.0.8")
      # define targets depend on different 'org:c's
      JarLibrary("c", [nay])
      JarLibrary("b", [yea])
      # then depend on those targets transitively, and override to the correct version
      l = JarLibrary(
        "a",
        dependencies=[Pants(":c")],
        overrides=[":b"])

      # confirm that resolving includes the correct version
      resolved = set(l.resolve())
      self.assertTrue(yea in resolved)
      # and attaches an exclude directly to the JarDependency
      self.assertTrue(Exclude(org, name) in nay.excludes)

########NEW FILE########
__FILENAME__ = test_listtargets
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os

from textwrap import dedent

from twitter.pants.base.target import Target
from twitter.pants.tasks.listtargets import ListTargets
from twitter.pants.tasks.test_base import ConsoleTaskTest


class BaseListTargetsTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return ListTargets


class ListTargetsTestEmpty(BaseListTargetsTest):
  def test_list_all_empty(self):
    self.assertEqual('', self.execute_task())
    self.assertEqual('', self.execute_task(args=['--test-sep=###']))
    self.assertEqual([], self.execute_console_task())


class ListTargetsTest(BaseListTargetsTest):
  @classmethod
  def setUpClass(cls):
    super(ListTargetsTest, cls).setUpClass()

    # Setup a BUILD tree for various list tests

    repo_target = dedent('''
        repo(
          name='public',
          url='http://maven.twttr.com',
          push_db='/tmp/publish.properties'
        )
        ''').strip()
    cls.create_target('repos', repo_target)

    class Lib(object):
      def __init__(self, name, provides=False):
        self.name = name
        self.provides = dedent('''
            artifact(
              org='com.twitter',
              name='%s',
              repo=pants('repos:public')
            )
            ''' % name).strip() if provides else 'None'

    def create_library(path, *libs):
      libs = libs or [Lib(os.path.basename(os.path.dirname(cls.build_path(path))))]
      for lib in libs:
        target = "java_library(name='%s', provides=%s, sources=[])\n" % (lib.name, lib.provides)
        cls.create_target(path, target)

    create_library('a')
    create_library('a/b', Lib('b', provides=True))
    create_library('a/b/c', Lib('c'), Lib('c2', provides=True), Lib('c3'))
    create_library('a/b/d')
    create_library('a/b/e', Lib('e1'))
    cls.create_target('f', dedent('''
        dependencies(
          name='alias',
          dependencies=[
            pants('a/b/c/BUILD:c3'),
            pants('a/b/d/BUILD:d')
          ]
        ).with_description("""
        Exercises alias resolution.
        Further description.
        """)
        '''))

  def test_list_path(self):
    self.assert_console_output('a/b/BUILD:b', targets=[self.target('a/b')])

  def test_list_siblings(self):
    self.assert_console_output('a/b/BUILD:b', targets=self.targets('a/b:'))
    self.assert_console_output('a/b/c/BUILD:c', 'a/b/c/BUILD:c2', 'a/b/c/BUILD:c3',
                               targets=self.targets('a/b/c/:'))

  def test_list_descendants(self):
    self.assert_console_output('a/b/c/BUILD:c', 'a/b/c/BUILD:c2', 'a/b/c/BUILD:c3',
                               targets=self.targets('a/b/c/::'))

    self.assert_console_output(
        'a/b/BUILD:b',
        'a/b/c/BUILD:c',
        'a/b/c/BUILD:c2',
        'a/b/c/BUILD:c3',
        'a/b/d/BUILD:d',
        'a/b/e/BUILD:e1',
        targets=self.targets('a/b::'))

  def test_list_all(self):
    self.assert_entries('\n',
        'repos/BUILD:public',
        'a/BUILD:a',
        'a/b/BUILD:b',
        'a/b/c/BUILD:c',
        'a/b/c/BUILD:c2',
        'a/b/c/BUILD:c3',
        'a/b/d/BUILD:d',
        'a/b/e/BUILD:e1',
        'f/BUILD:alias')

    self.assert_entries(', ',
        'repos/BUILD:public',
        'a/BUILD:a',
        'a/b/BUILD:b',
        'a/b/c/BUILD:c',
        'a/b/c/BUILD:c2',
        'a/b/c/BUILD:c3',
        'a/b/d/BUILD:d',
        'a/b/e/BUILD:e1',
        'f/BUILD:alias',
        args=['--test-sep=, '])

    self.assert_console_output(
        'repos/BUILD:public',
        'a/BUILD:a',
        'a/b/BUILD:b',
        'a/b/c/BUILD:c',
        'a/b/c/BUILD:c2',
        'a/b/c/BUILD:c3',
        'a/b/d/BUILD:d',
        'a/b/e/BUILD:e1',
        'f/BUILD:alias')

  def test_list_provides(self):
    self.assert_console_output(
        'a/b/BUILD:b com.twitter#b',
        'a/b/c/BUILD:c2 com.twitter#c2',
        args=['--test-provides'])

  def test_list_provides_customcols(self):
    self.assert_console_output(
        '/tmp/publish.properties a/b/BUILD:b http://maven.twttr.com public com.twitter#b',
        '/tmp/publish.properties a/b/c/BUILD:c2 http://maven.twttr.com public com.twitter#c2',
        args=[
            '--test-provides',
            '--test-provides-columns=repo_db,address,repo_url,repo_name,artifact_id'
        ])

  def test_list_dedups(self):
    def expand(spec):
      for target in self.targets(spec):
        for tgt in target.resolve():
          if isinstance(tgt, Target) and tgt.is_concrete:
            yield tgt

    targets = []
    targets.extend(expand('a/b/d/::'))
    targets.extend(expand('f::'))

    self.assertEquals(3, len(targets), "Expected a duplicate of a/b/d/BUILD:d")
    self.assert_console_output(
      'a/b/c/BUILD:c3',
      'a/b/d/BUILD:d',
      targets=targets
    )

  def test_list_documented(self):
    self.assert_console_output(
      # Confirm empty listing
      args=['--test-documented'],
      targets=[self.target('a/b')]
    )

    self.assert_console_output(
      dedent('''
      f/BUILD:alias
        Exercises alias resolution.
        Further description.
      ''').strip(),
      args=['--test-documented']
    )


########NEW FILE########
__FILENAME__ = test_list_goals
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.goal import Goal
from twitter.pants.goal.phase import Phase
from twitter.pants.tasks import Task
from twitter.pants.tasks.list_goals import ListGoals

from . import ConsoleTaskTest


class ListGoalsTest(ConsoleTaskTest):
  _INSTALLED_HEADER = 'Installed goals:'
  _UNDOCUMENTED_HEADER = 'Undocumented goals:'
  _LIST_GOALS_NAME = 'goals'
  _LIST_GOALS_DESC = 'List all documented goals.'
  _LLAMA_NAME = 'llama'
  _LLAMA_DESC = 'With such handsome fiber, no wonder everyone loves Llamas.'
  _ALPACA_NAME = 'alpaca'

  @classmethod
  def task_type(cls):
    return ListGoals

  class LlamaTask(Task):
    pass

  class AlpacaTask(Task):
    pass

  def test_list_goals(self):
    Phase.clear()
    self.assert_console_output(self._INSTALLED_HEADER)

    Goal(name=self._LIST_GOALS_NAME, action=ListGoals)\
      .install().with_description(self._LIST_GOALS_DESC)
    self.assert_console_output(
      self._INSTALLED_HEADER,
      '  %s: %s' % (self._LIST_GOALS_NAME, self._LIST_GOALS_DESC),
    )

    Goal(name=self._LLAMA_NAME, action=ListGoalsTest.LlamaTask)\
      .install().with_description(self._LLAMA_DESC)
    self.assert_console_output(
      self._INSTALLED_HEADER,
      '  %s: %s' % (self._LIST_GOALS_NAME, self._LIST_GOALS_DESC),
      '  %s: %s' % (self._LLAMA_NAME, self._LLAMA_DESC),
    )

    Goal(name=self._ALPACA_NAME, action=ListGoalsTest.AlpacaTask, dependencies=[self._LLAMA_NAME])\
      .install()
    self.assert_console_output(
      self._INSTALLED_HEADER,
      '  %s: %s' % (self._LIST_GOALS_NAME, self._LIST_GOALS_DESC),
      '  %s: %s' % (self._LLAMA_NAME, self._LLAMA_DESC),
    )

  def test_list_goals_all(self):
    Phase.clear()

    Goal(name=self._LIST_GOALS_NAME, action=ListGoals)\
      .install().with_description(self._LIST_GOALS_DESC)
    Goal(name=self._LLAMA_NAME, action=ListGoalsTest.LlamaTask)\
      .install().with_description(self._LLAMA_DESC)
    Goal(name=self._ALPACA_NAME, action=ListGoalsTest.AlpacaTask, dependencies=[self._LLAMA_NAME])\
      .install()

    self.assert_console_output(
      self._INSTALLED_HEADER,
      '  %s: %s' % (self._LIST_GOALS_NAME, self._LIST_GOALS_DESC),
      '  %s: %s' % (self._LLAMA_NAME, self._LLAMA_DESC),
      '',
      self._UNDOCUMENTED_HEADER,
      '  %s' % self._ALPACA_NAME,
      args=['--test-all'],
    )

  def test_list_goals_graph(self):
    Phase.clear()

    Goal(name=self._LIST_GOALS_NAME, action=ListGoals)\
      .install().with_description(self._LIST_GOALS_DESC)
    Goal(name=self._LLAMA_NAME, action=ListGoalsTest.LlamaTask)\
      .install().with_description(self._LLAMA_DESC)
    Goal(name=self._ALPACA_NAME, action=ListGoalsTest.AlpacaTask, dependencies=[self._LLAMA_NAME])\
      .install()

    self.assert_console_output(
      'digraph G {\n  rankdir=LR;\n  graph [compound=true];',
      '  subgraph cluster_goals {\n    node [style=filled];\n    color = blue;\n    label = "goals";',
      '    goals_goals [label="goals"];',
      '  }',
      '  subgraph cluster_llama {\n    node [style=filled];\n    color = blue;\n    label = "llama";',
      '    llama_llama [label="llama"];',
      '  }',
      '  subgraph cluster_alpaca {\n    node [style=filled];\n    color = blue;\n    label = "alpaca";',
      '    alpaca_alpaca [label="alpaca"];',
      '  }',
      '  alpaca_alpaca -> llama_llama [ltail=cluster_alpaca lhead=cluster_llama];',
      '}',
      args=['--test-graph'],
    )

########NEW FILE########
__FILENAME__ = test_minimal_cover
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.tasks.minimal_cover import MinimalCover
from twitter.pants.tasks.test_base import ConsoleTaskTest


class BaseMinimalCovertTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return MinimalCover


class MinimalCoverEmptyTest(BaseMinimalCovertTest):
  def test(self):
    self.assert_console_output(targets=[])


class MinimalCoverTest(BaseMinimalCovertTest):

  @classmethod
  def setUpClass(cls):
    super(MinimalCoverTest, cls).setUpClass()

    def create_target(path, name, *deps):
      all_deps = ["pants('%s')" % dep for dep in list(deps)]
      cls.create_target(path, dedent('''
          python_library(name='%s',
            dependencies=[%s]
          )
          ''' % (name, ','.join(all_deps))))

    create_target('common/a', 'a')
    create_target('common/b', 'b')
    create_target('common/c', 'c')
    create_target('overlaps', 'one', 'common/a', 'common/b')
    create_target('overlaps', 'two', 'common/a', 'common/c')
    create_target('overlaps', 'three', 'common/a', 'overlaps:one')

  def test_roots(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      targets=[self.target('common/a')],
      extra_targets=[self.target('common/b')]
    )

  def test_nodups(self):
    targets = [self.target('common/a')] * 2
    self.assertEqual(2, len(targets))
    self.assert_console_output(
      'common/a/BUILD:a',
      targets=targets
    )

  def test_disjoint(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      'common/b/BUILD:b',
      'common/c/BUILD:c',
      targets=[
        self.target('common/a'),
        self.target('common/b'),
        self.target('common/c'),
      ]
    )

  def test_identical(self):
    self.assert_console_output(
      'common/a/BUILD:a',
      targets=[
        self.target('common/a'),
        self.target('common/a'),
        self.target('common/a'),
      ]
    )

  def test_intersection(self):
    self.assert_console_output(
      'overlaps/BUILD:one',
      'overlaps/BUILD:two',
      targets=[
        self.target('overlaps:one'),
        self.target('overlaps:two')
      ]
    )

    self.assert_console_output(
      'overlaps/BUILD:one',
      'common/c/BUILD:c',
      targets=[
        self.target('common/a'),
        self.target('common/b'),
        self.target('common/c'),
        self.target('overlaps:one'),
      ]
    )

    self.assert_console_output(
      'overlaps/BUILD:two',
      'overlaps/BUILD:three',
      targets=[
        self.target('common/a'),
        self.target('common/b'),
        self.target('common/c'),
        self.target('overlaps:one'),
        self.target('overlaps:two'),
        self.target('overlaps:three'),
      ]
    )

########NEW FILE########
__FILENAME__ = test_protobuf_gen
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# TODO(John Sirois): Test against protoc itself

import unittest

from twitter.common.contextutil import temporary_file

from twitter.pants.tasks.protobuf_gen import calculate_genfiles


class ProtobufGenCalculateGenfilesTestBase(unittest.TestCase):
  def assert_files(self, lang, rel_path, contents, *expected_files):
    with temporary_file() as fp:
      fp.write(contents)
      fp.close()
      self.assertEqual(set(expected_files), calculate_genfiles(fp.name, rel_path)[lang])


class ProtobufGenCalculateJavaTest(ProtobufGenCalculateGenfilesTestBase):

  def assert_java_files(self, rel_path, contents, *expected_files):
    self.assert_files('java', rel_path, contents, *expected_files)

  def test_plain(self):
    self.assert_java_files(
        'snake_case.proto',
        'package com.twitter.ads.revenue_tables;',
        'com/twitter/ads/revenue_tables/SnakeCase.java')

    self.assert_java_files(
        'a/b/jake.proto',
        'package com.twitter.ads.revenue_tables;',
        'com/twitter/ads/revenue_tables/Jake.java')

  def test_custom_package(self):
    self.assert_java_files(
        'fred.proto',
        '''
          package com.twitter.ads.revenue_tables;
          option java_package = "com.example.foo.bar";
        ''',
        'com/example/foo/bar/Fred.java')

    self.assert_java_files(
        'bam_bam.proto',
        'option java_package = "com.example.baz.bip";',
        'com/example/baz/bip/BamBam.java')

    self.assert_java_files(
        'bam_bam.proto',
        'option java_package="com.example.baz.bip" ;',
        'com/example/baz/bip/BamBam.java')

  def test_custom_outer(self):
    self.assert_java_files(
        'jack_spratt.proto',
        '''
          package com.twitter.lean;
          option java_outer_classname = "To";
        ''',
        'com/twitter/lean/To.java')

  def test_multiple_files(self):
    self.assert_java_files(
        'jack_spratt.proto',
        '''
          package com.twitter.lean;
          option java_multiple_files = false;
        ''',
        'com/twitter/lean/JackSpratt.java')

    self.assert_java_files(
        'jack_spratt.proto',
        '''
          package com.twitter.lean;
          option java_multiple_files = true;

          enum Jake { FOO=1; }
          message joe_bob {
          }
        ''',
        'com/twitter/lean/JackSpratt.java',
        'com/twitter/lean/Jake.java',
        'com/twitter/lean/joe_bob.java',
        'com/twitter/lean/joe_bobOrBuilder.java')

########NEW FILE########
__FILENAME__ = test_roots
import os

from contextlib import contextmanager

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.target import Target
from twitter.pants.targets.sources import SourceRoot
from twitter.pants.tasks.roots import ListRoots
from twitter.pants.tasks.test_base import ConsoleTaskTest


@contextmanager
def register_sourceroot():
  try:
    yield SourceRoot.register
  except (ValueError, IndexError) as e:
    print("SourceRoot Registration Failed.")
    raise e
  finally:
    SourceRoot.reset()


class ListRootsTest(ConsoleTaskTest):

  class TypeA(Target):
    pass

  class TypeB(Target):
    pass

  @classmethod
  def task_type(cls):
    return ListRoots

  def test_roots_without_register(self):
    try:
      self.assert_console_output()
    except AssertionError:
      self.fail("./pants goal roots failed without any registered SourceRoot.")

  def test_no_source_root(self):
    with register_sourceroot() as sourceroot:
      sourceroot(os.path.join(get_buildroot(), "fakeroot"))
      self.assert_console_output('fakeroot: *')

  def test_single_source_root(self):
    with register_sourceroot() as sourceroot:
      sourceroot(os.path.join(get_buildroot(), "fakeroot"), ListRootsTest.TypeA,
                                                            ListRootsTest.TypeB)
      self.assert_console_output("fakeroot: TypeA,TypeB")

  def test_multiple_source_root(self):
    with register_sourceroot() as sourceroot:
      sourceroot(os.path.join(get_buildroot(), "fakerootA"), ListRootsTest.TypeA)
      sourceroot(os.path.join(get_buildroot(), "fakerootB"), ListRootsTest.TypeB)
      self.assert_console_output('fakerootA: TypeA', 'fakerootB: TypeB')

########NEW FILE########
__FILENAME__ = test_scrooge_gen
# ==================================================================================================
# Copyright 2014 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import pytest

from textwrap import dedent

from twitter.pants.base_build_root_test import BaseBuildRootTest
from twitter.pants.tasks import TaskError
from twitter.pants.tasks.scrooge_gen import ScroogeGen


class ScroogeGenTest(BaseBuildRootTest):

  def test_validate(self):
    self.create_target('test_validate', dedent('''
      java_thrift_library(name='one',
        sources=None,
        dependencies=None,
      )
    '''))

    self.create_target('test_validate', dedent('''
      java_thrift_library(name='two',
        sources=None,
        dependencies=[pants(':one')],
      )
    '''))

    self.create_target('test_validate', dedent('''
      java_thrift_library(name='three',
        sources=None,
        dependencies=[pants(':one')],
        rpc_style='finagle',
      )
    '''))

    ScroogeGen._validate([self.target('test_validate:one')])
    ScroogeGen._validate([self.target('test_validate:two')])

    with pytest.raises(TaskError):
      ScroogeGen._validate([self.target('test_validate:three')])

########NEW FILE########
__FILENAME__ = test_sorttargets
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.tasks.sorttargets import SortTargets
from twitter.pants.tasks.test_base import ConsoleTaskTest


class BaseSortTargetsTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return SortTargets


class SortTargetsEmptyTest(BaseSortTargetsTest):
  def test(self):
    self.assert_console_output(targets=[])


class SortTargetsTest(BaseSortTargetsTest):

  @classmethod
  def setUpClass(cls):
    super(SortTargetsTest, cls).setUpClass()

    def create_target(path, name, *deps):
      all_deps = ["pants('%s')" % dep for dep in list(deps)]
      cls.create_target(path, dedent('''
          python_library(name='%s',
            dependencies=[%s]
          )
          ''' % (name, ','.join(all_deps))))

    create_target('common/a', 'a')
    create_target('common/b', 'b', 'common/a')
    create_target('common/c', 'c', 'common/a', 'common/b')

  def test_sort(self):
    targets = [self.target('common/a'), self.target('common/c'), self.target('common/b')]
    self.assertEqual(['common/a/BUILD:a', 'common/b/BUILD:b', 'common/c/BUILD:c'],
                     list(self.execute_console_task(targets=targets)))

  def test_sort_reverse(self):
    targets = [self.target('common/c'), self.target('common/a'), self.target('common/b')]
    self.assertEqual(['common/c/BUILD:c', 'common/b/BUILD:b', 'common/a/BUILD:a'],
                     list(self.execute_console_task(targets=targets, args=['--test-reverse'])))

########NEW FILE########
__FILENAME__ = test_targets_help
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import os.path

from twitter.pants.base.build_environment import get_buildroot
from twitter.pants.base.target import Target
from twitter.pants.targets.sources import SourceRoot
from twitter.pants.tasks.targets_help import TargetsHelp
from twitter.pants.tasks.test_base import ConsoleTaskTest


class TargetsHelpTest(ConsoleTaskTest):

  @classmethod
  def task_type(cls):
    return TargetsHelp

  @classmethod
  def setUpClass(cls):
    super(TargetsHelpTest, cls).setUpClass()
    SourceRoot.register(os.path.join(get_buildroot(), 'fakeroot'), TargetsHelpTest.MyTarget)

  def test_list_installed_targets(self):
    self.assert_console_output(
      TargetsHelp.INSTALLED_TARGETS_HEADER,
      '  %s: %s' % ('my_target'.rjust(TargetsHelp.MAX_ALIAS_LEN),
                    TargetsHelpTest.MyTarget.__doc__.split('\n')[0]))

  def test_get_details(self):
    self.assert_console_output(
      TargetsHelp.DETAILS_HEADER.substitute(
        name='my_target', desc=TargetsHelpTest.MyTarget.__doc__),
      '  name: The name of this target.',
      '   foo: Another argument.  (default: None)',
      args=['--test-details=my_target'])

  class MyTarget(Target):
    """One-line description of the target."""
    def __init__(self, name, foo=None):
      """
      :param name: The name of this target.
      :param string foo: Another argument.
      """
      Target.__init__(self, name)

########NEW FILE########
__FILENAME__ = test_what_changed
# ==================================================================================================
# Copyright 2012 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from textwrap import dedent

from twitter.pants.base.target import TargetDefinitionException
from twitter.pants.tasks.what_changed import WhatChanged, Workspace
from twitter.pants.tasks.test_base import ConsoleTaskTest


class BaseWhatChangedTest(ConsoleTaskTest):
  @classmethod
  def task_type(cls):
    return WhatChanged

  def workspace(self, files=None, parent=None):
    class MockWorkspace(Workspace):
      @staticmethod
      def touched_files(p):
        self.assertEqual(parent or 'HEAD', p)
        return files or []
    return MockWorkspace()


class WhatChangedTestBasic(BaseWhatChangedTest):
  def test_nochanges(self):
    self.assert_console_output(workspace=self.workspace())

  def test_parent(self):
    self.assert_console_output(args=['--test-parent=42'], workspace=self.workspace(parent='42'))

  def test_files(self):
    self.assert_console_output(
      'a/b/c',
      'd',
      'e/f',
      args=['--test-files'],
      workspace=self.workspace(files=['a/b/c', 'd', 'e/f'])
    )


class WhatChangedTest(BaseWhatChangedTest):
  @classmethod
  def setUpClass(cls):
    super(WhatChangedTest, cls).setUpClass()

    cls.create_target('root', dedent('''
      source_root('src/py', python_library)
      source_root('resources/a1', resources)
    '''))

    cls.create_target('root/src/py/a', dedent('''
      python_library(
        name='alpha',
        sources=['b/c', 'd'],
        resources=['test.resources']
      )

      jar_library(
        name='beta',
        dependencies=[
          jar(org='gamma', name='ray', rev='1.137.bruce_banner')
        ]
      )
    '''))

    cls.create_target('root/src/py/1', dedent('''
      python_library(
        name='numeric',
        sources=['2']
      )
    '''))

    cls.create_target('root/src/thrift', dedent('''
      java_thrift_library(
        name='thrift',
        sources=['a.thrift']
      )

      python_thrift_library(
        name='py-thrift',
        sources=['a.thrift']
      )
    '''))

    cls.create_target('root/resources/a', dedent('''
      resources(
        name='a_resources',
        sources=['a.resources']
      )
    '''))

    cls.create_target('root/src/java/a', dedent('''
      java_library(
        name='a_java',
        sources=['a.java'],
        resources=[pants('root/resources/a:a_resources')]
      )
    '''))

    cls.create_target('root/3rdparty/BUILD.twitter', dedent('''
      jar_library(
        name='dummy',
        dependencies=[
          jar(org='foo', name='ray', rev='1.45')
        ])
    '''))

    cls.create_target('root/3rdparty/BUILD', dedent('''
      jar_library(
        name='dummy1',
        dependencies=[
          jar(org='foo1', name='ray', rev='1.45')
        ])
    '''))

  def test_owned(self):
    self.assert_console_output(
      'root/src/py/a/BUILD:alpha',
      'root/src/py/1/BUILD:numeric',
      workspace=self.workspace(files=['root/src/py/a/b/c', 'root/src/py/a/d', 'root/src/py/1/2'])
    )

  def test_multiply_owned(self):
    self.assert_console_output(
      'root/src/thrift/BUILD:thrift',
      'root/src/thrift/BUILD:py-thrift',
      workspace=self.workspace(files=['root/src/thrift/a.thrift'])
    )

  def test_build(self):
    self.assert_console_output(
      'root/src/py/a/BUILD:alpha',
      'root/src/py/a/BUILD:beta',
      workspace=self.workspace(files=['root/src/py/a/BUILD'])
    )

  def test_resource_changed(self):
    self.assert_console_output(
      'root/src/py/a/BUILD:alpha',
      workspace=self.workspace(files=['root/src/py/a/test.resources'])
    )

  def test_resource_changed_for_java_lib(self):
    self.assert_console_output(
      'root/resources/a/BUILD:a_resources',
      workspace=self.workspace(files=['root/resources/a/a.resources'])
    )

  def test_build_sibling(self):
    self.assert_console_output(
      'root/3rdparty/BUILD.twitter:dummy',
      workspace=self.workspace(files=['root/3rdparty/BUILD.twitter'])
    )

  def test_resource_type_error(self):
    self.create_target('root/resources/a1', dedent('''
      java_library(
        name='a1',
        sources=['a1.test'],
        resources=[1]
      )
    '''))
    self.assert_console_raises(
      TargetDefinitionException,
      workspace=self.workspace(files=['root/resources/a1/a1.test'])
    )

########NEW FILE########
__FILENAME__ = base_mock_target_test
import unittest

from twitter.pants.base.target import Target


class BaseMockTargetTest(unittest.TestCase):
  """A baseclass useful for tests using ``MockTarget``s.."""

  def setUp(self):
    Target._clear_all_addresses()

########NEW FILE########
__FILENAME__ = mock_logger
import sys

from twitter.pants.reporting.report import Report


class MockLogger(object):
  """A standalone logger that writes to stderr.

  Useful for testing without requiring the full RunTracker reporting framework.
  """
  def __init__(self, level=Report.INFO):
    self._level = level

  def _maybe_log(self, level, *msg_elements):
    if level <= self._level:
      sys.stderr.write(''.join(msg_elements))

  def debug(self, *msg_elements): self._maybe_log(Report.DEBUG, *msg_elements)
  def info(self, *msg_elements): self._maybe_log(Report.INFO, *msg_elements)
  def warn(self, *msg_elements): self._maybe_log(Report.WARN, *msg_elements)
  def error(self, *msg_elements): self._maybe_log(Report.ERROR, *msg_elements)
  def fatal(self, *msg_elements): self._maybe_log(Report.FATAL, *msg_elements)

########NEW FILE########
__FILENAME__ = mock_target
__author__ = 'Ryan Williams'

from collections import defaultdict
from twitter.pants.base.parse_context import ParseContext
from twitter.pants.targets.internal import InternalTarget
from twitter.pants.targets.with_sources import TargetWithSources


class MockTarget(InternalTarget, TargetWithSources):
  def __init__(self, name, dependencies=None, num_sources=0, exclusives=None):
    with ParseContext.temp():
      InternalTarget.__init__(self, name, dependencies, exclusives=exclusives)
      TargetWithSources.__init__(self, name, exclusives=exclusives)
    self.num_sources = num_sources
    self.declared_exclusives = defaultdict(set)
    if exclusives is not None:
      for k in exclusives:
        self.declared_exclusives[k] = set([exclusives[k]])
    self.exclusives = None

  def resolve(self):
    yield self

  def walk(self, work, predicate=None):
    work(self)
    for dep in self.dependencies:
      dep.walk(work)


########NEW FILE########
__FILENAME__ = test_maven_layout
# ==================================================================================================
# Copyright 2013 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from twitter.pants.base_build_root_test import BaseBuildRootTest


class MavenLayoutTest(BaseBuildRootTest):
  @classmethod
  def setUpClass(cls):
    super(MavenLayoutTest, cls).setUpClass()

    cls.create_target('projectB/src/main/scala', 'scala_library(name="test", sources=[])')
    cls.create_file('projectB/BUILD', 'maven_layout()')

    cls.create_target('projectA/subproject/src/main/java', 'java_library(name="test", sources=[])')
    cls.create_file('BUILD', 'maven_layout("projectA/subproject")')

  def test_layout_here(self):
    self.assertEqual('projectB/src/main/scala',
                     self.target('projectB/src/main/scala:test').target_base)

  def test_subproject_layout(self):
    self.assertEqual('projectA/subproject/src/main/java',
                     self.target('projectA/subproject/src/main/java:test').target_base)

########NEW FILE########
__FILENAME__ = test_thrift_util
import os

from twitter.common.contextutil import temporary_dir
from twitter.common.dirutil import safe_open
from twitter.common.lang import Compatibility

if Compatibility.PY3:
  import unittest
else:
  import unittest2 as unittest

from twitter.pants.thrift_util import find_includes, find_root_thrifts


class ThriftUtilTest(unittest.TestCase):
  def write(self, path, contents):
    with safe_open(path, 'w') as fp:
      fp.write(contents)
    return path

  def test_find_includes(self):
    with temporary_dir() as dir:
      a = os.path.join(dir, 'a')
      b = os.path.join(dir, 'b')

      main = self.write(os.path.join(a, 'main.thrift'), '''
        include "sub/a_included.thrift" //Todo commet
        include "b_included.thrift"
        include "c_included.thrift" #jibberish
        include "d_included.thrift" some ramdon
      ''')

      a_included = self.write(os.path.join(a, 'sub', 'a_included.thrift'), '# noop')
      b_included = self.write(os.path.join(b, 'b_included.thrift'), '# noop')
      c_included = self.write(os.path.join(b, 'c_included.thrift'), '# noop')

      self.assertEquals(set([a_included, b_included, c_included]),
                        find_includes(basedirs=set([a, b]), source=main))

  def test_find_includes_exception(self):
    with temporary_dir() as dir:
      a = os.path.join(dir, 'a')

      main = self.write(os.path.join(a, 'main.thrift'), '''
        include "sub/a_included.thrift # Todo"
        include "b_included.thrift"
      ''')
      self.write(os.path.join(a, 'sub', 'a_included.thrift'), '# noop')
      self.assertRaises(ValueError, find_includes, basedirs=set([a]), source=main)

  def test_find_root_thrifts(self):
    with temporary_dir() as dir:
      root_1 = self.write(os.path.join(dir, 'root_1.thrift'), '# noop')
      root_2 = self.write(os.path.join(dir, 'root_2.thrift'), '# noop')
      self.assertEquals(set([root_1, root_2]),
                        find_root_thrifts(basedirs=[], sources=[root_1, root_2]))

    with temporary_dir() as dir:
      root_1 = self.write(os.path.join(dir, 'root_1.thrift'), 'include "mid_1.thrift"')
      self.write(os.path.join(dir, 'mid_1.thrift'), 'include "leaf_1.thrift"')
      self.write(os.path.join(dir, 'leaf_1.thrift'), '# noop')
      root_2 = self.write(os.path.join(dir, 'root_2.thrift'), 'include "root_1.thrift"')
      self.assertEquals(set([root_2]), find_root_thrifts(basedirs=[], sources=[root_1, root_2]))

########NEW FILE########
__FILENAME__ = conftest
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

# test.py config magic to get a cmd-line argument passed through to our test.

def pytest_addoption(parser):
  parser.addoption("--generate_golden_data", action="store", default=None,
    help="If specified, write the generated output to this path.  Use this only when you're " \
         "convinced that the generated data is correct and the old golden data is not.")


########NEW FILE########
__FILENAME__ = test_thrift_parser
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import os.path
import pkgutil
import pytest

from twitter.thrift.descriptors.thrift_parser import ThriftParser
from twitter.thrift.descriptors.thrift_parser_error import ThriftParserError
from twitter.thrift.text import thrift_json_encoder

def pytest_funcarg__generate_golden_data(request):
  """py.test magic for passing the --generate_golden_data flag=/path/to/golden/data to the test."""
  return request.config.option.generate_golden_data

def test_thrift_parser(generate_golden_data):
  """Tests that we can parse a complex file that tickles as many cases and corner cases
  as we can think of. We verify the result against golden data."""
  TEST_DATA_FILE = 'test_data/test_data.thrift'
  GOLDEN_DATA_FILE = TEST_DATA_FILE + '.golden'
  TEST_DATA_PATH = 'twitter.thrift.descriptors'

  test_data = pkgutil.get_data(TEST_DATA_PATH, TEST_DATA_FILE)
  golden_data = pkgutil.get_data(TEST_DATA_PATH, GOLDEN_DATA_FILE)
  parser = ThriftParser()
  print('Parsing file %s...' % TEST_DATA_FILE)
  program = parser.parse_string(test_data)
  print('OK.')
  res = thrift_json_encoder.thrift_to_json(program)

  if golden_data is not None:
    # Generate new golden data to the specified path. Use this only once you're
    # convinced that the generated data is correct and the old golden data is not.
    if generate_golden_data is not None:
      with open(generate_golden_data, 'w') as fd:
        fd.write(res)

    assert golden_data == res

def test_parse_various_files():
  """Tests that we can parse, without choking, test files that are part of the original
  thrift parser's test suite. We just check that parsing succeeds, and don't verify the
  results."""
  TEST_DATA_FILES = [
    "AnnotationTest.thrift", "ConstantsDemo.thrift", "DenseLinkingTest.thrift",
    "OptionalRequiredTest.thrift", "StressTest.thrift", "DebugProtoTest.thrift",
    "DocTest.thrift", "ManyTypedefs.thrift", "SmallTest.thrift", "ThriftTest.thrift"
  ]
  TEST_DATA_DIR = 'test_data'
  TEST_DATA_PATH = 'twitter.thrift.descriptors'

  parser = ThriftParser()
  for test_data_file in TEST_DATA_FILES:
    test_data = pkgutil.get_data(TEST_DATA_PATH, os.path.join(TEST_DATA_DIR, test_data_file))
    print('Parsing file %s...' % test_data_file, end='')
    # TODO: The parser may fail silently and return a partial program. Fix this.
    program = parser.parse_string(test_data)
    print('OK.')


def _parse_with_expected_error(test_data, expected_error_msg):
  parser = ThriftParser()
  with pytest.raises(ThriftParserError) as exception_info:
    program = parser.parse_string(test_data)
  assert str(exception_info.value) == expected_error_msg


def test_repeated_type_alias():
  test_data = """
    typedef i32 Foo
    typedef string Foo
  """
  _parse_with_expected_error(test_data, 'line 3, col 19: Type alias already exists: Foo')


def test_invalid_enum_value():
  test_data = """
    enum Foo {
      BAR = -1
      BAZ = 1
    }
  """
  _parse_with_expected_error(test_data, 'line 3, col 6: Enum value for BAR must be >= 0: -1')

  test_data = """
    enum Foo {
      BAR = 1
      BAZ = 2147483648
    }
  """
  _parse_with_expected_error(test_data,
                             'line 4, col 6: Enum value for BAZ must be < 2^31: 2147483648')

  test_data = """
    enum Foo {
      BAR = 3
      BAZ = 3
    }
  """
  _parse_with_expected_error(test_data, 'line 4, col 6: Enum value for BAZ must be >= 4: 3')


def test_invalid_field_identifier():
  test_data = """
    struct Foo {
      0: i32 foo
    }
  """
  _parse_with_expected_error(test_data, 'line 3, col 6: Field identifier for foo must be >= 1: 0')

  test_data = """
    struct Foo {
      32768: i32 foo
    }
  """
  _parse_with_expected_error(test_data,
                             'line 3, col 6: Field identifier for foo must be < 2^15: 32768')

  test_data = """
    struct Foo {
      123: required i32 foo,
      123: optional string bar
    }
  """
  _parse_with_expected_error(test_data,
                             'line 4, col 6: Field identifier 123 for bar already used for foo')


def test_repeated_field_name():
  test_data = """
    struct Foo {
      1: required i32 foo,
      2: optional string foo
    }
  """
  _parse_with_expected_error(test_data, 'line 4, col 25: Field name foo for identifier 2' +
                                        ' already used for identifier 1')

def test_repeated_function_name():
  test_data = """
    service Foo {
      void foo(),
      bool foo(1:string bar)
    }
  """
  _parse_with_expected_error(test_data,
                             'line 4, col 11: Function name foo already used in this service')

########NEW FILE########
__FILENAME__ = test_thrift_json_decoder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

import json
import pkgutil
import unittest

from twitter.thrift.text import thrift_json_decoder
from gen.twitter.thrift.text.testing import ttypes as structs_for_testing


class ThriftJsonDecoderTest(unittest.TestCase):
  def test_decode_json_to_thrift(self):
    json_str = \
"""{
  "field1": 7,
  "field2": true,
  "field3": "Hello, World",
  "field4": [
    2,
    4,
    6,
    8
  ],
  "field5": [
    "yes",
    "maybe",
    "no"
  ],
  "field6": {
    "foo": "bar",
    "color": 3,
    "numbers": {
      "1": "one",
      "2": "two",
      "3": "three"
    }
  },
  "field7": 1.2
}"""

    res = thrift_json_decoder.json_to_thrift(json_str, structs_for_testing.TestStruct)
    assert res.field1 == 7
    assert res.field2 == True
    assert res.field3 == 'Hello, World'
    assert res.field4 == [2, 4, 6, 8]
    assert res.field5 == set(['no', 'yes', 'maybe'])
    assert res.field6.foo == 'bar'
    assert res.field6.color == 3
    assert res.field6.numbers == { 1: 'one', 2: 'two', 3: 'three' }
    assert res.field7 == 1.2

########NEW FILE########
__FILENAME__ = test_thrift_json_encoder
# ==================================================================================================
# Copyright 2011 Twitter, Inc.
# --------------------------------------------------------------------------------------------------
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this work except in compliance with the License.
# You may obtain a copy of the License in the LICENSE file, or at:
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==================================================================================================

from __future__ import print_function

import json
import pkgutil
import unittest

from twitter.thrift.text import thrift_json_encoder
from gen.twitter.thrift.text.testing import ttypes as structs_for_testing


class ThriftJsonEncoderTest(unittest.TestCase):
  def test_encode_thrift_to_json(self):
    x = structs_for_testing.TestStruct()
    x.field2 = True
    x.field4 = [2, 4, 6, 8]
    x.field7 = 1.2

    expected1 = \
"""{
  "field2": true,
  "field4": [
    2,
    4,
    6,
    8
  ],
  "field7": 1.2
}"""

    json_str1 = thrift_json_encoder.thrift_to_json(x)
    assert expected1 == json_str1

    x.field1 = 42
    x.field2 = False
    x.field3 = '"not default"'
    x.field4.append(10)
    x.field5 = set(['b', 'c', 'a'])
    x.field6 = structs_for_testing.InnerTestStruct()
    x.field6.foo = "bar"
    x.field6.color =  structs_for_testing.Color.BLUE

    expected2 = \
"""{
  "field1": 42,
  "field2": false,
  "field3": "\\"not default\\"",
  "field4": [
    2,
    4,
    6,
    8,
    10
  ],
  "field5": [
    "a",
    "b",
    "c"
  ],
  "field6": {
    "color": 3,
    "foo": "bar"
  },
  "field7": 1.2
}"""

    json_str2 = thrift_json_encoder.thrift_to_json(x)
    print(json_str2)
    assert expected2 == json_str2

########NEW FILE########
__FILENAME__ = test_generic_struct_parser
# Copyright 2011 Twitter Inc. All rights reserved

__author__ = 'ugo'  # Ugo Di Girolamo

"""Tests for the generic_struct_parser class."""

import unittest

from thrift.transport import TTransport
from thrift.protocol import TBinaryProtocol
from twitter.thrift.util import generic_struct_parser
from gen.twitter.thrift.text.testing import ttypes as structs_for_testing

class GenericStructParserTest(unittest.TestCase):
  def test_read_binary_encoded(self):
    x = structs_for_testing.TestStruct()
    x.field2 = True
    x.field4 = [2, 4, 6, 8]
    x.field7 = 1.2
    x.field1 = 42
    x.field2 = False
    x.field3 = '"not default"'
    x.field4.append(10)
    x.field5 = set(['b', 'c', 'a'])
    x.field6 = structs_for_testing.InnerTestStruct()
    x.field6.foo = "bar"
    x.field6.color = structs_for_testing.Color.BLUE

    otransport = TTransport.TMemoryBuffer()
    oprot = TBinaryProtocol.TBinaryProtocol(otransport)
    x.write(oprot)
    itransport = TTransport.TMemoryBuffer(otransport.cstringio_buf.getvalue())
    iprot = TBinaryProtocol.TBinaryProtocol(itransport)

    actual = generic_struct_parser.read(iprot)
    expected_field6 = {"FIELD_1": ('STRING', "bar"),
                       "FIELD_2": ('I32', structs_for_testing.Color.BLUE),
                      }
    expected = {"FIELD_1": ('I32', 42),
                "FIELD_2": ('BOOL', False),
                "FIELD_3": ('STRING', '"not default"'),
                "FIELD_4": ('LIST', ('I16', list([2, 4, 6, 8, 10]))),
                "FIELD_5": ('SET', ('STRING', set(['a', 'b', 'c']))),
                "FIELD_6": ('STRUCT', expected_field6),
                "FIELD_7": ('DOUBLE', 1.2),
               }
    print("actual   = %s" % actual)
    print("expected = %s" % expected)
    # Note(ugo): assertDictEqual is only in python 2.7
    try:
      self.assertEquals(actual, expected)
    except AssertionError as e:
      all_keys = set(expected.keys())
      all_keys.union(actual.keys())
      print("differences:")
      for k in all_keys:
        a = actual.get(k, '__missing__')
        e = expected.get(k, '__missing__')
        if a != e:
          print('  %r: %s vs %s' % (k, a, e))
      raise

########NEW FILE########
