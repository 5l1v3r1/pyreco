__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Sphinx configuration for NBT documentation.
#
# This file is execfile()d by sphinx-build with the current directory set to its containing dir.

# To generate the documentation, run:
# sphinx-build -b rst -a path-to/NBT/doc path-to/NBT.wiki

import sys, os


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.

# Make sure we are importing the current NBT, and not an old version installed 
# in the site-packages
parentdir = os.path.abspath(os.path.join(os.path.dirname(__file__),os.pardir))
if not os.path.exists(os.path.join(parentdir, 'nbt')):
    raise ImportError("Can not find nbt module at %s" % parentdir)
if os.path.exists(os.path.join(parentdir, 'examples')):
    sys.path.insert(1, os.path.join(parentdir, 'examples'))
if os.path.exists(os.path.join(parentdir, 'tests')):
    sys.path.insert(1, os.path.join(parentdir, 'tests'))
sys.path.insert(1, parentdir)
# setuptools (which is used by pip) sometimes places a .pth file in the
# site-package folder which overrides sys.path by manipulating sys.modules.
# Undo this manipulation, to ensure we're not using nbt in the site-package
# folder when generating documentation.
if 'nbt' in sys.modules:
    del sys.modules['nbt']

import nbt

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'sphinxcontrib.restbuilder']
#, 'sphinx.ext.viewcode'

# Add any paths that contain templates here, relative to this directory.
templates_path = ['templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'NBT'
copyright = u'2010-2013, Thomas Woolford'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = nbt._get_version()
# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['build', 'NBT.wiki']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for Autodoc extension (which generated code from Python docstrings)

# Include only the class docstring to describe classes
# (The alternative is to also include the __init__ docstring)
autoclass_content = 'class'

# This value selects if automatically documented members are sorted 
# alphabetical (value 'alphabetical'), by member type (value 'groupwise') 
# or by source order (value 'bysource'). The default is alphabetical.
# If further control of the order is required, define __all__ in the module.
autodoc_member_order = 'bysource'

# Don't skip the following special functions.
def skip(app, what, name, obj, skip, options):
    if name in ("__init__", "__unicode__", "__str__", "__repr__", "__len__", 
                "__contains__", "__iter__", "__getitem__", "__setitem__", 
                "__delitem__"):
        return False
    return skip

def setup(app):
    app.connect("autodoc-skip-member", skip)


# -- Options for intersphinx extension (for links to doc.python.org)

# intersphinx_mapping = {'python': ('http://docs.python.org/3.2', None)}
intersphinx_mapping = {'python': ('http://docs.python.org/3.2', 'python.inv')}


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = []

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'NBTdoc'


# -- Options for reST output ---------------------------------------------------

# This is the file name suffix for reST files
rst_file_suffix = '.rst'

# This is the suffix for links to other reST files
rst_link_suffix = ''

# If set, function that changes the changes the reST file name.
# Default is docname + rst_file_suffix
def rst_file_transform(docname):
    if docname == 'index':
        docname = 'home'
    return docname.title() + rst_file_suffix

# If set, function that changes the changes the reST file name to a URI.
# Default is docname + rst_link_suffix
def rst_link_transform(docname):
    if docname == 'index':
        return 'wiki'
    return 'wiki/' + docname.title()

# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'a4'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '12pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'NBT.tex', u'NBT Documentation',
   u'Thomas Woolford', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


########NEW FILE########
__FILENAME__ = anvil_blockdata
#!/usr/bin/env python
"""
Print the block ID and data for a layer in a Anvil chunk
This supports regular blocks with ID 0-255 and non-standard blocks with ID 256-4095.
"""
import os, sys
import itertools

# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
import nbt


def array_4bit_to_byte(array):
    """Convert a 2048-byte array of 4096 4-bit values to an array of 4096 1-byte values.
    The result is of type bytearray().
    Note that the first byte of the created arrays contains the LEAST significant 
    bits of the first byte of the Data. NOT to the MOST significant bits, as you 
    might expected. This is because Minecraft stores data in that way.
    """
    def iterarray(array):
        for b in array:
            yield(b & 15) # Little end of the byte
            yield((b >> 4) & 15) # Big end of the byte
    return bytearray(iterarray(array))

def array_byte_to_4bit(array):
    """Convert an array of 4096 1-byte values to a 2048-byte array of 4096 4-bit values.
    The result is of type bytearray().
    Any values larger than 16 are taken modulo 16.
    Note that the first byte of the original array will be placed in the LEAST 
    significant bits of the first byte of the result. Thus NOT to the MOST 
    significant bits, as you might expected. This is because Minecraft stores 
    data in that way.
    """
    def iterarray(array):
        arrayiter = iter(array)
        for b1 in arrayiter:
            b2 = next(arrayiter, 0)
            yield(((b2 & 15) << 4) + (b1 & 15))
    return bytearray(iterarray(array))

def grouper(iterable, n, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx"
    # Taken from itertools recipe.
    args = [iter(iterable)] * n
    return itertools.zip_longest(*args, fillvalue=fillvalue)

def print_chunklayer(blocks, data, add, yoffset):
    blocks = blocks[yoffset*256:(yoffset+1)*256]
    data = array_4bit_to_byte(data[yoffset*128:(yoffset+1)*128])
    if add is not None:
        add = array_4bit_to_byte(add[yoffset*128:(yoffset+1)*128])
        for i,v in enumerate(add):
            blocks[i] += 256*v
    
    assert len(blocks) == 256
    assert len(data) == 256
    
    for row in grouper(zip(blocks,data), 16):
        print (" ".join(("%4d:%-2d" % block) for block in row))

def get_section(world, chunkx, chunky, chunkz):
    """Given a world folder, return the requested section.
    If it is not defined, raise InconceivedChunk."""
    chunk = world.get_nbt(chunkx, chunkz) # may raise InconceivedChunk
    for section in chunk['Level']['Sections']:
        if section['Y'].value == chunky:
            return section
    raise nbt.region.InconceivedChunk("Section not defined")

def main(world_folder, chunkx, chunkz, height):
    world = nbt.world.WorldFolder(world_folder)
    if not isinstance(world, nbt.world.AnvilWorldFolder):
        print("%s is not an Anvil world" % (world_folder))
        return 65 # EX_DATAERR
    chunky, yoffset = divmod(height, 16)
    try:
        section = get_section(world, chunkx, chunky, chunkz)
        try:
            blocks = section['Blocks'].value
            data = section['Data'].value
        except (KeyError, AttributeError):
            blocks = bytearray(4096)
            data = bytearray(2048)
        try:
            add = section['Add'].value
        except (KeyError, AttributeError):
            add = None
    except nbt.region.InconceivedChunk:
        print("Section undefined")
        blocks = bytearray(4096)
        data = bytearray(2048)
        add = None
    
    print_chunklayer(blocks, data, add, yoffset)
    return 0 # NOERR


def usage(message=None, appname=None):
    if appname == None:
        appname = os.path.basename(sys.argv[0])
    print("Usage: %s WORLD_FOLDER CHUNK-X CHUNK-Z BLOCKHEIGHT-Y" % appname)
    if message:
        print("%s: error: %s" % (appname, message))

if __name__ == '__main__':
    if (len(sys.argv) != 5):
        usage()
        sys.exit(64) # EX_USAGE
    world_folder = sys.argv[1]
    try:
        chunkx = int(sys.argv[2])
    except ValueError:
        usage('Chunk X-coordinate should be an integer')
        sys.exit(64) # EX_USAGE
    try:
        chunkz = int(sys.argv[3])
    except ValueError:
        usage('Chunk Z-coordinate should be an integer')
        sys.exit(64) # EX_USAGE
    try:
        height = int(sys.argv[4])
    except ValueError:
        usage('Block height Y-coordinate should be an integer')
        sys.exit(64) # EX_USAGE
    
    # clean path name, eliminate trailing slashes:
    world_folder = os.path.normpath(world_folder)
    if (not os.path.exists(world_folder)):
        usage("No such folder as "+world_folder)
        sys.exit(72) # EX_IOERR
    
    sys.exit(main(world_folder, chunkx, chunkz, height))

########NEW FILE########
__FILENAME__ = biome_analysis
#!/usr/bin/env python
"""
Counter the number of biomes in the world. Works only for Anvil-based world folders.
"""
import locale, os, sys
from struct import pack, unpack

# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.region import RegionFile
from nbt.chunk import Chunk
from nbt.world import AnvilWorldFolder,UnknownWorldFormat

BIOMES = {
    0 : "Ocean",
    1 : "Plains",
    2 : "Desert",
    3 : "Mountains",
    4 : "Forest",
    5 : "Taiga",
    6 : "Swamp",
    7 : "River",
    8 : "Nether",
    9 : "Sky",
    10: "Frozen Ocean",
    11: "Frozen River",
    12: "Ice Plains",
    13: "Ice Mountains",
    14: "Mushroom Island",
    15: "Mushroom Shore",
    16: "Beach",
    17: "Desert Hills",
    18: "Forest Hills",
    19: "Taiga Hills",
    20: "Mountains Edge",
    21: "Jungle",
    22: "Jungle Hills",
    # 255: "Not yet calculated",
}


def print_results(biome_totals):
    locale.setlocale(locale.LC_ALL, '')
    for id,count in enumerate(biome_totals):
        # Biome ID 255 is ignored. It means it is not calculated by Minecraft yet
        if id == 255 or (count == 0 and id not in BIOMES):
            continue
        if id in BIOMES:
            biome = BIOMES[id]+" (%d)" % id
        else:
            biome = "Unknown (%d)" % id
        print(locale.format_string("%-25s %10d", (biome,count)))


def main(world_folder):
    world = AnvilWorldFolder(world_folder)  # Not supported for McRegion
    if not world.nonempty():  # likely still a McRegion file
        sys.stderr.write("World folder %r is empty or not an Anvil formatted world\n" % world_folder)
        return 65  # EX_DATAERR
    biome_totals = [0]*256 # 256 counters for 256 biome IDs
    
    try:
        for chunk in world.iter_nbt():
            for biomeid in chunk["Level"]["Biomes"]:
                biome_totals[biomeid] += 1

    except KeyboardInterrupt:
        print_results(biome_totals)
        return 75 # EX_TEMPFAIL
    
    print_results(biome_totals)
    return 0 # NOERR


if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print("No world folder specified!")
        sys.exit(64) # EX_USAGE
    world_folder = sys.argv[1]
    # clean path name, eliminate trailing slashes:
    world_folder = os.path.normpath(world_folder)
    if (not os.path.exists(world_folder)):
        print("No such folder as "+world_folder)
        sys.exit(72) # EX_IOERR
    
    sys.exit(main(world_folder))

########NEW FILE########
__FILENAME__ = block_analysis
#!/usr/bin/env python
"""
Finds the contents of the different blocks in a level, taking different data values (sub block types) into account.
"""

import locale, os, sys
import glob
# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.region import RegionFile
from nbt.chunk import Chunk

def stats_per_chunk(chunk, block_data_totals):
    """Given a chunk, increment the block types with the number of blocks found"""
    for block_id, data_id in chunk.blocks.get_all_blocks_and_data():
        block_data_totals[block_id][data_id] += 1

def bounded_stats_per_chunk(chunk, block_data_totals, start, stop):
    """Given a chunk, return the number of blocks types within the specified selection"""
    chunk_z, chunk_x = chunk.get_coords()
    for z in range(16):
        world_z = z + chunk_z*16
        if ( (start != None and world_z < int(start[2])) or (stop != None and  world_z > int(stop[2])) ):
            # Outside the bounding box; skip to next iteration
            #print("Z break: %d,%d,%d" % (world_z,start[2],stop[2]))
            break
        for x in range(16):
            world_x = x + chunk_x*16
            if ( (start != None and world_x < int(start[0])) or (stop != None and world_x > int(stop[0])) ):
                # Outside the bounding box; skip to next iteration
                #print("X break: %d,%d,%d" % (world_x,start[0],stop[0]))
                break
            for y in range(128):
                if ( (start != None and y < int(start[1])) or (stop != None and y > int(stop[1])) ):
                    # Outside the bounding box; skip to next iteration
                    #print("Y break: %d,%d,%d" % (y,start[1],stop[1]))
                    break
                
                #print("Chunk: %d,%d Coord: %d,%d,%d" % (c['x'], c['z'],x,y,z))
                block_id,block_data = chunk.blocks.get_block_and_data(x,y,z)
                block_data_totals[block_id][block_data] += 1

def process_region_file(filename, start, stop):
    """Given a region filename, return the number of blocks of each ID in that file"""
    pieces = filename.split('.')
    rx = int(pieces[1])
    rz = int(pieces[2])
    
    block_data_totals = [[0]*16 for i in range(256)] # up to 16 data numbers in 256 block IDs
    
    # Does the region overlap the bounding box at all?
    if (start != None):
        if ( (rx+1)*512-1 < int(start[0]) or (rz+1)*512-1 < int(start[2]) ):
            return block_data_totals
    elif (stop != None):
        if ( rx*512-1 > int(stop[0]) or rz*512-1 > int(stop[2]) ):
            return block_data_totals
    
    file = RegionFile(filename)
    
    # Get all chunks
    chunks = file.get_chunks()
    print("Parsing %s... %d chunks" % (os.path.basename(filename),len(chunks)))
    for c in chunks:
        # Does the chunk overlap the bounding box at all?
        if (start != None):
            if ( (c['x']+1)*16 + rx*512 - 1 < int(start[0]) or (c['z']+1)*16 + rz*512 - 1 < int(start[2]) ):
                continue
        elif (stop != None):
            if ( c['x']*16 + rx*512 - 1 > int(stop[0]) or c['z']*16 + rz*512 - 1 > int(stop[2]) ):
                continue
        
        chunk = Chunk(file.get_chunk(c['x'], c['z']))
        assert chunk.get_coords() == (c['x'] + rx*32, c['z'] + rz*32)
        #print("Parsing chunk ("+str(c['x'])+", "+str(c['z'])+")")
        # Parse the blocks

        # Fast code if no start or stop coordinates are specified
        # TODO: also use this code if start/stop is specified, but the complete chunk is included
        if (start == None and stop == None):
            stats_per_chunk(chunk, block_data_totals)
        else:
            # Slow code that iterates through each coordinate
            bounded_stats_per_chunk(chunk, block_data_totals, start, stop)
    
    return block_data_totals


def print_results(block_data_totals):
    locale.setlocale(locale.LC_ALL, '')
    
    # Analyze blocks
    for block_id,data in enumerate(block_data_totals):
        if sum(data) > 0:
            datastr = ", ".join([locale.format_string("%d: %d", (i,c), grouping=True) for (i,c) in enumerate(data) if c > 0])
            print(locale.format_string("block id %3d: %12d (data id %s)", (block_id,sum(data),datastr), grouping=True))
    block_totals = [sum(data_totals) for data_totals in block_data_totals]
    
    total_blocks = sum(block_totals)
    solid_blocks = total_blocks - block_totals[0]
    solid_ratio = (solid_blocks+0.0)/total_blocks if (total_blocks > 0) else 0
    print(locale.format_string("%d total blocks in region, %d are non-air (%0.4f", (total_blocks, solid_blocks, 100.0*solid_ratio), grouping=True)+"%)")
    
    # Find valuable blocks
    print(locale.format_string("Diamond Ore:      %8d", block_totals[56], grouping=True))
    print(locale.format_string("Gold Ore:         %8d", block_totals[14], grouping=True))
    print(locale.format_string("Redstone Ore:     %8d", block_totals[73], grouping=True))
    print(locale.format_string("Iron Ore:         %8d", block_totals[15], grouping=True))
    print(locale.format_string("Coal Ore:         %8d", block_totals[16], grouping=True))
    print(locale.format_string("Lapis Lazuli Ore: %8d", block_totals[21], grouping=True))
    print(locale.format_string("Dungeons:         %8d", block_totals[52], grouping=True))
    
    print(locale.format_string("Clay:             %8d", block_totals[82], grouping=True))
    print(locale.format_string("Sugar Cane:       %8d", block_totals[83], grouping=True))
    print(locale.format_string("Cacti:            %8d", block_totals[81], grouping=True))
    print(locale.format_string("Pumpkin:          %8d", block_totals[86], grouping=True))
    print(locale.format_string("Dandelion:        %8d", block_totals[37], grouping=True))
    print(locale.format_string("Rose:             %8d", block_totals[38], grouping=True))
    print(locale.format_string("Brown Mushroom:   %8d", block_totals[39], grouping=True))
    print(locale.format_string("Red Mushroom:     %8d", block_totals[40], grouping=True))
    print(locale.format_string("Lava Springs:     %8d", block_totals[11], grouping=True))
    


def main(world_folder, start=None, stop=None):
    if (not os.path.exists(world_folder)):
        print("No such folder as "+world_folder)
        return 2 # ENOENT
    
    regions = glob.glob(os.path.join(world_folder,'region','*.mcr'))
    
    block_data_totals = [[0]*16 for i in range(256)] # up to 16 data numbers in 256 block IDs
    try:
        for filename in regions:
            region_totals = process_region_file(filename, start, stop)
            for i, data in enumerate(region_totals):
                for j, total in enumerate(data):
                    block_data_totals[i][j] += total
    
    except KeyboardInterrupt:
        print_results(block_data_totals)
        return 75 # EX_TEMPFAIL
    
    print_results(block_data_totals)
    return 0 # EX_OK


if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print("No world folder specified! Usage: %s <world folder> [minx,miny,minz maxx,maxy,maxz]" % sys.argv[0])
        sys.exit(64) # EX_USAGE
    world_folder = sys.argv[1]
    # clean path name, eliminate trailing slashes. required for os.path.basename()
    world_folder = os.path.normpath(world_folder)
    if (not os.path.exists(world_folder)):
        print("No such folder as "+world_folder)
        sys.exit(72) # EX_IOERR
    start,stop = None,None
    if (len(sys.argv) == 4):
        # A min/max corner was specified
        start_str = sys.argv[2][1:-1] # Strip parenthesis...
        start = tuple(start_str.split(',')) # and convert to tuple
        stop_str = sys.argv[3][1:-1] # Strip parenthesis...
        stop = tuple(stop_str.split(',')) # and convert to tuple
    
    sys.exit(main(world_folder, start, stop))

########NEW FILE########
__FILENAME__ = chest_analysis
#!/usr/bin/env python
"""
Finds and prints the contents of chests (including minecart chests)
"""
import locale, os, sys

# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.world import WorldFolder

class Position(object):
    def __init__(self, x,y,z):
        self.x = x
        self.y = y
        self.z = z

class Chest(object):
    def __init__(self, type, pos, items):
        self.type  = type
        self.pos   = Position(*pos)
        self.items = items

def items_from_nbt(nbtlist):
    items = {}  # block_id -> count
    for item in nbtlist:
        id = item['id'].value
        count = item['Count'].value
        if id not in items:
            items[id] = 0
        items[id] += count
    return items

def chests_per_chunk(chunk):
    """Given a chunk, increment the block types with the number of blocks found"""
    # if (len(chunk['Entities']) > 0) or (len(chunk['TileEntities']) > 0):
    #   print("Chunk %d,%d" % (chunk["xPos"],chunk["zPos"]))
    entities = []
    for entity in chunk['Entities']:
        if entity["id"].value == "Minecart" and entity["type"].value == 1:
            x,y,z = entity["Pos"]
            x,y,z = x.value,y,value,z.value
            items = items_from_nbt(entity["Items"])
            entities.append(Chest("Minecart with chest",(x,y,z),items))
    for entity in chunk['TileEntities']:
        if entity["id"].value == "Chest":
            x,y,z = entity["x"].value,entity["y"].value,entity["z"].value
            items = items_from_nbt(entity["Items"])
            entities.append(Chest("Chest",(x,y,z),items))
    return entities



def print_results(chests):
    locale.setlocale(locale.LC_ALL, '')
    for chest in chests:
        itemcount = sum(chest.items.values())
        print("%s at %s,%s,%s with %d items:" % \
            (chest.type,\
            locale.format("%0.1f",chest.pos.x,grouping=True),\
            locale.format("%0.1f",chest.pos.y,grouping=True),\
            locale.format("%0.1f",chest.pos.z,grouping=True),\
            itemcount))
        for blockid,count in chest.items.items():
            print("   %3dx Item %d" % (count, blockid))


def main(world_folder):
    world = WorldFolder(world_folder)
    
    try:
        for chunk in world.iter_nbt():
            print_results(chests_per_chunk(chunk["Level"]))

    except KeyboardInterrupt:
        return 75 # EX_TEMPFAIL
    
    return 0 # NOERR


if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print("No world folder specified!")
        sys.exit(64) # EX_USAGE
    world_folder = sys.argv[1]
    # clean path name, eliminate trailing slashes:
    world_folder = os.path.normpath(world_folder)
    if (not os.path.exists(world_folder)):
        print("No such folder as "+world_folder)
        sys.exit(72) # EX_IOERR
    
    sys.exit(main(world_folder))

########NEW FILE########
__FILENAME__ = generate_level_dat
#!/usr/bin/env python
"""
Create a file that can be used as a basic level.dat file with all required fields
"""
# http://www.minecraftwiki.net/wiki/Alpha_Level_Format#level.dat_Format

import os,sys
import time
import random

# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.nbt import NBTFile, TAG_Long, TAG_Int, TAG_String, TAG_Compound

def generate_level():
    level = NBTFile() # Blank NBT
    level.name = "Data"
    level.tags.extend([
        TAG_Long(name="Time", value=1),
        TAG_Long(name="LastPlayed", value=int(time.time())),
        TAG_Int(name="SpawnX", value=0),
        TAG_Int(name="SpawnY", value=2),
        TAG_Int(name="SpawnZ", value=0),
        TAG_Long(name="SizeOnDisk", value=0),
        TAG_Long(name="RandomSeed", value=random.randrange(1,9999999999)),
        TAG_Int(name="version", value=19132),
        TAG_String(name="LevelName", value="Testing")
    ])
    
    player = TAG_Compound()
    player.name = "Player"
    player.tags.extend([
        TAG_Int(name="Score", value=0),
        TAG_Int(name="Dimension", value=0)
    ])
    inventory = TAG_Compound()
    inventory.name = "Inventory"
    player.tags.append(inventory)
    level.tags.append(player)
    
    return level


if __name__ == '__main__':
    level = generate_level()
    print(level.pretty_tree())
    #level.write_file("level.dat")

########NEW FILE########
__FILENAME__ = map
#!/usr/bin/env python
"""
Prints a map of the entire world.
"""

import locale, os, sys
import re, math
from struct import pack, unpack
# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.region import RegionFile
from nbt.chunk import Chunk
from nbt.world import WorldFolder,McRegionWorldFolder
# PIL module (not build-in)
try:
    from PIL import Image
except ImportError:
    # PIL not in search path. Let's see if it can be found in the parent folder
    sys.stderr.write("Module PIL/Image not found. Pillow (a PIL fork) can be found at http://python-imaging.github.io/\n")
    # Note: it may also be possible that PIL is installed, but JPEG support is disabled or broken
    sys.exit(70) # EX_SOFTWARE

def get_heightmap_image(chunk, buffer=False, gmin=False, gmax=False):
    points = chunk.blocks.generate_heightmap(buffer, True)
    # Normalize the points
    hmin = min(points) if (gmin == False) else gmin # Allow setting the min/max explicitly, in case this is part of a bigger map
    hmax = max(points) if (gmax == False) else gmax
    hdelta = hmax-hmin+0.0
    pixels = ""
    for y in range(16):
        for x in range(16):
            # pix X => mc -Z
            # pix Y => mc X
            offset = (15-x)*16+y
            height = int((points[offset]-hmin)/hdelta*255)
            if (height < 0): height = 0
            if (height > 255): height = 255
            pixels += pack(">B", height)
    im = Image.fromstring('L', (16,16), pixels)
    return im

def get_map(chunk):
    # Show an image of the chunk from above
    pixels = b""
    block_colors = {
        0: {'h':0, 's':0, 'l':0},       # Air
        1: {'h':0, 's':0, 'l':32},      # Stone
        2: {'h':94, 's':42, 'l':32},    # Grass
        3: {'h':27, 's':51, 'l':15},    # Dirt
        4: {'h':0, 's':0, 'l':25},      # Cobblestone
        8: {'h':228, 's':50, 'l':23},   # Water
        9: {'h':228, 's':50, 'l':23},   # Water
        10: {'h':16, 's':100, 'l':48},  # Lava
        11: {'h':16, 's':100, 'l':48},  # Lava
        12: {'h':53, 's':22, 'l':58},   # Sand
        13: {'h':21, 's':18, 'l':20},   # Gravel
        17: {'h':35, 's':93, 'l':15},   # Wood
        18: {'h':114, 's':64, 'l':22},  # Leaves
        24: {'h':48, 's':31, 'l':40},   # Sandstone
        37: {'h':60, 's':100, 'l':60},  # Yellow Flower
        38: {'h':0, 's':100, 'l':50},   # Red Flower
        50: {'h':60, 's':100, 'l':50},  # Torch
        51: {'h':55, 's':100, 'l':50},  # Fire
        59: {'h':123, 's':60, 'l':50},  # Crops
        60: {'h':35, 's':93, 'l':15},   # Farmland
        78: {'h':240, 's':10, 'l':85},  # Snow
        79: {'h':240, 's':10, 'l':95},  # Ice
        81: {'h':126, 's':61, 'l':20},  # Cacti
        82: {'h':7, 's':62, 'l':23},    # Clay
        83: {'h':123, 's':70, 'l':50},  # Sugarcane
        86: {'h':24, 's':100, 'l':45},  # Pumpkin
        91: {'h':24, 's':100, 'l':45},  # Jack-o-lantern
    }
    for z in range(16):
        for x in range(16):
            # Find the highest block in this column
            ground_height = 127
            tints = []
            for y in range(127,-1,-1):
                block_id = chunk.blocks.get_block(x,y,z)
                block_data = chunk.blocks.get_data(x,y,z)
                if (block_id == 8 or block_id == 9):
                    tints.append({'h':228, 's':50, 'l':23}) # Water
                elif (block_id == 18):
                    if (block_data == 1):
                        tints.append({'h':114, 's':64, 'l':22}) # Redwood Leaves
                    elif (block_data == 2):
                        tints.append({'h':93, 's':39, 'l':10}) # Birch Leaves
                    else:
                        tints.append({'h':114, 's':64, 'l':22}) # Normal Leaves
                elif (block_id == 79):
                    tints.append({'h':240, 's':5, 'l':95}) # Ice
                elif (block_id == 51):
                    tints.append({'h':55, 's':100, 'l':50}) # Fire
                elif (block_id != 0 or y == 0):
                    # Here is ground level
                    ground_height = y
                    break

            color = block_colors[block_id] if (block_id in block_colors) else {'h':0, 's':0, 'l':100}
            height_shift = (ground_height-64)*0.25
            
            final_color = {'h':color['h'], 's':color['s'], 'l':color['l']+height_shift}
            if final_color['l'] > 100: final_color['l'] = 100
            if final_color['l'] < 0: final_color['l'] = 0
            
            # Apply tints from translucent blocks
            for tint in reversed(tints):
                final_color = hsl_slide(final_color, tint, 0.4)

            rgb = hsl2rgb(final_color['h'], final_color['s'], final_color['l'])

            pixels += pack("BBB", rgb[0], rgb[1], rgb[2])
    im = Image.frombytes('RGB', (16,16), pixels)
    return im


## Color functions for map generation ##

# Hue given in degrees,
# saturation and lightness given either in range 0-1 or 0-100 and returned in kind
def hsl_slide(hsl1, hsl2, ratio):
    if (abs(hsl2['h'] - hsl1['h']) > 180):
        if (hsl1['h'] > hsl2['h']):
            hsl1['h'] -= 360
        else:
            hsl1['h'] += 360
    
    # Find location of two colors on the H/S color circle
    p1x = math.cos(math.radians(hsl1['h']))*hsl1['s']
    p1y = math.sin(math.radians(hsl1['h']))*hsl1['s']
    p2x = math.cos(math.radians(hsl2['h']))*hsl2['s']
    p2y = math.sin(math.radians(hsl2['h']))*hsl2['s']
    
    # Slide part of the way from tint to base color
    avg_x = p1x + ratio*(p2x-p1x)
    avg_y = p1y + ratio*(p2y-p1y)
    avg_h = math.atan(avg_y/avg_x)
    avg_s = avg_y/math.sin(avg_h)
    avg_l = hsl1['l'] + ratio*(hsl2['l']-hsl1['l'])
    avg_h = math.degrees(avg_h)
    
    #print('tint: %s base: %s avg: %s %s %s' % (tint,final_color,avg_h,avg_s,avg_l))
    return {'h':avg_h, 's':avg_s, 'l':avg_l}


# From http://www.easyrgb.com/index.php?X=MATH&H=19#text19
def hsl2rgb(H,S,L):
    H = H/360.0
    S = S/100.0 # Turn into a percentage
    L = L/100.0
    if (S == 0):
        return (int(L*255), int(L*255), int(L*255))
    var_2 = L * (1+S) if (L < 0.5) else (L+S) - (S*L)
    var_1 = 2*L - var_2

    def hue2rgb(v1, v2, vH):
        if (vH < 0): vH += 1
        if (vH > 1): vH -= 1
        if ((6*vH)<1): return v1 + (v2-v1)*6*vH
        if ((2*vH)<1): return v2
        if ((3*vH)<2): return v1 + (v2-v1)*(2/3.0-vH)*6
        return v1
        
    R = int(255*hue2rgb(var_1, var_2, H + (1.0/3)))
    G = int(255*hue2rgb(var_1, var_2, H))
    B = int(255*hue2rgb(var_1, var_2, H - (1.0/3)))
    return (R,G,B)


def main(world_folder, show=True):
    world = McRegionWorldFolder(world_folder)  # map still only supports McRegion maps
    bb = world.get_boundingbox()
    map = Image.new('RGB', (16*bb.lenx(),16*bb.lenz()))
    t = world.chunk_count()
    try:
        i =0.0
        for chunk in world.iter_chunks():
            if i % 50 ==0:
                sys.stdout.write("Rendering image")
            elif i % 2 == 0:
                sys.stdout.write(".")
                sys.stdout.flush()
            elif i % 50 == 49:
                sys.stdout.write("%5.1f%%\n" % (100*i/t))
            i +=1
            chunkmap = get_map(chunk)
            x,z = chunk.get_coords()
            map.paste(chunkmap, (16*(x-bb.minx),16*(z-bb.minz)))
        print(" done\n")
        filename = os.path.basename(world_folder)+".png"
        map.save(filename,"PNG")
        print("Saved map as %s" % filename)
    except KeyboardInterrupt:
        print(" aborted\n")
        filename = os.path.basename(world_folder)+".partial.png"
        map.save(filename,"PNG")
        print("Saved map as %s" % filename)
        return 75 # EX_TEMPFAIL
    if show:
        map.show()
    return 0 # NOERR


if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print("No world folder specified!")
        sys.exit(64) # EX_USAGE
    if sys.argv[1] == '--noshow' and len(sys.argv) > 2:
        show = False
        world_folder = sys.argv[2]
    else:
        show = True
        world_folder = sys.argv[1]
    # clean path name, eliminate trailing slashes. required for os.path.basename()
    world_folder = os.path.normpath(world_folder)
    if (not os.path.exists(world_folder)):
        print("No such folder as "+world_folder)
        sys.exit(72) # EX_IOERR
    
    sys.exit(main(world_folder, show))

########NEW FILE########
__FILENAME__ = mob_analysis
#!/usr/bin/env python
"""
Finds and prints different entities in a game file, including mobs, items, and vehicles.
"""

import locale, os, sys
# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.world import WorldFolder

class Position(object):
    def __init__(self, x,y,z):
        self.x = x
        self.y = y
        self.z = z

class Entity(object):
    def __init__(self, type, pos):
        self.type  = type
        self.pos   = Position(*pos)


def entities_per_chunk(chunk):
    """Given a chunk, find all entities (mobs, items, vehicles)"""
    entities = []
    for entity in chunk['Entities']:
        x,y,z = entity["Pos"]
        entities.append(Entity(entity["id"].value, (x.value,y.value,z.value)))
    return entities


def print_results(entities):
    locale.setlocale(locale.LC_ALL, '')
    for entity in entities:
        print("%s at %s,%s,%s" % \
            (entity.type,\
            locale.format("%0.1f",entity.pos.x,grouping=True),\
            locale.format("%0.1f",entity.pos.y,grouping=True),\
            locale.format("%0.1f",entity.pos.z,grouping=True)))


def main(world_folder):
    world = WorldFolder(world_folder)
    
    try:
        for chunk in world.iter_nbt():
            print_results(entities_per_chunk(chunk["Level"]))

    except KeyboardInterrupt:
        return 75 # EX_TEMPFAIL
    return 0 # NOERR


if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print("No world folder specified!")
        sys.exit(64) # EX_USAGE
    world_folder = sys.argv[1]
    # clean path name, eliminate trailing slashes:
    world_folder = os.path.normpath(world_folder)
    if (not os.path.exists(world_folder)):
        print("No such folder as "+world_folder)
        sys.exit(72) # EX_IOERR
    
    sys.exit(main(world_folder))

########NEW FILE########
__FILENAME__ = regionfile_analysis
#!/usr/bin/env python
"""
Defragment a given region file.
"""

import locale, os, sys
import collections
from optparse import OptionParser
import gzip
import zlib
from struct import unpack

# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.region import RegionFile, RegionFileFormatError


class ChunkMetadata(object):
    def __init__(self, x, z):
        self.x = x
        self.z = z
        self.sectorstart = None
        self.sectorlen = None
        self.timestamp = None
        self.length = None
        self.compression = None
        self.status = None
    def __repr__(self):
        return "chunk %02s,%02s  [%d]  @%-5d %2d  %-5s %-5s %d" % (self.x, self.z, self.status, self.sectorstart, self.sectorlen, self.length, self.compression, self.timestamp)


class Statuses(object):
    """Keep track of the number of statuses for all chunks.
    The different types of status are defined in RegionFile"""
    def __init__(self):
        self.counts = {}
        self.names = {}
        # Read status names from STATUS_CHUNK_* constants in RegionFile.
        for var in dir(RegionFile):
            if var.startswith("STATUS_CHUNK_"):
                name = var[13:].title().replace("_"," ")
                value = getattr(RegionFile, var)
                self.counts[value] = 0
                self.names[value] = name
    def count(self, status, count=1):
        if status not in self.counts:
            self.counts[status] = 0
            self.names = "Status %s" % status
        self.counts[status] += count
    def get_name(self, status):
        if status in self.names:
            return self.names[status]
        else:
            return "Status %s" % status
    def results(self):
        for value in sorted(self.counts.keys()):
            yield value, self.counts[value], self.get_name(value)
    def total(self):
        return sum(self.counts.values())

class ByteCounter(object):
    """Keep track of types of bytes in a binary stream."""
    def __init__(self):
        self.counts = {}
    def count(self, bytestream):
        if isinstance(bytestream, collections.Iterable):
            for byte in bytestream:
                if byte not in self.counts:
                    self.counts[byte] = 0
                self.counts[byte] += 1
        else:
            if bytestream not in self.counts:
                self.counts[bytestream] = 0
            self.counts[bytestream] += 1
    def results(self):
        for value in sorted(self.counts.keys()):
            yield value, self.counts[value]
    

def analyse_regionfile(filename, warnings=True):
    region = RegionFile(filename)
    
    statuscounts = Statuses()
    errors = []
    if region.size % 4096 != 0:
        errors.append("File size is %d bytes, which is not a multiple of 4096" % region.size)
    sectorsize = region._bytes_to_sector(region.size)
    sectors = sectorsize*[None]
    if region.size == 0:
        errors.append("File size is 0 bytes")
        sectors = []
    elif sectorsize < 2:
        errors.append("File size is %d bytes, too small for the 8192 byte header" % region.size)
    else:
        sectors[0] = "locations"
        sectors[1] = "timestamps"
    chunks = {}
    for x in range(32):
        for z in range(32):
            c = ChunkMetadata(x,z)
            (c.sectorstart, c.sectorlen, c.timestamp, status) = region.header[x,z]
            (c.length, c.compression, c.status) = region.chunk_headers[x,z]
            c.uncompressedlength = 0
            chunks[x,z] = c
            
            statuscounts.count(c.status)
            if c.status < 0:
                errors.append("chunk %d,%d has status %d: %s" % \
                    (x, z, c.status, statuscounts.get_name(c.status)))
            
            try:
                if c.sectorstart == 0:
                    if c.sectorlen != 0:
                        errors.append("chunk %d,%d is not created, but is %d sectors in length" % (x, z, c.sectorlen))
                    if c.timestamp != 0:
                        errors.append("chunk %d,%d is not created, but has timestamp %d" % (x, z, c.timestamp))
                    raise RegionFileFormatError('')
                allocatedbytes = 4096 * c.sectorlen
                if c.timestamp == 0:
                    errors.append("chunk %d,%d has no timestamp" % (x, z))
                if c.sectorstart < 2:
                    errors.append("chunk %d,%d starts at sector %d, which is in the header" % (x, z, c.sectorstart))
                    raise RegionFileFormatError('')
                if 4096 * c.sectorstart >= region.size:
                    errors.append("chunk %d,%d starts at sector %d, while the file is only %d sectors" % (x, z, c.sectorstart, sectorsize))
                    raise RegionFileFormatError('')
                elif 4096 * c.sectorstart + 5 > region.size:
                    # header of chunk only partially fits
                    errors.append("chunk %d,%d starts at sector %d, but only %d bytes of sector %d are present in the file" % (x, z, c.sectorstart, sectorsize))
                    raise RegionFileFormatError('')
                elif not c.length:
                    errors.append("chunk %d,%d length is undefined." % (x, z))
                    raise RegionFileFormatError('')
                elif c.length == 1:
                    errors.append("chunk %d,%d has length 0 bytes." % (x, z))
                elif 4096 * c.sectorstart + 4 + c.length > region.size:
                    # header of chunk fits, but not the complete chunk
                    errors.append("chunk %d,%d is %d bytes in length, which is behind the file end" % (x, z, c.length))
                requiredsectors = region._bytes_to_sector(c.length + 4)
                if c.sectorlen <= 0:
                    errors.append("chunk %d,%d is %d sectors in length" % (x, z, c.sectorlen))
                    raise RegionFileFormatError('')
                if c.compression == 0:
                    errors.append("chunk %d,%d is uncompressed. This is deprecated." % (x, z))
                elif c.compression == 1:
                    errors.append("chunk %d,%d uses GZip compression. This is deprecated." % (x, z))
                elif c.compression > 2:
                    errors.append("chunk %d,%d uses an unknown compression type (%d)." % (x, z, c.compression))
                if c.length + 4 > allocatedbytes: # TODO 4 or 5?
                    errors.append("chunk %d,%d is %d bytes (4+1+%d) and requires %d sectors, " \
                        "but only %d %s allocated" % \
                        (x, z, c.length+4, c.length-1, requiredsectors, c.sectorlen, \
                        "sector is" if (c.sectorlen == 1) else "sectors are"))
                elif c.length + 4 + 4096 == allocatedbytes:
                    # If the block fits in exactly n sectors, Minecraft seems to allocated n+1 sectors
                    # Threat this as a warning instead of an error.
                    if warnings:
                        errors.append("chunk %d,%d is %d bytes (4+1+%d) and requires %d %s, " \
                            "but %d sectors are allocated" % \
                            (x, z, c.length+4, c.length-1, requiredsectors, \
                            "sector" if (requiredsectors == 1) else "sectors", c.sectorlen))
                elif c.sectorlen > requiredsectors:
                    errors.append("chunk %d,%d is %d bytes (4+1+%d) and requires %d %s, " \
                        "but %d sectors are allocated" % \
                        (x, z, c.length+4, c.length-1, requiredsectors, \
                        "sector" if (requiredsectors == 1) else "sectors", c.sectorlen))
                

                # Decompress chunk, check if that succeeds.
                # Check if the header and footer indicate this is a NBT file.
                # (without parsing it in detail)
                compresseddata = None
                data = None
                try:
                    if 0 <= c.compression <= 2:
                        region.file.seek(4096*c.sectorstart + 5)
                        compresseddata = region.file.read(c.length - 1)
                except Exception as e:
                    errors.append("Error reading chunk %d,%d: %s" % (x, z, str(e)))
                if (c.compression == 0):
                    data = compresseddata
                if (c.compression == 1):
                    try:
                        data = gzip.decompress(compresseddata)
                    except Exception as e:
                        errors.append("Error decompressing chunk %d,%d using gzip: %s" % (x, z, str(e)))
                elif (c.compression == 2):
                    try:
                        data = zlib.decompress(compresseddata)
                    except Exception as e:
                        errors.append("Error decompressing chunk %d,%d using zlib: %s" % (x, z, str(e)))
                if data:
                    c.uncompressedlength = len(data)
                    if data[0] != 10:
                        errors.append("chunk %d,%d is not a valid NBT file: outer object is not a TAG_Compound, but %r" % (x, z, data[0]))
                    elif data[-1] != 0:
                        errors.append("chunk %d,%d is not a valid NBT file: files does not end with a TAG_End." % (x, z))
                    else:
                        (length, ) = unpack(">H", data[1:3])
                        name = data[3:3+length]
                        try:
                            name.decode("utf-8", "strict") 
                        except Exception as e:
                            errors.append("Error decompressing chunk %d,%d using unknown compression: %s" % (x, z, str(e)))
                
                if warnings:
                    # Read the unused bytes in a sector and check if all bytes are zeroed.
                    unusedlen = 4096*c.sectorlen - (c.length+4)
                    if unusedlen > 0:
                        try:
                            region.file.seek(4096*c.sectorstart + 4 + c.length)
                            unused = region.file.read(unusedlen)
                            zeroes = unused.count(b'\x00')
                            if zeroes < unusedlen:
                                errors.append("%d of %d unused bytes are not zeroed in sector %d after chunk %d,%d" % \
                                    (unusedlen-zeroes, unusedlen, c.sectorstart + c.sectorlen - 1, x, z))
                        except Exception as e:
                            errors.append("Error reading tail of chunk %d,%d: %s" % (x, z, str(e)))
            
            except RegionFileFormatError:
                pass
            
            if c.sectorlen and c.sectorstart:
                # Check for overlapping chunks
                for b in range(c.sectorlen):
                    m = "chunk %-2d,%-2d part %d/%d" % (x, z, b+1, c.sectorlen)
                    p = c.sectorstart + b
                    if p > sectorsize:
                        errors.append("%s outside file" % (m))
                        break
                    if sectors[p] != None:
                        errors.append("overlap in sector %d: %s and %s" % (p, sectors[p], m))
                    if (b == 0):
                        if (c.uncompressedlength > 0):
                            m += " (4+1+%d bytes compressed: %d bytes uncompressed)" % (c.length-1, c.uncompressedlength)
                        elif c.length:
                            m += " (4+1+%d bytes compressed)" % (c.length-1)
                        else:
                            m += " (4+1+0 bytes)"
                    if sectors[p] != None:
                        m += " (overlapping!)"
                    sectors[p] = m
    
    e = sectors.count(None)
    if e > 0:
        if warnings:
            errors.append("Fragementation: %d of %d sectors are unused" % (e, sectorsize))
        for sector, content in enumerate(sectors):
            if content == None:
                sectors[sector] = "empty"
                if warnings:
                    region.file.seek(4096*sector)
                    unused = region.file.read(4096)
                    zeroes = unused.count(b'\x00')
                    if zeroes < 4096:
                        errors.append("%d bytes are not zeroed in unused sector %d" % (4096-zeroes, sector))

    return errors, statuscounts, sectors, chunks
    

def debug_regionfile(filename, warnings=True):
    print(filename)
    errors, statuscounts, sectors, chunks = analyse_regionfile(filename, warnings)

    print("File size is %d sectors" % (len(sectors)))
    print("Chunk statuses (as reported by nbt.region.RegionFile):")
    for value, count, name in statuscounts.results():
        print("status %2d %-21s%4d chunks" % (value, ("(%s):" % name), count))
    print("%d chunks in total" % statuscounts.total()) #q should be 1024

    if len(errors) == 0:
        print("No errors or warnings found")
    elif warnings:
        print("Errors and Warnings:")
    else:
        print("Errors:")
    for error in errors:
        print(error)

    print("File content by sector:")
    for i,s in enumerate(sectors):
        print("sector %03d: %s" % (i, s))

def print_errors(filename, warnings=True):
    errors, statuscounts, sectors, chunks = analyse_regionfile(filename, warnings)
    print(filename)
    for error in errors:
        print(error)



if __name__ == '__main__':
    parser = OptionParser()
    parser.add_option("-v", "--verbose", dest="verbose", default=False,
                    action="store_true", help="Show detailed info about region file")
    parser.add_option("-q", "--quiet", dest="warnings", default=True,
                    action="store_false", help="Only show errors, no warnings")

    (options, args) = parser.parse_args()
    if (len(args) == 0):
        print("No region file specified! Use -v for verbose results; -q for errors only (no warnings)")
        sys.exit(64) # EX_USAGE

    for filename in args:
        try:
            if options.verbose:
                debug_regionfile(filename, options.warnings)
            else:
                print_errors(filename, options.warnings)
        except IOError as e:
            sys.stderr.write("%s: %s\n" % (e.filename, e.strerror))
            # sys.exit(72) # EX_IOERR
    sys.exit(0)

########NEW FILE########
__FILENAME__ = seed
#!/usr/bin/env python
"""
Prints the seed of a world.
"""

import os, sys

# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.nbt import NBTFile

def main(world_folder):
    filename = os.path.join(world_folder,'level.dat')
    level = NBTFile(filename)
    print(level["Data"]["RandomSeed"])
    return 0 # NOERR


if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print("No world folder specified!")
        sys.exit(64) # EX_USAGE
    world_folder = sys.argv[1]
    # clean path name, eliminate trailing slashes:
    world_folder = os.path.normpath(world_folder)
    if (not os.path.exists(world_folder)):
        print("No such folder as "+world_folder)
        sys.exit(72) # EX_IOERR
    
    sys.exit(main(world_folder))

########NEW FILE########
__FILENAME__ = utilities
#!/usr/bin/env python

"""
Useful utility functions for handling large NBT structures elegantly and
Pythonically.
"""

import os,sys

# local module
try:
    import nbt
except ImportError:
    # nbt not in search path. Let's see if it can be found in the parent folder
    extrasearchpath = os.path.realpath(os.path.join(__file__,os.pardir,os.pardir))
    if not os.path.exists(os.path.join(extrasearchpath,'nbt')):
        raise
    sys.path.append(extrasearchpath)
from nbt.nbt import NBTFile, TAG_Long, TAG_Int, TAG_String, TAG_List, TAG_Compound

def unpack_nbt(tag):
    """
    Unpack an NBT tag into a native Python data structure.
    """

    if isinstance(tag, TAG_List):
        return [unpack_nbt(i) for i in tag.tags]
    elif isinstance(tag, TAG_Compound):
        return dict((i.name, unpack_nbt(i)) for i in tag.tags)
    else:
        return tag.value

def pack_nbt(s):
    """
    Pack a native Python data structure into an NBT tag. Only the following
    structures and types are supported:

     * int
     * float
     * str
     * unicode
     * dict

    Additionally, arbitrary iterables are supported.

    Packing is not lossless. In order to avoid data loss, TAG_Long and
    TAG_Double are preferred over the less precise numerical formats.

    Lists and tuples may become dicts on unpacking if they were not homogenous
    during packing, as a side-effect of NBT's format. Nothing can be done
    about this.

    Only strings are supported as keys for dicts and other mapping types. If
    your keys are not strings, they will be coerced. (Resistance is futile.)
    """

    if isinstance(s, int):
        return TAG_Long(s)
    elif isinstance(s, float):
        return TAG_Double(s)
    elif isinstance(s, (str, unicode)):
        return TAG_String(s)
    elif isinstance(s, dict):
        tag = TAG_Compound()
        for k, v in s:
            v = pack_nbt(v)
            v.name = str(k)
            tag.tags.append(v)
        return tag
    elif hasattr(s, "__iter__"):
        # We arrive at a slight quandry. NBT lists must be homogenous, unlike
        # Python lists. NBT compounds work, but require unique names for every
        # entry. On the plus side, this technique should work for arbitrary
        # iterables as well.
        tags = [pack_nbt(i) for i in s]
        t = type(tags[0])
        # If we're homogenous...
        if all(t == type(i) for i in tags):
            tag = TAG_List(type=t)
            tag.tags = tags
        else:
            tag = TAG_Compound()
            for i, item in enumerate(tags):
                item.name = str(i)
            tag.tags = tags
        return tag
    else:
        raise ValueError("Couldn't serialise type %s!" % type(s))

########NEW FILE########
__FILENAME__ = chunk
"""
Handles a single chunk of data (16x16x128 blocks) from a Minecraft save.
Chunk is currently McRegion only.
"""
from io import BytesIO
from struct import pack, unpack
import array, math

class Chunk(object):
    """Class for representing a single chunk."""
    def __init__(self, nbt):
        chunk_data = nbt['Level']
        self.coords = chunk_data['xPos'],chunk_data['zPos']
        self.blocks = BlockArray(chunk_data['Blocks'].value, chunk_data['Data'].value)

    def get_coords(self):
        """Return the coordinates of this chunk."""
        return (self.coords[0].value,self.coords[1].value)

    def __repr__(self):
        """Return a representation of this Chunk."""
        return "Chunk("+str(self.coords[0])+","+str(self.coords[1])+")"


class BlockArray(object):
    """Convenience class for dealing with a Block/data byte array."""
    def __init__(self, blocksBytes=None, dataBytes=None):
        """Create a new BlockArray, defaulting to no block or data bytes."""
        if isinstance(blocksBytes, (bytearray, array.array)):
            self.blocksList = list(blocksBytes)
        else:
            self.blocksList = [0]*32768 # Create an empty block list (32768 entries of zero (air))

        if isinstance(dataBytes, (bytearray, array.array)):
            self.dataList = list(dataBytes)
        else:
            self.dataList = [0]*16384 # Create an empty data list (32768 4-bit entries of zero make 16384 byte entries)

    # Get all block entries
    def get_all_blocks(self):
        """Return the blocks that are in this BlockArray."""
        return self.blocksList

    # Get all data entries
    def get_all_data(self):
        """Return the data of all the blocks in this BlockArray."""
        bits = []
        for b in self.dataList:
            # The first byte of the Blocks arrays correspond
            # to the LEAST significant bits of the first byte of the Data.
            # NOT to the MOST significant bits, as you might expected.
            bits.append(b & 15) # Little end of the byte
            bits.append((b >> 4) & 15) # Big end of the byte
        return bits

    # Get all block entries and data entries as tuples
    def get_all_blocks_and_data(self):
        """Return both blocks and data, packed together as tuples."""
        return list(zip(self.get_all_blocks(), self.get_all_data()))

    def get_blocks_struct(self):
        """Return a dictionary with block ids keyed to (x, y, z)."""
        cur_x = 0
        cur_y = 0
        cur_z = 0
        blocks = {}
        for block_id in self.blocksList:
            blocks[(cur_x,cur_y,cur_z)] = block_id
            cur_y += 1
            if (cur_y > 127):
                cur_y = 0
                cur_z += 1
                if (cur_z > 15):
                    cur_z = 0
                    cur_x += 1
        return blocks

    # Give blockList back as a byte array
    def get_blocks_byte_array(self, buffer=False):
        """Return a list of all blocks in this chunk."""
        if buffer:
            length = len(self.blocksList)
            return BytesIO(pack(">i", length)+self.get_blocks_byte_array())
        else:
            return array.array('B', self.blocksList).tostring()

    def get_data_byte_array(self, buffer=False):
        """Return a list of data for all blocks in this chunk."""
        if buffer:
            length = len(self.dataList)
            return BytesIO(pack(">i", length)+self.get_data_byte_array())
        else:
            return array.array('B', self.dataList).tostring()

    def generate_heightmap(self, buffer=False, as_array=False):
        """Return a heightmap, representing the highest solid blocks in this chunk."""
        non_solids = [0, 8, 9, 10, 11, 38, 37, 32, 31]
        if buffer:
            return BytesIO(pack(">i", 256)+self.generate_heightmap()) # Length + Heightmap, ready for insertion into Chunk NBT
        else:
            bytes = []
            for z in range(16):
                for x in range(16):
                    for y in range(127, -1, -1):
                        offset = y + z*128 + x*128*16
                        if (self.blocksList[offset] not in non_solids or y == 0):
                            bytes.append(y+1)
                            break
            if (as_array):
                return bytes
            else:
                return array.array('B', bytes).tostring()

    def set_blocks(self, list=None, dict=None, fill_air=False):
        """
        Sets all blocks in this chunk, using either a list or dictionary.  
        Blocks not explicitly set can be filled to air by setting fill_air to True.
        """
        if list:
            # Inputting a list like self.blocksList
            self.blocksList = list
        elif dict:
            # Inputting a dictionary like result of self.get_blocks_struct()
            list = []
            for x in range(16):
                for z in range(16):
                    for y in range(128):
                        coord = x,y,z
                        offset = y + z*128 + x*128*16
                        if (coord in dict):
                            list.append(dict[coord])
                        else:
                            if (self.blocksList[offset] and not fill_air):
                                list.append(self.blocksList[offset])
                            else:
                                list.append(0) # Air
            self.blocksList = list
        else:
            # None of the above...
            return False
        return True

    def set_block(self, x,y,z, id, data=0):
        """Sets the block a x, y, z to the specified id, and optionally data."""
        offset = y + z*128 + x*128*16
        self.blocksList[offset] = id
        if (offset % 2 == 1):
            # offset is odd
            index = (offset-1)//2
            b = self.dataList[index]
            self.dataList[index] = (b & 240) + (data & 15) # modify lower bits, leaving higher bits in place
        else:
            # offset is even
            index = offset//2
            b = self.dataList[index]
            self.dataList[index] = (b & 15) + (data << 4 & 240) # modify ligher bits, leaving lower bits in place

    # Get a given X,Y,Z or a tuple of three coordinates
    def get_block(self, x,y,z, coord=False):
        """Return the id of the block at x, y, z."""
        """
        Laid out like:
        (0,0,0), (0,1,0), (0,2,0) ... (0,127,0), (0,0,1), (0,1,1), (0,2,1) ... (0,127,1), (0,0,2) ... (0,127,15), (1,0,0), (1,1,0) ... (15,127,15)
        
        ::
        
          blocks = []
          for x in range(15):
            for z in range(15):
              for y in range(127):
                blocks.append(Block(x,y,z))
        """

        offset = y + z*128 + x*128*16 if (coord == False) else coord[1] + coord[2]*128 + coord[0]*128*16
        return self.blocksList[offset]

    # Get a given X,Y,Z or a tuple of three coordinates
    def get_data(self, x,y,z, coord=False):
        """Return the data of the block at x, y, z."""
        offset = y + z*128 + x*128*16 if (coord == False) else coord[1] + coord[2]*128 + coord[0]*128*16
        # The first byte of the Blocks arrays correspond
        # to the LEAST significant bits of the first byte of the Data.
        # NOT to the MOST significant bits, as you might expected.
        if (offset % 2 == 1):
            # offset is odd
            index = (offset-1)//2
            b = self.dataList[index]
            return b & 15 # Get little (last 4 bits) end of byte
        else:
            # offset is even
            index = offset//2
            b = self.dataList[index]
            return (b >> 4) & 15 # Get big end (first 4 bits) of byte

    def get_block_and_data(self, x,y,z, coord=False):
        """Return the tuple of (id, data) for the block at x, y, z"""
        return (self.get_block(x,y,z,coord),self.get_data(x,y,z,coord))


########NEW FILE########
__FILENAME__ = nbt
"""
Handle the NBT (Named Binary Tag) data format
"""

from struct import Struct, error as StructError
from gzip import GzipFile
import zlib
from collections import MutableMapping, MutableSequence, Sequence
import os, io

try:
    unicode
    basestring
except NameError:
    unicode = str  # compatibility for Python 3
    basestring = str  # compatibility for Python 3


TAG_END = 0
TAG_BYTE = 1
TAG_SHORT = 2
TAG_INT = 3
TAG_LONG = 4
TAG_FLOAT = 5
TAG_DOUBLE = 6
TAG_BYTE_ARRAY = 7
TAG_STRING = 8
TAG_LIST = 9
TAG_COMPOUND = 10
TAG_INT_ARRAY = 11

class MalformedFileError(Exception):
    """Exception raised on parse error."""
    pass

class TAG(object):
    """TAG, a variable with an intrinsic name."""
    id = None

    def __init__(self, value=None, name=None):
        self.name = name
        self.value = value

    #Parsers and Generators
    def _parse_buffer(self, buffer):
        raise NotImplementedError(self.__class__.__name__)

    def _render_buffer(self, buffer):
        raise NotImplementedError(self.__class__.__name__)

    #Printing and Formatting of tree
    def tag_info(self):
        """Return Unicode string with class, name and unnested value."""
        return self.__class__.__name__ + \
                ('(%r)' % self.name if self.name else "") + \
                ": " + self.valuestr()
    def valuestr(self):
        """Return Unicode string of unnested value. For iterators, this returns a summary."""
        return unicode(self.value)

    def pretty_tree(self, indent=0):
        """Return formated Unicode string of self, where iterable items are recursively listed in detail."""
        return ("\t"*indent) + self.tag_info()

    # Python 2 compatibility; Python 3 uses __str__ instead.
    def __unicode__(self):
        """Return a unicode string with the result in human readable format. Unlike valuestr(), the result is recursive for iterators till at least one level deep."""
        return unicode(self.value)

    def __str__(self):
        """Return a string (ascii formated for Python 2, unicode for Python 3) with the result in human readable format. Unlike valuestr(), the result is recursive for iterators till at least one level deep."""
        return str(self.value)
    # Unlike regular iterators, __repr__() is not recursive.
    # Use pretty_tree for recursive results.
    # iterators should use __repr__ or tag_info for each item, like regular iterators
    def __repr__(self):
        """Return a string (ascii formated for Python 2, unicode for Python 3) describing the class, name and id for debugging purposes."""
        return "<%s(%r) at 0x%x>" % (self.__class__.__name__,self.name,id(self))

class _TAG_Numeric(TAG):
    """_TAG_Numeric, comparable to int with an intrinsic name"""
    def __init__(self, value=None, name=None, buffer=None):
        super(_TAG_Numeric, self).__init__(value, name)
        if buffer:
            self._parse_buffer(buffer)

    #Parsers and Generators
    def _parse_buffer(self, buffer):
        # Note: buffer.read() may raise an IOError, for example if buffer is a corrupt gzip.GzipFile
        self.value = self.fmt.unpack(buffer.read(self.fmt.size))[0]

    def _render_buffer(self, buffer):
        buffer.write(self.fmt.pack(self.value))

class _TAG_End(TAG):
    id = TAG_END
    fmt = Struct(">b")

    def _parse_buffer(self, buffer):
        # Note: buffer.read() may raise an IOError, for example if buffer is a corrupt gzip.GzipFile
        value = self.fmt.unpack(buffer.read(1))[0]
        if value != 0:
            raise ValueError("A Tag End must be rendered as '0', not as '%d'." % (value))

    def _render_buffer(self, buffer):
        buffer.write(b'\x00')

#== Value Tags ==#
class TAG_Byte(_TAG_Numeric):
    """Represent a single tag storing 1 byte."""
    id = TAG_BYTE
    fmt = Struct(">b")

class TAG_Short(_TAG_Numeric):
    """Represent a single tag storing 1 short."""
    id = TAG_SHORT
    fmt = Struct(">h")

class TAG_Int(_TAG_Numeric):
    """Represent a single tag storing 1 int."""
    id = TAG_INT
    fmt = Struct(">i")
    """Struct(">i"), 32-bits integer, big-endian"""

class TAG_Long(_TAG_Numeric):
    """Represent a single tag storing 1 long."""
    id = TAG_LONG
    fmt = Struct(">q")

class TAG_Float(_TAG_Numeric):
    """Represent a single tag storing 1 IEEE-754 floating point number."""
    id = TAG_FLOAT
    fmt = Struct(">f")

class TAG_Double(_TAG_Numeric):
    """Represent a single tag storing 1 IEEE-754 double precision floating point number."""
    id = TAG_DOUBLE
    fmt = Struct(">d")

class TAG_Byte_Array(TAG, MutableSequence):
    """
    TAG_Byte_Array, comparable to a collections.UserList with
    an intrinsic name whose values must be bytes
    """
    id = TAG_BYTE_ARRAY
    def __init__(self, name=None, buffer=None):
        super(TAG_Byte_Array, self).__init__(name=name)
        if buffer:
            self._parse_buffer(buffer)

    #Parsers and Generators
    def _parse_buffer(self, buffer):
        length = TAG_Int(buffer=buffer)
        self.value = bytearray(buffer.read(length.value))

    def _render_buffer(self, buffer):
        length = TAG_Int(len(self.value))
        length._render_buffer(buffer)
        buffer.write(bytes(self.value))

    # Mixin methods
    def __len__(self):
        return len(self.value)

    def __iter__(self):
        return iter(self.value)

    def __contains__(self, item):
        return item in self.value

    def __getitem__(self, key):
        return self.value[key]

    def __setitem__(self, key, value):
        # TODO: check type of value
        self.value[key] = value

    def __delitem__(self, key):
        del(self.value[key])

    def insert(self, key, value):
        # TODO: check type of value, or is this done by self.value already?
        self.value.insert(key, value)

    #Printing and Formatting of tree
    def valuestr(self):
        return "[%i byte(s)]" % len(self.value)

    def __unicode__(self):
        return '['+",".join([unicode(x) for x in self.value])+']'
    def __str__(self):
        return '['+",".join([str(x) for x in self.value])+']'

class TAG_Int_Array(TAG, MutableSequence):
    """
    TAG_Int_Array, comparable to a collections.UserList with
    an intrinsic name whose values must be integers
    """
    id = TAG_INT_ARRAY
    def __init__(self, name=None, buffer=None):
        super(TAG_Int_Array, self).__init__(name=name)
        if buffer:
            self._parse_buffer(buffer)

    def update_fmt(self, length):
        """ Adjust struct format description to length given """
        self.fmt = Struct(">" + str(length) + "i")

    #Parsers and Generators
    def _parse_buffer(self, buffer):
        length = TAG_Int(buffer=buffer).value
        self.update_fmt(length)
        self.value = list(self.fmt.unpack(buffer.read(self.fmt.size)))

    def _render_buffer(self, buffer):
        length = len(self.value)
        self.update_fmt(length)
        TAG_Int(length)._render_buffer(buffer)
        buffer.write(self.fmt.pack(*self.value))

    # Mixin methods
    def __len__(self):
        return len(self.value)

    def __iter__(self):
        return iter(self.value)

    def __contains__(self, item):
        return item in self.value

    def __getitem__(self, key):
        return self.value[key]

    def __setitem__(self, key, value):
        self.value[key] = value

    def __delitem__(self, key):
        del(self.value[key])

    def insert(self, key, value):
        self.value.insert(key, value)

    #Printing and Formatting of tree
    def valuestr(self):
        return "[%i int(s)]" % len(self.value)


class TAG_String(TAG, Sequence):
    """
    TAG_String, comparable to a collections.UserString with an
    intrinsic name
    """
    id = TAG_STRING
    def __init__(self, value=None, name=None, buffer=None):
        super(TAG_String, self).__init__(value, name)
        if buffer:
            self._parse_buffer(buffer)

    #Parsers and Generators
    def _parse_buffer(self, buffer):
        length = TAG_Short(buffer=buffer)
        read = buffer.read(length.value)
        if len(read) != length.value:
            raise StructError()
        self.value = read.decode("utf-8")

    def _render_buffer(self, buffer):
        save_val = self.value.encode("utf-8")
        length = TAG_Short(len(save_val))
        length._render_buffer(buffer)
        buffer.write(save_val)

    # Mixin methods
    def __len__(self):
        return len(self.value)

    def __iter__(self):
        return iter(self.value)

    def __contains__(self, item):
        return item in self.value

    def __getitem__(self, key):
        return self.value[key]

    #Printing and Formatting of tree
    def __repr__(self):
        return self.value

#== Collection Tags ==#
class TAG_List(TAG, MutableSequence):
    """
    TAG_List, comparable to a collections.UserList with an intrinsic name
    """
    id = TAG_LIST
    def __init__(self, type=None, value=None, name=None, buffer=None):
        super(TAG_List, self).__init__(value, name)
        if type:
            self.tagID = type.id
        else:
            self.tagID = None
        self.tags = []
        if buffer:
            self._parse_buffer(buffer)
        if self.tagID == None:
            raise ValueError("No type specified for list: %s" % (name))

    #Parsers and Generators
    def _parse_buffer(self, buffer):
        self.tagID = TAG_Byte(buffer=buffer).value
        self.tags = []
        length = TAG_Int(buffer=buffer)
        for x in range(length.value):
            self.tags.append(TAGLIST[self.tagID](buffer=buffer))

    def _render_buffer(self, buffer):
        TAG_Byte(self.tagID)._render_buffer(buffer)
        length = TAG_Int(len(self.tags))
        length._render_buffer(buffer)
        for i, tag in enumerate(self.tags):
            if tag.id != self.tagID:
                raise ValueError("List element %d(%s) has type %d != container type %d" %
                         (i, tag, tag.id, self.tagID))
            tag._render_buffer(buffer)

    # Mixin methods
    def __len__(self):
        return len(self.tags)

    def __iter__(self):
        return iter(self.tags)

    def __contains__(self, item):
        return item in self.tags

    def __getitem__(self, key):
        return self.tags[key]

    def __setitem__(self, key, value):
        self.tags[key] = value

    def __delitem__(self, key):
        del(self.tags[key])

    def insert(self, key, value):
        self.tags.insert(key, value)

    #Printing and Formatting of tree
    def __repr__(self):
        return "%i entries of type %s" % (len(self.tags), TAGLIST[self.tagID].__name__)

    #Printing and Formatting of tree
    def valuestr(self):
        return "[%i %s(s)]" % (len(self.tags), TAGLIST[self.tagID].__name__)
    def __unicode__(self):
        return "["+", ".join([tag.tag_info() for tag in self.tags])+"]"
    def __str__(self):
        return "["+", ".join([tag.tag_info() for tag in self.tags])+"]"

    def pretty_tree(self, indent=0):
        output = [super(TAG_List, self).pretty_tree(indent)]
        if len(self.tags):
            output.append(("\t"*indent) + "{")
            output.extend([tag.pretty_tree(indent + 1) for tag in self.tags])
            output.append(("\t"*indent) + "}")
        return '\n'.join(output)

class TAG_Compound(TAG, MutableMapping):
    """
    TAG_Compound, comparable to a collections.OrderedDict with an
    intrinsic name
    """
    id = TAG_COMPOUND
    def __init__(self, buffer=None):
        super(TAG_Compound, self).__init__()
        self.tags = []
        self.name = ""
        if buffer:
            self._parse_buffer(buffer)

    #Parsers and Generators
    def _parse_buffer(self, buffer):
        while True:
            type = TAG_Byte(buffer=buffer)
            if type.value == TAG_END:
                #print("found tag_end")
                break
            else:
                name = TAG_String(buffer=buffer).value
                try:
                    tag = TAGLIST[type.value](buffer=buffer)
                    tag.name = name
                    self.tags.append(tag)
                except KeyError:
                    raise ValueError("Unrecognised tag type")

    def _render_buffer(self, buffer):
        for tag in self.tags:
            TAG_Byte(tag.id)._render_buffer(buffer)
            TAG_String(tag.name)._render_buffer(buffer)
            tag._render_buffer(buffer)
        buffer.write(b'\x00') #write TAG_END

    # Mixin methods
    def __len__(self):
        return len(self.tags)

    def __iter__(self):
        for key in self.tags:
            yield key.name

    def __contains__(self, key):
        if isinstance(key, int):
            return key <= len(self.tags)
        elif isinstance(key, basestring):
            for tag in self.tags:
                if tag.name == key:
                    return True
            return False
        elif isinstance(key, TAG):
            return key in self.tags
        return False

    def __getitem__(self, key):
        if isinstance(key, int):
            return self.tags[key]
        elif isinstance(key, basestring):
            for tag in self.tags:
                if tag.name == key:
                    return tag
            else:
                raise KeyError("Tag %s does not exist" % key)
        else:
            raise TypeError("key needs to be either name of tag, or index of tag, not a %s" % type(key).__name__)

    def __setitem__(self, key, value):
        assert isinstance(value, TAG), "value must be an nbt.TAG"
        if isinstance(key, int):
            # Just try it. The proper error will be raised if it doesn't work.
            self.tags[key] = value
        elif isinstance(key, basestring):
            value.name = key
            for i, tag in enumerate(self.tags):
                if tag.name == key:
                    self.tags[i] = value
                    return
            self.tags.append(value)

    def __delitem__(self, key):
        if isinstance(key, int):
            del(self.tags[key])
        elif isinstance(key, basestring):
            self.tags.remove(self.__getitem__(key))
        else:
            raise ValueError("key needs to be either name of tag, or index of tag")

    def keys(self):
        return [tag.name for tag in self.tags]

    def iteritems(self):
        for tag in self.tags:
            yield (tag.name, tag)

    #Printing and Formatting of tree
    def __unicode__(self):
        return "{"+", ".join([tag.tag_info() for tag in self.tags])+"}"
    def __str__(self):
        return "{"+", ".join([tag.tag_info() for tag in self.tags])+"}"

    def valuestr(self):
        return '{%i Entries}' % len(self.tags)

    def pretty_tree(self, indent=0):
        output = [super(TAG_Compound, self).pretty_tree(indent)]
        if len(self.tags):
            output.append(("\t"*indent) + "{")
            output.extend([tag.pretty_tree(indent + 1) for tag in self.tags])
            output.append(("\t"*indent) + "}")
        return '\n'.join(output)


TAGLIST = {TAG_END: _TAG_End, TAG_BYTE:TAG_Byte, TAG_SHORT:TAG_Short, TAG_INT:TAG_Int, TAG_LONG:TAG_Long, TAG_FLOAT:TAG_Float, TAG_DOUBLE:TAG_Double, TAG_BYTE_ARRAY:TAG_Byte_Array, TAG_STRING:TAG_String, TAG_LIST:TAG_List, TAG_COMPOUND:TAG_Compound, TAG_INT_ARRAY:TAG_Int_Array}

class NBTFile(TAG_Compound):
    """Represent an NBT file object."""
    def __init__(self, filename=None, buffer=None, fileobj=None):
        super(NBTFile, self).__init__()
        self.filename = filename
        self.type = TAG_Byte(self.id)
        closefile = True
        #make a file object
        if filename:
            self.file = GzipFile(filename, 'rb')
        elif buffer:
            if hasattr(buffer, 'name'):
                self.filename = buffer.name
            self.file = buffer
            closefile = False
        elif fileobj:
            if hasattr(fileobj, 'name'):
                self.filename = fileobj.name
            self.file = GzipFile(fileobj=fileobj)
        else:
            self.file = None
            closefile = False
        #parse the file given initially
        if self.file:
            self.parse_file()
            if closefile:
                # Note: GzipFile().close() does NOT close the fileobj, 
                # So the caller is still responsible for closing that.
                try:
                    self.file.close()
                except (AttributeError, IOError):
                    pass
            self.file = None

    def parse_file(self, filename=None, buffer=None, fileobj=None):
        """Completely parse a file, extracting all tags."""
        if filename:
            self.file = GzipFile(filename, 'rb')
        elif buffer:
            if hasattr(buffer, 'name'):
                self.filename = buffer.name
            self.file = buffer
        elif fileobj:
            if hasattr(fileobj, 'name'):
                self.filename = fileobj.name
            self.file = GzipFile(fileobj=fileobj)
        if self.file:
            try:
                type = TAG_Byte(buffer=self.file)
                if type.value == self.id:
                    name = TAG_String(buffer=self.file).value
                    self._parse_buffer(self.file)
                    self.name = name
                    self.file.close()
                else:
                    raise MalformedFileError("First record is not a Compound Tag")
            except StructError as e:
                raise MalformedFileError("Partial File Parse: file possibly truncated.")
        else:
            raise ValueError("NBTFile.parse_file(): Need to specify either a filename or a file object")

    def write_file(self, filename=None, buffer=None, fileobj=None):
        """Write this NBT file to a file."""
        closefile = True
        if buffer:
            self.filename = None
            self.file = buffer
            closefile = False
        elif filename:
            self.filename = filename
            self.file = GzipFile(filename, "wb")
        elif fileobj:
            self.filename = None
            self.file = GzipFile(fileobj=fileobj, mode="wb")
        elif self.filename:
            self.file = GzipFile(self.filename, "wb")
        elif not self.file:
            raise ValueError("NBTFile.write_file(): Need to specify either a filename or a file object")
        #Render tree to file
        TAG_Byte(self.id)._render_buffer(self.file)
        TAG_String(self.name)._render_buffer(self.file)
        self._render_buffer(self.file)
        #make sure the file is complete
        try:
            self.file.flush()
        except (AttributeError, IOError):
            pass
        if closefile:
            try:
                self.file.close()
            except (AttributeError, IOError):
                pass

    def __repr__(self):
        """
        Return a string (ascii formated for Python 2, unicode
        for Python 3) describing the class, name and id for
        debugging purposes.
        """
        if self.filename:
            return "<%s(%r) with %s(%r) at 0x%x>" % (self.__class__.__name__, self.filename, \
                    TAG_Compound.__name__, self.name, id(self))
        else:
            return "<%s with %s(%r) at 0x%x>" % (self.__class__.__name__, \
                    TAG_Compound.__name__, self.name, id(self))

########NEW FILE########
__FILENAME__ = region
"""
Handle a region file, containing 32x32 chunks.
For more info of the region file format look:
http://www.minecraftwiki.net/wiki/Region_file_format
"""

from .nbt import NBTFile, MalformedFileError
from struct import pack, unpack
from gzip import GzipFile
from collections import Mapping
import zlib
import gzip
from io import BytesIO
import math, time
from os.path import getsize
from os import SEEK_END

# constants

SECTOR_LENGTH = 4096
"""Constant indicating the length of a sector. A Region file is divided in sectors of 4096 bytes each."""

# Status is a number representing:
# -5 = Error, the chunk is overlapping with another chunk
# -4 = Error, the chunk length is too large to fit in the sector length in the region header
# -3 = Error, chunk header has a 0 length
# -2 = Error, chunk inside the header of the region file
# -1 = Error, chunk partially/completely outside of file
#  0 = Ok
#  1 = Chunk non-existant yet
STATUS_CHUNK_OVERLAPPING = -5
"""Constant indicating an error status: the chunk is allocated a sector already occupied by another chunk"""
STATUS_CHUNK_MISMATCHED_LENGTHS = -4
"""Constant indicating an error status: the region header length and the chunk length are incompatible"""
STATUS_CHUNK_ZERO_LENGTH = -3
"""Constant indicating an error status: chunk header has a 0 length"""
STATUS_CHUNK_IN_HEADER = -2
"""Constant indicating an error status: chunk inside the header of the region file"""
STATUS_CHUNK_OUT_OF_FILE = -1
"""Constant indicating an error status: chunk partially/completely outside of file"""
STATUS_CHUNK_OK = 0
"""Constant indicating an normal status: the chunk exists and the metadata is valid"""
STATUS_CHUNK_NOT_CREATED = 1
"""Constant indicating an normal status: the chunk does not exist"""

COMPRESSION_NONE = 0
"""Constant indicating tha tthe chunk is not compressed."""
COMPRESSION_GZIP = 1
"""Constant indicating tha tthe chunk is GZip compressed."""
COMPRESSION_ZLIB = 2
"""Constant indicating tha tthe chunk is zlib compressed."""


# TODO: reconsider these errors. where are they catched? Where would an implementation make a difference in handling the different exceptions.

class RegionFileFormatError(Exception):
    """Base class for all file format errors.
    Note: InconceivedChunk is not a child class, because it is not considered a format error."""
    def __init__(self, msg=""):
        self.msg = msg
    def __str__(self):
        return self.msg

class NoRegionHeader(RegionFileFormatError):
    """The size of the region file is too small to contain a header."""

class RegionHeaderError(RegionFileFormatError):
    """Error in the header of the region file for a given chunk."""

class ChunkHeaderError(RegionFileFormatError):
    """Error in the header of a chunk, included the bytes of length and byte version."""

class ChunkDataError(RegionFileFormatError):
    """Error in the data of a chunk."""

class InconceivedChunk(LookupError):
    """Specified chunk has not yet been generated."""
    def __init__(self, msg=""):
        self.msg = msg


class ChunkMetadata(object):
    """
    Metadata for a particular chunk found in the 8 kiByte header and 5-byte chunk header.
    """

    def __init__(self, x, z):
        self.x = x
        """x-coordinate of the chunk in the file"""
        self.z = z
        """z-coordinate of the chunk in the file"""
        self.blockstart = 0
        """start of the chunk block, counted in 4 kiByte sectors from the
        start of the file. (24 bit int)"""
        self.blocklength = 0
        """amount of 4 kiBytes sectors in the block (8 bit int)"""
        self.timestamp = 0
        """a Unix timestamps (seconds since epoch) (32 bits), found in the
        second sector in the file."""
        self.length = 0
        """length of the block in bytes. This excludes the 4-byte length header,
        and includes the 1-byte compression byte. (32 bit int)"""
        self.compression = None
        """type of compression used for the chunk block. (8 bit int).
    
        - 0: uncompressed
        - 1: gzip compression
        - 2: zlib compression"""
        self.status = STATUS_CHUNK_NOT_CREATED
        """status as determined from blockstart, blocklength, length, file size
        and location of other chunks in the file.
        
        - STATUS_CHUNK_OVERLAPPING
        - STATUS_CHUNK_MISMATCHED_LENGTHS
        - STATUS_CHUNK_ZERO_LENGTH
        - STATUS_CHUNK_IN_HEADER
        - STATUS_CHUNK_OUT_OF_FILE
        - STATUS_CHUNK_OK
        - STATUS_CHUNK_NOT_CREATED"""
    def __str__(self):
        return "%s(%d, %d, sector=%s, length=%s, timestamp=%s, lenght=%s, compression=%s, status=%s)" % \
            (self.__class__.__name__, self.x, self.z, self.blockstart, self.blocklength, self.timestamp, \
            self.length, self.compression, self.status)
    def __repr__(self):
        return "%s(%d,%d)" % (self.__class__.__name__, self.x, self.z)
    def requiredblocks(self):
        # slightly faster variant of: floor(self.length + 4) / 4096))
        return (self.length + 3 + SECTOR_LENGTH) // SECTOR_LENGTH
    def is_created(self):
        """return True if this chunk is created according to the header.
        This includes chunks which are not readable for other reasons."""
        return self.blockstart != 0

class _HeaderWrapper(Mapping):
    """Wrapper around self.metadata to emulate the old self.header variable"""
    def __init__(self, metadata):
        self.metadata = metadata
    def __getitem__(self, xz):
        m = self.metadata[xz]
        return (m.blockstart, m.blocklength, m.timestamp, m.status)
    def __iter__(self):
        return iter(self.metadata) # iterates of the keys
    def __len__(self):
        return len(self.metadata)
class _ChunkHeaderWrapper(Mapping):
    """Wrapper around self.metadata to emulate the old self.chunk_headers variable"""
    def __init__(self, metadata):
        self.metadata = metadata
    def __getitem__(self, xz):
        m = self.metadata[xz]
        return (m.length if m.length > 0 else None, m.compression, m.status)
    def __iter__(self):
        return iter(self.metadata) # iterates of the keys
    def __len__(self):
        return len(self.metadata)

class RegionFile(object):
    """A convenience class for extracting NBT files from the Minecraft Beta Region Format."""
    
    # Redefine constants for backward compatibility.
    STATUS_CHUNK_OVERLAPPING = STATUS_CHUNK_OVERLAPPING
    """Constant indicating an error status: the chunk is allocated a sector
    already occupied by another chunk. 
    Deprecated. Use :const:`nbt.region.STATUS_CHUNK_OVERLAPPING` instead."""
    STATUS_CHUNK_MISMATCHED_LENGTHS = STATUS_CHUNK_MISMATCHED_LENGTHS
    """Constant indicating an error status: the region header length and the chunk
    length are incompatible. Deprecated. Use :const:`nbt.region.STATUS_CHUNK_MISMATCHED_LENGTHS` instead."""
    STATUS_CHUNK_ZERO_LENGTH = STATUS_CHUNK_ZERO_LENGTH
    """Constant indicating an error status: chunk header has a 0 length.
    Deprecated. Use :const:`nbt.region.STATUS_CHUNK_ZERO_LENGTH` instead."""
    STATUS_CHUNK_IN_HEADER = STATUS_CHUNK_IN_HEADER
    """Constant indicating an error status: chunk inside the header of the region file.
    Deprecated. Use :const:`nbt.region.STATUS_CHUNK_IN_HEADER` instead."""
    STATUS_CHUNK_OUT_OF_FILE = STATUS_CHUNK_OUT_OF_FILE
    """Constant indicating an error status: chunk partially/completely outside of file.
    Deprecated. Use :const:`nbt.region.STATUS_CHUNK_OUT_OF_FILE` instead."""
    STATUS_CHUNK_OK = STATUS_CHUNK_OK
    """Constant indicating an normal status: the chunk exists and the metadata is valid.
    Deprecated. Use :const:`nbt.region.STATUS_CHUNK_OK` instead."""
    STATUS_CHUNK_NOT_CREATED = STATUS_CHUNK_NOT_CREATED
    """Constant indicating an normal status: the chunk does not exist.
    Deprecated. Use :const:`nbt.region.STATUS_CHUNK_NOT_CREATED` instead."""
    
    def __init__(self, filename=None, fileobj=None):
        """
        Read a region file by filename of file object. 
        If a fileobj is specified, it is not closed after use; it is the callers responibility to close that.
        """
        self.file = None
        self.filename = None
        self._closefile = False
        if filename:
            self.filename = filename
            self.file = open(filename, 'r+b') # open for read and write in binary mode
            self._closefile = True
        elif fileobj:
            if hasattr(fileobj, 'name'):
                self.filename = fileobj.name
            self.file = fileobj
        elif not self.file:
            raise ValueError("RegionFile(): Need to specify either a filename or a file object")

        # Some variables
        self.metadata = {}
        """
        dict containing ChunkMetadata objects, gathered from metadata found in the
        8 kiByte header and 5-byte chunk header.
        
        ``metadata[x, z]: ChunkMetadata()``
        """
        self.header = _HeaderWrapper(self.metadata)
        """
        dict containing the metadata found in the 8 kiByte header:
        
        ``header[x, z]: (offset, sectionlength, timestamp, status)``
        
        :offset: counts in 4 kiByte sectors, starting from the start of the file. (24 bit int)
        :blocklength: is in 4 kiByte sectors (8 bit int)
        :timestamp: is a Unix timestamps (seconds since epoch) (32 bits)
        :status: can be any of:
        
            - STATUS_CHUNK_OVERLAPPING
            - STATUS_CHUNK_MISMATCHED_LENGTHS
            - STATUS_CHUNK_ZERO_LENGTH
            - STATUS_CHUNK_IN_HEADER
            - STATUS_CHUNK_OUT_OF_FILE
            - STATUS_CHUNK_OK
            - STATUS_CHUNK_NOT_CREATED
        
        Deprecated. Use :attr:`metadata` instead.
        """
        self.chunk_headers = _ChunkHeaderWrapper(self.metadata)
        """
        dict containing the metadata found in each chunk block:
        
        ``chunk_headers[x, z]: (length, compression, chunk_status)``
        
        :chunk length: in bytes, starting from the compression byte (32 bit int)
        :compression: is 1 (Gzip) or 2 (bzip) (8 bit int)
        :chunk_status: is equal to status in :attr:`header`.
        
        If the chunk is not defined, the tuple is (None, None, STATUS_CHUNK_NOT_CREATED)
        
        Deprecated. Use :attr:`metadata` instead.
        """

        self._init_header()
        self._parse_header()
        self._parse_chunk_headers()

    def get_size(self):
        """ Returns the file size in bytes. """
        # seek(0,2) jumps to 0-bytes from the end of the file.
        # Python 2.6 support: seek does not yet return the position.
        self.file.seek(0, SEEK_END)
        return self.file.tell()

    @staticmethod
    def _bytes_to_sector(bsize, sectorlength=SECTOR_LENGTH):
        """Given a size in bytes, return how many sections of length sectorlen are required to contain it.
        This is equivalent to ceil(bsize/sectorlen), if Python would use floating
        points for division, and integers for ceil(), rather than the other way around."""
        sectors, remainder = divmod(bsize, sectorlength)
        return sectors if remainder == 0 else sectors + 1
    
    def __del__(self):
        if self._closefile:
            self.file.close()
        # Parent object() has no __del__ method, otherwise it should be called here.

    def _init_file(self):
        """Initialise the file header. This will erase any data previously in the file."""
        header_length = 2*SECTOR_LENGTH
        if self.size > header_length:
            self.file.truncate(header_length)
        self.file.seek(0)
        self.file.write(header_length*b'\x00')
        self.size = header_length

    def _init_header(self):
        for x in range(32):
            for z in range(32):
                self.metadata[x,z] = ChunkMetadata(x, z)

    def _parse_header(self):
        """Read the region header and stores: offset, length and status."""
        # update the file size, needed when parse_header is called after
        # we have unlinked a chunk or writed a new one
        self.size = self.get_size()

        if self.size == 0:
            # Some region files seems to have 0 bytes of size, and
            # Minecraft handle them without problems. Take them
            # as empty region files.
            return
        elif self.size < 2*SECTOR_LENGTH:
            raise NoRegionHeader('The region file is %d bytes, too small in size to have a header.' % self.size)
        
        for index in range(0, SECTOR_LENGTH, 4):
            x = int(index//4) % 32
            z = int(index//4)//32
            m = self.metadata[x, z]
            
            self.file.seek(index)
            offset, length = unpack(">IB", b"\0"+self.file.read(4))
            m.blockstart, m.blocklength = offset, length
            self.file.seek(index + SECTOR_LENGTH)
            m.timestamp = unpack(">I", self.file.read(4))[0]
            
            if offset == 0 and length == 0:
                m.status = STATUS_CHUNK_NOT_CREATED
            elif length == 0:
                m.status = STATUS_CHUNK_ZERO_LENGTH
            elif offset < 2 and offset != 0:
                m.status = STATUS_CHUNK_IN_HEADER
            elif SECTOR_LENGTH * offset + 5 > self.size:
                # Chunk header can't be read.
                m.status = STATUS_CHUNK_OUT_OF_FILE
            else:
                m.status = STATUS_CHUNK_OK
        
        # Check for chunks overlapping in the file
        for chunks in self._sectors()[2:]:
            if len(chunks) > 1:
                # overlapping chunks
                for m in chunks:
                    # Update status, unless these more severe errors take precedence
                    if m.status not in (STATUS_CHUNK_ZERO_LENGTH, STATUS_CHUNK_IN_HEADER, 
                                        STATUS_CHUNK_OUT_OF_FILE):
                        m.status = STATUS_CHUNK_OVERLAPPING

    def _parse_chunk_headers(self):
        for x in range(32):
            for z in range(32):
                m = self.metadata[x, z]
                if m.status not in (STATUS_CHUNK_OK, STATUS_CHUNK_OVERLAPPING, \
                                    STATUS_CHUNK_MISMATCHED_LENGTHS):
                    continue
                try:
                    self.file.seek(m.blockstart*SECTOR_LENGTH) # offset comes in sectors of 4096 bytes
                    length = unpack(">I", self.file.read(4))
                    m.length = length[0] # unpack always returns a tuple, even unpacking one element
                    compression = unpack(">B",self.file.read(1))
                    m.compression = compression[0]
                except IOError:
                    m.status = STATUS_CHUNK_OUT_OF_FILE
                    continue
                if m.length <= 1: # chunk can't be zero length
                    m.status = STATUS_CHUNK_ZERO_LENGTH
                elif m.length + 4 > m.blocklength * SECTOR_LENGTH:
                    # There are not enough sectors allocated for the whole block
                    m.status = STATUS_CHUNK_MISMATCHED_LENGTHS

    def _sectors(self, ignore_chunk=None):
        """
        Return a list of all sectors, each sector is a list of chunks occupying the block.
        """
        sectorsize = self._bytes_to_sector(self.size)
        sectors = [[] for s in range(sectorsize)]
        sectors[0] = True # locations
        sectors[1] = True # timestamps
        for m in self.metadata.values():
            if not m.is_created():
                continue
            if ignore_chunk == m:
                continue
            if m.blocklength and m.blockstart:
                for b in range(m.blockstart, m.blockstart + max(m.blocklength, m.requiredblocks())):
                    if 2 <= b < sectorsize:
                        sectors[b].append(m)
        return sectors

    def _locate_free_sectors(self, ignore_chunk=None):
        """Return a list of booleans, indicating the free sectors."""
        sectors = self._sectors(ignore_chunk=ignore_chunk)
        # Sectors are considered free, if the value is an empty list.
        return [not i for i in sectors]

    def _find_free_location(self, free_locations, required_sectors=1, preferred=None):
        """
        Given a list of booleans, find a list of <required_sectors> consecutive True values.
        If no such list is found, return length(free_locations).
        Assumes first two values are always False.
        """
        # check preferred (current) location
        if preferred and all(free_locations[preferred:preferred+required_sectors]):
            return preferred
        
        # check other locations
        # Note: the slicing may exceed the free_location boundary.
        # This implementation relies on the fact that slicing will work anyway,
        # and the any() function returns True for an empty list. This ensures
        # that blocks outside the file are considered Free as well.
        
        i = 2 # First two sectors are in use by the header
        while i < len(free_locations):
            if all(free_locations[i:i+required_sectors]):
                break
            i += 1
        return i

    def get_metadata(self):
        """
        Return a list of the metadata of each chunk that is defined in te regionfile.
        This includes chunks which may not be readable for whatever reason,
        but excludes chunks that are not yet defined.
        """
        return [m for m in self.metadata.values() if m.is_created()]

    def get_chunks(self):
        """
        Return the x,z coordinates and length of the chunks that are defined in te regionfile.
        This includes chunks which may not be readable for whatever reason.

        Warning: despite the name, this function does not actually return the chunk,
        but merely it's metadata. Use get_chunk(x,z) to get the NBTFile, and then Chunk()
        to get the actual chunk.
        
        This method is deprecated. Use :meth:`get_metadata` instead.
        """
        return self.get_chunk_coords()

    def get_chunk_coords(self):
        """
        Return the x,z coordinates and length of the chunks that are defined in te regionfile.
        This includes chunks which may not be readable for whatever reason.
        
        This method is deprecated. Use :meth:`get_metadata` instead.
        """
        chunks = []
        for x in range(32):
            for z in range(32):
                m = self.metadata[x,z]
                if m.is_created():
                    chunks.append({'x': x, 'z': z, 'length': m.blocklength})
        return chunks

    def iter_chunks(self):
        """
        Yield each readable chunk present in the region.
        Chunks that can not be read for whatever reason are silently skipped.
        Warning: this function returns a :class:`nbt.nbt.NBTFile` object, use ``Chunk(nbtfile)`` to get a
        :class:`nbt.chunk.Chunk` instance.
        """
        for m in self.get_metadata():
            try:
                yield self.get_chunk(m.x, m.z)
            except RegionFileFormatError:
                pass
    
    def __iter__(self):
        return self.iter_chunks()

    def get_timestamp(self, x, z):
        """Return the timestamp of when this region file was last modified."""
        # TODO: raise an exception if chunk does not exist?
        # TODO: return a datetime.datetime object using datetime.fromtimestamp()
        return self.metadata[x,z].timestamp

    def chunk_count(self):
        """Return the number of defined chunks. This includes potentially corrupt chunks."""
        return len(self.get_metadata())

    def get_blockdata(self, x, z):
        """Return the decompressed binary data representing a chunk."""
        # read metadata block
        m = self.metadata[x, z]
        if m.status == STATUS_CHUNK_NOT_CREATED:
            raise InconceivedChunk("Chunk is not created")
        elif m.status == STATUS_CHUNK_IN_HEADER:
            raise RegionHeaderError('Chunk %d,%d is in the region header' % (x,z))
        elif m.status == STATUS_CHUNK_OUT_OF_FILE:
            raise RegionHeaderError('Chunk %d,%d is partially/completely outside the file' % (x,z))
        elif m.status == STATUS_CHUNK_ZERO_LENGTH:
            if m.blocklength == 0:
                raise RegionHeaderError('Chunk %d,%d has zero length' % (x,z))
            else:
                raise ChunkHeaderError('Chunk %d,%d has zero length' % (x,z))

        # status is STATUS_CHUNK_OK, STATUS_CHUNK_MISMATCHED_LENGTHS or STATUS_CHUNK_OVERLAPPING.
        # The chunk is always read, but in case of an error, the exception may be different 
        # based on the status.

        # offset comes in sectors of 4096 bytes + length bytes + compression byte
        self.file.seek(m.blockstart * SECTOR_LENGTH + 5)
        chunk = self.file.read(m.length-1) # the length in the file includes the compression byte

        err = None
        if m.compression > 2:
            raise ChunkDataError('Unknown chunk compression/format (%d)' % m.compression)
        try:
            if (m.compression == COMPRESSION_GZIP):
                # Python 3.1 and earlier do not yet support gzip.decompress(chunk)
                f = gzip.GzipFile(fileobj=BytesIO(chunk))
                chunk = bytes(f.read())
                f.close()
            elif (m.compression == COMPRESSION_ZLIB):
                chunk = zlib.decompress(chunk)
            return chunk
        except Exception as e:
            # Deliberately catch the Exception and re-raise.
            # The details in gzip/zlib/nbt are irrelevant, just that the data is garbled.
            err = str(e)
        if err:
            # don't raise during exception handling to avoid the warning 
            # "During handling of the above exception, another exception occurred".
            # Python 3.3 solution (see PEP 409 & 415): "raise ChunkDataError(str(e)) from None"
            if m.status == STATUS_CHUNK_MISMATCHED_LENGTHS:
                raise ChunkHeaderError('The length in region header and the length in the header of chunk %d,%d are incompatible' % (x,z))
            elif m.status == STATUS_CHUNK_OVERLAPPING:
                raise ChunkHeaderError('Chunk %d,%d is overlapping with another chunk' % (x,z))
            else:
                raise ChunkDataError(err)

    def get_nbt(self, x, z):
        """
        Return a NBTFile of the specified chunk.
        Raise InconceivedChunk if the chunk is not included in the file.
        """
        data = self.get_blockdata(x, z) # This may raise a RegionFileFormatError.
        data = BytesIO(data)
        err = None
        try:
            return NBTFile(buffer=data)
            # this may raise a MalformedFileError. Convert to ChunkDataError.
        except MalformedFileError as e:
            err = str(e)
        if err:
            raise ChunkDataError(err)

    def get_chunk(self, x, z):
        """
        Return a NBTFile of the specified chunk.
        Raise InconceivedChunk if the chunk is not included in the file.
        
        Note: this function may be changed later to return a Chunk() rather 
        than a NBTFile() object. To keep the old functionality, use get_nbt().
        """
        return self.get_nbt(x, z)

    def write_blockdata(self, x, z, data):
        """
        Compress the data, write it to file, and add pointers in the header so it 
        can be found as chunk(x,z).
        """
        data = zlib.compress(data) # use zlib compression, rather than Gzip
        length = len(data)

        # 5 extra bytes are required for the chunk block header
        nsectors = self._bytes_to_sector(length + 5)

        if nsectors >= 256:
            raise ChunkDataError("Chunk is too large (%d sectors exceeds 255 maximum)" % (nsectors))

        # Ensure file has a header
        if self.size < 2*SECTOR_LENGTH:
            self._init_file()

        # search for a place where to write the chunk:
        current = self.metadata[x, z]
        free_sectors = self._locate_free_sectors(ignore_chunk=current)
        sector = self._find_free_location(free_sectors, nsectors, preferred=current.blockstart)

        # write out chunk to region
        self.file.seek(sector*SECTOR_LENGTH)
        self.file.write(pack(">I", length + 1)) #length field
        self.file.write(pack(">B", COMPRESSION_ZLIB)) #compression field
        self.file.write(data) #compressed data

        # Write zeros up to the end of the chunk
        remaining_length = SECTOR_LENGTH * nsectors - length - 5
        self.file.write(remaining_length * b"\x00")

        #seek to header record and write offset and length records
        self.file.seek(4 * (x + 32*z))
        self.file.write(pack(">IB", sector, nsectors)[1:])

        #write timestamp
        self.file.seek(SECTOR_LENGTH + 4 * (x + 32*z))
        timestamp = int(time.time())
        self.file.write(pack(">I", timestamp))

        # Update free_sectors with newly written block
        # This is required for calculating file truncation and zeroing freed blocks.
        free_sectors.extend((sector + nsectors - len(free_sectors)) * [True])
        for s in range(sector, sector + nsectors):
            free_sectors[s] = False
        
        # Check if file should be truncated:
        truncate_count = list(reversed(free_sectors)).index(False)
        if truncate_count > 0:
            self.size = SECTOR_LENGTH * (len(free_sectors) - truncate_count)
            self.file.truncate(self.size)
            free_sectors = free_sectors[:-truncate_count]
        
        # Calculate freed sectors
        for s in range(current.blockstart, min(current.blockstart + current.blocklength, len(free_sectors))):
            if free_sectors[s]:
                # zero sector s
                self.file.seek(SECTOR_LENGTH*s)
                self.file.write(SECTOR_LENGTH*b'\x00')
        
        # update file size and header information
        self.size = self.get_size()
        current.blockstart = sector
        current.blocklength = nsectors
        current.status = STATUS_CHUNK_OK
        current.timestamp = timestamp
        current.length = length + 1
        current.compression = COMPRESSION_ZLIB

        # self.parse_header()
        # self.parse_chunk_headers()

    def write_chunk(self, x, z, nbt_file):
        """
        Pack the NBT file as binary data, and write to file in a compressed format.
        """
        data = BytesIO()
        nbt_file.write_file(buffer=data) # render to buffer; uncompressed
        self.write_blockdata(x, z, data.getvalue())

    def unlink_chunk(self, x, z):
        """
        Remove a chunk from the header of the region file.
        Fragmentation is not a problem, chunks are written to free sectors when possible.
        """
        # This function fails for an empty file. If that is the case, just return.
        if self.size < 2*SECTOR_LENGTH:
            return

        # zero the region header for the chunk (offset length and time)
        self.file.seek(4 * (x + 32*z))
        self.file.write(pack(">IB", 0, 0)[1:])
        self.file.seek(SECTOR_LENGTH + 4 * (x + 32*z))
        self.file.write(pack(">I", 0))

        # Check if file should be truncated:
        current = self.metadata[x, z]
        free_sectors = self._locate_free_sectors(ignore_chunk=current)
        truncate_count = list(reversed(free_sectors)).index(False)
        if truncate_count > 0:
            self.size = SECTOR_LENGTH * (len(free_sectors) - truncate_count)
            self.file.truncate(self.size)
            free_sectors = free_sectors[:-truncate_count]
        
        # Calculate freed sectors
        for s in range(current.blockstart, min(current.blockstart + current.blocklength, len(free_sectors))):
            if free_sectors[s]:
                # zero sector s
                self.file.seek(SECTOR_LENGTH*s)
                self.file.write(SECTOR_LENGTH*b'\x00')

        # update the header
        self.metadata[x, z] = ChunkMetadata(x, z)

    def _classname(self):
        """Return the fully qualified class name."""
        if self.__class__.__module__ in (None,):
            return self.__class__.__name__
        else:
            return "%s.%s" % (self.__class__.__module__, self.__class__.__name__)

    def __str__(self):
        if self.filename:
            return "<%s(%r)>" % (self._classname(), self.filename)
        else:
            return '<%s object at %d>' % (self._classname(), id(self))
    
    def __repr__(self):
        if self.filename:
            return "%s(%r)" % (self._classname(), self.filename)
        else:
            return '<%s object at %d>' % (self._classname(), id(self))

########NEW FILE########
__FILENAME__ = world
"""
Handles a Minecraft world save using either the Anvil or McRegion format.
"""

import os, glob, re
from . import region
from . import chunk
from .region import InconceivedChunk

class UnknownWorldFormat(Exception):
    """Unknown or invalid world folder."""
    def __init__(self, msg=""):
        self.msg = msg



class _BaseWorldFolder(object):
    """
    Abstract class, representing either a McRegion or Anvil world folder.
    This class will use either Anvil or McRegion, with Anvil the preferred format.
    Simply calling WorldFolder() will do this automatically.
    """
    type = "Generic"

    def __init__(self, world_folder):
        """Initialize a WorldFolder."""
        self.worldfolder = world_folder
        self.regionfiles = {}
        self.regions     = {}
        self.chunks  = None
        # os.listdir triggers an OSError for non-existant directories or permission errors.
        # This is needed, because glob.glob silently returns no files.
        os.listdir(world_folder)
        self.set_regionfiles(self.get_filenames())

    def get_filenames(self):
        # Warning: glob returns a empty list if the directory is unreadable, without raising an Exception
        return list(glob.glob(os.path.join(self.worldfolder,'region','r.*.*.'+self.extension)))

    def set_regionfiles(self, filenames):
        """
        This method directly sets the region files for this instance to use.
        It assumes the filenames are in the form r.<x-digit>.<z-digit>.<extension>
        """
        for filename in filenames:
            # Assume that filenames have the name r.<x-digit>.<z-digit>.<extension>
            m = re.match(r"r.(\-?\d+).(\-?\d+)."+self.extension, os.path.basename(filename))
            if m:
                x = int(m.group(1))
                z = int(m.group(2))
            else:
                # Only raised if a .mca of .mcr file exists which does not comply to the
                #  r.<x-digit>.<z-digit>.<extension> filename format. This may raise false
                # errors if a copy is made, e.g. "r.0.-1 copy.mca". If this is an issue, override
                # get_filenames(). In most cases, it is an error, and we like to raise that.
                # Changed, no longer raise error, because we want to continue the loop.
                # raise UnknownWorldFormat("Unrecognized filename format %s" % os.path.basename(filename))
                # TODO: log to stderr using logging facility.
                pass
            self.regionfiles[(x,z)] = filename

    def nonempty(self):
        """Return True is the world is non-empty."""
        return len(self.regionfiles) > 0

    def get_regionfiles(self):
        """Return a list of full path of all region files."""
        return list(self.regionfiles.values())

    def get_region(self, x,z):
        """Get a region using x,z coordinates of a region. Cache results."""
        if (x,z) not in self.regions:
            if (x,z) in self.regionfiles:
                self.regions[(x,z)] = region.RegionFile(self.regionfiles[(x,z)])
            else:
                # Return an empty RegionFile object
                # TODO: this does not yet allow for saving of the region file
                self.regions[(x,z)] = region.RegionFile()
        return self.regions[(x,z)]

    def iter_regions(self):
        for x,z in self.regionfiles.keys():
            yield self.get_region(x,z)

    def iter_nbt(self):
        """
        Return an iterable list of all NBT. Use this function if you only
        want to loop through the chunks once, and don't need the block or data arrays.
        """
        # TODO: Implement BoundingBox
        # TODO: Implement sort order
        for region in self.iter_regions():
            for c in region.iter_chunks():
                yield c

    def iter_chunks(self):
        """
        Return an iterable list of all chunks. Use this function if you only
        want to loop through the chunks once or have a very large world.
        Use get_chunks() if you access the chunk list frequently and want to cache
        the results. Use iter_nbt() if you are concerned about speed and don't want
        to parse the block data.
        """
        # TODO: Implement BoundingBox
        # TODO: Implement sort order
        for c in self.iter_nbt():
            yield self.chunkclass(c)

    def get_nbt(self,x,z):
        """
        Return a NBT specified by the chunk coordinates x,z. Raise InconceivedChunk
        if the NBT file is not yet generated. To get a Chunk object, use get_chunk.
        """
        rx,x = divmod(x,32)
        rz,z = divmod(z,32)
        nbt = self.get_region(rx,rz).get_chunk(x,z)
        if nbt == None:
            raise InconceivedChunk("Chunk %s,%s not present in world" % (32*rx+x,32*rz+z))
        return nbt

    def set_nbt(self,x,z,nbt):
        """
        Set a chunk. Overrides the NBT if it already existed. If the NBT did not exists,
        adds it to the Regionfile. May create a new Regionfile if that did not exist yet.
        nbt must be a nbt.NBTFile instance, not a Chunk or regular TAG_Compound object.
        """
        raise NotImplemented()
        # TODO: implement

    def get_chunk(self,x,z):
        """
        Return a chunk specified by the chunk coordinates x,z. Raise InconceivedChunk
        if the chunk is not yet generated. To get the raw NBT data, use get_nbt.
        """
        return self.chunkclass(self.get_nbt(x, z))

    def get_chunks(self, boundingbox=None):
        """
        Return a list of all chunks. Use this function if you access the chunk
        list frequently and want to cache the result.
        Use iter_chunks() if you only want to loop through the chunks once or have a
        very large world.
        """
        if self.chunks == None:
            self.chunks = list(self.iter_chunks())
        return self.chunks

    def chunk_count(self):
        """Return a count of the chunks in this world folder."""
        c = 0
        for r in self.iter_regions():
            c += r.chunk_count()
        return c

    def get_boundingbox(self):
        """
        Return minimum and maximum x and z coordinates of the chunks that
        make up this world save
        """
        b = BoundingBox()
        for rx,rz in self.regionfiles.keys():
            region = self.get_region(rx,rz)
            rx,rz = 32*rx,32*rz
            for cc in region.get_chunk_coords():
                x,z = (rx+cc['x'],rz+cc['z'])
                b.expand(x,None,z)
        return b

    def cache_test(self):
        """
        Debug routine: loop through all chunks, fetch them again by coordinates,
        and check if the same object is returned.
        """
        # TODO: make sure this test succeeds (at least True,True,False, preferable True,True,True)
        # TODO: Move this function to test class.
        for rx,rz in self.regionfiles.keys():
            region = self.get_region(rx,rz)
            rx,rz = 32*rx,32*rz
            for cc in region.get_chunk_coords():
                x,z = (rx+cc['x'],rz+cc['z'])
                c1 = self.chunkclass(region.get_chunk(cc['x'],cc['z']))
                c2 = self.get_chunk(x,z)
                correct_coords = (c2.get_coords() == (x,z))
                is_comparable = (c1 == c2) # test __eq__ function
                is_equal = (id(c1) == id(c2)) # test if they point to the same memory location
                # DEBUG (prints a tuple)
                print((x,z,c1,c2,correct_coords,is_comparable,is_equal))

    def __repr__(self):
        return "%s(%r)" % (self.__class__.__name__,self.worldfolder)


class McRegionWorldFolder(_BaseWorldFolder):
    """Represents a world save using the old McRegion format."""
    type = "McRegion"
    extension = 'mcr'
    chunkclass = chunk.Chunk
    # chunkclass = chunk.McRegionChunk  # TODO: change to McRegionChunk when done

class AnvilWorldFolder(_BaseWorldFolder):
    """Represents a world save using the new Anvil format."""
    type = "Anvil"
    extension = 'mca'
    chunkclass = chunk.Chunk
    # chunkclass = chunk.AnvilChunk  # TODO: change to AnvilChunk when done


class _WorldFolderFactory():
    """Factory class: instantiate the subclassses in order, and the first instance 
    whose nonempty() method returns True is returned. If no nonempty() returns True,
    a UnknownWorldFormat exception is raised."""
    def __init__(self, subclasses):
        self.subclasses = subclasses
    def __call__(self, *args, **kwargs):
        for cls in self.subclasses:
            wf = cls(*args, **kwargs)
            if wf.nonempty(): # Check if the world is non-empty
                return wf
        raise UnknownWorldFormat("Empty world or unknown format: %r" % world_folder)

WorldFolder = _WorldFolderFactory([AnvilWorldFolder, McRegionWorldFolder])
"""
Factory instance that returns a AnvilWorldFolder or McRegionWorldFolder
instance, or raise a UnknownWorldFormat.
"""



class BoundingBox(object):
    """A bounding box of x,y,z coordinates."""
    def __init__(self, minx=None, maxx=None, miny=None, maxy=None, minz=None, maxz=None):
        self.minx,self.maxx = minx, maxx
        self.miny,self.maxy = miny, maxy
        self.minz,self.maxz = minz, maxz
    def expand(self,x,y,z):
        """
        Expands the bounding
        """
        if x != None:
            if self.minx is None or x < self.minx:
                self.minx = x
            if self.maxx is None or x > self.maxx:
                self.maxx = x
        if y != None:
            if self.miny is None or y < self.miny:
                self.miny = y
            if self.maxy is None or y > self.maxy:
                self.maxy = y
        if z != None:
            if self.minz is None or z < self.minz:
                self.minz = z
            if self.maxz is None or z > self.maxz:
                self.maxz = z
    def lenx(self):
        return self.maxx-self.minx+1
    def leny(self):
        return self.maxy-self.miny+1
    def lenz(self):
        return self.maxz-self.minz+1
    def __repr__(self):
        return "%s(%s,%s,%s,%s,%s,%s)" % (self.__class__.__name__,self.minx,self.maxx,
                self.miny,self.maxy,self.minz,self.maxz)

########NEW FILE########
__FILENAME__ = alltests
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import logging

import unittest
try:
    from unittest import skip as _skip
except ImportError:
    # Python 2.6 has an older unittest API. The backported package is available from pypi.
    import unittest2 as unittest

testmodules = ['examplestests', 'nbttests', 'regiontests']
"""Files to check for test cases. Do not include the .py extension."""


def get_testsuites_in_module(module):
    """
    Return a list of unittest.TestSuite subclasses defined in module.
    """
    suites = []
    for name in dir(module):
        obj = getattr(module, name)
        if isinstance(obj, type) and issubclass(obj, unittest.TestSuite):
            suites.append(obj)
    return suites


def load_tests_in_modules(modulenames):
    """
    Given a list of module names, import the modules, load and run the 
    test cases in these modules. The modules are typically files in the 
    current directory, but this is not a requirement.
    """
    loader = unittest.TestLoader()
    suites = []
    for name in modulenames:
        module = __import__(name)
        suite = loader.loadTestsFromModule(module)
        for suiteclass in get_testsuites_in_module(module):
            # Wrap suite in TestSuite classes
            suite = suiteclass(suite)
        suites.append(suite)
    suite = unittest.TestSuite(suites)
    return suite



if __name__ == "__main__":
    logger = logging.getLogger("nbt.tests")
    if len(logger.handlers) == 0:
        # Logging is not yet configured. Configure it.
        logging.basicConfig(level=logging.INFO, stream=sys.stderr, format='%(levelname)-8s %(message)s')
    testresult = unittest.TextTestRunner(verbosity=2).run(load_tests_in_modules(testmodules))
    sys.exit(not testresult.wasSuccessful())

########NEW FILE########
__FILENAME__ = downloadsample
#!/usr/bin/env python
# encoding: utf-8
"""
Download a Minecraft sample world from GitHub and verify the SHA256 checksum.
"""

import sys
import os
try:
    import urllib.request as urllib   # Python 3
except ImportError:
    import urllib2 as urllib   # Python 2
import logging
import tarfile
import hashlib

import glob
import tempfile
import shutil

URL = "https://github.com/downloads/twoolie/NBT/Sample_World.tar.gz"
"""URL to retrieve"""
workdir = os.path.dirname(__file__)
"""Directory for download and extracting the sample files"""
worlddir = os.path.join(workdir, 'Sample World')
"""Destination folder for the sample world."""
checksums = {
    'Sample World': None, 
    'Sample World/data': None, 
    'Sample World/level.dat': 'f252cf8b938fa1e41c9335ea1bdc70fca73ac5c63c2cf2db4b2ddc4cb2fa4d91', 
    'Sample World/level.dat_mcr': '933238e89a9f7f94c72f236da0d81d44d966c7a1544490e51e682ab42ccc50ff', 
    'Sample World/level.dat_old': 'c4b5a5c355d4f85c369604ca27ee349dba41adc4712a43a6f8c8399fe44071e7', 
    'Sample World/region': None, 
    'Sample World/region/r.-1.0.mca': '6e8ec8698e2e68ca3ee2090da72e4f24c85f9db3f36191e5e33ebc8cafb209f2', 
    'Sample World/region/r.-1.0.mcr': '3a9ccafc6f64b98c0424814f44f1d0d3429cbb33448ff97e2e84ca649bfa16ae', 
    'Sample World/region/r.-1.1.mca': 'c5f6fb5c72ca534d0f73662f2199dca977d2de1521b4823f73384aa6826c4b74', 
    'Sample World/region/r.-1.1.mcr': '8e8b545b412a6a2bb519aee0259a63e6a918cd25a49538451e752e3bf90d4cf1', 
    'Sample World/region/r.0.0.mca': 'd86e51c2adf35f82492e974f75fe83e9e5db56a267a3fe76150dc42f0aeb07c7', 
    'Sample World/region/r.0.0.mcr': 'a8e7fea4e40a70e0d70dea7ebb1328c7623ed01b77d8aff34d01e530fbdad9d5', 
    'Sample World/region/r.0.1.mca': '8a03d910c7fd185ae5efb1409c592e4a9688dfab1fbd31f8c736461157a7675d', 
    'Sample World/region/r.0.1.mcr': '08fcd50748d4633a3b1d52e1b323c7dd9c4299a28ec095d0261fd195d3c9a537', 
    'Sample World/session.lock': 'd05da686dd04cd2ad1f660ddaa7645abc9fd9af396357a5b3256b437af0d7dba', 
}
"""SHA256 checksums for each file in the tar file.
Directories MUST also be included (without trailing slash), with None as the checksum"""


def download(url, destination):
    """
    Download the file from the specified URL, and extract the contents.
    
    May raise an IOError (or one of it's subclasses) upon error, either
    in reading from the URL of writing to file.
    """
    logger = logging.getLogger("nbt.tests.downloadsample")
    localfile = None
    remotefile = None
    try:
        request = urllib.Request(url)
        remotefile = urllib.urlopen(request)
        localfile = open(destination, 'wb')
        logger.info("Downloading %s" % url)
        chunksize = 524288 # 0.5 MiB
        size = 0
        while True:
            data = remotefile.read(chunksize)
            if not data: break
            localfile.write(data)
            size += len(data)
            logging.info("Downloaded %0.1f MiByte..." % (float(size)/1048576))
    finally:
        try:
            remotefile.close()
            localfile.close()
        except (IOError, AttributeError):
            pass
    logging.info("Download complete")

def extract(filename, workdir, filelist):
    """
    Extract contents of a tar file in workdir. The tar file may be compressed 
    using gzip or bzip2.
    
    For security, only files listed in filelist are extracted.
    Extraneous files will be logged as warning.
    """
    logger = logging.getLogger("nbt.tests.downloadsample")
    logger.info("Extracting")
    def filefilter(members):
        for tarinfo in members:
            if tarinfo.name in filelist:
                logger.info("Extract %s" % tarinfo.name)
                yield tarinfo
            else:
                logger.warning("Skip %s" % tarinfo.name)
    # r:* means any compression (gzip or bz2 are supported)
    files = tarfile.open(filename, 'r:*')
    files.extractall(workdir, filefilter(files.getmembers()))
    files.close()


def verify(checksums):
    """
    Verify if all given files are present and their SHA256 
    checksum is correct. Any files not explicitly listed are deleted.
    
    checksums is a dict of file => checksum, with file a file relative to dir.
    
    Returns a boolean indicating that all checksums are correct, and all files 
    are present.
    
    Any warnings and errors are printer to logger.
    Errors or exceptions result in a return value of False.
    """
    logger = logging.getLogger("nbt.tests.downloadsample")
    success = True
    for path in checksums.keys():
        try:
            check = checksums[path]
            if check == None: continue  # Skip folders
            localfile = open(path, 'rb')
            h = hashlib.sha256()
            chunksize = 524288 # 0.5 MiB
            while True:
                data = localfile.read(chunksize)
                if not data: break
                h.update(data)
            localfile.close()
            calc = h.hexdigest()
            if calc != check:
                logger.error("Checksum failed %s: %s found, %s expected" % (path, calc, check))
                success = False
        except IOError as e:
            if e.errno == 2:
                logger.error('Checksum verificiation failed: file %s not found' % e.filename)
            else:
                logger.error('Checksum verificiation of %s failed: errno %d: %s' % \
                        (e.filename, e.errno, e.strerror))
            return False
    logger.info("Checksum of %d files verified" % len(checksums))
    return success


def install(url=URL, workdir=workdir, checksums=checksums):
    """
    Download and extract a sample world, used for testing.
    The download file and sample world are stored in workdir.
    
    Verifies the checksum of all files. Files without a checksum are not
    extracted.
    """
    # the paths in checksum are relative to the working dir, and UNIX-based. 
    # Normalise them to support Windows; and create the following three derivates:
    # - posixpaths: list of relative posix paths -- to filter tar extraction
    # - nchecksums: as checksum, but with normalised absolute paths
    # - files: list of normalised absolute path of files (non-directories)
    posixpaths = checksums.keys()
    nchecksums = dict([(os.path.join(workdir, os.path.normpath(path)), checksums[path]) \
            for path in posixpaths if checksums[path] != None])
    files = nchecksums.keys()
    tarfile = os.path.join(workdir, os.path.basename(url))
    
    try:
        if not any(map(os.path.exists, files)):
            # none destination file exists. We can safely download/extract without overwriting.
            if not os.path.exists(tarfile):
                download(url=URL, destination=tarfile)
            extract(filename=tarfile, workdir=workdir, filelist=posixpaths)
        return verify(checksums=nchecksums)
    except urllib.HTTPError as e:
        logger.error('Download %s failed: HTTP Error %d: %s\n' % (url, e.code, e.reason))
    except urllib.URLError as e:
        # e.reason may be a socket.error. If so, print e.reason.strerror.
        logger.error('Download %s failed: %s\n' % \
                (url, e.reason.strerror if hasattr(e.reason, "strerror") else e.reason))
    except tarfile.TarError as e:
        logger.error('Extract %s failed: %s\n' % (tarfile, e.message))
    except IOError as e:
        logger.error('Download to %s failed: %s' % (e.filename, e.strerror))
    return False



def _mkdir(dstdir, subdir):
    """Helper function: create folder /dstdir/subdir"""
    os.mkdir(os.path.join(dstdir, os.path.normpath(subdir)))
def _copyglob(srcdir, destdir, pattern):
    """Helper function: copies files from /srcdir/pattern to /destdir/pattern.
    pattern is a glob pattern."""
    for fullpath in glob.glob(os.path.join(srcdir, os.path.normpath(pattern))):
        relpath = os.path.relpath(fullpath, srcdir)
        shutil.copy2(fullpath, os.path.join(destdir, relpath))
def _copyrename(srcdir, destdir, src, dest):
    """Helper function: copy file from /srcdir/src to /destdir/dest."""
    shutil.copy2(os.path.join(srcdir, os.path.normpath(src)), \
                os.path.join(destdir, os.path.normpath(dest)))

def temp_mcregion_world(worldfolder=worlddir):
    """Create a McRegion worldfolder in a temporary directory, based on the 
    files in the given mixed worldfolder. Returns the temporary directory path."""
    tmpfolder = tempfile.mkdtemp(prefix="nbtmcregion")
    _mkdir(tmpfolder, 'region')
    _copyglob(worldfolder, tmpfolder, "region/*.mcr")
    _copyrename(worldfolder, tmpfolder, "level.dat_mcr", "level.dat")
    return tmpfolder
def temp_anvil_world(worldfolder=worlddir):
    """Create a Anvil worldfolder in a temporary directory, based on the 
    files in the given mixed worldfolder. Returns the temporary directory path."""
    tmpfolder = tempfile.mkdtemp(prefix="nbtanvil")
    _mkdir(tmpfolder, 'region')
    _copyglob(worldfolder, tmpfolder, "region/*.mca")
    _copyrename(worldfolder, tmpfolder, "level.dat", "level.dat")
    return tmpfolder
def cleanup_temp_world(tmpfolder):
    """Remove a temporary directory"""
    shutil.rmtree(tmpfolder, ignore_errors=True)

if __name__ == '__main__':
    logger = logging.getLogger("nbt.tests.downloadsample")
    if len(logger.handlers) == 0:
        # Logging is not yet configured. Configure it.
        logging.basicConfig(level=logging.INFO, stream=sys.stderr, format='%(levelname)-8s %(message)s')
    success = install()
    sys.exit(0 if success else 1)

########NEW FILE########
__FILENAME__ = examplestests
#!/usr/bin/env python
# encoding: utf-8
"""
Run all scripts in examples on specific sample world.
"""

import sys
import os
import subprocess
import shutil
import tempfile
import glob

import unittest
try:
    from unittest import skip as _skip
except ImportError:
    # Python 2.6 has an older unittest API. The backported package is available from pypi.
    import unittest2 as unittest

# local modules
import downloadsample

if sys.version_info[0] < 3:
    def _deletechars(text, deletechars):
        """Return string, with the deletechars removed"""
        return filter(lambda c: c not in deletechars, text)
else:
    def _deletechars(text, deletechars):
        """Return string, with the deletechars removed"""
        filter = str.maketrans('', '', deletechars)
        return text.translate(filter)

def _mkdir(dstdir, subdir):
    """Helper function: create folder /dstdir/subdir"""
    os.mkdir(os.path.join(dstdir, os.path.normpath(subdir)))
def _copyglob(srcdir, destdir, pattern):
    """Helper function: copies files from /srcdir/pattern to /destdir/pattern.
    pattern is a glob pattern."""
    for fullpath in glob.glob(os.path.join(srcdir, os.path.normpath(pattern))):
        relpath = os.path.relpath(fullpath, srcdir)
        shutil.copy2(fullpath, os.path.join(destdir, relpath))
def _copyrename(srcdir, destdir, src, dest):
    """Helper function: copy file from /srcdir/src to /destdir/dest."""
    shutil.copy2(os.path.join(srcdir, os.path.normpath(src)), \
                os.path.join(destdir, os.path.normpath(dest)))


class ScriptTestCase(unittest.TestCase):
    """Test Case with helper functions for running a script, and installing a 
    Minecraft sample world."""
    worldfolder = None
    mcregionfolder = None
    anvilfolder = None
    examplesdir = os.path.normpath(os.path.join(__file__, os.pardir, os.pardir, 'examples'))
    def runScript(self, script, args):
        scriptpath = os.path.join(self.examplesdir, script)
        args.insert(0, scriptpath)
        # Ensure we're using the same python version.
        # python = sys.argv[0]
        # args.insert(0, python)
        p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        p.wait()
        output = [r.decode('utf-8') for r in p.stdout.readlines()]
        for l in p.stderr.readlines():
            sys.stdout.write("%s: %s" % (script, l.decode('utf-8')))
        try:
            p.stdout.close()
            p.stderr.close()
        except IOError:
            pass
        self.assertEqual(p.returncode, 0, "return code is %d" % p.returncode)
        return output
    def assertEqualOutput(self, actual, expected):
        """Compare two lists of strings, ignoring whitespace at begin and end of line."""
        if len(actual) < len(expected):
            self.fail("Output is %d lines, expected at least %d lines" % \
                    (len(actual), len(expected)))
        for i,expline in enumerate(expected):
            self.assertEqual(actual[i].strip(), expline.strip(), \
                    "Output line %d is %r, expected %r" % (i+1, actual[i], expline))
    def assertEqualString(self, actual, expected):
        """Compare strings, ignoring whitespace at begin and end of line."""
        self.assertEqual(actual.strip(), expected.strip(), \
                    "Output line %r, expected %r" % (actual, expected))


class BiomeAnalysisScriptTest(ScriptTestCase):
    pass
    
    # TODO: Sample World was converted with simple script, but does not seem to have biome data.
    # This needs to be added. (opening the world with minecraft client will change the 
    # world a bit, which I like to avoid. Perhaps opening with the server will not change it, 
    # if "/stop" is called quickly enough. this may change the amount of generated chunks to 
    # everything in a 380x380 block though.)
    
    # @classmethod
    # def setUpClass(cls):
    #   cls.installsampleworld()
    #   cls.extractAnvilWorld()
    # def testAnvilWorld(self):
    #   output = self.runScript('biome_analysis.py', [self.anvilfolder])

class BlockAnalysisScriptTest(ScriptTestCase):
    expected = [
        "DiamondOre:1743",
        "GoldOre:4838",
        "RedstoneOre:14487",
        "IronOre:52906",
        "CoalOre:97597",
        "LapisLazuliOre:2051",
        "Dungeons:26",
        "Clay:897",
        "SugarCane:22",
        "Cacti:0",
        "Pumpkin:6",
        "Dandelion:513",
        "Rose:131",
        "BrownMushroom:40",
        "RedMushroom:31",
        "LavaSprings:47665",
    ]
    def testMcRegionWorld(self):
        output = self.runScript('block_analysis.py', [self.mcregionfolder])
        self.assertTrue(len(output) >= 73, "Expected output of at least 73 lines long")
        output = [_deletechars(l, " ,.") for l in output[-16:]]
        self.assertEqualOutput(output, self.expected)
    # TODO: Anvil does not yet work.
    # def testAnvilWorld(self):
    #   output = self.runScript('block_analysis.py', [self.anvilfolder])
    #   print repr(output)
    #   self.assertTrue(len(output) >= 73, "Expected output of at least 73 lines long")
    #   output = [_deletechars(l, " ,.") for l in output[-16:]]
    #   self.assertEqualOutput(output, self.expected)

class ChestAnalysisScriptTest(ScriptTestCase):
    def testMcRegionWorld(self):
        output = self.runScript('chest_analysis.py', [self.mcregionfolder])
        self.assertEqual(len(output), 178)
        count = len(list(filter(lambda l: l.startswith('Chest at '), output)))
        self.assertEqual(count, 38)
    def testAnvilWorld(self):
        output = self.runScript('chest_analysis.py', [self.anvilfolder])
        self.assertEqual(len(output), 178)
        count = len(list(filter(lambda l: l.startswith('Chest at '), output)))
        self.assertEqual(count, 38)

def has_PIL():
    try:
        from PIL import Image
        return True
    except ImportError:
        return False

class MapScriptTest(ScriptTestCase):
    @unittest.skipIf(not has_PIL(), "PIL library not available")
    def testMcRegionWorld(self):
        output = self.runScript('map.py', ['--noshow', self.mcregionfolder])
        self.assertTrue(output[-1].startswith("Saved map as "))
    # TODO: this currently writes the map to tests/nbtmcregion*.png files. 
    # The locations should be a tempfile, and the file should be deleted afterwards.
    
    # @skipIf(not has_PIL(), "PIL library not available")
    # def testAnvilWorld(self):
    #   output = self.runScript('map.py', ['--noshow', self.anvilfolder])
    #   self.assertEqualString(output[-1], "Saved map as Sample World.png")

class MobAnalysisScriptTest(ScriptTestCase):
    def testMcRegionWorld(self):
        output = self.runScript('mob_analysis.py', [self.mcregionfolder])
        self.assertEqual(len(output), 413)
        output = sorted(output)
        self.assertEqualString(output[0], "Chicken at 107.6,88.0,374.5")
        self.assertEqualString(output[400], "Zombie at 249.3,48.0,368.1")
    def testAnvilWorld(self):
        output = self.runScript('mob_analysis.py', [self.anvilfolder])
        self.assertEqual(len(output), 413)
        output = sorted(output)
        self.assertEqualString(output[0], "Chicken at 107.6,88.0,374.5")
        self.assertEqualString(output[400], "Zombie at 249.3,48.0,368.1")

class SeedScriptTest(ScriptTestCase):
    def testMcRegionWorld(self):
        output = self.runScript('seed.py', [self.mcregionfolder])
        self.assertEqualOutput(output, ["-3195717715052600521"])
    def testAnvilWorld(self):
        output = self.runScript('seed.py', [self.anvilfolder])
        self.assertEqualOutput(output, ["-3195717715052600521"])

class GenerateLevelDatScriptTest(ScriptTestCase):
    expected = [
        "NBTFile('Data'): {10 Entries}",
        "{",
        "   TAG_Long('Time'): 1",
        "   TAG_Long('LastPlayed'): *",
        "   TAG_Int('SpawnX'): 0",
        "   TAG_Int('SpawnY'): 2",
        "   TAG_Int('SpawnZ'): 0",
        "   TAG_Long('SizeOnDisk'): 0",
        "   TAG_Long('RandomSeed'): *",
        "   TAG_Int('version'): 19132",
        "   TAG_String('LevelName'): Testing",
        "   TAG_Compound('Player'): {3 Entries}",
        "   {",
        "       TAG_Int('Score'): 0",
        "       TAG_Int('Dimension'): 0",
        "       TAG_Compound('Inventory'): {0 Entries}",
        "   }",
        "}"
    ]
    def testNBTGeneration(self):
        output = self.runScript('generate_level_dat.py', [])
        self.assertEqual(len(output), 18)
        self.assertEqualString(output[0],  self.expected[0])
        self.assertEqualString(output[10], self.expected[10])
        self.assertEqualString(output[11], self.expected[11])
        self.assertEqualString(output[13], self.expected[13])


def setUpModule():
    """Download sample world, and copy Anvil and McRegion files to temporary folders."""
    if ScriptTestCase.worldfolder == None:
        downloadsample.install()
        ScriptTestCase.worldfolder = downloadsample.worlddir
    if ScriptTestCase.mcregionfolder == None:
        ScriptTestCase.mcregionfolder = downloadsample.temp_mcregion_world()
    if ScriptTestCase.anvilfolder == None:
        ScriptTestCase.anvilfolder = downloadsample.temp_anvil_world()

def tearDownModule():
    """Remove temporary folders with Anvil and McRegion files."""
    if ScriptTestCase.mcregionfolder != None:
        downloadsample.cleanup_temp_world(ScriptTestCase.mcregionfolder)
    if ScriptTestCase.anvilfolder != None:
        downloadsample.cleanup_temp_world(ScriptTestCase.anvilfolder)
    ScriptTestCase.worldfolder = None
    ScriptTestCase.mcregionfolder = None
    ScriptTestCase.anvilfolder = None


if __name__ == '__main__':
    unittest.main(verbosity=2, failfast=True)

########NEW FILE########
__FILENAME__ = nbttests
#!/usr/bin/env python
import sys,os
import tempfile, shutil
from io import BytesIO
from gzip import GzipFile

import unittest
try:
    from unittest import skip as _skip
except ImportError:
    # Python 2.6 has an older unittest API. The backported package is available from pypi.
    import unittest2 as unittest

# Search parent directory first, to make sure we test the local nbt module, 
# not an installed nbt module.
parentdir = os.path.realpath(os.path.join(os.path.dirname(__file__),os.pardir))
if parentdir not in sys.path:
    sys.path.insert(1, parentdir)  # insert ../ just after ./

from nbt.nbt import _TAG_Numeric, TAG_Int, MalformedFileError, NBTFile, TAGLIST

NBTTESTFILE = os.path.join(os.path.dirname(__file__), 'bigtest.nbt')


class BugfixTest(unittest.TestCase):
    """Bugfix regression tests."""
    def testEmptyFiles(self):
        """
        Opening an empty file causes an uncaught exception.
        https://github.com/twoolie/NBT/issue/4
        """
        temp = BytesIO(b"")
        temp.seek(0)
        self.assertRaises(MalformedFileError, NBTFile, buffer=temp)


class ReadWriteTest(unittest.TestCase):
    """test that we can read the test file correctly"""

    def setUp(self):
        # copy the file (don't work on the original test file)
        self.tempdir = tempfile.mkdtemp()
        self.filename = os.path.join(self.tempdir, 'bigtest.nbt')
        shutil.copy(NBTTESTFILE, self.filename)

    def tearDown(self):
        # remove the temporary file
        try:
            shutil.rmtree(self.tempdir)
        except OSError as e:
            raise
    
    def testReadBig(self):
        mynbt = NBTFile(self.filename)
        self.assertTrue(mynbt.filename != None)
        self.assertEqual(len(mynbt.tags), 11)

    def testWriteBig(self):
        mynbt = NBTFile(self.filename)
        output = BytesIO()
        mynbt.write_file(buffer=output)
        self.assertEqual(GzipFile(NBTTESTFILE).read(), output.getvalue())

    def testWriteback(self):
        mynbt = NBTFile(self.filename)
        mynbt.write_file()

    def testProperlyClosed(self):
        """
        test that files opened from a file name are closed after 
        being written to. i.e. will read correctly in the future
        """
        #open the file
        mynbt = NBTFile(self.filename)
        mynbt['test'] = TAG_Int(123)
        mynbt.write_file()
        if hasattr(mynbt.file, "closed"):
            self.assertTrue(mynbt.file.closed)
        else: # GZipFile does not yet have a closed attribute in Python 2.6
            self.assertTrue(mynbt.file.fileobj == None)
        # make sure it can be read again directly after closing, 
        # and contains the updated contents.
        mynbt = NBTFile(self.filename)
        self.assertEqual(mynbt['test'].value, 123)

# TODO: test if self.file is NOT closed for NBTFile.write_file([fileobject])
# TODO: test if self.file is NOT closed for NBTFile.write_file([buffer])
# TODO: test if NBTFile([buffer]) followed by mynbt.write_file() fails

class TreeManipulationTest(unittest.TestCase):

    def setUp(self):
        self.nbtfile = NBTFile()

    def testRootNodeSetup(self):
        self.nbtfile.name = "Hello World"
        self.assertEqual(self.nbtfile.name, "Hello World")

    def testTagNumeric(self):
        for tag in TAGLIST:
            if isinstance(TAGLIST[tag], _TAG_Numeric):
                tagobj = TAGLIST[tag](name="Test", value=10)
                self.assertEqual(byte.name, "Test", "Name not set correctly for %s" % TAGLIST[tag].__class__.__name__)
                self.assertEqual(byte.value, 10, "Value not set correctly for %s" % TAGLIST[tag].__class__.__name__)
                self.nbtfile.tags.append(tagobj)

    #etcetera..... will finish later

    def tearDown(self):
        del self.nbtfile

class EmptyStringTest(unittest.TestCase):

    def setUp(self):
        self.golden_value = b"\x0A\0\x04Test\x08\0\x0Cempty string\0\0\0"
        self.nbtfile = NBTFile(buffer=BytesIO(self.golden_value))

    def testReadEmptyString(self):
        self.assertEqual(self.nbtfile.name, "Test")
        self.assertEqual(self.nbtfile["empty string"].value, "")

    def testWriteEmptyString(self):
        buffer = BytesIO()
        self.nbtfile.write_file(buffer=buffer)
        self.assertEqual(buffer.getvalue(), self.golden_value)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = regiontests
#!/usr/bin/env python
import sys,os
import tempfile, shutil
from io import BytesIO
import logging
import random
import time
import zlib

import unittest
try:
    from unittest import skip as _skip
except ImportError:
    # Python 2.6 has an older unittest API. The backported package is available from pypi.
    import unittest2 as unittest

# Search parent directory first, to make sure we test the local nbt module, 
# not an installed nbt module.
parentdir = os.path.realpath(os.path.join(os.path.dirname(__file__),os.pardir))
if parentdir not in sys.path:
    sys.path.insert(1, parentdir) # insert ../ just after ./

from nbt.region import RegionFile, RegionFileFormatError, NoRegionHeader, \
    RegionHeaderError, ChunkHeaderError, ChunkDataError, InconceivedChunk
from nbt.nbt import NBTFile, TAG_Compound, TAG_Byte_Array, TAG_Long, TAG_Int, TAG_String

REGIONTESTFILE = os.path.join(os.path.dirname(__file__), 'regiontest.mca')


def generate_level(bytesize = 4):
    """Generate a level, which is a given size in bytes."""
    level = NBTFile() # Blank NBT
    def append_byte_array(name, bytesize=1000):
        bytesize -= len(name)
        bytesize -= 7
        # byte_array = TAG_Byte_Array(name=name, value=bytearray([random.randrange(256) for i in range(bytesize)]))
        # level.append(byte_array)
        byte_array = TAG_Byte_Array(name=name)
        byte_array.value = bytearray([random.randrange(256) for i in range(bytesize)])
        level.tags.append(byte_array)
    random.seed(123456789) # fixed seed to give predictable results.
    if bytesize < 13:
        raise ValueError("NBT file size is at least 13 bytes")
    # 4 bytes TAG_Compound overhead, 13 bytes TAG_Byte_Array overhead.
    bytesize -= 4 # deduce overhead bytes
    i = 1
    while bytesize > 1500:
        append_byte_array("%06d" % i, 1000)
        i += 1
        bytesize -= 1000
    append_byte_array("last", bytesize)
    return level

def generate_compressed_level(minsize = 2000, maxsize = None):
    """
    Generate a level, which -when zlib compressed- is the given size in bytes.
    Note: this returns the *UNCOMPRESSED* NBT file.
    """
    logger = logging.getLogger("nbt.tests.regiontests")
    if maxsize == None:
        maxsize = minsize
    targetsize = (minsize + maxsize) // 2
    bytesize = targetsize
    c = None
    tries = 0
    while True:
        # Generate a level, encode to binary and compress
        level = generate_level(bytesize)
        b = BytesIO()
        level.write_file(buffer=b)
        b = b.getvalue()
        assert len(b) == bytesize
        c = zlib.compress(b, 1)
        # check if the compressed size is sufficient.
        resultsize = len(c)
        logger.debug("try %d: uncompressed %d bytes -> compressed %d bytes" % (tries, bytesize, resultsize))
        if minsize <= resultsize <= maxsize:
            break
        # size is not good enough. Try again, with new targetsize.
        bytesize = int(round(bytesize * targetsize / resultsize))
        tries += 1
        if tries > 20:
            sys.stderr.write("Failed to generate NBT file of %d bytes after %d tries. Result is %d bytes.\n" % (targetsize, tries, resultsize))
            break
    return level

class ReadWriteTest(unittest.TestCase):
    """Test to read, write and relocate chunks in a region file."""
    
    """
    All tests operate on regiontest.mca, is a 27-sector large region file, which looks like:
    sector 000: locations
    sector 001: timestamps
    sector 002: chunk 6 ,0  part 1/1
    sector 003: chunk 7 ,0  part 1/1 <<-- minor warning: unused bytes not zeroed
    sector 004: empty                <<-- minor warning: bytes not zeroed
    sector 005: chunk 8 ,0  part 1/1
    sector 006: chunk 9 ,0  part 1/1
    sector 007: chunk 10,0  part 1/1 <<-- deprecated encoding (gzip = 1)
    sector 008: chunk 11,0  part 1/1 <<-- unknown encoding (3)
    sector 009: chunk 2 ,0  part 1/1 <<-- uncompressed (encoding 0)
    sector 010: empty
    sector 011: empty
    sector 012: chunk 3 ,0  part 1/1 <<-- garbled data (can't be decoded)
    sector 013: chunk 1 ,0  part 1/1
    sector 014: chunk 4 ,0  part 1/3 <<-- 1 sector required, but 3 sectors allocated
    sector 015: chunk 12,0  part 1/1 <<-- part 2 of chunk 4,0 overlaps
    sector 016: chunk 4, 0  part 3/3
    sector 017: chunk 16,0  part 1/2
    sector 018: chunk 16,0  part 2/2
    sector 019: chunk 5 ,1  part 1/2 <<-- correct encoding, but not a valid NBT file
    sector 020: chunk 5 ,1  part 2/2
    sector 021: chunk 6 ,1  part 1/1 <<-- potential overlap with empty chunk 13,0
    sector 022: chunk 7 ,1  part 1/1 <<-- no timestamp
    sector 023: chunk 4 ,1  part 1/1 <<-- zero-byte length value in chunk (invalid header)
    sector 024: chunk 8 ,1  part 1/1 <<-- one-byte length value in chunk (no data)
    sector 025: chunk 3 ,1  part 1/1 <<-- 2 sectors required, but 1 sector allocated (length 4+1+4092)
    sector 026: empty                <<-- unregistered overlap from chunk 3,1
    
    in addition, the following (corrupted) chunks are defined in the header of regiontest.mca:
    sector 021: 0-sector length chunk 13,0 (and overlapping with chunk 6,1)
    sector 001: chunk 14,0 (in header)
    sector 030: chunk 15,0 (out of file)
    ----------: chunk 17,0 timestamp without data
    
    Thus:
    01. chunk 1 ,0  Readable  
    02. chunk 2 ,0  Readable   <<-- uncompressed (encoding 0)
    03. chunk 3 ,0  Unreadable <<-- garbled data (can't be decoded)
    04. chunk 4 ,0  Readable   <<-- overlaps with chunk 12,0.
    05. chunk 6 ,0  Readable 
    06. chunk 7 ,0  Readable 
    07. chunk 8 ,0  Readable 
    08. chunk 9 ,0  Readable 
    09. chunk 10,0  Readable   <<-- deprecated encoding (gzip = 1)
    10. chunk 11,0  Unreadable <<-- unknown encoding (3)
    11. chunk 12,0  Readable   <<-- Overlaps with chunk 4,0.
    12. chunk 13,0  Unreadable <<-- 0-sector length in header
    13. chunk 14,0  Unreadable <<-- in header
    14. chunk 15,0  Unreadable <<-- out of file
    15. chunk 16,0  Readable  
    --  chunk 17,0  Unreadable <<-- timestamp without data
    16. chunk 3 ,1  Readable   <<-- 2 sectors required, but 1 sector allocated (length 4+1+4092)
    17. chunk 4 ,1  Unreadable <<-- zero-byte length value in chunk (invalid header)
    18. chunk 5 ,1  Readable   <<-- Not a valid NBT file
    19. chunk 6 ,1  Readable   <<-- potential overlap with empty chunk 13,0
    20. chunk 7 ,1  Readable   <<-- no timestamp
    21. chunk 8 ,1  Unreadable <<-- one-byte length value in chunk (no data)
    """

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        self.filename = os.path.join(self.tempdir, 'regiontest.mca')
        shutil.copy(REGIONTESTFILE, self.filename)
        self.region = RegionFile(filename = self.filename)

    def tearDown(self):
        del self.region
        try:
            shutil.rmtree(self.tempdir)
        except OSError as e:
            raise

    def test000MethodFileSize(self):
        """
        Test of the get_size() method.
        The regionfile has 27 sectors.
        """
        self.assertEqual(self.region.get_size(), 27*4096)

    def test001MethodChunkCount(self):
        """
        Test of the chunk_count() method.
        The regionfile has 21 chunks, including 3-out of file chunks.
        """
        self.assertEqual(self.region.chunk_count(), 21)

    def test002MethodGetChunkCoords(self):
        """
        Test of get_chunk_coords() method.
        Note: this function may be deprecated in a later version of NBT.
        """
        coords_and_lengths = self.region.get_chunk_coords()
        coords = []
        for coord in coords_and_lengths:
            coords.append((coord['x'], coord['z']))
        
        self.assertIn((1, 0), coords)
        self.assertIn((2, 0), coords)
        self.assertIn((3, 0), coords)
        self.assertIn((4, 0), coords)
        self.assertIn((6, 0), coords)
        self.assertIn((7, 0), coords)
        self.assertIn((8, 0), coords)
        self.assertIn((9, 0), coords)
        self.assertIn((10, 0), coords)
        self.assertIn((11, 0), coords)
        self.assertIn((12, 0), coords)
        self.assertIn((13, 0), coords)
        self.assertIn((14, 0), coords) # note: length is undefined
        self.assertIn((15, 0), coords) # note: length is undefined
        self.assertIn((16, 0), coords)
        self.assertNotIn((17, 0), coords)
        self.assertIn((3, 1), coords)
        self.assertIn((4, 1), coords)
        self.assertIn((5, 1), coords)
        self.assertIn((6, 1), coords)
        self.assertIn((7, 1), coords)
        self.assertIn((8, 1), coords)
        self.assertEqual(len(coords_and_lengths), 21)

    def test003MethodIterChunks(self):
        """
        Test of iter_chunks() method.
        """
        chunks = []
        for chunk in self.region.iter_chunks():
            self.assertIsInstance(chunk, TAG_Compound)
            chunks.append(chunk)
        self.assertEqual(len(chunks), 13)

    def test004SyntaxIterChunks(self):
        """
        Test of iter(RegionFile) syntax.
        """
        chunks = []
        for chunk in self.region:
            self.assertIsInstance(chunk, TAG_Compound)
            chunks.append(chunk)
        self.assertEqual(len(chunks), 13)
    
    def test005ParameterHeaders(self):
        """
        read headers of chunk 9,0: 
        sector 6, 1 sector length, timestamp 1334530101, status STATUS_CHUNK_OK.
        read chunk headers of chunk 9,0: 
        lenght (incl. compression byte): 3969 bytes, zlip (2) compression, status STATUS_CHUNK_OK.
        """
        self.assertEqual(self.region.header[9,0], (6, 1, 1334530101, RegionFile.STATUS_CHUNK_OK))
        self.assertEqual(self.region.chunk_headers[9,0], (3969, 2, RegionFile.STATUS_CHUNK_OK))
    
    def test006ParameterHeadersUndefinedChunk(self):
        """
        read headers & chunk_headers of chunk 2,2
        """
        self.assertEqual(self.region.header[2,2], (0, 0, 0, RegionFile.STATUS_CHUNK_NOT_CREATED))
        self.assertEqual(self.region.chunk_headers[2,2], (None, None, RegionFile.STATUS_CHUNK_NOT_CREATED))
    
    def test010ReadChunkZlibCompression(self):
        """
        chunk 9,0: regular Zlib compression. Should be read OK.
        """
        nbt = self.region.get_nbt(9, 0)
        self.assertIsInstance(nbt, TAG_Compound)
        # get_chunk is currently an alias of get_nbt
        chunk = self.region.get_chunk(9, 0)
        self.assertIsInstance(chunk, TAG_Compound)

    def test011ReadChunkGzipCompression(self):
        """
        chunk 10,0: deprecated GZip compression. Should be read OK.
        """
        nbt = self.region.get_nbt(10, 0)
        self.assertIsInstance(nbt, TAG_Compound)

    def test012ReadChunkUncompressed(self):
        """
        chunk 2,0: no compression. Should be read OK.
        """
        nbt = self.region.get_nbt(2, 0)
        self.assertIsInstance(nbt, TAG_Compound)

    def test013ReadUnknownEncoding(self):
        """
        chunk 11,0 has unknown encoding (3). Reading should raise a ChunkDataError.
        """
        self.assertRaises(ChunkDataError, self.region.get_nbt, 11, 0)

    def test014ReadMalformedEncoding(self):
        """
        chunk 3,0 has malformed content. Reading should raise a ChunkDataError.
        This should not raise a MalformedFileError.
        """
        self.assertRaises(ChunkDataError, self.region.get_nbt, 3, 0)

    # TODO: raise nbt.region.ChunkDataError instead of nbt.nbt.MalformedFileError, or make them the same.
    def test015ReadMalformedNBT(self):
        """
        read chunk 5,1: valid compression, but not a valid NBT file. Reading should raise a ChunkDataError.
        """
        self.assertRaises(ChunkDataError, self.region.get_nbt, 5, 1)

    def test016ReadChunkNonExistent(self):
        """
        read chunk 2,2: does not exist. Reading should raise a InconceivedChunk.
        """
        self.assertRaises(InconceivedChunk, self.region.get_nbt, 2, 2)

    def test017ReadableChunks(self):
        """
        Test which chunks are readable.
        """
        coords = []
        for cc in self.region.get_chunk_coords():
            try:
                nbt = self.region.get_chunk(cc['x'], cc['z'])
                coords.append((cc['x'], cc['z']))
            except RegionFileFormatError:
                pass

        self.assertIn((1, 0), coords)
        self.assertIn((2, 0), coords)
        self.assertNotIn((3, 0), coords) # garbled data
        self.assertIn((4, 0), coords) # readable, despite overlapping with chunk 12,0
        self.assertIn((6, 0), coords)
        self.assertIn((7, 0), coords)
        self.assertIn((8, 0), coords)
        self.assertIn((9, 0), coords)
        self.assertIn((10, 0), coords)
        self.assertNotIn((11, 0), coords) # unknown encoding
        self.assertIn((12, 0), coords) # readable, despite overlapping with chunk 4,1
        self.assertNotIn((13, 0), coords) # zero-length (in header)
        self.assertNotIn((14, 0), coords) # in header
        self.assertNotIn((15, 0), coords) # out of file
        self.assertIn((16, 0), coords)
        self.assertNotIn((17, 0), coords) # timestamp without data
        self.assertIn((3, 1), coords)
        self.assertNotIn((4, 1), coords) # invalid length (in chunk)
        self.assertNotIn((5, 1), coords) # not a valid NBT file
        self.assertIn((6, 1), coords)
        self.assertIn((7, 1), coords)
        self.assertNotIn((8, 1), coords) # zero-length (in chunk)
        self.assertEqual(len(coords), 13)

    def test020ReadInHeader(self):
        """
        read chunk 14,0: supposedly located in the header. 
        Reading should raise a RegionHeaderError.
        """
        self.assertRaises(RegionHeaderError, self.region.get_nbt, 14, 0)
        # TODO:
        self.assertEqual(self.region.header[14,0], (1, 1, 1376433960, RegionFile.STATUS_CHUNK_IN_HEADER))
        self.assertEqual(self.region.chunk_headers[14,0], (None, None, RegionFile.STATUS_CHUNK_IN_HEADER))

    def test021ReadOutOfFile(self):
        """
        read chunk 15,0: error (out of file)
        """
        self.assertRaises(RegionHeaderError, self.region.get_nbt, 15, 0)
        self.assertEqual(self.region.header[15,0], (30, 1, 1376433961, RegionFile.STATUS_CHUNK_OUT_OF_FILE))
        self.assertEqual(self.region.chunk_headers[15,0], (None, None, RegionFile.STATUS_CHUNK_OUT_OF_FILE))

    def test022ReadZeroLengthHeader(self):
        """
        read chunk 13,0: error (zero-length)
        """
        self.assertRaises(RegionHeaderError, self.region.get_nbt, 13, 0)
        self.assertEqual(self.region.header[13,0], (21, 0, 1376433958, RegionFile.STATUS_CHUNK_ZERO_LENGTH))
        self.assertEqual(self.region.chunk_headers[13,0], (None, None, RegionFile.STATUS_CHUNK_ZERO_LENGTH))

    def test023ReadInvalidLengthChunk(self):
        """
        zero-byte lengths in chunk. (4,1)
        read chunk 4,1: error (invalid)
        """
        self.assertRaises(ChunkHeaderError, self.region.get_nbt, 4, 1)

    def test024ReadZeroLengthChunk(self):
        """
        read chunk 8,1: error (zero-length chunk)
        """
        self.assertRaises(ChunkHeaderError, self.region.get_nbt, 8, 1)

    def test025ReadChunkSizeExceedsSectorSize(self):
        """
        read chunk 3,1: can be read, despite that the chunk content is longer than the allocated sectors.
        In general, reading should either succeeds, or raises a ChunkDataError.
        The status should be STATUS_CHUNK_MISMATCHED_LENGTHS.
        """
        self.assertEqual(self.region.chunk_headers[3,1][2], RegionFile.STATUS_CHUNK_MISMATCHED_LENGTHS)
        # reading should succeed, despite the overlap (next chunk is free)
        nbt = self.region.get_nbt(3, 1)

    def test026ReadChunkOverlapping(self):
        """
        chunk 4,0 and chunk 12,0 overlap: status should be STATUS_CHUNK_OVERLAPPING
        """
        self.assertEqual(self.region.chunk_headers[4,0][2], RegionFile.STATUS_CHUNK_OVERLAPPING)
        self.assertEqual(self.region.chunk_headers[12,0][2], RegionFile.STATUS_CHUNK_OVERLAPPING)

    def test030GetTimestampOK(self):
        """
        get_timestamp
        read chunk 9,0: OK
        """
        self.assertEqual(self.region.get_timestamp(9,0), 1334530101)

    def test031GetTimestampBadChunk(self):
        """
        read chunk 15,0: OK
        Data is out-out-of-file, but timestamp is still there.
        """
        self.assertEqual(self.region.get_timestamp(15,0), 1376433961)

    def test032GetTimestampNoChunk(self):
        """
        read chunk 17,0: OK
        no data, but a timestamp
        """
        self.assertEqual(self.region.get_timestamp(17,0), 1334530101)

    def test033GetTimestampMissing(self):
        """
        read chunk 7,1: OK
        data, but no timestamp
        """
        self.assertEqual(self.region.get_timestamp(7,1), 0)

    def test040WriteNewChunk(self):
        """
        read chunk 0,2: InconceivedError
        write 1 sector chunk 0,2
        - read location (<= 026), size (001), timestamp (non-zero).
        """
        chunk_count = self.region.chunk_count()
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.assertRaises(InconceivedChunk, self.region.get_nbt, 0, 2)
        timebefore = int(time.time())
        self.region.write_chunk(0, 2, nbt)
        timeafter = time.time()
        header = self.region.header[0,2]
        self.assertEqual(header[1], 1, "Chunk length must be 1 sector")
        self.assertGreaterEqual(header[0], 2, "Chunk must not be written in the header")
        self.assertLessEqual(header[0], 26, "Chunk must not be written in an empty sector")
        self.assertGreaterEqual(header[2], timebefore, "Timestamp must be time.time()")
        self.assertLessEqual(header[2], timeafter, "Timestamp must be time.time()")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)
        self.assertEqual(self.region.chunk_count(), chunk_count + 1)

    def test041WriteAndReadNewChunk(self):
        """
        write 1 sector chunk 0,2
        read chunk 0,2: OK
        - compare writen and read NBT file
        """
        nbtwrite = generate_compressed_level(minsize = 100, maxsize = 4000)
        writebuffer = BytesIO()
        nbtwrite.write_file(buffer=writebuffer)
        nbtsize = writebuffer.seek(0,2)
        self.region.write_chunk(0, 2, nbtwrite)
        nbtread = self.region.get_nbt(0, 2)
        readbuffer = BytesIO()
        nbtread.write_file(buffer=readbuffer)
        self.assertEqual(nbtsize, readbuffer.seek(0,2))
        writebuffer.seek(0)
        writtendata = writebuffer.read()
        readbuffer.seek(0)
        readdata = readbuffer.read()
        self.assertEqual(writtendata, readdata)

    def test042WriteExistingChunk(self):
        """
        write 1 sector chunk 9,0 (should stay in 006)
        - read location (006) and size (001).
        """
        chunk_count = self.region.chunk_count()
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(9, 0, nbt)
        header = self.region.header[9, 0]
        self.assertEqual(header[0], 6, "Chunk should remain at sector 6")
        self.assertEqual(header[1], 1, "Chunk length must be 1 sector")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)
        self.assertEqual(self.region.chunk_count(), chunk_count)

    def test043DeleteChunk(self):
        """
        read chunk 6,0: OK
        unlink chunk 6,0
        - check location, size, timestamp (all should be 0)
        read chunk 6,0: InconceivedError
        """
        chunk_count = self.region.chunk_count()
        nbt = self.region.get_nbt(6, 0)
        self.region.unlink_chunk(6, 0)
        self.assertRaises(InconceivedChunk, self.region.get_nbt, 6, 0)
        header = self.region.header[6, 0]
        self.assertEqual(header[0], 0)
        self.assertEqual(header[1], 0)
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_NOT_CREATED)
        self.assertEqual(self.region.chunk_count(), chunk_count - 1)

    def test044UseEmptyChunks(self):
        """
        write 1 sector chunk 1,2 (should go to 004)
        write 1 sector chunk 2,2 (should go to 010)
        write 1 sector chunk 3,2 (should go to 011)
        verify file size remains 027*4096
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        availablelocations = (4, 10, 11, 26)
        self.region.write_chunk(1, 2, nbt)
        self.assertIn(self.region.header[1, 2][0], availablelocations)
        self.region.write_chunk(2, 2, nbt)
        self.assertIn(self.region.header[2, 2][0], availablelocations)
        self.region.write_chunk(3, 2, nbt)
        self.assertIn(self.region.header[3, 2][0], availablelocations)

    def test050WriteNewChunk2sector(self):
        """
        write 2 sector chunk 1,2 (should go to 010-011)
        """
        nbt = generate_compressed_level(minsize = 5000, maxsize = 7000)
        self.region.write_chunk(1, 2, nbt)
        header = self.region.header[1, 2]
        self.assertEqual(header[1], 2, "Chunk length must be 2 sectors")
        self.assertEqual(header[0], 10, "Chunk should be placed in sector 10")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)

    def test051WriteNewChunk4096byte(self):
        """
        write 4091+5-byte (1 sector) chunk 1,2 (should go to 004)
        """
        nbt = generate_compressed_level(minsize = 4091, maxsize = 4091)
        self.region.write_chunk(1, 2, nbt)
        header = self.region.header[1, 2]
        chunk_header = self.region.chunk_headers[1, 2]
        if chunk_header[0] != 4092:
            raise unittest.SkipTest("Can't create chunk of 4091 bytes compressed")
        self.assertEqual(header[1], 1, "Chunk length must be 2 sectors")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)

    def test052WriteNewChunk4097byte(self):
        """
        write 4092+5-byte (2 sector) chunk 1,2 (should go to 010-011)
        """
        nbt = generate_compressed_level(minsize = 4092, maxsize = 4092)
        self.region.write_chunk(1, 2, nbt)
        header = self.region.header[1, 2]
        chunk_header = self.region.chunk_headers[1, 2]
        if chunk_header[0] != 4093:
            raise unittest.SkipTest("Can't create chunk of 4092 bytes compressed")
        self.assertEqual(header[1], 2, "Chunk length must be 2 sectors")
        self.assertEqual(header[0], 10, "Chunk should be placed in sector 10")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)

    def test053WriteNewChunkIncreaseFile(self):
        """
        write 3 sector chunk 2,2 (should go to 026-028 or 027-029) (increase file size)
        verify file size is 29*4096
        """
        nbt = generate_compressed_level(minsize = 9000, maxsize = 11000)
        self.region.write_chunk(1, 2, nbt)
        header = self.region.header[1, 2]
        self.assertEqual(header[1], 3, "Chunk length must be 3 sectors")
        self.assertIn(header[0], (26, 27), "Chunk should be placed in sector 26")
        self.assertEqual(self.region.get_size(), (header[0] + header[1])*4096, "File size should be multiple of 4096")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)

    def test054WriteExistingChunkDecreaseSector(self):
        """
        write 1 sector chunk 16,0 (should go to existing 017) (should free sector 018)
        write 1 sector chunk 1,2 (should go to 004)
        write 1 sector chunk 2,2 (should go to 010)
        write 1 sector chunk 3,2 (should go to 011)
        write 1 sector chunk 4,2 (should go to freed 018)
        write 1 sector chunk 5,2 (should go to 026)
        verify file size remains 027*4096
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        header = self.region.header[16, 0]
        self.assertEqual(header[1], 2)
        self.region.write_chunk(16, 0, nbt)
        header = self.region.header[16, 0]
        self.assertEqual(header[1], 1, "Chunk length must be 1 sector1")
        self.assertEqual(header[0], 17, "Chunk should remain in sector 17")
        # Write 1-sector chunks to check which sectors are "free"
        locations = []
        self.region.write_chunk(1, 2, nbt)
        locations.append(self.region.header[1, 2][0])
        self.region.write_chunk(2, 2, nbt)
        locations.append(self.region.header[2, 2][0])
        self.region.write_chunk(3, 2, nbt)
        locations.append(self.region.header[3, 2][0])
        self.region.write_chunk(4, 2, nbt)
        locations.append(self.region.header[4, 2][0])
        self.region.write_chunk(5, 2, nbt)
        locations.append(self.region.header[5, 2][0])
        self.assertIn(18, locations)
        # self.assertEqual(locations, [4, 10, 11, 18, 26])
        # self.assertEqual(self.region.get_size(), 27*4096)

    @unittest.skip('Test takes too much time')
    def test055WriteChunkTooLarge(self):
        """
        Chunks of size >= 256 sectors are not supported by the file format
        attempt to write a chunk 256 sectors in size
        should raise Exception
        """
        maxsize = 256 * 4096
        nbt = generate_compressed_level(minsize = maxsize + 100, maxsize = maxsize + 4000)
        self.assertRaises(ChunkDataError, self.region.write_chunk, 2, 2, nbt)

    def test060WriteExistingChunkIncreaseSectorSameLocation(self):
        """
        write 2 sector chunk 7,0 (should go to 003-004) (increase chunk size)
        """
        nbt = generate_compressed_level(minsize = 5000, maxsize = 7000)
        self.region.write_chunk(7, 0, nbt)
        header = self.region.header[7, 0]
        self.assertEqual(header[1], 2, "Chunk length must be 2 sectors")
        self.assertEqual(header[0], 3, "Chunk should remain in sector 3")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)
        # self.assertEqual(self.region.get_size(), 27*4096)

    def test061WriteExistingChunkCorrectSize(self):
        """
        write 2 sector chunk 3,1 (should go to 025-026) (increase sector size)
        """
        nbt = self.region.get_chunk(3, 1)
        self.region.write_chunk(3, 1, nbt)
        header = self.region.header[3, 1]
        self.assertEqual(header[1], 2, "Chunk length must be 2 sectors")
        self.assertEqual(header[0], 25, "Chunk should remain in sector 25")
        self.assertEqual(header[3], RegionFile.STATUS_CHUNK_OK)
        self.assertEqual(self.region.get_size(), 27*4096)

    def test062WriteExistingChunkIncreaseSectorNewLocation(self):
        """
        write 2 sector chunk 8,0 (should go to 004-005 or 010-011)
        verify chunk_count remains 18
        write 2 sector chunk 2,2 (should go to 010-011 or 004-005)
        verify that file size is not increased <= 027*4096
        verify chunk_count is 19
        """
        locations = []
        chunk_count = self.region.chunk_count()
        nbt = generate_compressed_level(minsize = 5000, maxsize = 7000)
        self.region.write_chunk(8, 0, nbt)
        header = self.region.header[8, 0]
        self.assertEqual(header[1], 2) # length
        locations.append(header[0]) # location
        self.assertEqual(self.region.chunk_count(), chunk_count)
        self.region.write_chunk(2, 2, nbt)
        header = self.region.header[2, 2]
        self.assertEqual(header[1], 2) # length
        locations.append(header[0]) # location
        self.assertEqual(sorted(locations), [4, 10]) # locations
        self.assertEqual(self.region.chunk_count(), chunk_count + 1)

    def test063WriteNewChunkFreedSectors(self):
        """
        unlink chunk 6,0
        unlink chunk 7,0
        write 3 sector chunk 2,2 (should go to 002-004) (file size should remain the same)
        """
        self.region.unlink_chunk(6, 0)
        self.region.unlink_chunk(7, 0)
        nbt = generate_compressed_level(minsize = 9000, maxsize = 11000)
        self.region.write_chunk(2, 2, nbt)
        header = self.region.header[2, 2]
        self.assertEqual(header[1], 3, "Chunk length must be 3 sectors")
        self.assertEqual(header[0], 2, "Chunk should be placed in sector 2")

    def test070WriteOutOfFileChunk(self):
        """
        write 1 sector chunk 13,0 (should go to 004)
        Should not go to sector 30 (out-of-file location)
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(13, 0, nbt)
        header = self.region.header[13, 0]
        self.assertEqual(header[1], 1) # length
        self.assertLessEqual(header[0], 26, "Previously out-of-file chunk should be written in-file")

    def test071WriteZeroLengthSectorChunk(self):
        """
        write 1 sector chunk 13,0 (should go to 004)
        Verify sector 19 remains untouched.
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(13, 0, nbt)
        header = self.region.header[13, 0]
        self.assertEqual(header[1], 1) # length
        self.assertNotEqual(header[0], 19, "Previously 0-length chunk should not overwrite existing chunk")

    def test072WriteOverlappingChunkLong(self):
        """
        write 2 sector chunk 4,0 (should go to 010-011) (free 014 & 016)
        verify location is NOT 014 (because of overlap)
        write 1 sector chunk 1,2 (should go to 004)
        write 1 sector chunk 2,2 (should go to freed 014)
        write 1 sector chunk 3,2 (should go to freed 016)
        write 1 sector chunk 4,2 (should go to 018)
        write 1 sector chunk 5,2 (should go to 026)
        verify file size remains 027*4096
        """
        nbt = generate_compressed_level(minsize = 5000, maxsize = 7000)
        self.region.write_chunk(4, 0, nbt)
        header = self.region.header[4, 0]
        self.assertEqual(header[1], 2) # length
        self.assertNotEqual(header[0], 14, "Chunk should not be written to same location when it overlaps")
        self.assertEqual(header[0], 10, "Chunk should not be written to same location when it overlaps")
        # Write 1-sector chunks to check which sectors are "free"
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        locations = []
        self.region.write_chunk(1, 2, nbt)
        locations.append(self.region.header[1, 2][0])
        self.region.write_chunk(2, 2, nbt)
        locations.append(self.region.header[2, 2][0])
        self.region.write_chunk(3, 2, nbt)
        locations.append(self.region.header[3, 2][0])
        self.region.write_chunk(4, 2, nbt)
        locations.append(self.region.header[4, 2][0])
        self.region.write_chunk(5, 2, nbt)
        locations.append(self.region.header[5, 2][0])
        self.assertIn(14, locations)
        self.assertIn(16, locations)
        # self.assertEqual(locations, [4, 14, 16, 18, 26])
        # self.assertEqual(self.region.get_size(), 27*4096)

    def test073WriteOverlappingChunkSmall(self):
        """
        write 1 sector chunk 12,0 (should go to 004) ("free" 015 for use by 4,0)
        verify location is NOT 015
        verify sectors 15 and 16 are not marked as "free", but remain in use by 4,0
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(12, 0, nbt)
        header = self.region.header[12, 0]
        self.assertEqual(header[1], 1) # length
        self.assertNotEqual(header[0], 15, "Chunk should not be written to same location when it overlaps")
        # Write 1-sector chunks to check which sectors are "free"
        locations = []
        self.region.write_chunk(1, 2, nbt)
        locations.append(self.region.header[1, 2][0])
        self.region.write_chunk(2, 2, nbt)
        locations.append(self.region.header[2, 2][0])
        self.region.write_chunk(3, 2, nbt)
        locations.append(self.region.header[3, 2][0])
        self.assertNotIn(15, locations)
        self.assertNotIn(16, locations)
        # self.assertEqual(locations, [10, 11, 26])
        # self.assertEqual(self.region.get_size(), 27*4096)

    def test074WriteOverlappingChunkSameLocation(self):
        """
        write 1 sector chunk 12,0 (should go to 004) ("free" 012 for use by 4,0)
        write 3 sector chunk 4,0 (should stay in 014-016)
        verify file size remains <= 027*4096
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(12, 0, nbt)
        header = self.region.header[12, 0]
        self.assertEqual(header[1], 1) # length
        self.assertNotEqual(header[0], 15, "Chunk should not be written to same location when it overlaps")
        nbt = generate_compressed_level(minsize = 9000, maxsize = 11000)
        self.region.write_chunk(4, 0, nbt)
        header = self.region.header[4, 0]
        self.assertEqual(header[1], 3) # length
        self.assertEqual(header[0], 14, "No longer overlapping chunks should be written to same location when when possible")

    def test080FileTruncateLastChunkDecrease(self):
        """
        write 1 sector chunk 3,1 (should remain in 025) (free 026)
        verify file size is truncated: 26*4096 bytes
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(3, 1, nbt)
        self.assertEqual(self.region.get_size(), 26*4096, "File should be truncated when last chunk is reduced in size")

    def test081FileTruncateFreeTail(self):
        """
        delete chunk 3,1 (free 025: truncate file size)
        verify file size: 25*4096 bytes
        """
        self.region.unlink_chunk(3, 1)
        self.assertEqual(self.region.get_size(), 25*4096, "File should be truncated when last sector(s) are freed")

    def test082FileTruncateMergeFree(self):
        """
        delete chunk 8,1 (free 024)
        delete chunk 3,1 (free 025: truncate file size, including 024)
        verify file size: 24*4096 bytes
        """
        self.region.unlink_chunk(8, 1)
        self.region.unlink_chunk(3, 1)
        self.assertEqual(self.region.get_size(), 24*4096, "File should be truncated as far as possible when last sector(s) are freed")

    def test090DeleteNonExistingChunk(self):
        """
        delete chunk 2,2
        """
        self.region.unlink_chunk(2, 2)
        self.assertFalse(self.region.metadata[2, 2].is_created())

    def test091DeleteNonInHeaderChunk(self):
        """
        delete chunk 14,0. This should leave sector 1 untouched.
        verify sector 1 is unmodified, with the exception of timestamp for chunk 14,0.
        """
        self.region.file.seek(4096)
        before = self.region.file.read(4096)
        chunklocation = 4 * (14 + 32*1)
        before = before[:chunklocation] + before[chunklocation+4:]
        self.region.unlink_chunk(14, 1)
        self.region.file.seek(4096)
        after = self.region.file.read(4096)
        after = after[:chunklocation] + after[chunklocation+4:]
        self.assertEqual(before, after)

    def test092DeleteOutOfFileChunk(self):
        """
        delete chunk 15,1
        verify file size is not increased.
        """
        size = self.region.get_size()
        self.region.unlink_chunk(15, 1)
        self.assertLessEqual(self.region.get_size(), size)

    def test093DeleteChunkZeroTimestamp(self):
        """
        delete chunk 17,0
        verify timestamp is zeroed. both in get_timestamp() and get_metadata()
        """
        self.assertEqual(self.region.get_timestamp(17, 0), 1334530101)
        self.region.unlink_chunk(17, 0)
        self.assertEqual(self.region.get_timestamp(17, 0), 0)

    def test100WriteZeroPadding(self):
        """
        write 1 sector chunk 16,0 (should go to existing 017) (should free sector 018)
        Check if unused bytes in sector 017 and all bytes in sector 018 are zeroed.
        """
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(16, 0, nbt)
        header = self.region.header[16, 0]
        chunk_header = self.region.chunk_headers[16, 0]
        sectorlocation = header[0]
        oldsectorlength = 2 * 4096
        chunklength = 4 + chunk_header[0]
        unusedlength = oldsectorlength - chunklength
        self.region.file.seek(4096*sectorlocation + chunklength)
        unused = self.region.file.read(unusedlength)
        zeroes = unused.count(b'\x00')
        self.assertEqual(zeroes, unusedlength, "All unused bytes should be zeroed after writing a chunk")
    
    def test101DeleteZeroPadding(self):
        """
        unlink chunk 7,1
        Check if all bytes in sector 022 are zeroed.
        """
        header = self.region.header[7, 1]
        sectorlocation = header[0]
        self.region.unlink_chunk(7, 1)
        self.region.file.seek(sectorlocation*4096)
        unused = self.region.file.read(4096)
        zeroes = unused.count(b'\x00')
        self.assertEqual(zeroes, 4096, "All bytes should be zeroed after deleting a chunk")
    
    def test102DeleteOverlappingNoZeroPadding(self):
        """
        unlink chunk 4,0. Due to overlapping chunks, bytes should not be zeroed.
        Check if bytes in sector 015 are not all zeroed.
        """
        header = self.region.header[4, 0]
        sectorlocation = header[0]
        self.region.unlink_chunk(4, 0)
        self.region.file.seek((sectorlocation + 1) * 4096)
        unused = self.region.file.read(4096)
        zeroes = unused.count(b'\x00')
        self.assertNotEqual(zeroes, 4096, "Bytes should not be zeroed after deleting an overlapping chunk")
        self.region.file.seek((sectorlocation) * 4096)
        unused = self.region.file.read(4096)
        zeroes = unused.count(b'\x00')
        self.assertEqual(zeroes, 4096, "Bytes should be zeroed after deleting non-overlapping portions of a chunk")
        self.region.file.seek((sectorlocation + 2) * 4096)
        unused = self.region.file.read(4096)
        zeroes = unused.count(b'\x00')
        self.assertEqual(zeroes, 4096, "Bytes should be zeroed after deleting non-overlapping portions of a chunk")
    
    def test103MoveOverlappingNoZeroPadding(self):
        """
        write 2 sector chunk 4,0 to a different location. Due to overlapping chunks, bytes should not be zeroed.
        Check if bytes in sector 015 are not all zeroed.
        """
        header = self.region.header[4, 0]
        sectorlocation = header[0]
        nbt = generate_compressed_level(minsize = 5000, maxsize = 7000)
        self.region.write_chunk(4, 0, nbt)
        self.region.file.seek((sectorlocation + 1) * 4096)
        unused = self.region.file.read(4096)
        zeroes = unused.count(b'\x00')
        self.assertNotEqual(zeroes, 4096, "Bytes should not be zeroed after moving an overlapping chunk")
    
    def test104DeleteZeroPaddingMismatchLength(self):
        """
        unlink chunk 3,1. (which has a length mismatch)
        Check if bytes in sector 025 are all zeroed.
        Check if first byte in sector 026 is not zeroed.
        """
        raise unittest.SkipTest("Test can't use this testfile")



class EmptyFileTest(unittest.TestCase):
    """Test for 0-byte file support.
    These files should be treated as a valid region file without any stored chunk."""
    
    @staticmethod
    def generate_level():
        level = NBTFile() # Blank NBT
        level.name = "Data"
        level.tags.extend([
            TAG_Long(name="Time", value=1),
            TAG_Long(name="LastPlayed", value=1376031942),
            TAG_Int(name="SpawnX", value=0),
            TAG_Int(name="SpawnY", value=2),
            TAG_Int(name="SpawnZ", value=0),
            TAG_Long(name="SizeOnDisk", value=0),
            TAG_Long(name="RandomSeed", value=123456789),
            TAG_Int(name="version", value=19132),
            TAG_String(name="LevelName", value="Testing")
        ])
        player = TAG_Compound()
        player.name = "Player"
        player.tags.extend([
            TAG_Int(name="Score", value=0),
            TAG_Int(name="Dimension", value=0)
        ])
        inventory = TAG_Compound()
        inventory.name = "Inventory"
        player.tags.append(inventory)
        level.tags.append(player)
        return level

    def setUp(self):
        self.stream = BytesIO(b"")
        self.stream.seek(0)

    def test01ReadFile(self):
        region = RegionFile(fileobj=self.stream)
        self.assertEqual(region.chunk_count(), 0)

    def test02WriteFile(self):
        chunk = self.generate_level()
        region = RegionFile(fileobj=self.stream)
        region.write_chunk(0, 0, chunk)
        self.assertEqual(region.get_size(), 3*4096)
        self.assertEqual(region.chunk_count(), 1)


# class PartialHeaderFileTest(EmptyFileTest):
#   """Test for file support with only a partial header file.
#   These files should be treated as a valid region file without any stored chunk."""
#   
#   def setUp(self):
#       self.stream = BytesIO(4096*b"\x00" + b"\x52\x1E\x8B\xE6")
#       self.stream.seek(0)
# 
#   def test03GetTimestampNoChunk(self):
#       region = RegionFile(fileobj=self.stream)
#       self.assertEqual(region.get_timestamp(0,0), 1377733606)


class TruncatedFileTest(unittest.TestCase):
    """Test for truncated file support.
    These files should be treated as a valid region file without any stored chunk."""

    def setUp(self):
        data = b'\x00\x00\x02\x01' + 8188*b'\x00' + \
               b'\x00\x00\x00\x27\x02\x78\xda\xe3\x62\x60\x71\x49\x2c\x49\x64\x61\x60\x09\xc9\xcc\x4d' + \
               b'\x65\x80\x00\x46\x0e\x06\x16\xbf\x44\x20\x97\x25\x24\xb5\xb8\x84\x01\x00\x6b\xb7\x06\x52'
        self.length = 8235
        self.assertEqual(len(data), self.length)
        stream = BytesIO(data)
        stream.seek(0)
        self.region = RegionFile(fileobj=stream)

    def tearDown(self):
        del self.region

    def test00FileProperties(self):
        self.assertEqual(self.region.get_size(), self.length)
        self.assertEqual(self.region.chunk_count(), 1)
    
    def test01ReadChunk(self):
        """Test if a block can be read, even when the file is truncated right after the block data."""
        data = self.region.get_blockdata(0,0) # This may raise a RegionFileFormatError.
        data = BytesIO(data)
        nbt = NBTFile(buffer=data)
        self.assertEqual(nbt["Time"].value, 1)
        self.assertEqual(nbt["Name"].value, "Test")

    def test02ReplaceChunk(self):
        """Test if writing the last block in a truncated file will extend the file size to the sector boundary."""
        nbt = self.region.get_nbt(0, 0)
        self.region.write_chunk(0, 0, nbt)
        size = self.region.size
        self.assertEqual(size, self.region.get_size())
        self.assertEqual(size, 3*4096)
    
    def test03WriteChunk(self):
        nbt = generate_compressed_level(minsize = 100, maxsize = 4000)
        self.region.write_chunk(0, 1, nbt)
        self.assertEqual(self.region.get_size(), 4*4096)
        self.assertEqual(self.region.chunk_count(), 2)
        self.region.file.seek(self.length)
        unusedlength = 3*4096 - self.length
        unused = self.region.file.read(unusedlength)
        zeroes = unused.count(b'\x00')
        self.assertEqual(unusedlength, zeroes)


# TODO: test suite to test the different __init__ parameters of RegionFile
# (filename or fileobj), write to it, delete RegionFile object, and test if:
# - file is properly written to
# - file is closed (for filename)
# - file is not closed (for fileobj)
# Also test if an exception is raised if RegionFile is called incorrectly (e.g. both filename and fileobj are specified, or none)

# TODO: avoid the following functions to support Python 2.6:
# AttributeError: 'module' object has no attribute 'SkipTest'
# AttributeError: 'ReadWriteTest' object has no attribute 'assertGreaterEqual'
# AttributeError: 'ReadWriteTest' object has no attribute 'assertIn'
# AttributeError: 'ReadWriteTest' object has no attribute 'assertIsInstance'
# AttributeError: 'ReadWriteTest' object has no attribute 'assertLessEqual'
# AttributeError: 'ReadWriteTest' object has no attribute 'assertNotIn'


if __name__ == '__main__':
    logger = logging.getLogger("nbt.tests.regiontests")
    if len(logger.handlers) == 0:
        # Logging is not yet configured. Configure it.
        logging.basicConfig(level=logging.INFO, stream=sys.stderr, format='%(levelname)-8s %(message)s')
    unittest.main()

########NEW FILE########
