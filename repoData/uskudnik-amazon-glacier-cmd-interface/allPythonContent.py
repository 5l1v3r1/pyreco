__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# glacier-cmd documentation build configuration file, created by
# sphinx-quickstart on Thu Sep 27 09:32:58 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
#extensions = []
extensions = ['sphinxcontrib.programoutput', 'sphinx.ext.autodoc',
              'sphinx.ext.intersphinx']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'glacier-cmd'
copyright = u'2012, uskudnik'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.1'
# The full version, including alpha/beta/rc tags.
release = '0.1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'glacier-cmddoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'glacier-cmd.tex', u'glacier-cmd Documentation',
   u'uskudnik', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'glacier-cmd', u'glacier-cmd Documentation',
     [u'uskudnik'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'glacier-cmd', u'glacier-cmd Documentation',
   u'uskudnik', 'glacier-cmd', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

intersphinx_mapping = {
    'python': ('http://python.readthedocs.org/en/latest/', None),
    'boto': ('http://boto.readthedocs.org/en/latest/', None),
}

########NEW FILE########
__FILENAME__ = glacier
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
.. module:: glacier.py
   :platform: Unix, Windows
   :synopsis: Command line interface for amazon glacier
"""

import sys
import os
import ConfigParser
import argparse
import re
import locale
import glob
import csv
import json

from prettytable import PrettyTable

from GlacierWrapper import GlacierWrapper

from functools import wraps
from glacierexception import *

def output_headers(headers, output):
    """
    Prints a list of headers - single item output.

    :param headers: the output to be printed as {'header1':'data1',...}
    :type headers: dict
    """
    rows = [(k, headers[k]) for k in headers.keys()]
    if output == 'print':
        table = PrettyTable(["Header", "Value"])
        for row in rows:
            if len(str(row[1])) < 100:
                table.add_row(row)
        
        print table
        
    if output == 'csv':
        csvwriter = csv.writer(sys.stdout, quoting=csv.QUOTE_ALL)
        for row in rows:
            csvwriter.writerow(row)
        
    if output == 'json':
        print json.dumps(headers)

def output_table(results, output, keys=None, sort_key=None):
    """
    Prettyprints results. Expects a list of identical dicts.
    Use the dict keys as headers unless keys is given; one line for each item.

    Expected format of data is a list of dicts:
    [{'key1':'data1.1', 'key2':'data1.2', ... },
     {'key1':'data1.2', 'key2':'data2.2', ... },
     ...]
    keys: dict of headers to be printed for each key:
    {'key1':'header1', 'key2':'header2',...}

    sort_key: the key to use for sorting the table.
    """

    if output == 'print':
        if len(results) == 0:
            print 'No output!'
            return

        headers = [keys[k] for k in keys.keys()] if keys else results[0].keys()
        table = PrettyTable(headers)
        for line in results:
            table.add_row([line[k] if k in line else '' for k in (keys.keys() if keys else headers)])

        if sort_key:
            table.sortby = keys[sort_key] if keys else sort_key
            
        print table
        
    if output == 'csv':
        csvwriter = csv.writer(sys.stdout, quoting=csv.QUOTE_ALL)
        keys = results[0].keys()
        csvwriter.writerow(keys)
        for row in results:
            csvwriter.writerow([row[k] for k in keys])
            
    if output == 'json':
        print json.dumps(results)

def output_msg(msg, output, success=True):
    """
    In case of a single message output, e.g. nothing found.

    :param msg: a single message to output.
    :type msg: str
    :param success: whether the operation was a success or not.
    :type success: boolean
    """
    if output == 'print':
        print msg
        
    if output == 'csv':
        csvwriter = csv.writer(sys.stdout, quoting=csv.QUOTE_ALL)
        csvwriter.writerow(msg)
            
    if output == 'json':
        print json.dumps(msg)
        
    if not success:
        sys.exit(125)

def size_fmt(num, decimals = 1):
    """
    Formats file sizes in human readable format. Anything bigger than TB
    is returned is TB. Number of decimals is optional, defaults to 1.
    """
    fmt = "%%3.%sf %%s"% decimals
    for x in ['bytes','KB','MB','GB']:
        if num < 1024.0:
            return fmt % (num, x)
        
        num /= 1024.0
        
    return fmt % (num, 'TB')

def default_glacier_wrapper(args, **kwargs):
    """
    Convenience function to call an instance of GlacierWrapper
    with all required arguments.
    """
    return GlacierWrapper(args.aws_access_key,
                          args.aws_secret_key,
                          args.region,
                          bookkeeping=args.bookkeeping,
                          no_bookkeeping=args.no_bookkeeping,
                          bookkeeping_domain_name=args.bookkeeping_domain_name,
                          sdb_access_key=args.sdb_access_key,
                          sdb_secret_key=args.sdb_secret_key,
                          sdb_region=args.sdb_region,
                          # sns_enable=args.sns_enable,
                          # sns_topic=args.sns_topic,
                          # sns_monitored_vaults=args.sns_monitored_vaults,
                          # sns_options=args.sns_options,
                          # config_object=args.config_object,
                          logfile=args.logfile,
                          loglevel=args.loglevel,
                          logtostdout=args.logtostdout)

def handle_errors(fn):
    """
    Decorator for exception handling.
    """
    @wraps(fn)
    def wrapper(*args, **kwargs):
        try:
            return fn(*args, **kwargs)
        except GlacierException as e:

            # We are only interested in the error message in case it is a
            # self-caused exception.
            e.write(indentation='||  ', stack=False, message=True)
            sys.exit(e.exitcode)

    return wrapper

@handle_errors
def lsvault(args):
    """
    Returns a list of vaults (if any).
    """
    glacier = default_glacier_wrapper(args)
    vault_list = glacier.lsvault()
    keys = {'VaultName': "Vault name",
            'VaultARN': "ARN",
            'CreationDate': "Created",
            'SizeInBytes': "Size"}
    output_table(vault_list, args.output, keys=keys)

@handle_errors
def mkvault(args):
    """
    Create a new vault.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.mkvault(args.vault)
    output_headers(response, args.output)

@handle_errors
def rmvault(args):
    """
    Remove a vault.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.rmvault(args.vault)
    output_headers(response, args.output)

@handle_errors
def describevault(args):
    """
    Give the description of a vault.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.describevault(args.vault)
    headers = {'LastInventoryDate': "LastInventory",
               'NumberOfArchives': "Archives",
               'SizeInBytes': "Size",
               'VaultARN': "ARN",
               'CreationDate': "Created"}
    output_headers(response, args.output)

@handle_errors
def listmultiparts(args):
    """
    Give an overview of all multipart uploads that are not finished.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.listmultiparts(args.vault)
    if not response:
        output_msg('No active multipart uploads.', args.output, success=False)
    else:
        output_table(response, args.output)

@handle_errors
def abortmultipart(args):
    """
    Abort a multipart upload which is in progress.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.abortmultipart(args.vault, args.uploadId)
    output_headers(response, args.output)

@handle_errors
def listjobs(args):
    """
    List all the active jobs for a vault.
    """
    glacier = default_glacier_wrapper(args)
    job_list = glacier.list_jobs(args.vault)
    if job_list == []:
        output_msg('No jobs.', args.output, success=False)
        return

    headers = {'Action': "Action",
               'ArchiveId': "Archive ID",
               'StatusCode': "Status",
               'CreationDate': "Initiated",
               'VaultARN': "VaultARN",
               'JobId': "Job ID"}
    output_table(job_list, args.output, keys=headers)

@handle_errors
def describejob(args):
    """
    Give the description of a job.'
    """
    glacier = default_glacier_wrapper(args)
    job = glacier.describejob(args.vault, args.jobid)
    output_headers(job, args.output)

@handle_errors
def download(args):
    """
    Download an archive.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.download(args.vault, args.archive, args.partsize,
                                out_file_name=args.outfile, overwrite=args.overwrite)
    if args.outfile:
        output_msg(response, args.output, success=True)

@handle_errors
def upload(args):
    """
    Upload a file or a set of files to a Glacier vault.
    """

    # See if we got a bacula-style file set.
    # This is /path/to/vol001|vol002|vol003
    if args.bacula:
        if len(args.filename) > 1:
            raise InputException(
                'Bacula-style file name input can accept only one file name argument.')
        
        fileset = args.filename[0].split('|')
        if len(fileset) > 1:
            dirname = os.path.dirname(fileset[0])
            args.filename = [fileset[0]]
            args.filename += [os.path.join(dirname, fileset[i]) for i in range(1, len(fileset))]

    glacier = default_glacier_wrapper(args)
    results = []

    # If we have one or more file names, they appear in a list.
    # Iterate over these file names; do path expansion and wildcard expansion
    # just in case the shell didn't take care of that.
    # If no file name given it's an empty list, and we expect the file to
    # be read over stdin.
    if args.filename:
        for f in args.filename:
    
            # In case the shell does not expand wildcards, if any, do this here.
            if f[0] == '~':
                f = os.path.expanduser(f)

            globbed = glob.glob(f)
            if globbed:
                for g in globbed:
                    response = glacier.upload(args.vault, g, args.description, args.region, args.stdin,
                                              args.name, args.partsize, args.uploadid, args.resume)
                    results.append({"Uploaded file": g,
                                    "Created archive with ID": response[0],
                                    "Archive SHA256 tree hash": response[1]})
            else:
                raise InputException(
                    "File name given for upload can not be found: %s."% f,
                    code='CommandError')
            
    elif args.stdin:

        # No file name; using stdin.
        response = glacier.upload(args.vault, None, args.description, args.region, args.stdin,
                                  args.name, args.partsize, args.uploadid, args.resume)
        results = [{"Created archive with ID": response[0],
                    "Archive SHA256 tree hash": response[1]}]

    else:
        raise InputException(
            '''No input given. Either give a file name or file names
on the command line, or use the --stdin switch and pipe
in the data over stdin.''',
            cause='No file name and no stdin pipe.',
            code='CommandError')
            
    output_table(results, args.output) if len(results) > 1 \
                          else output_headers(results[0], args.output)

@handle_errors
def getarchive(args):
    """
    Initiate an archive retrieval job.
    """
    glacier = default_glacier_wrapper(args)
    status, job, jobid = glacier.getarchive(args.vault, args.archive)
    output_headers(job, args.output)

@handle_errors
def rmarchive(args):
    """
    Remove an archive from a vault.
    """
    glacier = default_glacier_wrapper(args)
    glacier.rmarchive(args.vault, args.archive)
    output_msg("Archive removed.", args.output, success=True)

@handle_errors
def search(args):
    """
    Search the database for file name or description.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.search(vault=args.vault,
                              region=args.region,
                              search_term=args.searchterm,
                              file_name=args.filename)
    output_table(response, args.output)

@handle_errors
def inventory(args):
    """
    Fetch latest inventory (or start a retrieval job if not ready).
    """
    glacier = default_glacier_wrapper(args)
    output = args.output
    if sys.stdout.isatty() and output == 'print':
        print 'Checking inventory, please wait.\r',
        sys.stdout.flush()
        
    job, inventory = glacier.inventory(args.vault, args.refresh)
    if inventory:
        if sys.stdout.isatty() and output == 'print':
            print "Inventory of vault: %s" % (inventory["VaultARN"],)
            print "Inventory Date: %s\n" % (inventory['InventoryDate'],)
            print "Content:"
            
        headers = {'ArchiveDescription': 'Archive Description',
                   'CreationDate': 'Uploaded',
                   'Size': 'Size',
                   'ArchiveId': 'Archive ID',
                   'SHA256TreeHash': 'SHA256 tree hash'}
        output_table(inventory['ArchiveList'], args.output, keys=headers)
        if sys.stdout.isatty() and output == 'print':
            size = 0
            for item in inventory['ArchiveList']:
                size += int(item['Size'])

            print 'This vault contains %s items, total size %s.'% (len(inventory['ArchiveList']), size_fmt(size))

    else:
        result = {'Status':'Inventory retrieval in progress.',
                  'Job ID':job['JobId'],
                  'Job started (time in UTC)':job['CreationDate']}
        output_headers(result, args.output)

@handle_errors
def treehash(args):
    """
    Calculates the tree hash of the given file(s).
    """
    glacier = default_glacier_wrapper(args)
    hash_results = []
    for f in args.filename:
        if f:

            # In case the shell does not expand wildcards, if any, do this here.
            if f[0] == '~':
                f = os.path.expanduser(f)
                
            globbed = glob.glob(f)
            if globbed:
                for g in globbed:
                    hash_results.append(
                        {'File name': g,
                         'SHA256 tree hash': glacier.get_tree_hash(g)})
        else:
            raise InputException(
                'No file name given.',
                code='CommandError')

    output_table(hash_results, args.output)

def snssync(args):
    """
    If monitored_vaults is specified in configuration file, subscribe vaults
    specificed in it to notifications, otherwiser subscribe all vault.
    """
    glacier = default_glacier_wrapper(args)
    response = glacier.sns_sync(sns_options=args.sns_options, output=args.output)
    output_table(response, args.output)

def snssubscribe(args):
    """
    Subscribe individual vaults to notifications by method specified by user.
    """
    protocol = args.protocol
    endpoint = args.endpoint
    vault_names = args.vault
    topic = args.topic

    glacier = default_glacier_wrapper(args)
    response = glacier.sns_subscribe(protocol, endpoint, topic, vault_names=vault_names, sns_options=args.sns_options)
    output_table(response, args.output)

def snslistsubscriptions(args):
    """
    List subscriptions.
    """
    protocol = args.protocol
    endpoint = args.endpoint
    topic = args.topic

    glacier = default_glacier_wrapper(args)
    response = glacier.sns_list_subscriptions(protocol, endpoint, topic, sns_options=args.sns_options)
    output_table(response, args.output)

def snslisttopics(args):
    glacier = default_glacier_wrapper(args)
    response = glacier.sns_list_topics(sns_options=args.sns_options)
    output_table(response, args.output)

def snsunsubscribe(args):
    """
    Unsubscribe individual vaults from notifications for specified protocol, 
    endpoint and vault.
    """
    protocol = args.protocol
    endpoint = args.endpoint
    topic = args.topic

    glacier = default_glacier_wrapper(args)
    response = glacier.sns_unsubscribe(protocol, endpoint, topic, sns_options=args.sns_options)
    output_table(response, args.output)    

def main():
    program_description = u"""
    Command line interface for Amazon Glacier
    """

    # Config parser
    conf_parser = argparse.ArgumentParser(
                                formatter_class=argparse.ArgumentDefaultsHelpFormatter,
                                add_help=False)

    conf_parser.add_argument("-c", "--conf", default="~/.glacier-cmd",
        help="Name of the file to log messages to.", metavar="FILE")
    conf_parser.add_argument('--logtostdout', action='store_true',
        help='Send log messages to stdout instead of the config file.')

    args, remaining_argv = conf_parser.parse_known_args()

    # Here we parse config from files in home folder or in current folder
    # We use separate topics for aws and glacier specific configs
    aws = glacier = sdb = {}
    config = ConfigParser.SafeConfigParser()

    sns = {'topics_present':False, 'topic':'aws-glacier-notifications'}

    configs_read = config.read(['/etc/glacier-cmd.conf',
                    os.path.expanduser('~/.glacier-cmd'),
                    args.conf])
    if configs_read:
        try:
            aws = dict(config.items("aws"))
        except ConfigParser.NoSectionError:
            pass
        try:
            glacier = dict(config.items("glacier"))
        except ConfigParser.NoSectionError:
            pass
        try:
            sdb = dict(config.items("sdb"))
            for key,value in sdb.items():
                sdb["sdb_%s"%key]=value
                del sdb[key]
        except ConfigParser.NoSectionError:
            pass

        topics_present = any(topic for topic in config.sections() if topic.startswith("SNS:"))
        if topics_present:
            sns = { 'topics_present':True }

            sns_topics = []
            for topic in config.sections():
                if topic.startswith("SNS:"):
                    s = {
                        'topic':topic.split("SNS:")[-1],
                        'options':dict(config.items(topic))
                    }
                    sns_topics += [s]
            
            if sns_topics:
                sns['topics'] = sns_topics
        elif any(topic for topic in config.sections() if topic == "SNS"):
            sns = { 'topics_present':False }
            if not config.get('SNS', 'topic', vars={ "topic":None }):
                sns['topic'] = "aws-glacier-notifications"
            else:
                sns['topic'] = config.get('SNS', 'topic')

    # Join config options with environments
    aws = dict(os.environ.items() + aws.items() )
    glacier = dict(os.environ.items() + glacier.items() )
    sdb = dict(os.environ.items() + sdb.items() )

    # Helper functions
    filt_s= lambda x: x.lower().replace("_","-")
    filt = lambda x,y="": dict(((y+"-" if y not in filt_s(k) else "") +
                             filt_s(k), v) for (k, v) in x.iteritems())
    """
    >>> a = {'notifications': 'True', 'monitored_vaults': 'vvt,vv1', "aws-foo":"neki"}
    >>> filt(a, "aws").get('aws-foo')
    'neki'
    """
    a_required = lambda x: x not in filt(aws, "aws")
    s_required = lambda x: x not in filt(sdb, "sdb")
    required = lambda x: x not in filt(glacier)
    a_default = lambda x: filt(aws, "aws").get(x)
    s_default = lambda x: filt(sdb, "sdb").get(x)
    default = lambda x: filt(glacier).get(x)

    # Main configuration parser
    parser = argparse.ArgumentParser(parents=[conf_parser],
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter,
                                     description=program_description)
    subparsers = parser.add_subparsers(title='Subcommands',
        help=u"For subcommand help, use: glacier-cmd <subcommand> -h")

    # Amazon Web Services settings
    group = parser.add_argument_group('aws')
    help_msg_config = u"(Required if you have not created a \
                        ~/.glacier-cmd or /etc/glacier-cmd.conf config file)"
    group.add_argument('--aws-access-key',
                       required=a_required("aws-access-key"),
                       default=a_default("aws-access-key"),
                       help="Your aws access key " + help_msg_config)
    group.add_argument('--aws-secret-key',
                       required=a_required("aws-secret-key"),
                       default=a_default("aws-secret-key"),
                       help="Your aws secret key " + help_msg_config)

    # Glacier settings
    group = parser.add_argument_group('glacier')
    group.add_argument('--region',
                       required=required("region"),
                       default=default("region"),
                       help="Region where you want to store \
                             your archives " + help_msg_config)
    bookkeeping = True if default('bookkeeping') == 'True' else False
    group.add_argument('--bookkeeping',
                       required=False,
                       default=bookkeeping,
                       action="store_true",
                       help="Should we keep book of all created archives.\
                             This requires a Amazon SimpleDB account and its \
                             bookkeeping domain name set")
    group.add_argument('--no-bookkeeping',
                   required=False,
                   default=False,
                   action="store_true",
                   help="Explicitly disables bookkeeping, regardless of other\
                        configuration or command line options.")
    group.add_argument('--bookkeeping-domain-name',
                        required=False,
                        default=default("bookkeeping-domain-name"),
                        help="Amazon SimpleDB domain name for bookkeeping.")
    group.add_argument('--logfile',
                       required=False,
                       default=os.path.expanduser('~/.glacier-cmd.log'),
                       help='File to write log messages to.')
    group.add_argument('--loglevel',
                       required=False,
                       default=default('loglevel') if default('loglevel') else 'WARNING',
                       choices=["-1", "DEBUG", "0", "INFO", "1", "WARNING",
                                "2", "ERROR", "3", "CRITICAL"],
                       help="Set the lowest level of messages you want to log.")
    group.add_argument('--output',
                       required=False,
                       default=default('output') if default('output') else 'print',
                       choices=['print', 'csv', 'json'],
                       help='Set how to return results: print to the screen, or as csv resp. json string.')

    # SimpleDB settings
    group = parser.add_argument_group('sdb')
    group.add_argument('--sdb-access-key',
        required=False,
        default=s_default("sdb-access-key") or a_default("aws-access-key"),
        help="aws access key to be used with bookkeeping" + help_msg_config)
    group.add_argument('--sdb-secret-key',
        required=False,
        default=s_default("sdb-secret-key") or a_default("aws-secret-key"),
        help="aws secret key to be used with bookkeeping" + help_msg_config)
    group.add_argument('--sdb-region',
        required=False,
        default=s_default("sdb-region") or default("region"),
        help="Region where you want to store \
                             your bookkeeping " + help_msg_config)

    # glacier-cmd mkvault <vault>
    parser_mkvault = subparsers.add_parser("mkvault",
        help="Create a new vault.")
    parser_mkvault.add_argument('vault',
        help='The vault to be created.')
    parser_mkvault.set_defaults(func=mkvault)

    # glacier-cmd lsvault    
    parser_lsvault = subparsers.add_parser("lsvault",
        help="List available vaults.")
    parser_lsvault.set_defaults(func=lsvault)

    # glacier-cmd describevault <vault>
    parser_describevault = subparsers.add_parser('describevault',
        help='Describe a vault.')
    parser_describevault.add_argument('vault',
        help='The vault to be described.')
    parser_describevault.set_defaults(func=describevault)

    # glacier-cmd rmvault <vault>
    parser_rmvault = subparsers.add_parser('rmvault',
        help='Remove a vault.')
    parser_rmvault.add_argument('vault',
        help='The vault to be removed.')
    parser_rmvault.set_defaults(func=rmvault)

    # glacier-cmd upload <vault> <filename> [--description <description>] [--name <store file name>] [--partsize <part size>]
    # glacier-cmd upload <vault> --stdin [--description <description>] [--name <store file name>] [--partsize <part size>]
    parser_upload = subparsers.add_parser('upload',
        formatter_class=argparse.RawTextHelpFormatter,
        help='Upload an archive to Amazon Glacier.')
    parser_upload.add_argument('vault',
        help='The vault the archive is to be stored in.')
##    group = parser_upload.add_mutually_exclusive_group(required=True)
    parser_upload.add_argument('filename', nargs='*', default=None,
        help='''\
The name(s) of the local file(s) to be uploaded. Wildcards
are accepted. Can not be used if --stdin is used.''')
    parser_upload.add_argument('--stdin', action='store_true',
        help='''\
Read data from stdin, instead of local file. 
Can not be used if <filename> is given.''')
    parser_upload.add_argument('--name', default=None,
        help='''\
Use the given name as the filename for bookkeeping 
purposes. To be used in conjunction with --stdin or 
when the file being uploaded is a temporary file.''')
    parser_upload.add_argument('--partsize', type=int, default=-1,
        help='''\
Part size to use for upload (in MB). Must
be a power of 2 in the range:
    1, 2, 4, 8, ..., 2,048, 4,096.
Values that are not a power of 2 will be
adjusted upwards to the next power of 2.

Amazon accepts up to 10,000 parts per upload.

Smaller parts result in more frequent progress
updates, and less bandwidth wasted if a part
needs to be re-transmitted. On the other hand,
smaller parts limit the size of the archive that
can be uploaded. Some examples:

partsize  MaxArchiveSize
    1        1*1024*1024*10000 ~= 9.7 GB
    4        4*1024*1024*10000 ~= 39 GB
   16       16*1024*1024*10000 ~= 156 GB
  128      128*1024*1024*10000 ~= 1.2 TB
 4096     4096*1024*1024*10000 ~= 39 TB

If not given, the smallest possible part size
will be used when uploading a file, and 128 MB
when uploading from stdin.''')
    parser_upload.add_argument('--description', default=None,
        help='''\
Description of the file to be uploaded. Use quotes
if your file name contains spaces. (optional).''')
    parser_upload.add_argument('--uploadid', default=None,
        help='''\
The uploadId of a multipart upload that is not
finished yet. If given, glacier-cmd will attempt
to resume this upload using the given file, or by
re-reading the data from stdin.''')
    parser_upload.add_argument('--resume', action='store_true',
        help='''\
Attempt to resume an interrupted multi-part upload.
Does not work in combination with --stdin, and
requires bookkeeping to be enabled.
(not implemented yet)''')
    parser_upload.add_argument('--bacula', action='store_true',
        help='''\
The (single!) file name will be parsed using Bacula's
style of providing multiple names on the command line.
E.g.: /path/to/backup/vol001|vol002|vol003''')
    parser_upload.set_defaults(func=upload)

    # glacier-cmd listmultiparts <vault>
    parser_listmultiparts = subparsers.add_parser('listmultiparts',
        help='List all active multipart uploads.')
    parser_listmultiparts.add_argument('vault',
        help='The vault to check the active multipart uploads for.')
    parser_listmultiparts.set_defaults(func=listmultiparts)

    # glacier-cmd abortmultipart <vault> <uploadId>
    parser_abortmultipart = subparsers.add_parser('abortmultipart',
        help='Abort a multipart upload.')
    parser_abortmultipart.add_argument('vault',
        help='The vault the upload is for.')
    parser_abortmultipart.add_argument('uploadId',
        help='The id of the upload to be aborted, try listmultiparts.')
    parser_abortmultipart.set_defaults(func=abortmultipart)

    # glacier-cmd inventory <vault> [--refresh]
    parser_inventory = subparsers.add_parser('inventory',
        help='List inventory of a vault, if available. If not available, \
              creates inventory retrieval job if none running already.')
    parser_inventory.add_argument('vault',
        help='The vault to list the inventory of.')
    parser_inventory.add_argument('--refresh', action='store_true',
        help='Create an inventory retrieval job, even if inventory is \
              available or with another retrieval job running.')
    parser_inventory.set_defaults(func=inventory)

    # glacier-cmd getarchive <vault> <archive>
    parser_getarchive = subparsers.add_parser('getarchive',
        help='Requests to make an archive available for download.')
    parser_getarchive.add_argument('vault',
        help='The vault the archive is stored in.')
    parser_getarchive.add_argument('archive',
        help='The archive id.')
    parser_getarchive.set_defaults(func=getarchive)

    # glacier-cmd download <vault> <archive> [--outfile <file name>]
    parser_download = subparsers.add_parser('download',
        formatter_class=argparse.RawTextHelpFormatter,
        help='Download a file by archive id.')
    parser_download.add_argument('vault',
        help="Specify the vault in which archive is located.")
    parser_download.add_argument('archive',
        help='The archive to be downloaded.')
    parser_download.add_argument('--outfile',
        help='''\
The name of the local file to store the archive.
If omitted, stdout will be used.''')
    parser_download.add_argument('--overwrite', action='store_true',
        help='''
Overwrite an existing local file if one exists when
downloading an archive.''')
    parser_download.add_argument('--partsize', type=int, default=-1,
        help='''\
Part size to use for download (in MB). Must
be a power of 2 in the range:
    1, 2, 4, 8, ..., 2,048, 4,096.
Values that are not a power of 2 will be
adjusted upwards to the next power of 2.

Amazon accepts up to 10,000 parts per download.

Smaller parts result in more frequent progress
updates, and less bandwidth wasted if a part
needs to be re-transmitted. On the other hand,
smaller parts limit the size of the archive that
can be downloaded and result in slower overall
performance. Some examples:

partsize  MaxArchiveSize
    1        1*1024*1024*10000 ~= 9.7 GB
    4        4*1024*1024*10000 ~= 39 GB
   16       16*1024*1024*10000 ~= 156 GB
  128      128*1024*1024*10000 ~= 1.2 TB
 4096     4096*1024*1024*10000 ~= 39 TB

If not given, the smallest possible part size
will be used depending on the size of the job
at hand.''')
    parser_download.set_defaults(func=download)

    # glacier-cmd rmarchive <vault> <archive>
    parser_rmarchive = subparsers.add_parser('rmarchive',
        help='Remove archive from Amazon Glacier.')
    parser_rmarchive.add_argument('vault',
        help='The vault the archive is stored in.')
    parser_rmarchive.add_argument('archive',
        help='The archive id of the archive to be removed.')
    parser_rmarchive.set_defaults(func=rmarchive)

    # glacier-cmd search [<vault>] [--filename <file name>] [--searchterm <search term>]
    parser_search = subparsers.add_parser('search',
        help='Search Amazon SimpleDB database for available archives \
              (requires bookkeeping to be enabled).')
    parser_search.add_argument('vault', nargs='?', default=None,
        help='The vault to search in. Searching all if omitted.')
    parser_search.add_argument('--filename', default=None,
        help='Search key for searching by (part of) file names.')
    parser_search.add_argument('--searchterm', default=None,
        help='Search key for searching (part of) description fields.')
    parser_search.set_defaults(func=search)

    # glacier-cmd listjobs <vault>
    parser_listjobs = subparsers.add_parser('listjobs',
        help='List active jobs in a vault.')
    parser_listjobs.add_argument('vault',
        help='The vault to list the jobs for.')
    parser_listjobs.set_defaults(func=listjobs)

    # glacier-cmd describejob <vault>
    parser_describejob = subparsers.add_parser('describejob',
        help='Describe a job.')
    parser_describejob.add_argument('vault',
        help='The vault the job is listed for.')
    parser_describejob.add_argument('jobid',
        help='The job ID of the job to be described.')
    parser_describejob.set_defaults(func=describejob)

    # glacier-cmd hash <filename>
    parser_describejob = subparsers.add_parser('treehash',
        help='Calculate the tree-hash (Amazon style sha256-hash) of a file.')
    parser_describejob.add_argument('filename', nargs='*',
        help='The filename to calculate the treehash of.')
    parser_describejob.set_defaults(func=treehash)

    # SNS related commands are located in their own subparser 
    parser_sns = subparsers.add_parser('sns', 
        help="Subcommands related to SNS")
    sns_subparsers = parser_sns.add_subparsers(title="Subcommands related to SNS")

    # glacier-cmd sns syncs
    sns_parser_sync = sns_subparsers.add_parser('sync',
        help="Go through configuration file and either subscribe all vaults to default topic or, if sections are present, create separate topics and subscribe specified vaults to that topic.")
    sns_parser_sync.set_defaults(func=snssync, sns_options=sns)

    # glacier-cmd sns subscribe protocol endpoint topic [--vault]
    sns_parser_subscribe = sns_subparsers.add_parser('subscribe',
        help="Subscribe to topic.")
    sns_parser_subscribe.add_argument("protocol",
        help="Protocol used for notifications. Can be email, http, https or sms.")
    sns_parser_subscribe.add_argument("endpoint",
        help="Valid applicable endpoint - email address, URL or phone number.")
    sns_parser_subscribe.add_argument("topic", 
        help="Topic for which notifications will be sent to specified protocol and endpoint.")
    sns_parser_subscribe.add_argument("--vault",
        help="Optional vault names, seperated by comma, for this a new topic will be created and subscribed to.")
    sns_parser_subscribe.set_defaults(func=snssubscribe, sns_options={ "options":sns, })

    # glacier-cmd sns unsubscribe [--protocol <protocol>] [--endpoint <endpoint>] [--topic <topic>]
    sns_parser_unsubscribe = sns_subparsers.add_parser('unsubscribe',
        help="Unsubscribe from a specified topic.")
    sns_parser_unsubscribe.add_argument("--protocol",
        help="Protocol used for notifications. Can be email, http, https or sms.")
    sns_parser_unsubscribe.add_argument("--endpoint",
        help="Valid applicable endpoint - email address, URL or phone number.")
    sns_parser_unsubscribe.add_argument("--topic",
        help="Topic for which notifications will be sent to specified protocol and endpoint.")
    sns_parser_unsubscribe.set_defaults(func=snsunsubscribe, sns_options=sns)

    # glacier-cmd sns lssub [--protocol <protocol>] [--endpoint <endpoint>] [--topic <topic>]
    sns_parser_listsubs = sns_subparsers.add_parser('lssub', 
        help="List subscriptions. Other arguments are ANDed together.")
    sns_parser_listsubs.add_argument("--protocol",
        help="Show only subscriptions on a specified protocol.")
    sns_parser_listsubs.add_argument("--endpoint",
        help="Show only subscriptions to a specified endpoint.")
    sns_parser_listsubs.add_argument("--topic",
        help="Show only subscriptions for a specified topic.")
    sns_parser_listsubs.set_defaults(func=snslistsubscriptions, sns_options=sns)

    # glacier-cmd sns lstopic
    sns_parser_listtopics = sns_subparsers.add_parser('lstopic',
        help="List all topics.")
    sns_parser_listtopics.set_defaults(func=snslisttopics, sns_options=sns)
    

    # TODO args.logtostdout becomes false when parsing the remaining_argv
    # so here we bridge this. An ugly hack but it works.
    logtostdout = args.logtostdout

    # Process the remaining arguments.
    args = parser.parse_args(remaining_argv)
    
    args.logtostdout = logtostdout
    
    # Run the subcommand.
    args.func(args)

if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = glaciercorecalls
#!/usr/bin/env python
# encoding: utf-8
"""
.. module:: botocorecalls
   :platform: Unix, Windows
   :synopsis: boto calls to access Amazon Glacier.
   
This depends on the boto library, use version 2.6.0 or newer.

     
     writer = GlacierWriter(glacierconn, GLACIER_VAULT)
     writer.write(block of data)
     writer.close()
     # Get the id of the newly created archive
     archive_id = writer.get_archive_id()from boto.connection import AWSAuthConnection
"""

import urllib
import hashlib
import math
import json
import sys
import time

import boto.glacier.layer1

from glacierexception import *

# Placeholder, effectively renaming the class.
class GlacierConnection(boto.glacier.layer1.Layer1):

    pass
    

def chunk_hashes(data):
    """
    Break up the byte-string into 1MB chunks and return sha256 hashes
    for each.
    """
    chunk = 1024*1024
    chunk_count = int(math.ceil(len(data)/float(chunk)))
    return [hashlib.sha256(data[i*chunk:(i+1)*chunk]).digest() for i in range(chunk_count)]

def tree_hash(fo):
    """
    Given a hash of each 1MB chunk (from chunk_hashes) this will hash
    together adjacent hashes until it ends up with one big one. So a
    tree of hashes.
    """
    hashes = []
    hashes.extend(fo)
    while len(hashes) > 1:
        new_hashes = []
        while True:
            if len(hashes) > 1:
                first = hashes.pop(0)
                second = hashes.pop(0)
                new_hashes.append(hashlib.sha256(first + second).digest())
            elif len(hashes) == 1:
                only = hashes.pop(0)
                new_hashes.append(only)
            else:
                break
        hashes.extend(new_hashes)
    return hashes[0]

def bytes_to_hex(str):
    return ''.join( [ "%02x" % ord( x ) for x in str] ).strip()

class GlacierWriter(object):
    """
    Presents a file-like object for writing to a Amazon Glacier
    Archive. The data is written using the multi-part upload API.
    """
    DEFAULT_PART_SIZE = 128 # in MB, power of 2.
    
    def __init__(self, connection, vault_name,
                 description=None, part_size_in_bytes=DEFAULT_PART_SIZE*1024*1024,
                 uploadid=None, logger=None):

        self.part_size = part_size_in_bytes
        self.vault_name = vault_name
        self.connection = connection
##        self.location = None
        self.logger = logger

        if uploadid:
            self.uploadid = uploadid
        else:
            response = self.connection.initiate_multipart_upload(self.vault_name,
                                                                 self.part_size,
                                                                 description)
            self.uploadid = response['UploadId']

        self.uploaded_size = 0
        self.tree_hashes = []
        self.closed = False
##        self.upload_url = response.getheader("location")

    def write(self, data):
        
        if self.closed:
            raise CommunicationError(
                "Tried to write to a GlacierWriter that is already closed.",
                code='InternalError')

        if len(data) > self.part_size:
            raise InputException (
                'Block of data provided must be equal to or smaller than the set block size.',
                code='InternalError')
        
        part_tree_hash = tree_hash(chunk_hashes(data))
        self.tree_hashes.append(part_tree_hash)
        headers = {
                   "x-amz-glacier-version": "2012-06-01",
                    "Content-Range": "bytes %d-%d/*" % (self.uploaded_size,
                                                       (self.uploaded_size+len(data))-1),
                    "Content-Length": str(len(data)),
                    "Content-Type": "application/octet-stream",
                    "x-amz-sha256-tree-hash": bytes_to_hex(part_tree_hash),
                    "x-amz-content-sha256": hashlib.sha256(data).hexdigest()
                  }

        response = self.connection.upload_part(self.vault_name,
                                    self.uploadid,
                                    hashlib.sha256(data).hexdigest(),
                                    bytes_to_hex(part_tree_hash),
                                    (self.uploaded_size, self.uploaded_size+len(data)-1),
                                    data)
        response.read()

##        retries = 0
##        while True:
##            response = self.connection.make_request(
##                "PUT",
##                self.upload_url,
##                headers,
##                data)
##
##            # Success.
##            if response.status == 204:
##                break
##
##            # Time-out recieved: sleep for 5 minutes and try again.
##            # Do not try more than five times; after that it's over.
##            elif response.status == 408:
##                if retries >= 5:
##                    resp = json.loads(response.read())
##                    raise ResonseException(
##                        resp['message'],
##                        cause='Timeout',
##                        code=resp['code'])
##                        
##                if self.logger:
##                    logger.warning(resp['message'])
##                    logger.warning('sleeping 300 seconds (5 minutes) before retrying.')
##                    
##                retries += 1
##                time.sleep(300)
##
##            else:
##                raise ResponseException(
##                    "Multipart upload part expected response status 204 (got %s):\n%s"\
##                        % (response.status, response.read()),
##                    cause=resp['message'],
##                    code=resp['code'])

##        response.read()
        self.uploaded_size += len(data)

    def close(self):
        
        if self.closed:
            return
            
        # Complete the multiplart glacier upload
        response = self.connection.complete_multipart_upload(self.vault_name,
                                                             self.uploadid,
                                                             bytes_to_hex(tree_hash(self.tree_hashes)),
                                                             self.uploaded_size)
        self.archive_id = response['ArchiveId']
        self.location = response['Location']
        self.hash_sha256 = bytes_to_hex(tree_hash(self.tree_hashes))
        self.closed = True

    def get_archive_id(self):
        self.close()
        return self.archive_id

    def get_location(self):
        self.close()
        return self.location

    def get_hash(self):
        self.close()
        return self.hash_sha256

########NEW FILE########
__FILENAME__ = glacierexception
import traceback
import re
import sys
import logging

"""

**********
Note by wvmarle:

This file contains the complete code from chained_exception.py plus the
error handling code from GlacierWrapper.py, allowing it to be used in other
modules like glaciercorecalls as well.
**********

"""
class GlacierException(Exception):
    """
    An extension of the built-in Exception class, this handles
    an additional cause keyword argument, adding it as cause
    attribute to the exception message.
    It logs the error message (amount of information depends on the log
    level) and passes it on to a higher level to handle.
    Furthermore it allows for the upstream handler to call for a
    complete stack trace or just a simple error and cause message.

    TODO: describe usage.
    """

    ERRORCODE = {'InternalError': 127,        # Library internal error.
                 'UndefinedErrorCode': 126,   # Undefined code.
                 'NoResults': 125,            # Operation yielded no results.
                 'GlacierConnectionError': 1,  # Can not connect to Glacier. 
                 'SdbConnectionError': 2,     # Can not connect to SimpleDB.
                 'CommandError': 3,           # Command line is invalid.
                 'VaultNameError': 4,         # Invalid vault name.
                 'DescriptionError': 5,       # Invalid archive description.
                 'IdError': 6,                # Invalid upload/archive/job ID given.
                 'RegionError': 7,            # Invalid region given.
                 'FileError': 8,              # Error related to reading/writing a file.
                 'ResumeError': 9,            # Problem resuming a multipart upload.
                 'NotReady': 10,              # Requested download is not ready yet.
                 'BookkeepingError': 11,      # Bookkeeping not available.
                 'SdbCommunicationError': 12, # Problem reading/writing SimpleDB data.
                 'ResourceNotFoundException': 13, # Glacier can not find the requested resource.
                 'InvalidParameterValueException': 14,  # Parameter not accepted.
                 'DownloadError': 15,         # Downloading an archive failed.
                 'SNSConnectionError': 126,   # Can not connect to SNS
                 'SNSConfigurationError': 127,  # Problem with configuration file
                 'SNSParameterError':128,     # Problem with arguments passed to SNS
    }
                 
    def __init__(self, message, code=None, cause=None):
        """
        Constructor. Logs the error.

        :param message: the error message.
        :type message: str
        :param code: the error code.
        :type code: str
        :param cause: explanation on what caused the error.
        :type cause: str
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.exitcode = self.ERRORCODE[code] if code in self.ERRORCODE else 254
        self.code = code
        if cause:
            self.logger.error('ERROR: %s'% cause)
            self.cause = cause if isinstance(cause, tuple) else (cause,)
            self.stack = traceback.format_stack()[:-2]

        else:
            self.logger.error('An error occurred, exiting.')
            self.cause = ()

            # Just wrap up a cause-less exception.
            # Get the stack trace for this exception.
            self.stack = (
                traceback.format_stack()[:-2] +
                traceback.format_tb(sys.exc_info()[2]))
            # ^^^ let's hope the information is still there; caller must take
            #     care of this.
            
        self.message = message
        self.logger.info(self.fetch(message=True))
        self.logger.debug(self.fetch(stack=True))
        if self.exitcode == 254:
            self.logger.debug('Unknown error code: %s.'% code)

    # Works as a generator to help get the stack trace and the cause
    # written out.
    def causeTree(self, indentation='  ', alreadyMentionedTree=[], stack=False, message=False):
        """
        Returns a complete stack tree, an error message, or both.
        Returns a warning if neither stack or message are True.
        """
        if stack:
            yield "Traceback (most recent call last):\n"
            ellipsed = 0
            for i, line in enumerate(self.stack):
                if (ellipsed is not False
                    and i < len(alreadyMentionedTree)
                    and line == alreadyMentionedTree[i]):
                    ellipsed += 1
                else:
                    if ellipsed:
                        yield "  ... (%d frame%s repeated)\n" % (
                            ellipsed,
                            "" if ellipsed == 1 else "s")
                        ellipsed = False  # marker for "given out"
                        
                    yield line

        if message:
            exc = self if self.message is None else self.message
            for line in traceback.format_exception_only(exc.__class__, exc):
                yield line
                
            if self.cause:
                yield ("Caused by: %d exception%s\n" %
                    (len(self.cause), "" if len(self.cause) == 1 else "s"))
                
                for causePart in self.cause:
                    if hasattr(causePart,"causeTree"):
                        for line in causePart.causeTree(indentation, self.stack):
                            yield re.sub(r'([^\n]*\n)', indentation + r'\1', line)
                    else:
                        for line in traceback.format_exception_only(causePart.__class__, causePart):
                            yield re.sub(r'([^\n]*\n)', indentation + r'\1', line)

        if not message and not stack:
            yield ('No output. Specify message=True and/or stack=True \
to get output when calling this function.\n')

    def write(self, stream=None, indentation='  ', message=False, stack=False):
        """
        Writes the error details to sys.stderr or a stream.
        """
        
        stream = sys.stderr if stream is None else stream
        for line in self.causeTree(indentation, message=message, stack=stack):
            stream.write(line)

    def fetch(self, indentation='  ', message=False, stack=False):
        """
        Fetches the error details and returns them as string.
        """
        out = ''
        for line in self.causeTree(indentation, message=message, stack=stack):
            out += line

        return out

class InputException(GlacierException):
    """
    Exception that is raised when there is someting wrong with the
    user input.
    """
    
    VaultNameError = 1
    VaultDescriptionError = 2
    def __init__(self, message, code=None, cause=None):
        """ Handles the exception.

        :param message: the error message.
        :type message: str
        :param code: the error code.
        :type code: 
        :param cause: explanation on what caused the error.
        :type cause: str
        """
        GlacierException.__init__(self, message, code=code, cause=cause)

class ConnectionException(GlacierException):
    """
    Exception that is raised when there is something wrong with
    the connection.
    """
    
    GlacierConnectionError = 1
    SdbConnectionError = 2
    def __init__(self, message, code=None, cause=None):
        """ Handles the exception.

        :param message: the error message.
        :type message: str
        :param code: the error code.
        :type code: 
        :param cause: explanation on what caused the error.
        :type cause: str
        """
        GlacierException.__init__(self, message, code=code, cause=cause)

class CommunicationException(GlacierException):
    """
    Exception that is raised when there is something wrong in
    the communication with an external library like boto.
    """
    def __init__(self, message, code=None, cause=None):
        """ Handles the exception.

        :param message: the error message.
        :type message: str
        :param code: the error code.
        :type code: 
        :param cause: explanation on what caused the error.
        :type cause: str
        """
        GlacierException.__init__(self, message, code=code, cause=cause)

class ResponseException(GlacierException):
    """
    Exception that is raised when there is an http response error.
    """
    def __init__(self, message, code=None, cause=None):
        GlacierException.__init__(self, message, code=code, cause=cause)

if __name__ == '__main__':
    class ChildrenException(Exception):
        def __init__(self, message):
            Exception.__init__(self, message)

    class ParentException(GlacierException):
        def __init__(self, message, cause=None):
            if cause:
                GlacierException.__init__(self, message, cause=cause)
            else:
                 GlacierException.__init__(self, message)

    try:
        try:
            raise ChildrenException("parent")
        except ChildrenException, e:
            raise ParentException("children", cause=e)
    except ParentException, e:
        e.write(indentation='||  ')

########NEW FILE########
__FILENAME__ = GlacierWrapper
# -*- coding: utf-8 -*-
"""
.. module:: GlacierWrapper
   :platform: Unix, Windows
   :synopsis: Wrapper for accessing Amazon Glacier, with Amazon SimpleDB 
   support and other features.
"""

import math
import json
import pytz
import re
import logging
import os.path
import time
import sys
import traceback
import glaciercorecalls
import select
import hashlib
import fcntl
import termios
import struct

import boto
import boto.sdb
from boto import sns

from functools import wraps
from dateutil.parser import parse as dtparse
from datetime import datetime
from pprint import pformat

from glaciercorecalls import GlacierConnection, GlacierWriter

from glacierexception import *

class log_class_call(object):
    """
    Decorator that logs class calls to specific functions.

    .. note::

        Set loglevel to DEBUG to see these logs.
    """

    def __init__(self, start, finish, getter=None):
        """
        Decorator constructor.

        :param start: Message logged when starting the class.
        :type start: str.
        :param finish: Message logged when finishing the class.
        :type finish: str.
        """

        self.start = start
        self.finish = finish
        self.getter = getter

    def __call__(self, fn):
        def wrapper(*args, **kwargs):
            that = args[0]
            that.logger.debug(self.start)
            ret = fn(*args, **kwargs)
            that.logger.debug(self.finish)
            if self.getter:
                that.logger.debug(pformat(self.getter(ret)))
            else:
                that.logger.debug(pformat(ret))

            return ret

        wrapper.func_name = fn.func_name
        if hasattr(fn, '__name__'):
            wrapper.__name__ = self.name = fn.__name__

        if hasattr(fn, '__doc__'):
            wrapper.__doc__ = fn.__doc__

        if hasattr(fn, '__module__'):
            wrapper.__module__ = fn.__module__

        return wrapper


class mmap(object):
    """
    Not really a mmap, just a simple read-only substitute that
    does not require having a lot of RAM to upload large files.
    """
    def __init__(self, file):
        self.file = file
        self.size = os.fstat(self.file.fileno()).st_size

    def __getitem__(self, key):
        self.file.seek(key.start)
        if key.stop is None:
            stop = self.size
        else:
            stop = key.stop
        return self.file.read(stop - key.start)


class GlacierWrapper(object):
    """
    Wrapper for accessing Amazon Glacier, with Amazon SimpleDB support
    and other features.
    """

    VAULT_NAME_ALLOWED_CHARACTERS = "[a-zA-Z\.\-\_0-9]+"
    ID_ALLOWED_CHARACTERS = "[a-zA-Z\-\_0-9]+"
    MAX_VAULT_NAME_LENGTH = 255
    MAX_VAULT_DESCRIPTION_LENGTH = 1024
    MAX_PARTS = 10000
    AVAILABLE_REGIONS = ('us-east-1', 'us-west-2', 'us-west-1',
                         'eu-west-1', 'ap-northeast-1', 'ap-southeast-2')
    AVAILABLE_REGIONS_MESSAGE = """\
Invalid region. Available regions for Amazon Glacier are:
us-east-1 (US - Virginia)
us-west-1 (US - N. California)
us-west-2 (US - Oregon)
eu-west-1 (EU - Ireland)
ap-northeast-1 (Asia-Pacific - Tokyo)
ap-southeast-2 (Asia-Pacific - Sydney)\
"""

    def setuplogging(self, logfile, loglevel, logtostdout):
        """
        Set up the logging facility.

        * If no logging parameters are given, WARNING-level logging will be printed to stdout.
        * If logtostdout is True, messages will be sent to stdout, even if a logfile is given.
        * If a logfile is given but can not be written to, logs are sent to stderr instead.

        :param logfile: the fully qualified file name of where to log to.
        :type logfile: str
        :param loglevel: the level of logging::

           * CRITICAL
           * ERROR
           * WARNING
           * INFO
           * DEBUG

        :type loglevel: str
        :param logtostdout: whether to sent log messages to stdout.
        :type logtostdout: boolean
        """

        levels = {'3': logging.CRITICAL,
                  'CRITICAL': logging.CRITICAL,
                  '2': logging.ERROR,
                  'ERROR': logging.ERROR,
                  '1': logging.WARNING,
                  'WARNING': logging.WARNING,
                  '0': logging.INFO,
                  'INFO': logging.INFO,
                  '-1': logging.DEBUG,
                  'DEBUG': logging.DEBUG}

        loglevel = 'WARNING' if not loglevel in levels.keys() else levels[loglevel]

        datefmt = '%b %d %H:%M:%S'
        logformat = '%(asctime)s %(levelname)-8s glacier-cmd %(message)s'

        if logtostdout:
            logging.basicConfig(level=loglevel,
                                stream=sys.stdout,
                                format=logformat,
                                datefmt=datefmt)
        elif logfile:
            try:
                open(logfile, 'a')
            except IOError:

                # Can't open the specified log file, log to stderr instead.
                logging.basicConfig(level=loglevel,
                                    stream=sys.stderr,
                                    format=logformat,
                                    datefmt=datefmt)
            else:
                logging.basicConfig(level=loglevel,
                                    filename=logfile,
                                    format=logformat,
                                    datefmt=datefmt)

        else:
            logging.basicConfig(level='WARNING',
                                stream=sys.stdout,
                                format=logformat,
                                datefmt=datefmt)

    def glacier_connect(func):
        """
        Decorator which handles the connection to Amazon Glacier.

        :param func: Function to wrap
        :type func: function

        :returns: wrapper function
        :rtype: function
        :raises: :py:exc:`glacier.glacierexception.ConnectionException`
        """

        @wraps(func)
        @log_class_call("Connecting to Amazon Glacier.",
                        "Connection to Amazon Glacier successful.")
        def glacier_connect_wrap(*args, **kwargs):
            self = args[0]
            if not hasattr(self, "glacierconn") or \
                (hasattr(self, "glacierconn") and not self.glacierconn):
                try:
                    self.logger.debug("""\
                        Connecting to Amazon Glacier with
                        aws_access_key %s
                        aws_secret_key %s
                        region %s\
                        """,
                                      self.aws_access_key,
                                      self.aws_secret_key,
                                      self.region)
                    self.glacierconn = GlacierConnection(self.aws_access_key,
                                                         self.aws_secret_key,
                                                         region_name=self.region)
                except boto.exception.AWSConnectionError as e:
                    raise ConnectionException(
                        "Cannot connect to Amazon Glacier.",
                        cause=e.cause,
                        code="GlacierConnectionError")

            return func(*args, **kwargs)
        return glacier_connect_wrap

    def sdb_connect(func):
        """
        Decorator which connects to Amazon SimpleDB.

        :param func: Function to wrap
        :type func: function

        :returns: wrapper function
        :rtype: function
        :raises: :py:exc:`glacier.glacierexception.ConnectionException`
        """

        @wraps(func)
        @log_class_call("Connecting to Amazon SimpleDB.",
                        "Connection to Amazon SimpleDB successful.")
        def sdb_connect_wrap(*args, **kwargs):
            self = args[0]
            if not self.bookkeeping:
                return func(*args, **kwargs)

            # TODO: give SimpleDB its own class? Or move the few calls
            # we need to glaciercorecalls?

            if not self.bookkeeping_domain_name:
                raise InputException(
                    '''\
Bookkeeping enabled but no Amazon SimpleDB domain given.
Provide a domain in either the config file or via the
command line, or disable bookkeeping.''',
                    code="SdbConnectionError")

            if not hasattr(self, 'sdb_conn'):
                try:
                    self.logger.debug("""\
Connecting to Amazon SimpleDB domain %s with
aws_access_key %s
aws_secret_key %s\
""",
                                      self.bookkeeping_domain_name,
                                      self.aws_access_key,
                                      self.aws_secret_key)
                    self.sdb_conn = boto.sdb.connect_to_region(
                        self.sdb_region,
                        aws_access_key_id=self.sdb_access_key,
                        aws_secret_access_key=self.sdb_secret_key)
                    domain_name = self.bookkeeping_domain_name
                    self.sdb_domain = self.sdb_conn.create_domain(domain_name)
                except (boto.exception.AWSConnectionError, boto.exception.SDBResponseError) as e:
                    raise ConnectionException(
                        "Cannot connect to Amazon SimpleDB.",
                        cause=e,
                        code="SdbConnectionError")

            return func(*args, **kwargs)

        return sdb_connect_wrap

    def sns_connect(func):
        """
        Decorator which connects to Amazon SNS.

        :param func: Function to wrap
        :type func: function

        :returns: wrapper function
        :rtype: function
        :raises: GlacierWrapper.ConnectionException
        """
        @wraps(func)
        def sns_connect_wrap(*args, **kwargs):
            self = args[0]

            if not hasattr(self, "sns_conn"):
                try:
                    self.sns_conn = boto.sns.connect_to_region(
                        aws_access_key_id=self.aws_access_key,
                        aws_secret_access_key=self.aws_secret_key,
                        region_name=self.region)
                except boto.exception.AWSConnectionError as e:
                    raise ConnectionException(
                        "Cannot connect to Amazon SNS.",
                        cause=e.cause,
                        code="SNSConnectionError")
            return func(*args, **kwargs)
        return sns_connect_wrap

    @log_class_call('Checking whether vault name is valid.',
                    'Vault name is valid.')
    def _check_vault_name(self, name):
        """
        Checks whether we have a valid vault name.

        :param name: Vault name
        :type name: str

        :returns: True if valid, raises exception otherwise.
        :rtype: boolean
        :raises: :py:exc:`glacier.glacierexception.InputException`
        """

        if len(name) > self.MAX_VAULT_NAME_LENGTH:
            raise InputException(
                u"Vault name can be at most %s characters long." % self.MAX_VAULT_NAME_LENGTH,
                cause="Vault name more than %s characters long." % self.MAX_VAULT_NAME_LENGTH,
                code="VaultNameError")

        if len(name) == 0:
            raise InputException(
                u"Vault name has to be at least 1 character long.",
                cause='Vault name has to be at least 1 character long.',
                code="VaultNameError")

        # If the name starts with an illegal character, then result
        # m is None. In that case the expression becomes '0 != len(name)'
        # which of course is always True.
        m = re.match(self.VAULT_NAME_ALLOWED_CHARACTERS, name)
        if (m.end() if m else 0) != len(name):
            raise InputException(
                u"""Allowed characters are a-z, A-Z, 0-9, '_' (underscore), '-' (hyphen), and '.' (period)""",
                cause='Illegal characters in the vault name.',
                code="VaultNameError")

        return True

    @log_class_call('Checking whether vault description is valid.',
                    'Vault description is valid.')
    def _check_vault_description(self, description):
        """
        Checks whether a vault description is valid (at least one character,
        not too long, no illegal characters).

        :param description: Vault description
        :type description: str

        :returns: True if valid, raises exception otherwise.
        :rtype: boolean
        :raises: :py:exc:`glacier.glacierexception.InputException`
        """

        if len(description) > self.MAX_VAULT_DESCRIPTION_LENGTH:
            raise InputException(
                u"Description must be no more than %s characters."% self.MAX_VAULT_DESCRIPTION_LENGTH,
                cause='Vault description contains more than %s characters.'% self.MAX_VAULT_DESCRIPTION_LENGTH,
                code="VaultDescriptionError")

        for char in description:
            n = ord(char)
            if n < 32 or n > 126:
                raise InputException(
                    u"""The allowed characters are 7-bit ASCII without \
control codes, specifically ASCII values 32-126 decimal \
or 0x20-0x7E hexadecimal.""",
                    cause="Invalid characters in the vault name.",
                    code="VaultDescriptionError")

        return True

    @log_class_call('Checking whether id is valid.',
                    'Id is valid.')
    def _check_id(self, amazon_id, id_type):
        """
        Checks if an id (jobID, uploadID, archiveID) is valid.
        A jobID or uploadID is 92 characters long, an archiveID is
        138 characters long.
        Valid characters are a-z, A-Z, 0-9, '-' and '_'.

        :param amazon_id: id to be validated
        :type amazon_id: str
        :param id_type: the case-sensity type of id (JobId, UploadId, ArchiveId).
        :type id_type: str

        :returns: True if valid, raises exception otherwise.
        :rtype: boolean
        :raises: :py:exc:`glacier.glacierexception.InputException`
        """

        length = {'JobId': 92,
                  'UploadId': 92,
                  'ArchiveId': 138}
        self.logger.debug('Checking a %s.' % id_type)
        if len(amazon_id) != length[id_type]:
            raise InputException(
                'A %s must be %s characters long. This ID is %s characters.'% (id_type, length[id_type], len(amazon_id)),
                cause='Incorrect length of the %s string.'% id_type,
                code="IdError")

        m = re.match(self.ID_ALLOWED_CHARACTERS, amazon_id)
        if (m.end() if m else 0) != len(amazon_id):
            raise InputException(u"""\
This %s contains invalid characters. \
Allowed characters are a-z, A-Z, 0-9, '_' (underscore) and '-' (hyphen)\
""" % id_type,
                cause='Illegal characters in the %s string.' % id_type,
                code="IdError")

        return True

    @log_class_call('Validating region.',
                    'Region is valid.')
    def _check_region(self, region):
        """
        Checks whether the region given is valid.

        :param region: the region to be validated.
        :type region: str

        :returns: True if valid, raises exception otherwise.
        :rtype: boolean
        :raises: GlacierWrapper.InputException
        """

        if not region in self.AVAILABLE_REGIONS:
            raise InputException(
                self.AVAILABLE_REGIONS_MESSAGE,
                cause='Invalid region code: %s.' % region,
                code='RegionError')

        return True

    def _check_part_size(self, part_size, total_size):
        """
        Check the part size:

        - check whether we have a part size, if not: use default.
        - check whether part size is a power of two: if not,
            increase until it is.
        - check wehther part size is big enough for the archive
            total size: if not, increase until it is.

        Return part size to use.
        """
        def _part_size_for_total_size(total_size):
            return self._next_power_of_2(
                int(math.ceil(
                    float(total_size) / (1024 * 1024 * self.MAX_PARTS)
                )))

        if part_size < 0:
            if total_size > 0:
                part_size = _part_size_for_total_size(total_size)
            else:
                part_size = GlacierWriter.DEFAULT_PART_SIZE
        else:
            ps = self._next_power_of_2(part_size)
            if not ps == part_size:
                self.logger.warning("""\
Part size in MB must be a power of 2, \
e.g. 1, 2, 4, 8 MB; automatically increased part size from %s to %s.\
""" % (part_size, ps))

            part_size = ps

        # Check if user specified value is big enough, and adjust if needed.
        if total_size > part_size * 1024 * 1024 * self.MAX_PARTS:
            part_size = _part_size_for_total_size(total_size)
            self.logger.warning("Part size given is too small; \
using %s MB parts to upload." % part_size)

        return part_size

    def _next_power_of_2(self, v):
        """
        Returns the next power of 2, or the argument if it's
        already a power of 2.

        :param v: the value to be tested.
        :type v: int

        :returns: the next power of 2.
        :rtype: int
        """

        if v == 0:
            return 1

        v -= 1
        v |= v >> 1
        v |= v >> 2
        v |= v >> 4
        v |= v >> 8
        v |= v >> 16
        return v + 1

    def _bold(self, msg):
        """
        Uses ANSI codes to make text bold for printing on the tty.
        """

        return u'\033[1m%s\033[0m' % msg

    def _progress(self, msg):
        """
        A progress indicator. Prints the progress message if stdout
        is connected to a tty (i.e. run from the command prompt).

        :param msg: the progress message to be printed.
        :type msg: str
        """

        if sys.stdout.isatty():

            # Get the current screen width.
            cols = struct.unpack('hh',  fcntl.ioctl(sys.stdout, termios.TIOCGWINSZ, '1234'))[1]

            # Make sure the message fits on a single line, strip if not,
            # and add spaces to fill the line if it's shorter (to erase
            # old characters from longer lines)
            msg = msg[:cols] if len(msg) > cols else msg
            if len(msg) < cols:
                for i in range(cols - len(msg)):
                    msg += ' '

            sys.stdout.write(msg + '\r')
            sys.stdout.flush()

    def _size_fmt(self, num, decimals=1):
        """
        Formats byte sizes in human readable format. Anything bigger
        than TB is returned as TB.
        Number of decimals is optional, defaults to 1.

        :param num: the size in bytes.
        :type num: int
        :param decimals: the number of decimals to return.
        :type decimals: int

        :returns: the formatted number.
        :rtype: str
        """

        fmt = "%%3.%sf %%s" % decimals
        for x in ['bytes', 'KB', 'MB', 'GB']:
            if num < 1024.0:
                return fmt % (num, x)

            num /= 1024.0

        return fmt % (num, 'TB')

    def _decode_error_message(self, e):
        try:
            e = json.loads(e)['message']
        except:
            e = None

        return e

    @glacier_connect
    @log_class_call("Listing vaults.",
                    "Listing vaults complete.")
    def lsvault(self, limit=None):
        """
        Lists available vaults.

        :returns: List of vault descriptions.

            .. code-block:: python

                [{u'CreationDate': u'2012-09-20T14:29:14.710Z',
                  u'LastInventoryDate': u'2012-10-01T02:10:12.497Z',
                  u'NumberOfArchives': 15,
                  u'SizeInBytes': 33932739443L,
                  u'VaultARN': u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name',
                  u'VaultName': u'your_vault_name'},
                  ...
                ]

        :rtype: list
        :raises: :py:exc:`glacier.glacierexception.CommunicationException`,
                 :py:exc:`glacier.glacierexception.ResponseException`
        """

        marker = None
        vault_list = []
        while True:
            try:
                response = self.glacierconn.list_vaults(marker=marker)
            except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
                raise ResponseException(
                    'Failed to recieve vault list.',
                    cause=self._decode_error_message(e.body),
                    code=e.code)

            vault_list += response.copy()['VaultList']
            marker = response.copy()['Marker']
            if limit and len(vault_list) >= limit:
                vault_list = vault_list[:limit]
                break

            if not marker:
                break

        return vault_list

    @glacier_connect
    @log_class_call("Creating vault.",
                    "Vault creation completed.")
    def mkvault(self, vault_name):
        """
        Creates a new vault.

        :param vault_name: Name of vault to be created.
        :type vault_name: str

        :returns: Response data.
        :rtype: :py:class:`boto.glacier.response.GlacierResponse`
        :raises: :py:exc:`glacier.glacierexception.CommunicationException`
        """

        self._check_vault_name(vault_name)
        try:
            response = self.glacierconn.create_vault(vault_name)
        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
            raise ResponseException(
                'Failed to create vault with name %s.' % vault_name,
                cause=self._decode_error_message(e.body),
                code=e.code)

        return response.copy()

    @glacier_connect
    @sdb_connect
    @log_class_call("Removing vault.",
                    "Vault removal complete.")
    def rmvault(self, vault_name):
        """
        Removes a vault. Vault must be empty before it can be removed.

        :param vault_name: Name of vault to be removed.
        :type vault_name: str

        :returns: Response data. Raises exception on failure.

            .. code-block:: python

                [('x-amzn-requestid', 'Example_rkQ-xzxHfrI-997hphbfdcIbL74IhDf_Example'),
                 ('date', 'Mon, 01 Oct 2012 13:54:06 GMT')]

        :rtype: list
        :raises: :py:exc:`glacier.glacierexception.CommunicationException`
        """

        self._check_vault_name(vault_name)
        try:
            response = self.glacierconn.delete_vault(vault_name)
        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
            raise ResponseException(
                'Failed to remove vault with name %s.' % vault_name,
                cause=self._decode_error_message(e.body),
                code=e.code)

        # Check for orphaned entries in the bookkeeping database, and
        # remove them.
        if self.bookkeeping:
            query = "select * from `%s` where vault='%s'" % (self.bookkeeping_domain_name, vault_name)
            result = self.sdb_domain.select(query)
            try:
                for item in result:
                    self.sdb_domain.delete_item(item)
                    self.logger.debug('Deleted orphaned archive from the database: %s.' % item.name)
            except boto.exception.SDBResponseError as e:
                raise ResponseException(
                        'SimpleDB did not respond correctly to our orphaned listings check.',
                        cause=self._decode_error_message(e.body),
                        code=e.code)

        return response.copy()

    @glacier_connect
    @log_class_call("Requesting vault description.",
                    "Vault description received.")
    def describevault(self, vault_name):
        """
        Describes vault inventory and other details.

        :param vault_name: Name of vault.
        :type vault_name: str

        :returns: vault description.

            .. code-block:: python

                {u'CreationDate': u'2012-10-01T13:24:55.791Z',
                 u'LastInventoryDate': None,
                 u'NumberOfArchives': 0,
                 u'SizeInBytes': 0,
                 u'VaultARN': u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name',
                 u'VaultName': u'your_vault_name'}

        :rtype: dict
        :raises: :py:exc:`glacier.glacierexception.CommunicationException`
        """

        self._check_vault_name(vault_name)
        try:
            response = self.glacierconn.describe_vault(vault_name)
        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
            raise ResponseException(
                'Failed to get description of vault with name %s.' % vault_name,
                cause=self._decode_error_message(e.body),
                code=e.code)

        return response.copy()

    @glacier_connect
    @log_class_call("Requesting jobs list.",
                    "Active jobs list received.")
    def list_jobs(self, vault_name, completed=None,
                  status_code=None, limit=None):
        """
        Provides a list of current Glacier jobs with status and other
        job details.
        If no jobs active it returns an empty list.

        :param vault_name: Name of vault.
        :type vault_name: str

        :returns: job list

            .. code-block:: python

                [{u'Action': u'InventoryRetrieval',
                  u'ArchiveId': None,
                  u'ArchiveSizeInBytes': None,
                  u'Completed': False,
                  u'CompletionDate': None,
                  u'CreationDate': u'2012-10-01T14:54:51.919Z',
                  u'InventorySizeInBytes': None,
                  u'JobDescription': None,
                  u'JobId': u'Example_rctvAMVd3tgAbCuQkD2vjNQ6aw9ifwACvhjhIeKtNnZqeSIuMYRo3JUKsK_0M-VNYvb0-eEreSUp_Example',
                  u'SHA256TreeHash': None,
                  u'SNSTopic': None,
                  u'StatusCode': u'InProgress',
                  u'StatusMessage': None,
                  u'VaultARN': u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name'},
                  {...}]

        :rtype: list
        :raises: :py:exc:`glacier.glacierexception.ResponseException`
        """

        self._check_vault_name(vault_name)
        marker = None
        job_list = []
        while True:
            try:
                response = self.glacierconn.list_jobs(vault_name,
                                                      completed=completed,
                                                      status_code=status_code,
                                                      marker=marker)
            except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
                raise ResponseException(
                    'Failed to recieve the jobs list for vault %s.' % vault_name,
                    cause=self._decode_error_message(e.body),
                    code=e.code)
            job_list += response.copy()['JobList']
            marker = response.copy()['Marker']
            if limit and len(job_list) >= limit:
                job_list = job_list[:limit]
                break

            if not marker:
                break

        return job_list

    @glacier_connect
    @log_class_call("Requesting job description.",
                    "Job description received.")
    def describejob(self, vault_name, job_id):
        """
        Gives detailed description of a job.

        :param vault_name: Name of vault.
        :type vault_name: str
        :param job_id: id of job to be described.
        :type job_id: str

        :returns: List of job properties.

            .. code-block:: python

                {u'Action': u'InventoryRetrieval',
                 u'ArchiveId': None,
                 u'ArchiveSizeInBytes': None,
                 u'Completed': False,
                 u'CompletionDate': None,
                 u'CreationDate': u'2012-10-01T14:54:51.919Z',
                 u'InventorySizeInBytes': None,
                 u'JobDescription': None,
                 u'JobId': u'Example_d3tgAbCuQ9vPRqRJkD2vjNQ6wBgga7Xaw9ifwACvhjhIeKtNnZqeSIuMYRo3JUKsK_0M-VNYvb0-_Example',
                 u'SHA256TreeHash': None,
                 u'SNSTopic': None,
                 u'StatusCode': u'InProgress',
                 u'StatusMessage': None,
                 u'VaultARN': u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name'}

        :rtype: dict
        :raises: :py:exc:`glacier.glacierexception.CommunicationException`
        """

        self._check_vault_name(vault_name)
        self._check_id(job_id, 'JobId')
        try:
            response = self.glacierconn.describe_job(vault_name, job_id)
        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
            raise ResponseException(
                'Failed to get description of job with job id %s.' % job_id,
                cause=self._decode_error_message(e.body),
                code=e.code)

        return response.copy()

    @glacier_connect
    @log_class_call("Aborting multipart upload.",
                    "Multipart upload successfully aborted.")
    def abortmultipart(self, vault_name, upload_id):
        """
        Aborts an incomplete multipart upload, causing any uploaded data to be
        removed from Amazon Glacier.

        :param vault_name: Name of the vault.
        :type vault_name: str
        :param upload_id: the UploadId of the multipart upload to be aborted.
        :type upload_id: str

        :returns: server response.

            .. code-block:: python

                [('x-amzn-requestid', 'Example_ZJwjlLbvg8Dg_lnYUnC8bjV6cvlTBTO_Example'),
                 ('date', 'Mon, 01 Oct 2012 16:08:23 GMT')]

        :rtype: list
        :raises: :py:exc:`glacier.glacierexception.CommunicationException`
        """

        self._check_vault_name(vault_name)
        self._check_id(upload_id, "UploadId")
        try:
            response = self.glacierconn.abort_multipart_upload(vault_name, upload_id)
        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
            raise ResponseException(
                'Failed to abort multipart upload with id %s.' % upload_id,
                cause=self._decode_error_message(e.body),
                code=e.code)

        return response.copy()

    @glacier_connect
    @log_class_call("Listing multipart uploads.",
                    "Multipart uploads list received successfully.")
    def listmultiparts(self, vault_name, limit=None):
        """
        Provids a list of all currently active multipart uploads.

        :param vault_name: Name of the vault.
        :type vault_name: str

        :return: list of uploads, or None.

            .. code-block:: python

                [{u'ArchiveDescription': u'myfile.tgz',
                  u'CreationDate': u'2012-09-30T15:21:35.890Z',
                  u'MultipartUploadId': u'Example_oiuhncYLvBRZLzYgVw7MO_OO4l6i78va8N83R9xLNqrFaa8Vyz4W_JsaXhLNicCCbi_OdsHD8dHK_Example',
                  u'PartSizeInBytes': 134217728,
                  u'VaultARN': u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name'},
                  {...}]

        :rtype: list
        :raises: :py:exc:`glacier.glacierexception.CommunicationException`
        """

        self._check_vault_name(vault_name)
        marker = None
        uploads = []
        while True:
            try:
                response = self.glacierconn.list_multipart_uploads(vault_name,
                                                                   marker=marker)
            except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
                raise ResponseException(
                    'Failed to get a list of multipart uploads for vault %s.' % vault_name,
                    cause=self._decode_error_message(e.body),
                    code=e.code)

            uploads += response.copy()['UploadsList']
            marker = response.copy()['Marker']
            if limit and len(uploads) >= limit:
                uploads = uploads[:limit]
                break

            if not marker:
                break

        return uploads

    @glacier_connect
    @sdb_connect
    @log_class_call("Uploading archive.",
                    "Upload of archive finished.")
    def upload(self, vault_name, file_name, description, region,
               stdin, alternative_name, part_size, uploadid, resume):
        """
        Uploads a file to Amazon Glacier.

        :param vault_name: Name of the vault.
        :type vault_name: str
        :param file_name: Name of the file to upload.
        :type file_name: str
        :param description: Description of the upload.
        :type description: str
        :param region: region where to upload to.
        :type region: str
        :param stdin: whether to use stdin to read data from.
        :type stdin: boolan
        :param part_size: the size (in MB) of the blocks to upload.
        :type part_size: int

        :returns: Tupple of (archive_id, sha256hash)
        :rtype: tupple
        :raises: :py:exc:`glacier.glacierexception.InputException`,
                 :py:exc:`glacier.glacierexception.ResponseException`
        """

        # Switch off debug logging for boto, as otherwise it's
        # filling up the log with the data sent!
        if self.logger.getEffectiveLevel() == 10:
            logging.getLogger('boto').setLevel(logging.INFO)

        # Do some sanity checking on the user values.
        self._check_vault_name(vault_name)
        self._check_region(region)
        if not description:
            description = file_name if file_name else 'No description.'

        if description:
            self._check_vault_description(description)

        if uploadid:
            self._check_id(uploadid, 'UploadId')

        if resume and stdin:
            raise InputException(
                'You must provide the UploadId to resume upload of streams from stdin.\nUse glacier-cmd listmultiparts <vault> to find the UploadId.',
                code='CommandError')

        # If file_name is given, try to use this file(s).
        # Otherwise try to read data from stdin.
        total_size = 0
        reader = None
        mmapped_file = None
        if not stdin:
            if not file_name:
                raise InputException(
                    "No file name given for upload.",
                    code='CommandError')

            try:
                f = open(file_name, 'rb')
                mmapped_file = mmap(f)
                total_size = os.path.getsize(file_name)
            except IOError as e:
                raise InputException(
                    "Could not access file: %s."% file_name,
                    cause=e,
                    code='FileError')

        elif select.select([sys.stdin,],[],[],0.0)[0]:
            reader = sys.stdin
            total_size = 0
        else:
            raise InputException(
                "There is nothing to upload.",
                code='CommandError')

        # Log the kind of upload we're going to do.
        if uploadid:
            self.logger.info('Attempting resumption of upload of %s to %s.'% (file_name if file_name else 'data from stdin', vault_name))
        elif resume:
            self.logger.info('Attempting resumption of upload of %s to %s.'% (file_name, vault_name))
        else:
            self.logger.info('Starting upload of %s to %s.\nDescription: %s'% (file_name if file_name else 'data from stdin', vault_name, description))

        # If user did not specify part_size, compute the optimal (i.e. lowest
        # value to stay within the self.MAX_PARTS (10,000) block limit).
        part_size = self._check_part_size(part_size, total_size)
        part_size_in_bytes = part_size * 1024 * 1024

        # If we have an UploadId, check whether it is linked to a current
        # job. If so, check whether uploaded data matches the input data and
        # try to resume uploading.
        upload = None
        if uploadid:
            uploads = self.listmultiparts(vault_name)
            for upload in uploads:
                if uploadid == upload['MultipartUploadId']:
                    self.logger.debug('Found a matching upload id. Continuing upload resumption attempt.')
                    self.logger.debug(upload)
                    part_size_in_bytes = upload['PartSizeInBytes']
                    break
            else:
                raise InputException(
                    'Can not resume upload of this data as no existing job with this uploadid could be found.',
                    code='IdError')

        # Initialise the writer task.
        writer = GlacierWriter(self.glacierconn, vault_name, description=description,
                               part_size_in_bytes=part_size_in_bytes, uploadid=uploadid, logger=self.logger)

        if upload:
            marker = None
            while True:

                # Fetch a list of already uploaded parts and their SHA hashes.
                try:
                    response = self.glacierconn.list_parts(vault_name, uploadid, marker=marker)
                except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
                    raise ResponseException(
                        'Failed to get a list already uploaded parts for interrupted upload %s.'% uploadid,
                        cause=self._decode_error_message(e.body),
                        code=e.code)

                list_parts_response = response.copy()
                current_position = 0
                stop = 0
                # Process the parts list.
                # For each part of data, take the matching data range from
                # the local file, and compare hashes.
                # If recieving data over stdin, the parts must be sequential
                # and the first must start at 0. For file, we can use the seek()
                # function to handle non-sequential parts.
                for part in list_parts_response['Parts']:
                    start, stop = (int(p) for p in part['RangeInBytes'].split('-'))
                    stop += 1
                    if not start == current_position:
                        if stdin:
                            raise InputException(
                                'Cannot verify non-sequential upload data from stdin.',
                                code='ResumeError')
                        if reader:
                            reader.seek(start)

                    if mmapped_file and stop > mmapped_file.size:
                        raise InputException(
                            'File does not match uploaded data; please check your uploadid and try again.',
                            cause='File is smaller than uploaded data.',
                            code='ResumeError')

                    # Try to read the chunk of data, and take the hash if we
                    # have received anything.
                    # If no data or hash mismatch, stop checking raise an
                    # exception.
                    data = None
                    data = reader.read(stop-start) if reader else mmapped_file[start:stop]
                    if data:
                        data_hash = glaciercorecalls.tree_hash(glaciercorecalls.chunk_hashes(data))
                        if glaciercorecalls.bytes_to_hex(data_hash) == part['SHA256TreeHash']:
                            self.logger.debug('Part %s hash matches.'% part['RangeInBytes'])
                            writer.tree_hashes.append(data_hash)
                        else:
                            raise InputException(
                                'Received data does not match uploaded data; please check your uploadid and try again.',
                                cause='SHA256 hash mismatch.',
                                code='ResumeError')

                    else:
                        raise InputException(
                            'Received data does not match uploaded data; please check your uploadid and try again.',
                            cause='No or not enough data to match.',
                            code='ResumeError')

                    current_position += stop - start

                # If a marker is present, this means there are more pages
                # of parts available. If no marker, we have the last page.
                marker = list_parts_response['Marker']
                writer.uploaded_size = stop
                if not marker:
                    break

                if total_size > 0:
                    msg = 'Checked %s of %s (%s%%).' \
                          % (self._size_fmt(writer.uploaded_size),
                             self._size_fmt(total_size),
                             self._bold(str(int(100 * writer.uploaded_size/total_size))))
                else:
                    msg = 'Checked %s.' \
                          % (self._size_fmt(writer.uploaded_size))

                self._progress(msg)

            # Finished checking; log this and print the final status update
            # before resuming the upload.
            self.logger.info('Already uploaded: %s. Continuing from there.'% self._size_fmt(stop))
            if total_size > 0:
                msg = 'Checked %s of %s (%s%%). Check done; resuming upload.' \
                      % (self._size_fmt(writer.uploaded_size),
                         self._size_fmt(total_size),
                         self._bold(str(int(100 * writer.uploaded_size/total_size))))
            else:
                msg = 'Checked %s. Check done; resuming upload.' \
                      % (self._size_fmt(writer.uploaded_size))

            self._progress(msg)

        # Read file in parts so we don't fill the whole memory.
        start_time = current_time = previous_time = time.time()
        start_bytes = writer.uploaded_size
        while True:
            if reader:
                part = reader.read(part_size_in_bytes)
            else:
                if mmapped_file.size > writer.uploaded_size+part_size_in_bytes:
                    part = mmapped_file[writer.uploaded_size:writer.uploaded_size+part_size_in_bytes]
                else:
                    part = mmapped_file[writer.uploaded_size:]

            if not part:
                break

            writer.write(part)
            current_time = time.time()
            overall_rate = int((writer.uploaded_size-start_bytes)/(current_time - start_time))
            if total_size > 0:

                # Calculate transfer rates in bytes per second.
                current_rate = int(part_size_in_bytes/(current_time - previous_time))

                # Estimate finish time, based on overall transfer rate.
                if overall_rate > 0:
                    time_left = (total_size - writer.uploaded_size)/overall_rate
                    eta_seconds = current_time + time_left
                    if datetime.fromtimestamp(eta_seconds).day is not\
                            datetime.now().day:
                        eta_template = "%a, %d %b, %H:%M:%S"
                    else:
                        eta_template = "%H:%M:%S"
                    eta = time.strftime(eta_template,
                                        time.localtime(eta_seconds))
                else:
                    time_left = "Unknown"
                    eta = "Unknown"

                msg = 'Wrote %s of %s (%s%%). Rate %s/s, average %s/s, ETA %s.' \
                      % (self._size_fmt(writer.uploaded_size),
                         self._size_fmt(total_size),
                         self._bold(str(int(100 * writer.uploaded_size/total_size))),
                         self._size_fmt(current_rate, 2),
                         self._size_fmt(overall_rate, 2),
                         eta)

            else:
                msg = 'Wrote %s. Rate %s/s.' \
                      % (self._size_fmt(writer.uploaded_size),
                         self._size_fmt(overall_rate, 2))

            self._progress(msg)
            previous_time = current_time
            self.logger.debug(msg)

        writer.close()
        if not stdin:
            f.close()
        current_time = time.time()
        overall_rate = int(writer.uploaded_size/(current_time - start_time))
        msg = 'Wrote %s. Rate %s/s.\n' % (self._size_fmt(writer.uploaded_size),
                                            self._size_fmt(overall_rate, 2))
        self._progress(msg)
        self.logger.info(msg)

        archive_id = writer.get_archive_id()
        sha256hash = writer.get_hash()
        location = writer.get_location()

        if self.bookkeeping:
            self.logger.info('Writing upload information into the bookkeeping database.')

            # Use the alternative name as given by --name <name> if we have it.
            file_name = alternative_name if alternative_name else file_name

            # If still no name this is an stdin job, so set name accordingly.
            file_name = file_name if file_name else 'Data from stdin.'
            file_attrs = {
                'region': region,
                'vault': vault_name,
                'filename': file_name,
                'archive_id': archive_id,
                'location': location,
                'description': description,
                'date':'%s' % datetime.utcnow().replace(tzinfo=pytz.utc),
                'hash': sha256hash,
                'size': writer.uploaded_size
            }

##            if file_name:
##                file_attrs['filename'] = file_name
##            elif stdin:
##                file_attrs['filename'] = 'data from stdin'

            self.sdb_domain.put_attributes(file_attrs['filename'], file_attrs)

        return (archive_id, sha256hash)


    @glacier_connect
    @log_class_call("Processing archive retrieval job.",
                    "Archive retrieval job response received.")
    def getarchive(self, vault_name, archive_id):
        """
        Requests Amazon Glacier to make archive available for download.

        If retrieval job is not yet initiated:

        - initiate a job,
        - return tuple ("initiated", job, None)

        If retrieval job is already initiated:

        - return tuple ("running", job, None).

        If the file is ready for download:

        - return tuple ("ready", job, jobId).

        :param vault: Vault name from where we want to retrieve the archive.
        :type vault: str
        :param archive: ArchiveID of archive to be retrieved.
        :type archive: str

        :returns: Tuple of (status, job, JobId)

        TODO: Return example

        :rtype: (str, dict, str)
        :raises: :py:exc:`glacier.glacierexception.ResponseException`
        """

        results = None
        self._check_vault_name(vault_name)
        self._check_id(archive_id, 'ArchiveId')

        # Check whether we have a retrieval job for the archive.
        job_list = self.list_jobs(vault_name)
        for job in job_list:
            if job['ArchiveId'] == archive_id:
                if job['Completed']:
                    return ('ready', job, job['JobId'])

                return ('running', job, None)

        # No job found related to this archive, start a new job.
        job_data = {'ArchiveId': archive_id,
                    'Type': 'archive-retrieval'}
        try:
            response = self.glacierconn.initiate_job(vault_name, job_data)
        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
            raise ResponseException(
                'Failed to initiate an archive retrieval job for archive %s in vault %s.'% (archive_id, vault_name),
                cause=self._decode_error_message(e.body),
                code=e.code)

        job = response.copy()
        return ('initiated', job, None)

    @glacier_connect
    @sdb_connect
    @log_class_call("Download an archive.",
                    "Download archive done.")
    def download(self, vault_name, archive_id, part_size,
                 out_file_name=None, overwrite=False):
        """
        Download a file from Glacier, and store it in out_file.
        If no out_file is given, the file will be dumped on stdout.
        """

        # Sanity checking on the input.
        self._check_vault_name(vault_name)
        self._check_id(archive_id, 'ArchiveId')

        # Check whether the requested file is available from Amazon Glacier.
        job_list = self.list_jobs(vault_name)
        job_id = None
        for job in job_list:
            if job['ArchiveId'] == archive_id:
                download_job = job
                if not job['Completed']:
                    raise CommunicationException(
                        "Archive retrieval request not completed yet. Please try again later.",
                        code='NotReady')
                self.logger.debug('Archive retrieval completed; archive is available for download now.')
                break

        else:
            raise InputException(
                "Requested archive not available. Please make sure \
your archive ID is correct, and start a retrieval job using \
'getarchive' if necessary.",
                code='IdError')

        # Check whether we can access the file the archive has to be written to.
        out_file = None
        if out_file_name:
            if os.path.isfile(out_file_name) and not overwrite:
                raise InputException(
                    "File exists already, aborting. Use the overwrite flag to overwrite existing file.",
                    code="FileError")
            try:
                out_file = open(out_file_name, 'w')
            except IOError as e:
                raise InputException(
                    "Cannot access the ouput file.",
                    cause=e,
                    code='FileError')

        # Sanity checking done; start downloading the file, part by part.
        total_size = download_job['ArchiveSizeInBytes']
        part_size_in_bytes = self._check_part_size(part_size, total_size) * 1024 * 1024
        start_bytes = downloaded_size = 0
        hash_list = []
        start_time = current_time = previous_time = time.time()

        # Log our pending action.
        if out_file:
            self.logger.debug('Starting download of archive to file %s.'% out_file_name)
        else:
            self.logger.debug('Starting download of archive to stdout.')

        # Download the data, one part at a time.
        while downloaded_size < total_size:

            # Read a part of data.
            from_bytes = downloaded_size
            to_bytes = min(downloaded_size + part_size_in_bytes, total_size)
            try:
                response = self.glacierconn.get_job_output(vault_name,
                                                            download_job['JobId'],
                                                            byte_range=(from_bytes, to_bytes-1))
                data = response.read()
            except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
                raise ResponseException(
                    'Failed to download archive %s.'% archive_id,
                    cause=self._decode_error_message(e.body),
                    code=e.code)

            hash_list.append(glaciercorecalls.chunk_hashes(data)[0])
            downloaded_size = to_bytes
            if out_file:
                try:
                    out_file.write(response.read())
                except IOError as e:
                    raise InputException(
                        "Cannot write data to the specified file.",
                        cause=e,
                        code='FileError')
            else:
                sys.stdout.write(response.read())
                sys.stdout.flush()

            # Calculate progress statistics.
            current_time = time.time()
            overall_rate = int((downloaded_size-start_bytes)/(current_time - start_time))
            current_rate = int(part_size_in_bytes/(current_time - previous_time))

            # Estimate finish time, based on overall transfer rate.
            time_left = (total_size - downloaded_size)/overall_rate
            eta_seconds = current_time + time_left
            if datetime.fromtimestamp(eta_seconds).day is not\
                    datetime.now().day:
                eta_template = "%a, %d %b, %H:%M:%S"
            else:
                eta_template = "%H:%M:%S"

            eta = time.strftime(eta_template, time.localtime(eta_seconds))
            msg = 'Read %s of %s (%s%%). Rate %s/s, average %s/s, ETA %s.' \
                  % (self._size_fmt(downloaded_size),
                     self._size_fmt(total_size),
                     self._bold(str(int(100 * downloaded_size/total_size))),
                     self._size_fmt(current_rate, 2),
                     self._size_fmt(overall_rate, 2),
                     eta)
            self._progress(msg)
            previous_time = current_time
            self.logger.debug(msg)

        if out_file:
            out_file.close()
        if glaciercorecalls.bytes_to_hex(glaciercorecalls.tree_hash(hash_list)) != download_job['SHA256TreeHash']:
            raise CommunicationException(
                "Downloaded data hash mismatch",
                code="DownloadError",
                cause=None)

        self.logger.debug('Download of archive finished successfully.')
        current_time = time.time()
        overall_rate = int(downloaded_size/(current_time - start_time))
        msg = 'Wrote %s. Rate %s/s.\n' % (self._size_fmt(downloaded_size),
                                            self._size_fmt(overall_rate, 2))
        self._progress(msg)
        self.logger.info(msg)

    @glacier_connect
    @sdb_connect
    @log_class_call("Searching for archive.",
                    "Search done.")
    def search(self, vault=None, region=None, file_name=None, search_term=None):
        """
        Searches for archives using SimpleDB

        :param vault: Vault name where you want to search.
        :type vault: str
        :param region: Region where you want to search.
        :type region: str
        :param file_name: Name of the file
        :type file_name: str
        :param search_term: Additional search term to use
        :type search_term: str

        TODO: Search examples

        :returns: List of archives that match

        TODO: Return example

        :rtype: list
        """

        # Sanity checking.
        if not self.bookkeeping:
            raise InputException(
                "You must enable bookkeeping to be able to do searches.",
                cause='Bookkeeping not enabled.',
                code='BookkeepingError')

        if vault:
            self._check_vault_name(vault)

        if region:
            self._check_region(region)

##        if file_name and ('"' in file_name or "'" in file_name):
##            raise InputException(
##                'Quotes like \' and \" are not allowed in search terms.',
##                cause='Invalid search term %s: contains quotes.'% file_name)
##
##
##        if search_term and ('"' in search_term or "'" in search_term):
##            raise InputException(
##                'Quotes like \' and \" are not allowed in search terms.',
##                cause='Invalid search term %s: contains quotes.'% search_term)

        self.logger.debug('Search terms: vault %s, region %s, file name %s, search term %s'%
                          (vault, region, file_name, search_term))
        search_params = []
        if region:
            search_params += ["region='%s'" % (region,)]

        if vault:
            search_params += ["vault='%s'" % (vault,)]

        if file_name:
            search_params += ["filename like '%"+file_name.replace("'", "''")+"%'"]

        if search_term:
            search_params += ["description like '%"+search_term.replace("'", "''")+"%'"]

        if search_params:
            search_params = " and ".join(search_params)
            query = 'select * from `%s` where %s' % (self.bookkeeping_domain_name, search_params)
        else:
            query = 'select * from `%s`' % self.bookkeeping_domain_name

        self.logger.debug('Query: "%s"'% query)
        result = self.sdb_domain.select(query)
        items = []

        # Get the results; filter out incomplete uploads (those without
        # an archive_id attribute).
        try:
            for item in result:
                self.logger.debug('Next search result:\n%s'% item)
                if item.has_key('archive_id'):
                    items.append(item)
        except boto.exception.SDBResponseError as e:
            raise ResponseException(
                    'SimpleDB did not like your query with parameters %s.'% search_params,
                    cause=self._decode_error_message(e.body),
                    code=e.code)

        return items

    @glacier_connect
    @sdb_connect
    @log_class_call("Deleting archive.", "Archive deleted.")
    def rmarchive(self, vault_name, archive_id):
        """
        Remove an archive from an Amazon Glacier vault.

        :param vault: the vault name.
        :type vault: str
        :param archive: the archive ID
        :type archive: str

        :raises: :py:exc:`glacier.glacierexception.CommunicationException`,
                 :py:exc:`glacier.glacierexception.ResponseException`
        """

        self._check_vault_name(vault_name)
        self._check_id(archive_id, 'ArchiveId')
        try:
            self.glacierconn.delete_archive(vault_name, archive_id)
        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
            raise ResponseException(
                'Failed to remove archive %s from vault %s.'% (archive_id, vault_name),
                cause=self._decode_error_message(e.body),
                code=e.code)

        # Remove the listing from the bookkeeping database.
        if self.bookkeeping:
            try:
                item = self.sdb_domain.get_item(archive_id)
                if item:
                    self.sdb_domain.delete_item(item)
            except boto.exception.SDBResponseError as e:
                raise CommunicationException(
                    "Cannot delete item from Amazon SimpleDB.",
                    code="SdbWriteError",
                    cause=e)

    @glacier_connect
    @sdb_connect
    @log_class_call("Requesting inventory overview.",
                    "Inventory response received.")
    def inventory(self, vault_name, refresh):
        """
        Retrieves inventory and returns retrieval job, or if it's already retrieved
        returns overview of the inventoy. If force=True it will force start a new
        inventory taking job.

        :param vault_name: Vault name
        :type vault_name: str
        :param refresh: Force new inventory retrieval.
        :type refresh: boolean

        :returns: Tuple of retrieval job and inventory data (as list) if available.

            .. code-block:: python

                ({u'CompletionDate': None,
                  u'VaultARN':
                  u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name',
                  u'SNSTopic': None,
                  u'SHA256TreeHash': None,
                  u'Completed': False,
                  u'InventorySizeInBytes': None,
                  u'JobId': u'Example_d3tgAbCuQ9vPRqRJkD2vjNQ6wBgga7Xaw9ifwACvhjhIeKtNnZqeSIuMYRo3JUKsK_0M-VNYvb0-_Example',
                  u'ArchiveId': None,
                  u'JobDescription': None,
                  u'StatusCode': u'InProgress',
                  u'Action': u'InventoryRetrieval',
                  u'CreationDate': u'2012-10-01T14:54:51.919Z',
                  u'StatusMessage': None,
                  u'ArchiveSizeInBytes': None},
                  None
                )
        :rtype: (list, list)

        :raises: :py:exc:`glacier.glacierexception.CommunicationException`,
                 :py:exc:`glacier.glacierexception.ResponseException`
        """

        self._check_vault_name(vault_name)
        inventory = None
        inventory_job = None
        if not refresh:

            # List active jobs and check whether any inventory retrieval
            # has been completed, and whether any is in progress. We want
            # to find the latest finished job, or that failing the latest
            # in progress job.
            job_list = self.list_jobs(vault_name)
            inventory_done = False
            for job in job_list:
                if job['Action'] == "InventoryRetrieval":

                    # As soon as a finished inventory job is found, we're done.
                    if job['Completed']:
                        self.logger.debug('Found finished inventory job %s.'% job)
                        d = dtparse(job['CompletionDate']).replace(tzinfo=pytz.utc)
                        job['inventory_date'] = d
                        inventory_done = True
                        inventory_job = job
                        break

                    self.logger.debug('Found running inventory job %s.'% job)
                    inventory_job = job

            # If inventory retrieval is complete, process it.
            if inventory_done:
                self.logger.debug('Fetching results of finished inventory retrieval.')
                response = self.glacierconn.get_job_output(vault_name, inventory_job['JobId'])
                inventory = response.copy()
                archives = []

                # If bookkeeping is enabled, update cache.
                # Add all inventory information to the database, then check
                # for any archives listed in the database for that vault and
                # remove those.
                if self.bookkeeping and len(inventory['ArchiveList']) > 0:
                    self.logger.debug('Updating the bookkeeping with the latest inventory.')
                    items = {}

                    # Add items to the inventory, 25 at a time (maximum batch).
                    for item in inventory['ArchiveList']:
                        items[item['ArchiveId']] = {
                            'vault': vault_name,
                            'archive_id': item['ArchiveId'],
                            'description': item['ArchiveDescription'],
                            'date':'%s' % dtparse(item['CreationDate']).replace(tzinfo=pytz.utc),
                            'hash': item['SHA256TreeHash'],
                            'size': item['Size'],
                            'region': self.region
                            }
                        archives.append(item['ArchiveId'])
                        if len(items) == 25:
                            self.logger.debug('Writing batch of 25 inventory items to the bookkeeping db.')
                            try:
                                self.sdb_domain.batch_put_attributes(items)
                            except boto.exception.SDBResponseError as e:
                                raise CommunicationException(
                                    "Cannot update inventory cache, Amazon SimpleDB is not happy.",
                                    cause=e,
                                    code="SdbWriteError")
                            items = {}

                    # Add the remaining batch of items, if any, to the
                    # database.
                    if items:
                        self.logger.debug('Writing final batch of %s inventory items to the bookkeeping db.'% len(items))
                        try:
                            self.sdb_domain.batch_put_attributes(items)
                        except boto.exception.SDBResponseError as e:
                            raise CommunicationException(
                                "Cannot update inventory cache, Amazon SimpleDB is not happy.",
                                cause=e,
                                code="SdbWriteError")

                    # Get the inventory from the database for this vault,
                    # and delete any orphaned items.
                    query = "select * from `%s` where vault='%s'" % (self.bookkeeping_domain_name, vault_name)
                    result = self.sdb_domain.select(query)
                    try:
                        for item in result:
                            if not item.name in archives:
                                self.sdb_domain.delete_item(item)
                                self.logger.debug('Deleted orphaned archive from the database: %s.'% item.name)

                    except boto.exception.SDBResponseError as e:
                        raise ResponseException(
                                'SimpleDB did not respond correctly to our inventory check.',
                                cause=self._decode_error_message(e.body),
                                code=e.code)

        # If refresh == True or no current inventory jobs either finished or
        # in progress, we have to start a new job. Then request the job details
        # through describejob to return.
        if refresh or not inventory_job:
            self.logger.debug('No inventory jobs finished or running; starting a new job.')
            job_data = {'Type': 'inventory-retrieval'}
            try:
                new_job = self.glacierconn.initiate_job(vault_name, job_data)
            except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:
                raise ResponseException(
                    'Failed to create a new inventory retrieval job for vault %s.'% vault_name,
                    cause=self._decode_error_message(e.body),
                    code=e.code)

            inventory_job = self.describejob(vault_name, new_job['JobId'])

        return (inventory_job, inventory)

    def get_tree_hash(self, file_name):
        """
        Calculate the tree hash of a file.

        :param file_name: the file name to calculate a hash of.
        :type file_name: str

        :returns: the tree hash of the file.
        :rtype: str
        """

        try:
            reader = open(file_name, 'rb')
        except IOError as e:
            raise InputException(
                "Could not access the file given: %s." % file_name,
                cause=e,
                code='FileError')

        hashes = [hashlib.sha256(part).digest() for part in iter((lambda:reader.read(1024 * 1024)), '')]
        return glaciercorecalls.bytes_to_hex(glaciercorecalls.tree_hash(hashes))

    def _init_events_for_vault(self, vault_name, topic):
        config = {
            'SNSTopic': topic,
            'Events': ['ArchiveRetrievalCompleted',
                       'InventoryRetrievalCompleted']
        }

        return self.glacierconn.set_vault_notifications(
            vault_name=vault_name,
            notification_config=config)

    def _init_events_for_vaults(self, vaults, topic):
        results = []
        for vault_name in vaults:
            try:
                import collections
                result = collections.OrderedDict()
            except AttributeError:
                result = dict()

            result["Vault Name"] = vault_name
            result["Request Id"] = \
                self._init_events_for_vault(vault_name, topic)[u'RequestId']
            results += [result]
        return results

    @glacier_connect
    @sns_connect
    def sns_sync(self, sns_options, output):
        options = sns_options

        if not options['topics_present']:
            topic = self.sns_conn.create_topic(options['topic'])['CreateTopicResponse']['CreateTopicResult']['TopicArn']
            if options.get('vaults', None):
                vaults = options['vaults'].split(",")  # monitored vaults
            else:
                vaults = [fvault[u'VaultARN'].split("vaults/")[-1] for fvault in self.lsvault()]  # fvault - full vault information
            return self._init_events_for_vaults(vaults, topic)
        else:
            results = []
            for topic in options["topics"]:
                vaults = []
                if topic.get("options", None):
                    if topic.get("options").get('vaults', None):
                        vaults = topic.get("options").get('vaults').split(",")
                        vaults = vaults[:-1] if vaults[-1] == "" else vaults

                methods = []
                if topic.get('options', None):
                    if topic.get('options').get('method', None):
                        methods = topic.get('options').get('method').split(";")
                        methods = methods[:-1] if methods[-1] == "" else methods

                if not vaults:
                    vaults = [fvault[u'VaultARN'].split("vaults/")[-1] for fvault in self.lsvault()]

                topic_arn = self.sns_conn.create_topic(topic['topic'])['CreateTopicResponse']['CreateTopicResult']['TopicArn']

                topic_results = self._init_events_for_vaults(vaults, topic_arn)

                if methods:
                    for method in methods:
                        try:
                            protocol, endpoint = method.split(',')
                        except ValueError:
                            raise InputException(
                                ("If you specify method, you should use format "
                                 "'protocol1,endpoint1;protocol2,endpoint2'."),
                                cause='Wrong method format in configuration file.',
                                code='SNSConfigurationError')
                        results_subscribe = self.sns_subscribe(protocol, endpoint, topic=topic_arn.split(":")[-1], sns_options=sns_options)

                for i, vault in enumerate(topic_results):
                    try:
                        import collections
                        result = collections.OrderedDict()
                    except AttributeError:
                        result = dict()

                    if output == "print":
                        if not i:
                            result['Topic'] = topic['topic']
                            if methods:
                                result["Subscribe Result"] = results_subscribe[0]['SubscribeResult']
                            else:
                                result["Subscribe Result"] = ""
                        else:
                            result['Topic'] = ""
                            result["Subscribe Result"] = ""
                    else:
                        result['Topic'] = topic['topic']
                        if methods:
                            result["Subscribe Result"] = results_subscribe[0]['SubscribeResult']
                        else:
                            result["Subscribe Result"] = ""

                    result["Vault Name"] = vault['Vault Name']
                    result["Request Id"] = vault['Request Id']
                    results += [result]

            return results


    @glacier_connect
    @sns_connect
    def sns_subscribe(self, protocol, endpoint, topic, sns_options, vault_names=None):
        all_topics = self.sns_conn.get_all_topics()['ListTopicsResponse']['ListTopicsResult']['Topics']

        topic_arn = self.sns_conn.create_topic(topic)['CreateTopicResponse']['CreateTopicResult']['TopicArn']


        if vault_names:
            vaults = vault_names.split(",")
            self._init_events_for_vaults(vaults, topic_arn)
        
        topic_arns = [topic_arn]

        if len(topic_arns):
            try:
                results = []
                for arn in topic_arns:
                    result = self.sns_conn.subscribe(arn, protocol, endpoint)['SubscribeResponse']
                    results += [{'SubscribeResult': result['SubscribeResult']['SubscriptionArn'],
                                'RequestId': result['ResponseMetadata']['RequestId']}]
                return results
            except boto.exception.BotoServerError as e:
                raise ResponseException("Failed to subscribe to notifications for vault %s." % vault_name,
                    cause=self._decode_error_message(e.body),
                    code=e.code)
        else:
            raise Exception("No vaults matching that name found.")

    @glacier_connect
    @sns_connect
    def sns_list_topics(self, sns_options):
        topics = self.sns_conn.get_all_topics()['ListTopicsResponse']['ListTopicsResult']['Topics']
        
        results = []
        for topic in topics:
            results += [{"Topic":topic['TopicArn'].split(":")[-1], "Topic ARN":topic['TopicArn']}]
        return results

    @glacier_connect
    @sns_connect
    def sns_list_subscriptions(self, protocol, endpoint, topic, sns_options):
        subscriptions = self.sns_conn.get_all_subscriptions()['ListSubscriptionsResponse']['ListSubscriptionsResult']['Subscriptions']

        results = []
        for sub in subscriptions:
            try:
                import collections
                result = collections.OrderedDict()
            except AttributeError:
                result = dict()

            subtopic = sub['TopicArn'].split(":")[-1]
            subprotocol = sub['Protocol']
            subendpoint = sub['Endpoint']
            subarn = sub['SubscriptionArn']

            if topic and subtopic != topic:
                continue
            if protocol and subprotocol != protocol:
                continue
            if endpoint and subendpoint != endpoint:
                continue

            result['Account #'] = sub['Owner']
            result['Section'] = subtopic
            result['Protocol'] = subprotocol
            result['Endpoint'] = subendpoint
            result['ARN'] = subarn
            results += [result]
        return results

    @glacier_connect
    @sns_connect
    def sns_unsubscribe(self, protocol, endpoint, topic, sns_options):
        if not protocol or not endpoint or not topic:
            raise InputException(
                ("You must specify at least one parameter that will "
                 "by which subscriptions will be canceled."),
                cause='No parameters given for unsubscribe.',
                code='SNSParameterError')

        matching_subs = self.sns_list_subscriptions(protocol,
                                                    endpoint,
                                                    topic,
                                                    sns_options=sns_options)

        unsubscribed = []
        for res in matching_subs:
            if res['ARN'] != u'PendingConfirmation':
                self.sns_conn.unsubscribe(res['ARN'])
                unsubscribed += [res]

        return unsubscribed

    def __init__(self, aws_access_key, aws_secret_key, region,
                 bookkeeping=False, no_bookkeeping=None, bookkeeping_domain_name=None,
                 sdb_access_key=None, sdb_secret_key=None, sdb_region=None,
                 logfile=None, loglevel='WARNING', logtostdout=True):
        """
        Constructor, sets up important variables and so for GlacierWrapper.

        :param aws_access_key: your AWS access key.
        :type aws_access_key: str
        :param aws_secret_key: your AWS secret key.
        :type aws_secret_key: str
        :param region: name of your default region, see :ref:`regions`.
        :type region: str
        :param bookkeeping: whether to enable bookkeeping, see :reg:`bookkeeping`.
        :type bookkeeping: boolean
        :param bookkeeping_domain_name: your Amazon SimpleDB domain name where the bookkeeping information will be stored.
        :type bookkeeping_domain_name: str
        :param sdb_access_key: your SimpleDB access key.
        :type sdb_access_key: str
        :param sdb_secret_key: your SimpleDB secret key.
        :type sdb_secret_key: str
        :param sdb_region: name of your sdb region, see :ref:`regions`.
        :type sdb_region: str
        :param logfile: complete file name of where to log messages.
        :type logfile: str
        :param loglevel: the desired loglevel, see :py:func:`setuplogging`
        :type loglevel: str
        :param logtostdout: whether to log messages to stdout instead of to file.
        :type logtostdout: boolean
        """

        self.aws_access_key = aws_access_key
        self.aws_secret_key = aws_secret_key
        self.bookkeeping = bookkeeping

        if no_bookkeeping:
            self.bookkeeping = False

        self.bookkeeping_domain_name = bookkeeping_domain_name

        self.region = region

        self.sdb_access_key = sdb_access_key if sdb_access_key else aws_access_key
        self.sdb_secret_key = sdb_secret_key if sdb_secret_key else aws_secret_key
        self.sdb_region = sdb_region if sdb_region else region

        self.setuplogging(logfile, loglevel, logtostdout)
        self.logger = logging.getLogger(self.__class__.__name__)

        self._check_region(region)

        self.logger.debug("""\
Creating GlacierWrapper instance with
    aws_access_key=%s,
    aws_secret_key=%s,
    bookkeeping=%s,
    nobookkeeping=%s,
    bookkeeping_domain_name=%s,
    region=%s,
    sdb_access_key=%s,
    sdb_secret_key=%s,
    sdb_region=%s,
    logfile %s,
    loglevel %s,
    logging to stdout %s.""",
                          aws_access_key, aws_secret_key, bookkeeping,
                          no_bookkeeping,
                          bookkeeping_domain_name, region,
                          sdb_access_key, sdb_secret_key, sdb_region,
                          logfile, loglevel, logtostdout)

########NEW FILE########
__FILENAME__ = sdb
import unittest


########NEW FILE########
__FILENAME__ = sns
import unittest

import ConfigParser
import os
import sys

sys.path.append("/".join(sys.path[0].split("/")[:-1]))

from GlacierWrapper import GlacierWrapper

from boto.glacier.exceptions import UnexpectedHTTPResponseError

import localsettings

class TestGlacierSNS(unittest.TestCase):
    def setUp(self):
        config = ConfigParser.SafeConfigParser()
        config.read(['/etc/glacier-cmd.conf',
                    os.path.expanduser('~/.glacier-cmd')])

        secs = config.sections()
        for sec in secs:
            if sec != "aws":
                config.remove_section(sec)

        prepand_options = lambda section: [(section + "_" + k, v)
                                           for k, v in config.items(section)]
        self.args = dict(prepand_options("aws"))
        self.args.update({"region": "us-east-1"})

    def tearDown(self):
        for vault in self.gw.lsvault():
            if \
                vault[u'VaultARN'].split("vaults/")[-1]\
                    .startswith("test_vvault"):
                self.gw.rmvault(vault[u'VaultARN'].split("vaults/")[-1])

        topics = self.gw.sns_conn.get_all_topics()\
['ListTopicsResponse']\
['ListTopicsResult']\
['Topics']

        for topic in topics:
            if topic['TopicArn'].split(":")[-1].startswith("test_topic"):
                self.gw.sns_conn.delete_topic(topic['TopicArn'])


class TestGlacierSNSAuto(TestGlacierSNS):
    def test_sync_auto_basic(self):
        """
        No configuration
        """
        sns_options = {'topic': 'aws-glacier-notifications',
                       'topics_present': False}

        self.gw = GlacierWrapper(**self.args)

        vault_name = "test_vvault0"

        # Lets create one vault for our testing purposes
        self.gw.mkvault(vault_name)

        # Only after a call to one of gw functions was executed
        # is glacierconn available
        gc = vars(self.gw)['glacierconn']

        # No vault notifications set for fresh vault
        with self.assertRaises(UnexpectedHTTPResponseError) as cm:
            gc.get_vault_notifications(vault_name)
        self.assertEqual(cm.exception.status, 404)
        self.assertEqual(cm.exception.message,
                         ("Expected 200, got (404, "
                         "code=ResourceNotFoundException, "
                         "message=No notification configuration "
                         "is set for vault: %s)") % (vault_name,))

        # Set all vaults
        response = self.gw.sns_sync(sns_options=sns_options, output="csv")
        successful_vaults = [r["Vault Name"] for r in response]
        self.assertIn(vault_name, successful_vaults)

        # Check out vault has set notifications
        vaults = [vault[u'VaultARN'].split("vaults/")[-1]
                  for vault in self.gw.lsvault()]
        for vault in vaults:
            response = gc.get_vault_notifications(vault)
            events = response['Events']
            self.assertIn(u"ArchiveRetrievalCompleted", events)
            self.assertIn(u"InventoryRetrievalCompleted", events)

        # Remove test vault
        self.gw.rmvault(vault_name)


class TestGlacierSNSMultiConfig(TestGlacierSNS):
    def test_withOUT_method(self):
        """
        Configuration

        [SNS:test_topic_1]

        [SNS:test_topic_2]
        vaults=test_vvault0,test_vvault2

        {'topics': [
        """
        vaults = ['test_vvault0', 'test_vvault1', 'test_vvault2']
        vaults_used = [vaults[0], vaults[2]]

        sns_options = {'topics': [
            {'topic': 'test_topic_1', 'options':{}},
            {'topic': 'test_topic_2', 'options':
                {'vaults': ','.join(vaults_used)}}
        ],
            'topics_present': True}

        self.gw = GlacierWrapper(**self.args)

        for vault in vaults:
            self.gw.mkvault(vault)

        response = self.gw.sns_sync(sns_options=sns_options, output="csv")

        for obj in response:
            del obj['Request Id']

        # Testing topic 1 - no vaults passed in,
        # should be subscribed to all vaults (our testing vaults and some more)
        for vault in vaults:
            self.assertIn(
                dict([('Topic', 'test_topic_1'),
                    ('Subscribe Result', u''),
                    ('Vault Name', vault)]),
                response)

        # Testing topic 2
        # should be subscribed only to test_vvault0, test_vvault2
        for vault in vaults_used:
            self.assertIn(
                dict([('Topic', 'test_topic_2'),
                    ('Subscribe Result', u''),
                    ('Vault Name', vault)]),
                response)

        for vault in vaults:
            self.gw.rmvault(vault)

    def test_with_method(self):
        """
        Configuration

        [SNS:test_topic_1]
        method=email,email.1@example.com;

        [SNS:test_topic_2]
        vaults=test_vvault0,test_vvault2
        method=email,email.1@example.com;email,email.2@example.com
        """
        vaults = ['test_vvault0', 'test_vvault1', 'test_vvault2']
        vaults_used = [vaults[0], vaults[2]]

        sns_options = {'topics': [
            {'topic': 'test_topic_1', 'options':
                {'method': '%s,%s;' % (
                    localsettings.protocol_1,
                    localsettings.endpoint_1
                )}},
            {'topic': 'test_topic_2', 'options':
                {'vaults': 'test_vvault0,test_vvault2',
                 'method': ('%s,%s;'
                            '%s,%s') % (
                                localsettings.protocol_1,
                                localsettings.endpoint_1,
                                localsettings.protocol_2,
                                localsettings.endpoint_2)}}
        ],
            'topics_present': True}

        self.gw = GlacierWrapper(**self.args)

        for vault in vaults:
            self.gw.mkvault(vault)

        response = self.gw.sns_sync(sns_options=sns_options, output="csv")

        for obj in response:
            del obj['Request Id']

        # Testing topic 1 - no vaults passed in,
        # should be subscribed to all vaults (our testing vaults and some more)
        for vault in vaults:
            self.assertIn(
                dict([('Topic', 'test_topic_1'),
                    ('Subscribe Result', u'pending confirmation'),
                    ('Vault Name', vault)]),
                response)

        # Testing topic 2
        # should be subscribed only to test_vvault0, test_vvault2
        for vault in vaults_used:
            self.assertIn(
                dict([('Topic', 'test_topic_2'),
                    ('Subscribe Result', u'pending confirmation'),
                    ('Vault Name', vault)]),
                response)

        for vault in vaults:
            self.gw.rmvault(vault)


class TestGlacierSNSManualSubscribe(TestGlacierSNS):
    def test_subscribe_to_existing_topic(self):
        """
        $ glacier-cmd subscribe email endpoint_1 test_topic_1
        """
        self.gw = GlacierWrapper(**self.args)

        topic = 'test_topic_existing_kind_off'

        # sns_subscribe actually creates a topic to "get it"
        response = self.gw.sns_subscribe(protocol="email",
                                         endpoint=localsettings.endpoint_1,
                                         topic=topic,
                                         sns_options={})

        for res in response:
            del res["RequestId"]

        self.assertIn(
            {'SubscribeResult': u'pending confirmation'},
            response)

        all_topics = self.gw.sns_conn.get_all_topics()\
['ListTopicsResponse']\
['ListTopicsResult']\
['Topics']
        topics = [t['TopicArn'].split(":")[-1] for t in all_topics]
        self.assertIn(topic, topics)

    def test_subscribe_create_topic_for_vaults(self):
        """
        $ glacier-cmd
            subscribe email endpoint_1 test_topic --vault test_vvault0,test_vvault
        """
        self.gw = GlacierWrapper(**self.args)

        vaults = ['test_vvault0', 'test_vvault1', 'test_vvault2']
        vaults_used = [vaults[0], vaults[2]]

        topic = 'test_topic_new_for_vaults'

        for vault in vaults:
            self.gw.mkvault(vault)

        response = self.gw.sns_subscribe(protocol="email",
                                         endpoint=localsettings.endpoint_1,
                                         topic=topic,
                                         vault_names=",".join(vaults_used),
                                         sns_options={})
        for res in response:
            del res["RequestId"]

        self.assertIn(
            {'SubscribeResult': u'pending confirmation'},
            response)

        # Lets check that the topic was created
        all_topics = self.gw.sns_conn.get_all_topics()\
['ListTopicsResponse']\
['ListTopicsResult']\
['Topics']

        topics = [t['TopicArn'].split(":")[-1] for t in all_topics]
        self.assertIn(topic, topics)

        for vault in vaults_used:
            notifications = self.gw.glacierconn.get_vault_notifications(vault)
            self.assertIn("ArchiveRetrievalCompleted",
                          notifications['Events'])
            self.assertIn("InventoryRetrievalCompleted",
                          notifications['Events'])
            self.assertEqual(topic,
                             notifications['SNSTopic'].split(":")[-1])

if __name__ == '__main__':
    # Use python -m unittest tests_sns.<test case> to run individual test cases
    # e. g. python -m unittest tests_sns.TestGlacierSNSManualSubscribe
    unittest.main()

########NEW FILE########
