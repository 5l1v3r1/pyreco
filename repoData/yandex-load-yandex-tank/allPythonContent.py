__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Yandex.Tank documentation build configuration file, created by
# sphinx-quickstart on Thu Oct 11 17:02:24 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo', 'sphinx.ext.coverage', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Yandex.Tank'
copyright = u'2012, Yandex'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.1'
# The full version, including alpha/beta/rc tags.
release = '1.1.1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'YandexTankdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'YandexTank.tex', u'Yandex.Tank Documentation',
   u'Yandex', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'yandextank', u'Yandex.Tank Documentation',
     [u'Yandex'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'YandexTank', u'Yandex.Tank Documentation',
   u'Yandex', 'YandexTank', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# -- Options for Epub output ---------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'Yandex.Tank'
epub_author = u'Yandex'
epub_publisher = u'Yandex'
epub_copyright = u'2012, Yandex'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# A tuple containing the cover image and cover page html template filenames.
#epub_cover = ()

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True

########NEW FILE########
__FILENAME__ = ConsoleWorker
""" Provides classes to run TankCore from console environment """
import datetime
import fnmatch
import logging
import os
import sys
import time
import traceback
import signal
from optparse import OptionParser

from Tank.Plugins.ConsoleOnline import RealConsoleMarkup
from tankcore import TankCore


class SingleLevelFilter(logging.Filter):
    """Exclude or approve one msg type at a time.    """

    def __init__(self, passlevel, reject):
        logging.Filter.__init__(self)
        self.passlevel = passlevel
        self.reject = reject

    def filter(self, record):
        if self.reject:
            return record.levelno != self.passlevel
        else:
            return record.levelno == self.passlevel


def signal_handler(sig, frame):
    """ required for non-tty python runs to interrupt """
    raise KeyboardInterrupt()


signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)


class ConsoleTank:
    """    Worker class that runs tank core accepting cmdline params    """

    IGNORE_LOCKS = "ignore_locks"

    def __init__(self, options, ammofile):
        self.core = TankCore()

        self.options = options
        self.ammofile = ammofile

        self.baseconfigs_location = '/etc/yandex-tank'

        self.log_filename = self.options.log
        self.core.add_artifact_file(self.log_filename)
        self.log = logging.getLogger(__name__)

        self.signal_count = 0
        self.scheduled_start = None


    def set_baseconfigs_dir(self, directory):
        """        Set directory where to read configs set        """
        self.baseconfigs_location = directory


    def init_logging(self):
        """        Set up logging, as it is very important for console tool        """
        logger = logging.getLogger()
        logger.setLevel(logging.DEBUG)

        # create file handler which logs even debug messages
        if self.log_filename:
            file_handler = logging.FileHandler(self.log_filename)
            file_handler.setLevel(logging.DEBUG)
            file_handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(name)s %(message)s"))
            logger.addHandler(file_handler)

        # create console handler with a higher log level
        console_handler = logging.StreamHandler(sys.stdout)
        stderr_hdl = logging.StreamHandler(sys.stderr)

        fmt_verbose = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s %(message)s")
        fmt_regular = logging.Formatter("%(asctime)s %(levelname)s: %(message)s", "%H:%M:%S")

        if self.options.verbose:
            console_handler.setLevel(logging.DEBUG)
            console_handler.setFormatter(fmt_verbose)
            stderr_hdl.setFormatter(fmt_verbose)
        elif self.options.quiet:
            console_handler.setLevel(logging.WARNING)
            console_handler.setFormatter(fmt_regular)
            stderr_hdl.setFormatter(fmt_regular)
        else:
            console_handler.setLevel(logging.INFO)
            console_handler.setFormatter(fmt_regular)
            stderr_hdl.setFormatter(fmt_regular)

        f_err = SingleLevelFilter(logging.ERROR, True)
        f_warn = SingleLevelFilter(logging.WARNING, True)
        f_crit = SingleLevelFilter(logging.CRITICAL, True)
        console_handler.addFilter(f_err)
        console_handler.addFilter(f_warn)
        console_handler.addFilter(f_crit)
        logger.addHandler(console_handler)

        f_info = SingleLevelFilter(logging.INFO, True)
        f_debug = SingleLevelFilter(logging.DEBUG, True)
        stderr_hdl.addFilter(f_info)
        stderr_hdl.addFilter(f_debug)
        logger.addHandler(stderr_hdl)


    def __override_config_from_cmdline(self):
        """ override config options from command line"""
        if self.options.option:
            self.core.apply_shorthand_options(self.options.option)


    def get_default_configs(self):
        """ returns default configs list, from /etc and home dir """
        configs = []
        try:
            conf_files = os.listdir(self.baseconfigs_location)
            conf_files.sort()
            for filename in conf_files:
                if fnmatch.fnmatch(filename, '*.ini'):
                    configs += [os.path.realpath(self.baseconfigs_location + os.sep + filename)]
        except OSError:
            self.log.warn(self.baseconfigs_location + ' is not acessible to get configs list')

        configs += [os.path.expanduser('~/.yandex-tank')]
        return configs


    def configure(self):
        """         Make all console-specific preparations before running Tank        """
        if self.options.ignore_lock:
            self.log.warn("Lock files ignored. This is highly unrecommended practice!")

        if self.options.lock_dir:
            self.core.set_option(self.core.SECTION, "lock_dir", self.options.lock_dir)

        while True:
            try:
                self.core.get_lock(self.options.ignore_lock)
                break
            except Exception, exc:
                if self.options.lock_fail:
                    raise RuntimeError("Lock file present, cannot continue")
                self.log.debug("Failed to get lock: %s", traceback.format_exc(exc))
                self.log.info("Waiting 5s for retry...")
                time.sleep(5)

        try:
            configs = []

            if not self.options.no_rc:
                configs = self.get_default_configs()

            if not self.options.config:
                if os.path.exists(os.path.realpath('load.ini')):
                    self.log.info("No config passed via cmdline, using ./load.ini")
                    configs += [os.path.realpath('load.ini')]
                    self.core.add_artifact_file(os.path.realpath('load.ini'), True)
                elif os.path.exists(os.path.realpath('load.conf')):
                    # just for old 'lunapark' compatibility
                    self.log.warn("Using 'load.conf' is unrecommended, please use 'load.ini' instead")
                    conf_file = os.path.realpath('load.conf')
                    configs += [conf_file]
                    self.core.add_artifact_file(conf_file, True)
            else:
                for config_file in self.options.config:
                    configs.append(config_file)

            self.core.load_configs(configs)

            if self.ammofile:
                self.log.debug("Ammofile: %s", self.ammofile)
                self.core.set_option("phantom", 'ammofile', self.ammofile[0])

            self.__override_config_from_cmdline()

            self.core.load_plugins()

            if self.options.scheduled_start:
                try:
                    self.scheduled_start = datetime.datetime.strptime(self.options.scheduled_start, '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    self.scheduled_start = datetime.datetime.strptime(
                        datetime.datetime.now().strftime('%Y-%m-%d ') + self.options.scheduled_start,
                        '%Y-%m-%d %H:%M:%S')

            if self.options.ignore_lock:
                self.core.set_option(self.core.SECTION, self.IGNORE_LOCKS, "1")

        except Exception, ex:
            self.log.info("Exception: %s", traceback.format_exc(ex))
            sys.stderr.write(RealConsoleMarkup.RED)
            self.log.error("%s", ex)
            sys.stderr.write(RealConsoleMarkup.RESET)
            sys.stderr.write(RealConsoleMarkup.TOTAL_RESET)
            self.core.release_lock()
            raise ex

    def __graceful_shutdown(self):
        """ call shutdown routines """
        retcode = 1
        self.log.info("Trying to shutdown gracefully...")
        retcode = self.core.plugins_end_test(retcode)
        retcode = self.core.plugins_post_process(retcode)
        self.log.info("Done graceful shutdown")
        return retcode


    def perform_test(self):
        """
        Run the test sequence via Tank Core
        """
        self.log.info("Performing test")
        retcode = 1
        try:
            self.core.plugins_configure()
            self.core.plugins_prepare_test()
            if self.scheduled_start:
                self.log.info("Waiting scheduled time: %s...", self.scheduled_start)
                while datetime.datetime.now() < self.scheduled_start:
                    self.log.debug("Not yet: %s < %s", datetime.datetime.now(), self.scheduled_start)
                    time.sleep(1)
                self.log.info("Time has come: %s", datetime.datetime.now())

            if self.options.manual_start:
                raw_input("Press Enter key to start test:")

            self.core.plugins_start_test()
            retcode = self.core.wait_for_finish()
            retcode = self.core.plugins_end_test(retcode)
            retcode = self.core.plugins_post_process(retcode)

        except KeyboardInterrupt as ex:
            sys.stdout.write(RealConsoleMarkup.YELLOW)
            self.log.info("Do not press Ctrl+C again, the test will be broken otherwise")
            sys.stdout.write(RealConsoleMarkup.RESET)
            sys.stdout.write(RealConsoleMarkup.TOTAL_RESET)
            self.signal_count += 1
            self.log.debug("Caught KeyboardInterrupt: %s", traceback.format_exc(ex))
            try:
                retcode = self.__graceful_shutdown()
            except KeyboardInterrupt as ex:
                self.log.debug("Caught KeyboardInterrupt again: %s", traceback.format_exc(ex))
                self.log.info("User insists on exiting, aborting graceful shutdown...")
                retcode = 1

        except Exception as ex:
            self.log.info("Exception: %s", traceback.format_exc(ex))
            sys.stderr.write(RealConsoleMarkup.RED)
            self.log.error("%s", ex)
            sys.stderr.write(RealConsoleMarkup.RESET)
            sys.stderr.write(RealConsoleMarkup.TOTAL_RESET)
            retcode = self.__graceful_shutdown()
            self.core.release_lock()

        self.log.info("Done performing test with code %s", retcode)
        return retcode


class DevNullOpts:
    def __init__(self):
        pass

    log = "/dev/null"


class CompletionHelperOptionParser(OptionParser):
    def __init__(self):
        OptionParser.__init__(self, add_help_option=False)
        self.add_option('--bash-switches-list', action='store_true', dest="list_switches", help="Options list")
        self.add_option('--bash-options-prev', action='store', dest="list_options_prev", help="Options list")
        self.add_option('--bash-options-cur', action='store', dest="list_options_cur", help="Options list")

    def error(self, msg):
        pass

    def exit(self, status=0, msg=None):
        pass

    def handle_request(self, parser):
        options = self.parse_args()[0]
        if options.list_switches:
            opts = []
            for option in parser.option_list:
                if not "--bash" in option.get_opt_string():
                    opts.append(option.get_opt_string())
            print ' '.join(opts)
            exit(0)

        if options.list_options_cur or options.list_options_prev:
            cmdtank = ConsoleTank(DevNullOpts(), None)
            cmdtank.core.load_configs(cmdtank.get_default_configs())
            cmdtank.core.load_plugins()

            opts = []
            for option in cmdtank.core.get_available_options():
                opts.append(cmdtank.core.SECTION + '.' + option + '=')

            plugin_keys = cmdtank.core.config.get_options(cmdtank.core.SECTION, cmdtank.core.PLUGIN_PREFIX)
            for (plugin_name, plugin_path) in plugin_keys:
                opts.append(cmdtank.core.SECTION + '.' + cmdtank.core.PLUGIN_PREFIX + plugin_name + '=')

            for plugin in cmdtank.core.plugins.values():
                for option in plugin.get_available_options():
                    opts.append(plugin.SECTION + '.' + option + '=')
            print ' '.join(sorted(opts))
            exit(0)


########NEW FILE########
__FILENAME__ = agent
#! /usr/bin/env python2
""" The agent bundle, contains all metric classes and agent running code """
from optparse import OptionParser
import base64
import logging
import os
import glob
import re
import socket
import subprocess
import sys
import time
from threading import Thread
import traceback
import signal

import ConfigParser
import commands


def signal_handler(sig, frame):
    """ required for non-tty python runs to interrupt """
    raise KeyboardInterrupt()


signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)


class AbstractMetric:
    """ Parent class for all metrics """

    def __init__(self):
        pass

    def columns(self):
        """ methods should return list of columns provided by metric class """
        raise NotImplementedError()

    def check(self):
        """ methods should return list of values provided by metric class """
        raise NotImplementedError()


class CpuLa(AbstractMetric):
    def columns(self, ):
        return ['System_la1', 'System_la5', 'System_la15']

    def check(self, ):
        loadavg_str = open('/proc/loadavg', 'r').readline().strip()
        return map(str, loadavg_str.split()[:3])


class CpuStat(AbstractMetric):
    """ read /proc/stat and calculate amount of time
        the CPU has spent performing different kinds of work.
    """

    def __init__(self):
        AbstractMetric.__init__(self)
        self.check_prev = None
        self.check_last = None
        self.current = None
        self.last = None

    def columns(self, ):
        columns = ['System_csw', 'System_int',
                   'CPU_user', 'CPU_nice', 'CPU_system', 'CPU_idle', 'CPU_iowait',
                   'CPU_irq', 'CPU_softirq', 'System_numproc', 'System_numthreads']
        return columns

    def check(self, ):
        # Empty symbol for no data
        empty = ''

        # resulting data array
        result = []

        # Context switches and interrups. Check.
        try:
            # TODO: change to simple file reading
            output = subprocess.Popen('cat /proc/stat | grep -E "^(ctxt|intr|cpu) "',
                                      shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except Exception, exc:
            logging.error("Problems running popen", traceback.format_exc(exc))
            result.append([empty] * 9)
        else:
            err = output.stderr.read()
            if err:
                result.extend([empty] * 9)
            else:
                info = output.stdout.read()

                # CPU. Fetch data
                cpus = info.split("\n")[0].split()[1:8]
                fetch_cpu = lambda: map(float, cpus)

                # Context switches and interrupts. Fetch data
                data = []
                for line in info.split("\n")[1:3]:
                    if line:
                        data.append(line.split()[1])
                fetch_data = lambda: map(float, data)

                # Context switches and interrups. Analyze.
                if self.last:
                    self.current = fetch_data()
                    delta = []
                    cnt = 0
                    for _ in self.current:
                        delta.append(self.current[cnt] - self.last[cnt])
                        cnt += 1
                    self.last = self.current
                    result.extend(map(str, delta))
                else:
                    self.last = fetch_data()
                    result.extend([empty] * 2)
                #                logger.debug("Result: %s" % result)

                # CPU. analyze.
                #                logger.debug("CPU start.")
                if self.check_prev is not None:
                    self.check_last = fetch_cpu()
                    delta = []
                    cnt = 0
                    sum_val = 0
                    for _ in self.check_last:
                        column_delta = self.check_last[cnt] - self.check_prev[cnt]
                        sum_val += column_delta
                        delta.append(column_delta)
                        cnt += 1

                    cnt = 0
                    for _ in self.check_last:
                        result.append(str((delta[cnt] / sum_val) * 100))
                        cnt += 1
                    self.check_prev = self.check_last
                else:
                    self.check_prev = fetch_cpu()
                    result.extend([empty] * 7)
                    #                logger.debug("Result: %s" % result)

        # Numproc, numthreads
        # TODO: change to simple file reading
        command = ['ps ax | wc -l', "cat /proc/loadavg | cut -d' ' -f 4 | cut -d'/' -f2"]
        for cmd2 in command:
            try:
                output = subprocess.Popen(cmd2, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            except Exception, exc:
                logging.error("Problems running popen", traceback.format_exc(exc))
                result.append(empty)
            else:
                err = output.stderr.read()
                if err:
                    result.append(empty)
                else:
                    result.append(str(int(output.stdout.read().strip()) - 1))
        return result


def is_number(s):
    try:
        float(s)
        return True
    except ValueError:
        return False


class Custom(AbstractMetric):
    """ custom metrics: call and tail """

    def __init__(self, call, tail):
        AbstractMetric.__init__(self)
        self.call = call
        self.tail = tail
        self.diff_values = {}

    def columns(self, ):
        cols = []
        for el in self.tail:
            cols.append("Custom:" + el)
        for el in self.call:
            cols.append("Custom:" + el)
        return cols

    def check(self, ):
        res = []
        for el in self.tail:
            cmnd = base64.b64decode(el.split(':')[1])
            output = subprocess.Popen(['tail', '-n', '1', cmnd], stdout=subprocess.PIPE).communicate()[0]
            res.append(self.diff_value(el, output.strip()))
        for el in self.call:
            cmnd = base64.b64decode(el.split(':')[1])
            output = subprocess.Popen(cmnd, shell=True, stdout=subprocess.PIPE).stdout.read()
            res.append(self.diff_value(el, output.strip()))
        return res

    def diff_value(self, config_line, value):
        if not is_number(value):
            #logging.warning("Non-numeric result string, defaulting value to 0: %s", value)
            value = 0
        params = config_line.split(':')
        if len(params) > 2 and int(params[2]):
            if config_line in self.diff_values:
                diffvalue = float(value) - self.diff_values[config_line]
            else:
                diffvalue = 0
            self.diff_values[config_line] = float(value)
            value = diffvalue
        return str(value)


class Disk(AbstractMetric):
    def __init__(self):
        AbstractMetric.__init__(self)
        self.read = 0
        self.write = 0
        self.devs = self._get_devs()

    def columns(self, ):
        return ['Disk_read', 'Disk_write']

    def check(self, ):
        # cluster size
        size = 512

        # Current data
        read, writed = 0, 0

        try:
            with open("/proc/diskstats") as mfd:
                stat = mfd.readlines()

            for el in stat:
                data = el.split()
                if data[2] in self.devs:
                    read += int(data[5])
                    writed += int(data[9])

            if self.read or self.write:
                result = [str(size * (read - self.read)), str(size * (writed - self.write))]
            else:
                result = ['', '']

            self.read, self.write = read, writed

        except Exception, exc:
            logging.error("%s: %s", exc, traceback.format_exc(exc))
            result = ['', '']
        return result

    @staticmethod
    def _get_devs():
        with open("/proc/mounts") as mfd:
            mounts = mfd.readlines()
        logging.info("Mounts: %s", mounts)
        devs = []
        for mount in mounts:
            if mount.startswith("/dev"):
                parts = mount.split(" ")
                rp = os.path.realpath(parts[0])
                short_name = rp.split(os.sep)[-1]
                if not os.path.exists(rp):
                    logging.info("File not exists for %s , will search in block devices", rp)
                    for dirc in glob.glob("/sys/devices/virtual/block/*"):
                        logging.debug("Checking %s", dirc)
                        name_path = "%s/dm/name" % dirc
                        if os.path.exists(name_path):
                            logging.debug("Checking %s", dirc)
                            try:
                                with open(name_path) as fds:
                                    nam = fds.read().strip()
                                    logging.info("Test: %s/%s", nam, short_name)
                                    if nam == short_name:
                                        dsk_name = dirc.split(os.sep)[-1]
                                        logging.info("Found: %s", dsk_name)
                                        devs.append(dsk_name)
                                        break
                            except Exception, exc:
                                logging.info("Failed: %s", traceback.format_exc(exc))
                else:
                    devs.append(short_name)
        logging.info("Devs: %s", devs)
        return devs


class Mem(AbstractMetric):
    """
    Memory statistics
    """
    empty = ''

    def __init__(self):
        AbstractMetric.__init__(self)
        self.name = 'advanced memory usage'
        self.nick = ('used', 'buff', 'cach', 'free', 'dirty')
        self.vars = ('MemUsed', 'Buffers', 'Cached', 'MemFree', 'Dirty', 'MemTotal')

    #        self.open('/proc/meminfo')

    def columns(self):
        columns = ['Memory_total', 'Memory_used', 'Memory_free', 'Memory_shared', 'Memory_buff', 'Memory_cached']
        logging.info("Start. Columns: %s" % columns)
        return columns

    def check(self):
        result = []
        try:
            #TODO: change to simple file reading
            output = subprocess.Popen('cat /proc/meminfo', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except Exception, e:
            logging.error("%s: %s" % (e.__class__, str(e)))
            result.append([self.empty] * 9)
        else:
            data = {}
            err = output.stderr.read()
            if err:
                result.extend([self.empty] * 9)
                logging.error(err.rstrip())
            else:
                info = output.stdout.read()

                for name in self.vars:
                    data[name] = 0

                for l in info.splitlines():
                    if len(l) < 2:
                        continue
                    [name, raw_value] = l.split(':')
                    #                    print "name: %s " % name
                    if name in self.vars:
                        #                       print name, raw_value
                        data.update({name: long(raw_value.split()[0]) / 1024.0})
                        #                print data
                data['MemUsed'] = data['MemTotal'] - data['MemFree'] - data['Buffers'] - data['Cached']
            result = [data['MemTotal'], data['MemUsed'], data['MemFree'], 0, data['Buffers'], data['Cached']]
            return map(str, result)


class NetRetrans(AbstractMetric):
    """ read netstat output and
    calculate tcp segment retransmition derivative """

    def __init__(self):
        AbstractMetric.__init__(self)
        self.retr_second = None
        self.retr_first = None
        self.fetch = None
        self.delta = []

    def columns(self, ):
        return ['Net_retransmit', ]

    def check(self, ):
        self.fetch = lambda: int(commands.getoutput('netstat -s | grep "segments retransmited" | awk \'{print $1}\''))
        if self.retr_second is not None:
            self.retr_first = self.fetch()
            self.delta = []
            self.delta.append(str(self.retr_first - self.retr_second))
            self.retr_second = self.retr_first
            return self.delta
        else:
            # first check
            self.retr_second = self.fetch()
            return ['0', ]


class NetTcp(AbstractMetric):
    """ Read ss util output and count TCP socket's number grouped by state """

    def __init__(self):
        AbstractMetric.__init__(self)
        self.fields = ['Net_closewait', 'Net_estab', 'Net_timewait', ]
        self.keys = ['closed', 'estab', 'timewait', ]

    def columns(self, ):
        return self.fields

    def check(self, ):
        """
        * check is there TCP connections in "field" state in last check
        if note set it to 0.
        * make output ordered as "fields" list
        """
        fetch = lambda: commands.getoutput("ss -s | sed -ne 's/).*//;/^TCP/s/^[^(]*(//p'")
        data = {}
        result = []
        raw_lines = fetch().split(',')
        for line in raw_lines:
            value = line.split()
            data[value[0].strip()] = int(value[1].strip())
        for field in self.keys:
            if field in data:
                result.append(str(data[field]))
            else:
                result.append('0')
        return result


class NetTxRx(AbstractMetric):
    """ Get upped iface names and read they Tx/Rx counters in bytes """

    def __init__(self):
        AbstractMetric.__init__(self)
        self.prev_rx = 0
        self.prev_tx = 0

    def columns(self, ):
        return ['Net_tx', 'Net_rx', ]

    def check(self, ):
        """
        get network interface name which have ip addr
        which resolved fron  host FQDN.
        If we have network bonding or need to collect multiple iface
        statistic beter to change that behavior.
        """
        data = commands.getoutput("/sbin/ifconfig -s | awk '{rx+=$8; tx+=$4} END {print rx, tx}'")
        logging.debug("TXRX output: %s", data)
        (rx, tx) = data.split(" ")
        rx = int(rx)
        tx = int(tx)

        if self.prev_rx == 0:
            t_tx = 0
            t_rx = 0
        else:
            t_rx = rx - self.prev_rx
            t_tx = tx - self.prev_tx
        self.prev_rx = rx
        self.prev_tx = tx

        return [str(t_rx), str(t_tx)]


class Net(AbstractMetric):
    def __init__(self):
        AbstractMetric.__init__(self)
        self.recv = 0
        self.send = 0
        self.rgx = re.compile('\S+\s(\d+)\s(\d+)')

    def columns(self, ):
        return ['Net_recv', 'Net_send']

    def check(self, ):
        # Current data
        recv, send = 0, 0

        # TODO: change to simple file reading
        cmnd = "cat /proc/net/dev | tail -n +3 | cut -d':' -f 1,2 --output-delimiter=' ' | awk '{print $1, $2, $10}'"
        logging.debug("Starting: %s", cmnd)
        try:
            stat = subprocess.Popen([cmnd], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        except Exception, exc:
            logging.error("Error getting net metrics: %s", exc)
            result = ['', '']

        else:
            err = stat.stderr.read()
            if err:
                logging.error("Error output: %s", err)
                result = ['', '']
            else:
                for elm in stat.stdout:
                    match = self.rgx.match(elm)
                    if match:
                        recv += int(match.group(1))
                        send += int(match.group(2))
                        logging.debug("Recv/send: %s/%s", recv, send)
                    else:
                        logging.debug("Not matched: %s", elm)
                if self.recv:
                    result = [str(recv - self.recv), str(send - self.send)]
                else:
                    result = ['', '']

        self.recv, self.send = recv, send
        logging.debug("Result: %s", result)
        return result


# ===========================



class AgentWorker(Thread):
    dlmtr = ';'

    def __init__(self):
        Thread.__init__(self)
        self.last_run_ts = None
        self.startup_processes = []
        self.c_interval = 1
        self.tails = None
        self.calls = None
        self.metrics_collected = []
        self.startups = []
        self.shutdowns = []
        self.c_host = None
        self.c_local_start = None
        self.c_start = None
        self.daemon = True  # Thread auto-shutdown
        self.finished = False
        # metrics we know about
        self.known_metrics = {
            'cpu-la': CpuLa(),
            'cpu-stat': CpuStat(),
            'mem': Mem(),
            'net-retrans': NetRetrans(),
            'net-tx-rx': NetTxRx(),
            'net-tcp': NetTcp(),
            'disk': Disk(),
            'net': Net(),
        }

    @staticmethod
    def popen(cmnd):
        return subprocess.Popen(cmnd, bufsize=0, preexec_fn=os.setsid, close_fds=True, shell=True,
                                stdout=subprocess.PIPE, stderr=subprocess.PIPE)


    def run(self):
        logging.info("Running startup commands")
        for cmnd in self.startups:
            logging.debug("Run: %s", cmnd)
            proc = self.popen(cmnd)
            self.startup_processes.append(proc)

        logging.info("Start polling thread")
        header = []

        sync_time = str(self.c_start + (int(time.time()) - self.c_local_start))
        header.extend(['start', self.c_host, sync_time])  # start compile init header

        # add metrics from config file to header
        for metric_name in self.metrics_collected:
            if metric_name:
                header.extend(self.known_metrics[metric_name].columns())

        # add custom metrics from config file to header
        custom = Custom(self.calls, self.tails)
        header.extend(custom.columns())

        sys.stdout.write(self.dlmtr.join(header) + '\n')
        sys.stdout.flush()

        logging.debug(self.dlmtr.join(header))

        # check loop
        while not self.finished:
            logging.debug('Start check')
            line = []
            sync_time = str(self.c_start + (int(time.time()) - self.c_local_start))
            line.extend([self.c_host, sync_time])

            # known metrics
            for metric_name in self.metrics_collected:
                try:
                    data = self.known_metrics[metric_name].check()
                    if len(data) != len(self.known_metrics[metric_name].columns()):
                        raise RuntimeError("Data len not matched columns count: %s" % data)
                except Exception, e:
                    logging.error('Can\'t fetch %s: %s', metric_name, e)
                    data = [''] * len(self.known_metrics[metric_name].columns())
                line.extend(data)

            logging.debug("line: %s" % line)
            # custom commands
            line.extend(custom.check())

            # print result line
            try:
                row = self.dlmtr.join(line)
                logging.debug("str: %s" % row)
                sys.stdout.write(row + '\n')
                sys.stdout.flush()
            except Exception, e:
                logging.error('Failed to convert line %s: %s', line, e)

            self.fixed_sleep(self.c_interval)

        logging.info("Terminating startup commands")
        for proc in self.startup_processes:
            logging.debug("Terminate: %s", proc)
            os.killpg(proc.pid, signal.SIGTERM)

        logging.info("Running shutdown commands")
        for cmnd in self.shutdowns:
            logging.debug("Run: %s", cmnd)
            subprocess.call(cmnd, shell=True)

        logging.info("Worker thread finished")

    def fixed_sleep(self, slp_interval):
        """ sleep 'interval' exclude processing time part """
        delay = slp_interval
        if self.last_run_ts is not None:
            delta = time.time() - self.last_run_ts
            delay = slp_interval - delta
            logging.debug("Sleep for: %s (delta %s)", delay, delta)

        time.sleep(delay if delay > 0 else 0)
        self.last_run_ts = time.time()


class AgentConfig:
    def __init__(self, def_cfg_path):
        self.c_interval = 1
        self.c_host = socket.getfqdn()
        logging.info("Start agent at host: %s\n" % self.c_host)
        self.c_start = None
        self.c_local_start = int(time.time())
        self.metrics_collected = []
        self.calls = []
        self.tails = []
        self.startups = []
        self.shutdowns = []

        options = self.parse_options(def_cfg_path)
        self.parse_config(options.cfg_file)

    def parse_options(self, def_cfg_path):
        # parse options
        parser = OptionParser()
        parser.add_option('-c', '--config', dest='cfg_file', type='str',
                          help='Config file path, default is: ./' + def_cfg_path,
                          default=def_cfg_path)

        parser.add_option('-t', '--timestamp', dest='timestamp', type='int',
                          help='Caller timestamp for synchronization', default=self.c_local_start)
        (options, args) = parser.parse_args()

        self.c_start = options.timestamp
        logging.debug("Caller timestamp: %s", options.timestamp)

        return options

    def parse_config(self, cfg_file='agent.cfg'):
        # parse cfg file
        config = ConfigParser.ConfigParser()
        config.readfp(open(cfg_file))

        # metric section
        if config.has_option('metric', 'names'):
            self.metrics_collected = config.get('metric', 'names').split(',')

        # main section
        if config.has_section('main'):
            if config.has_option('main', 'interval'):
                self.c_interval = config.getfloat('main', 'interval')
            if config.has_option('main', 'host'):
                self.c_host = config.get('main', 'host')
            if config.has_option('main', 'start'):
                self.c_start = config.getint('main', 'start')

        logging.info('Agent params: %s, %s' % (self.c_interval, self.c_host))

        # custom section
        if config.has_section('custom'):
            if config.has_option('custom', 'tail'):
                self.tails += config.get('custom', 'tail').split(',')
            if config.has_option('custom', 'call'):
                self.calls += config.get('custom', 'call').split(',')

        if config.has_section('startup'):
            for option in config.options('startup'):
                if option.startswith('cmd'):
                    self.startups.append(config.get('startup', option))

        if config.has_section('shutdown'):
            for option in config.options('shutdown'):
                if option.startswith('cmd'):
                    self.shutdowns.append(config.get('shutdown', option))


    def prepare_worker(self, wrk):
        # populate
        wrk.c_start = self.c_start
        wrk.c_local_start = self.c_local_start
        wrk.c_interval = self.c_interval
        wrk.c_host = self.c_host
        wrk.metrics_collected = self.metrics_collected
        wrk.calls = self.calls
        wrk.tails = self.tails
        wrk.startups = self.startups
        wrk.shutdowns = self.shutdowns


if __name__ == '__main__':
    fname = os.path.dirname(__file__) + "_agent.log"
    level = logging.DEBUG

    fmt = "%(asctime)s - %(filename)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(filename=fname, level=level, format=fmt)

    worker = AgentWorker()
    worker.setDaemon(False)

    agent_config = AgentConfig('agent.cfg')
    agent_config.prepare_worker(worker)

    worker.start()

    try:
        logging.debug("Check for stdin shutdown command")
        cmd = sys.stdin.readline()
        if cmd:
            logging.info("Stdin cmd received: %s", cmd)
            worker.finished = True
    except KeyboardInterrupt:
        logging.debug("Interrupted")
        worker.finished = True

########NEW FILE########
__FILENAME__ = collector
"""Target monitoring via SSH"""
import ConfigParser
from collections import defaultdict
from lxml import etree
from subprocess import PIPE, Popen
import base64
import logging
import os.path
import re
import select
import signal
import sys
import tempfile
import time
import fcntl
import traceback

import tankcore


class Config(object):
    """     Config reader helper    """

    def __init__(self, config):
        self.tree = etree.parse(config)

    def loglevel(self):
        """Get log level from config file. Possible values: info, debug"""

        log_level = 'info'
        log_level_raw = self.tree.xpath('/Monitoring')[0].get('loglevel')
        if log_level_raw in ('info', 'debug'):
            log_level = log_level_raw
        return log_level


class SSHWrapper:
    """     separate SSH calls to be able to unit test the collector    """

    def __init__(self, timeout):
        self.log = logging.getLogger(__name__)
        self.ssh_opts = ['-q', '-o', 'StrictHostKeyChecking=no', '-o', 'PasswordAuthentication=no', '-o',
                         'NumberOfPasswordPrompts=0', '-o', 'ConnectTimeout=' + str(timeout)]
        self.scp_opts = []
        self.host = None
        self.port = None

    def set_host_port(self, host, port):
        """        Set host and port to use        """
        self.host = host
        self.port = port
        self.scp_opts = self.ssh_opts + ['-P', self.port]
        self.ssh_opts = self.ssh_opts + ['-C', '-p', self.port]

    def get_ssh_pipe(self, cmd):
        """        Get open ssh pipe        """
        args = ['ssh'] + self.ssh_opts + [self.host] + cmd
        self.log.debug('Executing: %s', args)
        return Popen(args, stdout=PIPE, stderr=PIPE, stdin=PIPE, bufsize=0, preexec_fn=os.setsid, close_fds=True)

    def get_scp_pipe(self, cmd):
        """        Get open scp pipe         """
        args = ['scp'] + self.scp_opts + cmd
        self.log.debug('Executing: %s', args)
        return Popen(args, stdout=PIPE, stderr=PIPE, stdin=PIPE, bufsize=0, preexec_fn=os.setsid, close_fds=True)


class AgentClient(object):
    """    Agent client connection    """

    def __init__(self):
        self.run = []
        self.host = None

        self.port = 22
        self.ssh = None

        temp_config = tempfile.mkstemp('.cfg', 'agent_')
        os.close(temp_config[0])
        self.path = {
            # Destination path on remote host
            'AGENT_REMOTE_FOLDER': '/var/tmp/lunapark_monitoring',

            # Source path on tank
            'AGENT_LOCAL_FOLDER': os.path.dirname(__file__) + '/agent/',
            'METRIC_LOCAL_FOLDER': os.path.dirname(__file__) + '/agent/metric',

            # Temp config path
            'TEMP_CONFIG': temp_config[1]
        }
        self.interval = None
        self.metric = None
        self.custom = {}
        self.startups = []
        self.shutdowns = []
        self.python = '/usr/bin/env python2'

    def start(self):
        """        Start remote agent        """
        logging.debug('Start monitoring: %s', self.host)
        if not self.run:
            raise ValueError("Empty run string")
        self.run += ['-t', str(int(time.time()))]
        logging.debug(self.run)
        pipe = self.ssh.get_ssh_pipe(self.run)
        logging.debug("Started: %s", pipe)
        return pipe


    def create_agent_config(self, loglevel):
        """ Creating config """
        try:
            float(self.interval)
        except:
            strn = "Monitoring interval parameter is in wrong format: '%s'. Only numbers allowed."
            raise ValueError(strn % self.interval)

        cfg = ConfigParser.ConfigParser()
        cfg.add_section('main')
        cfg.set('main', 'interval', self.interval)
        cfg.set('main', 'host', self.host)
        cfg.set('main', 'loglevel', loglevel)

        cfg.add_section('metric')
        cfg.set('metric', 'names', self.metric)

        cfg.add_section('custom')
        for method in self.custom:
            if self.custom[method]:
                cfg.set('custom', method, ','.join(self.custom[method]))

        cfg.add_section('startup')
        for idx, cmd in enumerate(self.startups):
            cfg.set('startup', "cmd%s" % idx, cmd)

        cfg.add_section('shutdown')
        for idx, cmd in enumerate(self.shutdowns):
            cfg.set('shutdown', "cmd%s" % idx, cmd)

        with open(self.path['TEMP_CONFIG'], 'w') as fds:
            cfg.write(fds)

        return self.path['TEMP_CONFIG']

    def install(self, loglevel):
        """ Create folder and copy agent and metrics scripts to remote host """
        logging.info("Installing monitoring agent at %s...", self.host)
        agent_config = self.create_agent_config(loglevel)

        self.ssh.set_host_port(self.host, self.port)

        # getting remote temp dir
        cmd = [self.python + ' -c "import tempfile; print tempfile.mkdtemp();"']
        logging.debug("Get remote temp dir: %s", cmd)
        pipe = self.ssh.get_ssh_pipe(cmd)

        err = pipe.stderr.read().strip()
        if err:
            raise RuntimeError("[%s] ssh error: '%s'" % (self.host, err))
        pipe.wait()
        logging.debug("Return code [%s]: %s", self.host, pipe.returncode)
        if pipe.returncode:
            raise RuntimeError("Failed to get remote dir via SSH at %s, code %s: %s" % (
                self.host, pipe.returncode, pipe.stdout.read().strip()))

        remote_dir = pipe.stdout.read().strip()
        if remote_dir:
            self.path['AGENT_REMOTE_FOLDER'] = remote_dir
        logging.debug("Remote dir at %s:%s", self.host, self.path['AGENT_REMOTE_FOLDER'])

        # Copy agent
        cmd = [self.path['AGENT_LOCAL_FOLDER'] + '/agent.py',
               '[' + self.host + ']' + ':' + self.path['AGENT_REMOTE_FOLDER']]
        logging.debug("Copy agent to %s: %s", self.host, cmd)

        pipe = self.ssh.get_scp_pipe(cmd)
        pipe.wait()
        logging.debug("AgentClient copy exitcode: %s", pipe.returncode)
        if pipe.returncode != 0:
            raise RuntimeError("AgentClient copy exitcode: %s" % pipe.returncode)

        # Copy config
        cmd = [self.path['TEMP_CONFIG'], '[' + self.host + ']' + ':' + self.path['AGENT_REMOTE_FOLDER'] + '/agent.cfg']
        logging.debug("[%s] Copy config: %s", cmd, self.host)

        pipe = self.ssh.get_scp_pipe(cmd)
        pipe.wait()
        logging.debug("AgentClient copy config exitcode: %s", pipe.returncode)
        if pipe.returncode != 0:
            raise RuntimeError("AgentClient copy config exitcode: %s" % pipe.returncode)

        if os.getenv("DEBUG") or 1:
            debug = "DEBUG=1"
        else:
            debug = ""
        self.run = [debug, self.python, self.path['AGENT_REMOTE_FOLDER'] + '/agent.py', '-c',
                    self.path['AGENT_REMOTE_FOLDER'] + '/agent.cfg']
        return agent_config

    def uninstall(self):
        """ Remove agent's files from remote host"""
        fhandle, log_file = tempfile.mkstemp('.log', "agent_" + self.host + "_")
        os.close(fhandle)
        cmd = [self.host + ':' + self.path['AGENT_REMOTE_FOLDER'] + "_agent.log", log_file]
        logging.debug("Copy agent log from %s: %s", self.host, cmd)
        remove = self.ssh.get_scp_pipe(cmd)
        remove.wait()

        logging.info("Removing agent from: %s...", self.host)
        cmd = ['rm', '-r', self.path['AGENT_REMOTE_FOLDER']]
        remove = self.ssh.get_ssh_pipe(cmd)
        remove.wait()
        return log_file


class MonitoringCollector:
    """    Class to aggregate data from several collectors    """

    def __init__(self):
        self.log = logging.getLogger(__name__)
        self.config = None
        self.default_target = None
        self.agents = []
        self.agent_pipes = []
        self.filter_conf = {}
        self.listeners = []
        self.ssh_wrapper_class = SSHWrapper
        self.first_data_received = False
        self.send_data = ''
        self.artifact_files = []
        self.inputs, self.outputs, self.excepts = [], [], []
        self.filter_mask = defaultdict(str)
        self.ssh_timeout = 5


    def add_listener(self, obj):
        """         Add data line listener        """
        self.listeners.append(obj)


    def prepare(self):
        """ Prepare for monitoring - install agents etc"""
        # Parse config
        agent_config = []
        if self.config:
            [agent_config, self.filter_conf] = self.getconfig(self.config, self.default_target)

        self.log.debug("filter_conf: %s", self.filter_conf)

        # Filtering
        for host in self.filter_conf:
            self.filter_mask[host] = []
        self.log.debug("Filter mask: %s", self.filter_mask)

        # Creating agent for hosts
        logging.debug('Creating agents')
        for adr in agent_config:
            logging.debug('Creating agent: %s', adr)
            agent = AgentClient()
            agent.host = adr['host']
            agent.python = adr['python']
            agent.metric = adr['metric']
            agent.port = adr['port']
            agent.interval = adr['interval']
            agent.custom = adr['custom']
            agent.startups = adr['startups']
            agent.shutdowns = adr['shutdowns']
            agent.ssh = self.ssh_wrapper_class(self.ssh_timeout)
            self.agents.append(agent)

        # Mass agents install
        logging.debug("Agents: %s", self.agents)

        conf = Config(self.config)
        for agent in self.agents:
            logging.debug('Install monitoring agent. Host: %s', agent.host)
            self.artifact_files.append(agent.install(conf.loglevel()))

    def start(self):
        """ Start N parallel agents """
        for agent in self.agents:
            pipe = agent.start()
            self.agent_pipes.append(pipe)

            fds = pipe.stdout.fileno()
            flags = fcntl.fcntl(fds, fcntl.F_GETFL)
            fcntl.fcntl(fds, fcntl.F_SETFL, flags | os.O_NONBLOCK)
            self.outputs.append(pipe.stdout)

            fds = pipe.stderr.fileno()
            flags = fcntl.fcntl(fds, fcntl.F_GETFL)
            fcntl.fcntl(fds, fcntl.F_SETFL, flags | os.O_NONBLOCK)
            self.excepts.append(pipe.stderr)

        logging.debug("Pipes: %s", self.agent_pipes)


    def poll(self):
        """        Poll agents for data        """
        readable, writable, exceptional = select.select(self.outputs, self.inputs, self.excepts, 0)
        logging.debug("Streams: %s %s %s", readable, writable, exceptional)

        # if empty run - check children
        if (not readable) or exceptional:
            for pipe in self.agent_pipes:
                if pipe.returncode:
                    logging.debug("Child died returncode: %s", pipe.returncode)
                    self.outputs.remove(pipe.stdout)
                    self.agent_pipes.remove(pipe)

        # Handle exceptions
        for excepted in exceptional:
            data = excepted.readline()
            while data:
                logging.error("Got exception [%s]: %s", excepted, data)
                data = excepted.readline()

        while readable:
            to_read = readable.pop(0)
            # Handle outputs

            try:
                lines = to_read.read().split("\n")
            except IOError:
                self.log.debug("No data available")
                lines = []

            for data in lines:
                logging.debug("Got data from agent: %s", data.strip())
                self.send_data += self.filter_unused_data(self.filter_conf, self.filter_mask, data)
                logging.debug("Data after filtering: %s", self.send_data)

        if not self.first_data_received and self.send_data:
            self.first_data_received = True
            self.log.info("Monitoring received first data")
        else:
            self.send_collected_data()

        return len(self.outputs)


    def stop(self):
        """ Shutdown  agents       """
        logging.debug("Initiating normal finish")
        for pipe in self.agent_pipes:
            try:
                pipe.stdin.write("stop\n")
            except IOError, exc:
                logging.warn("Problems stopping agent: %s", traceback.format_exc(exc))

        time.sleep(1)

        for pipe in self.agent_pipes:
            if pipe.pid:
                first_try = True
                delay = 1
                while tankcore.pid_exists(pipe.pid):
                    if first_try:
                        logging.debug("Killing %s with %s", pipe.pid, signal.SIGTERM)
                        os.killpg(pipe.pid, signal.SIGTERM)
                        first_try = False
                        time.sleep(0.1)
                    else:
                        time.sleep(delay)
                        delay *= 2
                        logging.warn("Killing %s with %s", pipe.pid, signal.SIGKILL)
                        os.killpg(pipe.pid, signal.SIGKILL)

        for agent in self.agents:
            self.artifact_files.append(agent.uninstall())


    def send_collected_data(self):
        """ sends pending data set to listeners """
        for listener in self.listeners:
            listener.monitoring_data(self.send_data)
        self.send_data = ''

    # FIXME: a piese of shit and shame to a programmer
    def get_host_config(self, default, default_metric, filter_obj, host, names, target_hint):
        hostname = host.get('address')
        if hostname == '[target]':
            if not target_hint:
                raise ValueError("Can't use [target] keyword with no target parameter specified")
            logging.debug("Using target hint: %s", target_hint)
            hostname = target_hint
        stats = []
        startups = []
        shutdowns = []
        custom = {'tail': [], 'call': [], }
        metrics_count = 0
        for metric in host:
            # known metrics
            if metric.tag in default.keys():
                metrics_count += 1
                metr_val = default[metric.tag].split(',')
                if metric.get('measure'):
                    metr_val = metric.get('measure').split(',')
                for elm in metr_val:
                    if not elm:
                        continue
                    stat = "%s_%s" % (metric.tag, elm)
                    stats.append(stat)
                    agent_name = self.get_agent_name(metric.tag, elm)
                    if agent_name:
                        names[agent_name] = 1
                        # custom metric ('call' and 'tail' methods)
            elif (str(metric.tag)).lower() == 'custom':
                metrics_count += 1
                isdiff = metric.get('diff')
                if not isdiff:
                    isdiff = 0
                stat = "%s:%s:%s" % (base64.b64encode(metric.get('label')), base64.b64encode(metric.text), isdiff)
                stats.append('Custom:' + stat)
                custom[metric.get('measure', 'call')].append(stat)
            elif (str(metric.tag)).lower() == 'startup':
                startups.append(metric.text)
            elif (str(metric.tag)).lower() == 'shutdown':
                shutdowns.append(metric.text)

        logging.debug("Metrics count: %s", metrics_count)
        logging.debug("Host len: %s", len(host))
        logging.debug("keys: %s", host.keys())
        logging.debug("values: %s", host.values())

        # use default metrics for host
        if metrics_count == 0:
            for metric in default_metric:
                metr_val = default[metric].split(',')
                for elm in metr_val:
                    stat = "%s_%s" % (metric, elm)
                    stats.append(stat)
                    agent_name = self.get_agent_name(metric, elm)
                    if agent_name:
                        names[agent_name] = 1
        metric = ','.join(names.keys())
        tmp = {}
        if metric:
            tmp.update({'metric': metric})
        else:
            tmp.update({'metric': 'cpu-stat'})

        if host.get('interval'):
            tmp.update({'interval': host.get('interval')})
        else:
            tmp.update({'interval': 1})

        if host.get('priority'):
            tmp.update({'priority': host.get('priority')})
        else:
            tmp.update({'priority': 0})

        if host.get('port'):
            tmp.update({'port': host.get('port')})
        else:
            tmp.update({'port': '22'})

        if host.get('python'):
            tmp.update({'python': host.get('python')})
        else:
            tmp.update({'python': '/usr/bin/env python2'})

        tmp.update({'custom': custom})
        tmp.update({'host': hostname})

        tmp.update({'startups': startups})
        tmp.update({'shutdowns': shutdowns})

        filter_obj[hostname] = stats
        return tmp

    def getconfig(self, filename, target_hint):
        """ Prepare config data"""
        default = {
            'System': 'csw,int',
            'CPU': 'user,system,iowait',
            'Memory': 'free,cached,used',
            'Disk': 'read,write',
            'Net': 'recv,send',
        }

        default_metric = ['CPU', 'Memory', 'Disk', 'Net']

        try:
            tree = etree.parse(filename)
        except IOError, exc:
            logging.error("Error loading config: %s", exc)
            raise RuntimeError("Can't read monitoring config %s" % filename)

        hosts = tree.xpath('/Monitoring/Host')
        names = defaultdict()
        config = []
        filter_obj = defaultdict(str)
        for host in hosts:
            host_config = self.get_host_config(default, default_metric, filter_obj, host, names, target_hint)
            config.append(host_config)

        return [config, filter_obj]

    def filtering(self, mask, filter_list):
        """ Filtering helper """
        host = filter_list[0]
        initial = [0, 1]
        res = []
        if mask[host]:
            keys = initial + mask[host]
            for key in keys:
                try:
                    res.append(filter_list[key])
                except IndexError:
                    self.log.warn("Problems filtering data: %s with %s", mask, len(filter_list))
                    return None
        return ';'.join(res)

    def filter_unused_data(self, filter_conf, filter_mask, data):
        """ Filter unselected metrics from data """
        self.log.debug("Filtering data: %s", data)
        out = ''
        # Filtering data
        keys = data.rstrip().split(';')
        if re.match('^start;', data):  # make filter_conf mask
            host = keys[1]
            for i in xrange(3, len(keys)):
                if keys[i] in filter_conf[host]:
                    filter_mask[host].append(i - 1)
            self.log.debug("Filter mask: %s", filter_mask)
            out = 'start;'
            out += self.filtering(filter_mask, keys[1:]).rstrip(';') + '\n'
        elif re.match('^\[debug\]', data):  # log debug output
            logging.debug('agent debug: %s', data.rstrip())
        else:
            filtered = self.filtering(filter_mask, keys)
            if filtered:
                out = filtered + '\n'  # filtering values
        return out

    def get_agent_name(self, metric, param):
        """Resolve metric name"""
        depend = {
            'CPU': {
                'idle': 'cpu-stat',
                'user': 'cpu-stat',
                'system': 'cpu-stat',
                'iowait': 'cpu-stat',
                'nice': 'cpu-stat'
            },
            'System': {
                'la1': 'cpu-la',
                'la5': 'cpu-la',
                'la15': 'cpu-la',
                'csw': 'cpu-stat',
                'int': 'cpu-stat',
                'numproc': 'cpu-stat',
                'numthreads': 'cpu-stat',
            },
            'Memory': {
                'free': 'mem',
                'used': 'mem',
                'cached': 'mem',
                'buff': 'mem',
            },
            'Disk': {
                'read': 'disk',
                'write': 'disk',
            },
            'Net': {
                'recv': 'net',
                'send': 'net',
                'tx': 'net-tx-rx',
                'rx': 'net-tx-rx',
                'retransmit': 'net-retrans',
                'estab': 'net-tcp',
                'closewait': 'net-tcp',
                'timewait': 'net-tcp',
            }
        }
        if depend[metric][param]:
            return depend[metric][param]
        else:
            return ''


class MonitoringDataListener:
    """ Parent class for data listeners """

    def __init__(self):
        pass

    def monitoring_data(self, data_string):
        """ Notification about new monitoring data lines """
        raise NotImplementedError()


class StdOutPrintMon(MonitoringDataListener):
    """ Simple listener, writing data to stdout """

    def __init__(self):
        MonitoringDataListener.__init__(self)

    def monitoring_data(self, data_string):
        sys.stdout.write(data_string)


class MonitoringDataDecoder:
    """    The class that serves converting monitoring data lines to dict    """
    NA = 'n/a'

    def __init__(self):
        self.metrics = {}
        self.log = logging.getLogger()

    def decode_line(self, line):
        """ convert mon line to dict """
        is_initial = False
        data_dict = {}
        data = line.strip().split(';')
        timestamp = -1
        if data[0] == 'start':
            data.pop(0)  # remove 'start'
            host = data.pop(0)
            if not data:
                logging.warn("Wrong mon data line: %s", line)
            else:
                timestamp = data.pop(0)
                self.metrics[host] = []
                for metric in data:
                    if metric.startswith("Custom:"):
                        metric = base64.standard_b64decode(metric.split(':')[1])
                    self.metrics[host].append(metric)
                    data_dict[metric] = self.NA
                    is_initial = True
        else:
            host = data.pop(0)
            timestamp = data.pop(0)

            if host not in self.metrics.keys():
                raise ValueError("Host %s not in started metrics: %s" % (host, self.metrics))

            if len(self.metrics[host]) != len(data):
                raise ValueError("Metrics len and data len differs: %s vs %s" % (len(self.metrics[host]), len(data)))

            for metric in self.metrics[host]:
                data_dict[metric] = data.pop(0)

        self.log.debug("Decoded data %s: %s", host, data_dict)
        return host, data_dict, is_initial, timestamp


# FIXME: 3 synchronize times between agent and collector better

########NEW FILE########
__FILENAME__ = Aggregator
""" Core module to calculate aggregate data """
import copy
import datetime
import logging
import math
import time

from tankcore import AbstractPlugin
import tankcore


class AggregateResultListener:
    """ listener to be notified about aggregate data """

    def __init__(self):
        pass

    def aggregate_second(self, second_aggregate_data):
        """ notification about new aggregate data """
        raise NotImplementedError("Abstract method needs to be overridden")


class AggregatorPlugin(AbstractPlugin):
    """ Plugin that manages aggregation """
    default_time_periods = "1ms 2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100 " \
                           "150 200 250 300 350 400 450 500 600 650 700 750 800 850 900 950 1s " \
                           "1500 2s 2500 3s 3500 4s 4500 5s 5500 6s 6500 7s 7500 8s 8500 9s 9500 10s 11s"

    SECTION = 'aggregator'

    @staticmethod
    def get_key():
        return __file__

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.process = None
        self.second_data_listeners = []
        self.preproc_out_offset = 0
        self.buffer = []
        self.second_data_draft = []
        self.preproc_out_filename = None
        self.cumulative_data = SecondAggregateDataTotalItem()
        self.reader = None
        self.time_periods = [tankcore.expand_to_milliseconds(x)
                             for x in self.default_time_periods.split(' ')]
        self.last_sample_time = 0
        self.precise_cumulative = 1

    def get_available_options(self):
        return ["time_periods", "precise_cumulative"]

    def configure(self):
        periods = self.get_option(
            "time_periods", self.default_time_periods).split(" ")
        self.time_periods = [
            tankcore.expand_to_milliseconds(x) for x in periods]
        self.core.set_option(
            self.SECTION, "time_periods", " ".join([str(x) for x in periods]))
        self.precise_cumulative = int(
            self.get_option("precise_cumulative", '1'))

    def start_test(self):
        if not self.reader:
            self.log.warning("No one had set reader for aggregate data yet...")

    def is_test_finished(self):
        # read up to 2 samples in single pass
        self.__read_samples(2)
        return -1

    def end_test(self, retcode):
        self.__read_samples(force=True)
        if self.reader:
            self.reader.close_files()
        return retcode

    def add_result_listener(self, listener):
        """ add object to data listeners """
        self.second_data_listeners.append(listener)

    def __notify_listeners(self, data):
        """ notify all listeners about aggregate data """
        self.log.debug(
            "Notifying listeners about second: %s , %s/%s req/responses",
            data.time, data.overall.planned_requests, data.overall.RPS)
        for listener in self.second_data_listeners:
            listener.aggregate_second(data)

    def get_timeout(self):
        """ get timeout based on time_periods last val """
        return self.time_periods[-1:][0]

    def __generate_zero_samples(self, data):
        """ fill timeline gaps with zero samples """
        if not data:
            return
        while self.last_sample_time and int(time.mktime(data.time.timetuple())) - self.last_sample_time > 1:
            self.last_sample_time += 1
            self.log.warning("Adding zero sample: %s", self.last_sample_time)
            zero = self.reader.get_zero_sample(
                datetime.datetime.fromtimestamp(self.last_sample_time))
            self.__notify_listeners(zero)
        self.last_sample_time = int(time.mktime(data.time.timetuple()))

    def __read_samples(self, limit=0, force=False):
        """ call reader object to read next sample set """
        if self.reader:
            self.reader.check_open_files()
            data = self.reader.get_next_sample(force)
            count = 0
            while data:
                self.last_sample_time = int(time.mktime(data.time.timetuple()))
                self.__generate_zero_samples(data)
                self.__notify_listeners(data)
                if limit < 1 or count < limit:
                    data = self.reader.get_next_sample(force)
                else:
                    data = None
                count += 1


# ===============================================================
class SecondAggregateData:
    """ class holds aggregate data for the second """

    def __init__(self, cimulative_item=None):
        self.cases = {}
        self.time = None
        self.overall = SecondAggregateDataItem()
        self.cumulative = cimulative_item

    def __repr__(self):
        return "SecondAggregateData[%s][%s]" % (self.time, time.mktime(self.time.timetuple()))


class SecondAggregateDataItem:
    """ overall and case items has this type """
    QUANTILES = [0.25, 0.50, 0.75, 0.80, 0.85, 0.90, 0.95, 0.98, 0.99, 1.00]

    def __init__(self):
        self.log = logging.getLogger(__name__)
        self.case = None
        self.planned_requests = 0
        self.active_threads = 0
        self.selfload = 0
        self.RPS = 0
        self.http_codes = {}
        self.net_codes = {}
        self.times_dist = []
        self.quantiles = {}
        self.dispersion = 0
        self.input = 0
        self.output = 0
        self.avg_connect_time = 0
        self.avg_send_time = 0
        self.avg_latency = 0
        self.avg_receive_time = 0
        self.avg_response_time = 0


class SecondAggregateDataTotalItem:
    """ total cumulative data item """

    def __init__(self):
        self.avg_connect_time = 0
        self.avg_send_time = 0
        self.avg_latency = 0
        self.avg_receive_time = 0
        self.avg_response_time = 0
        self.total_count = 0
        self.times_dist = {}
        self.quantiles = {}

    def add_data(self, overall_item):
        """ add data to total """
        for time_item in overall_item.times_dist:
            self.total_count += time_item['count']
            timing = int(time_item['from'])
            if timing in self.times_dist.keys():
                self.times_dist[timing]['count'] += time_item['count']
            else:
                self.times_dist[timing] = time_item

    def add_raw_data(self, times_dist):
        """ add data to total """
        for time_item in times_dist:
            self.total_count += 1
            timing = int(time_item)
            dist_item = self.times_dist.get(
                timing, {'from': timing, 'to': timing, 'count': 0})
            dist_item['count'] += 1
            self.times_dist[timing] = dist_item
        logging.debug("Total times len: %s", len(self.times_dist))

    def calculate_total_quantiles(self):
        """ calculate total quantiles based on times dist """
        self.quantiles = {}
        quantiles = reversed(copy.copy(SecondAggregateDataItem.QUANTILES))
        timings = sorted(self.times_dist.keys(), reverse=True)
        level = 1.0
        timing = 0
        for quan in quantiles:
            while level >= quan:
                timing = timings.pop(0)
                level -= float(
                    self.times_dist[timing]['count']) / self.total_count
            self.quantiles[quan * 100] = self.times_dist[timing]['to']

        logging.debug("Total quantiles: %s", self.quantiles)
        return self.quantiles


# ===============================================================

class AbstractReader:
    """
    Parent class for all source reading adapters
    """

    def __init__(self, owner):
        self.aggregator = owner
        self.log = logging.getLogger(__name__)
        self.cumulative = SecondAggregateDataTotalItem()
        self.data_queue = []
        self.data_buffer = {}

    def check_open_files(self):
        """ open files if necessary """
        pass

    def close_files(self):
        """        Close opened handlers to avoid fd leak        """
        pass

    def get_next_sample(self, force):
        """ read next sample from file """
        pass

    def parse_second(self, next_time, data):
        """ parse buffered data to aggregate item """
        self.log.debug("Parsing second: %s", next_time)
        result = self.get_zero_sample(
            datetime.datetime.fromtimestamp(next_time))
        time_before = time.time()
        for item in data:
            self.__append_sample(result.overall, item)
            marker = item[0]
            if marker:
                if not marker in result.cases.keys():
                    result.cases[marker] = SecondAggregateDataItem()
                self.__append_sample(result.cases[marker], item)

        # at this phase we have raw response times in result.overall.times_dist
        if self.aggregator.precise_cumulative:
            self.cumulative.add_raw_data(result.overall.times_dist)

        self.log.debug(
            "Calculate aggregates for %s requests", result.overall.RPS)
        self.__calculate_aggregates(result.overall)
        for case in result.cases.values():
            self.__calculate_aggregates(case)

        if not self.aggregator.precise_cumulative:
            self.cumulative.add_data(result.overall)
        self.cumulative.calculate_total_quantiles()

        spent = time.time() - time_before
        if spent:
            self.log.debug(
                "Aggregating speed: %s lines/sec", int(len(data) / spent))
        return result

    def __calculate_aggregates(self, item):
        """ calculate aggregates on raw data """
        if item.RPS:
            item.selfload = 100 * item.selfload / item.RPS
            item.avg_connect_time /= item.RPS
            item.avg_send_time /= item.RPS
            item.avg_latency /= item.RPS
            item.avg_receive_time /= item.RPS
            item.avg_response_time /= item.RPS

            item.times_dist.sort()
            count = 0.0
            quantiles = copy.copy(SecondAggregateDataItem.QUANTILES)
            times = copy.copy(self.aggregator.time_periods)
            time_from = 0
            time_to = times.pop(0)
            times_dist_draft = []
            times_dist_item = {'from': time_from, 'to': time_to, 'count': 0}
            deviation = 0.0
            timing = 0
            for timing in item.times_dist:
                count += 1
                if quantiles and (count / item.RPS) >= quantiles[0]:
                    level = quantiles.pop(0)
                    item.quantiles[level * 100] = timing

                while times and timing > time_to:
                    time_from = time_to
                    time_to = times.pop(0)
                    if times_dist_item['count']:
                        times_dist_draft.append(times_dist_item)
                    times_dist_item = {
                        'from': time_from, 'to': time_to, 'count': 0}

                times_dist_item['count'] += 1
                deviation += math.pow(item.avg_response_time - timing, 2)

            while quantiles:
                level = quantiles.pop(0)
                item.quantiles[level * 100] = timing

            if times_dist_item['count']:
                times_dist_draft.append(times_dist_item)

            item.dispersion = deviation / item.RPS
            item.times_dist = times_dist_draft

    def __append_sample(self, result, item):
        """ add single sample to aggregator buffer """
        for check in item:
            if check < 0:
                self.log.error("Problem item: %s", item)
                raise ValueError("One of the sample items has negative value")

        (marker, threads, overall_rt, http_code, net_code, sent_bytes,
         received_bytes, connect, send, latency, receive, accuracy) = item
        result.case = marker
        result.active_threads = threads
        result.planned_requests = 0
        result.RPS += 1

        if http_code and http_code != '0':
            if not http_code in result.http_codes.keys():
                result.http_codes[http_code] = 0
            result.http_codes[http_code] += 1
        if not net_code in result.net_codes.keys():
            result.net_codes[net_code] = 0
        result.net_codes[net_code] += 1

        result.input += received_bytes
        result.output += sent_bytes

        result.avg_connect_time += connect
        result.avg_send_time += send
        result.avg_latency += latency
        result.avg_receive_time += receive
        result.avg_response_time += overall_rt
        result.selfload += accuracy

        result.times_dist.append(overall_rt)

    def get_zero_sample(self, date_time):
        """ instantiate new aggregate data item """
        res = SecondAggregateData(self.cumulative)
        res.time = date_time
        return res

    def pop_second(self):
        """ pop from out queue new aggregate data item """
        self.data_queue.sort()
        next_time = self.data_queue.pop(0)
        data = self.data_buffer[next_time]
        del self.data_buffer[next_time]
        res = self.parse_second(next_time, data)
        return res

########NEW FILE########
__FILENAME__ = ApacheBenchmark
''' AB load generator '''
from Tank.Plugins.Aggregator import AggregatorPlugin, AbstractReader
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin, AbstractInfoWidget
from tankcore import AbstractPlugin
import os
import subprocess
import tankcore

class ApacheBenchmarkPlugin(AbstractPlugin):
    ''' Apache Benchmark plugin '''
    SECTION = 'ab'
    
    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.out_file = None
        self.process = None
        self.concurrency = 0
        self.options = None
        self.url = None
        self.requests = None

    @staticmethod
    def get_key():
        return __file__
    
    def get_available_options(self):
        return ["options", "url", "requests", "concurrency"]
    
    def configure(self):
        self.options = self.get_option("options", '')
        self.url = self.get_option("url", 'http://localhost/')
        self.requests = self.get_option("requests", '100')
        self.concurrency = self.get_option("concurrency", '1')
        self.out_file = self.core.mkstemp('.log', 'ab_')
        self.core.add_artifact_file(self.out_file)

    def prepare_test(self):
        aggregator = None
        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
        except Exception, ex:
            self.log.warning("No aggregator found: %s", ex)

        if aggregator:
            aggregator.reader = ABReader(aggregator, self)
            
        try:
            console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
        except Exception, ex:
            self.log.debug("Console not found: %s", ex)
            console = None
            
        if console:    
            widget = ABInfoWidget(self)
            console.add_info_widget(widget)
        
            
    def start_test(self):
        args = ['ab', '-r', '-g', self.out_file,
                '-n', self.requests,
                '-c', self.concurrency, self.url]
        self.log.info("Starting ab with arguments: %s", args)
        self.process = subprocess.Popen(args, stderr=subprocess.PIPE, stdout=subprocess.PIPE, close_fds=True)
           
    def is_test_finished(self):
        retcode = self.process.poll()
        if retcode != None:
            self.log.debug("%s exit code: %s", self.SECTION, retcode)
            return retcode
        else:
            return -1

            
    def end_test(self, retcode):
        if self.process and self.process.poll() == None:
            self.log.warn("Terminating ab process with PID %s", self.process.pid)
            self.process.terminate()
        else:
            self.log.info("Seems ab finished OK")

        if self.process:
            tankcore.log_stdout_stderr(self.log, self.process.stdout, self.process.stderr, self.SECTION)
        return retcode
            

 
class ABReader(AbstractReader):
    '''
    Adapter to read AB files
    '''
    def __init__(self, aggregator, abench):
        AbstractReader.__init__(self, aggregator)
        self.abench = abench
        self.results = None
    
    def check_open_files(self):
        if not self.results and os.path.exists(self.abench.out_file):
            self.log.debug("Opening ab out file: %s", self.abench.out_file)
            self.results = open(self.abench.out_file, 'r')
            
    def close_files(self):
        if self.results:
            self.results.close()
    
    def get_next_sample(self, force):
        if self.results:
            read_lines = self.results.readlines()
            if read_lines:
                read_lines.pop(0)  # remove header line
            self.log.debug("About to process %s result lines", len(read_lines))
            for line in read_lines:
                line = line.strip()
                if not line:
                    return None 
                # Tue Sep 25 14:19:36 2012        1348568376      0       36      36      34
                data = line.split("\t")
                if len(data) != 6:
                    self.log.warning("Wrong ab log line, skipped: %s", line)
                    continue
                cur_time = int(data[1])
                ctime = int(data[2])
                # dtime = int(data[3])
                ttime = int(data[4])
                wait = int(data[5])
    
                if not cur_time in self.data_buffer.keys():
                    self.data_queue.append(cur_time)
                    self.data_buffer[cur_time] = []
                #        marker, threads, overallRT, httpCode, netCode
                data_item = ['', self.abench.concurrency, ttime, 0, 0]
                # bytes:     sent    received
                data_item += [0, 0]
                #        connect    send    latency    receive
                data_item += [ctime, 0, wait, ttime - ctime - wait]
                #        accuracy
                data_item += [0]
                self.data_buffer[cur_time].append(data_item)
                    
        if self.data_queue:
            return self.pop_second()
        else:
            return None 
    

class ABInfoWidget(AbstractInfoWidget):
    ''' Console widget '''    
    def __init__(self, abench):
        AbstractInfoWidget.__init__(self)
        self.abench = abench
        self.active_threads = 0

    def get_index(self):
        return 0

    def render(self, screen):        
        ab_text = " Apache Benchmark Test "
        space = screen.right_panel_width - len(ab_text) - 1 
        left_spaces = space / 2
        right_spaces = space / 2
        template = screen.markup.BG_BROWN + '~' * left_spaces + ab_text + ' ' + '~' * right_spaces + screen.markup.RESET + "\n" 
        template += "           URL: %s\n"
        template += "   Concurrency: %s\n"
        template += "Total Requests: %s"
        data = (self.abench.url, self.abench.concurrency, self.abench.requests)
        
        return template % data

########NEW FILE########
__FILENAME__ = Autostop
""" Autostop facility """
import copy
import logging
import re
import json

from Tank.Plugins.Aggregator import AggregatorPlugin, AggregateResultListener
from Tank.Plugins.ConsoleOnline import AbstractInfoWidget, ConsoleOnlinePlugin
from tankcore import AbstractPlugin
import tankcore


class AutostopPlugin(AbstractPlugin, AggregateResultListener):
    """ Plugin that accepts criteria classes and triggers autostop """
    SECTION = 'autostop'

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.cause_criteria = None
        self.criterias = []
        self.custom_criterias = []
        self.counting = []
        self.criteria_str = ''

    @staticmethod
    def get_key():
        return __file__

    def get_counting(self):
        """ get criterias that are activated """
        return self.counting

    def add_counting(self, obj):
        """ add criteria that activated """
        self.counting += [obj]

    def add_criteria_class(self, criteria_class):
        """ add new criteria class """
        self.custom_criterias += [criteria_class]

    def get_available_options(self):
        return ["autostop"]

    def configure(self):
        aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
        aggregator.add_result_listener(self)
        self.criteria_str = " ".join(self.get_option("autostop", '').split("\n"))
        self.add_criteria_class(AvgTimeCriteria)
        self.add_criteria_class(NetCodesCriteria)
        self.add_criteria_class(HTTPCodesCriteria)
        self.add_criteria_class(QuantileCriteria)
        self.add_criteria_class(SteadyCumulativeQuantilesCriteria)

    def prepare_test(self):
        for criteria_str in self.criteria_str.strip().split(")"):
            if not criteria_str:
                continue
            self.log.debug("Criteria string: %s", criteria_str)
            self.criterias.append(self.__create_criteria(criteria_str))

        self.log.debug("Criteria object: %s", self.criterias)

        try:
            console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
        except Exception, ex:
            self.log.debug("Console not found: %s", ex)
            console = None

        if console:
            console.add_info_widget(AutostopWidget(self))

    def is_test_finished(self):
        if self.cause_criteria:
            self.log.info("Autostop criteria requested test stop: %s", self.cause_criteria.explain())
            return self.cause_criteria.get_rc()
        else:
            return -1

    def __create_criteria(self, criteria_str):
        """ instantiate criteria from config string """
        parsed = criteria_str.split("(")
        type_str = parsed[0].strip().lower()
        parsed[1] = parsed[1].split(")")[0].strip()

        for criteria_class in self.custom_criterias:
            if criteria_class.get_type_string() == type_str:
                return criteria_class(self, parsed[1])
        raise ValueError("Unsupported autostop criteria type: %s" % criteria_str)

    def aggregate_second(self, second_aggregate_data):
        self.counting = []
        if not self.cause_criteria:
            for criteria in self.criterias:
                if criteria.notify(second_aggregate_data):
                    self.log.debug("Autostop criteria requested test stop: %s", criteria)
                    self.cause_criteria = criteria


class AutostopWidget(AbstractInfoWidget):
    """ widget that displays counting criterias """

    def __init__(self, sender):
        AbstractInfoWidget.__init__(self)
        self.owner = sender

    def get_index(self):
        return 25

    def render(self, screen):
        res = []
        candidates = self.owner.get_counting()
        for candidate in candidates:
            text, perc = candidate.widget_explain()
            if perc >= 0.95:
                res += [screen.markup.RED_DARK + text + screen.markup.RESET]
            elif perc >= 0.8:
                res += [screen.markup.RED + text + screen.markup.RESET]
            elif perc >= 0.5:
                res += [screen.markup.YELLOW + text + screen.markup.RESET]
            else:
                res += [text]

        if res:
            return "Autostop:\n  " + ("\n  ".join(res))
        else:
            return ''


class AbstractCriteria:
    """ parent class for all criterias """
    RC_TIME = 21
    RC_HTTP = 22
    RC_NET = 23
    RC_STEADY = 33

    def __init__(self):
        self.log = logging.getLogger(__name__)
        self.cause_second = None

    @staticmethod
    def count_matched_codes(codes_regex, codes_dict):
        """ helper to aggregate codes by mask """
        total = 0
        for code, count in codes_dict.items():
            if codes_regex.match(str(code)):
                total += count
        return total

    def notify(self, aggregate_second):
        """ notification about aggregate data goes here """
        raise NotImplementedError("Abstract methods requires overriding")

    def get_rc(self):
        """ get return code for test """
        raise NotImplementedError("Abstract methods requires overriding")

    def explain(self):
        """ long explanation to show after test stop """
        raise NotImplementedError("Abstract methods requires overriding")

    def widget_explain(self):
        """ short explanation to display in right panel """
        return self.explain(), 0

    @staticmethod
    def get_type_string():
        """ returns string that used as config name for criteria """
        raise NotImplementedError("Abstract methods requires overriding")


class AvgTimeCriteria(AbstractCriteria):
    """ average response time criteria """

    @staticmethod
    def get_type_string():
        return 'time'

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.rt_limit = tankcore.expand_to_milliseconds(param_str.split(',')[0])
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[1])
        self.autostop = autostop

    def notify(self, aggregate_second):
        if aggregate_second.overall.avg_response_time > self.rt_limit:
            if not self.seconds_count:
                self.cause_second = aggregate_second

            self.log.debug(self.explain())

            self.seconds_count += 1
            self.autostop.add_counting(self)
            if self.seconds_count >= self.seconds_limit:
                return True
        else:
            self.seconds_count = 0

        return False

    def get_rc(self):
        return self.RC_TIME

    def explain(self):
        items = (self.rt_limit, self.seconds_count, self.cause_second.time)
        return "Average response time higher than %sms for %ss, since %s" % items

    def widget_explain(self):
        items = (self.rt_limit, self.seconds_count, self.seconds_limit)
        return "Avg Time >%sms for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit


class HTTPCodesCriteria(AbstractCriteria):
    """ HTTP codes criteria """

    @staticmethod
    def get_type_string():
        return 'http'

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.codes_mask = param_str.split(',')[0].lower()
        self.codes_regex = re.compile(self.codes_mask.replace("x", '.'))
        self.autostop = autostop

        level_str = param_str.split(',')[1].strip()
        if level_str[-1:] == '%':
            self.level = float(level_str[:-1]) / 100
            self.is_relative = True
        else:
            self.level = int(level_str)
            self.is_relative = False
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[2])


    def notify(self, aggregate_second):
        matched_responses = self.count_matched_codes(self.codes_regex, aggregate_second.overall.http_codes)
        if self.is_relative:
            if aggregate_second.overall.RPS:
                matched_responses = float(matched_responses) / aggregate_second.overall.RPS
            else:
                matched_responses = 0
        self.log.debug("HTTP codes matching mask %s: %s/%s", self.codes_mask, matched_responses, self.level)

        if matched_responses >= self.level:
            if not self.seconds_count:
                self.cause_second = aggregate_second

            self.log.debug(self.explain())

            self.seconds_count += 1
            self.autostop.add_counting(self)
            if self.seconds_count >= self.seconds_limit:
                return True
        else:
            self.seconds_count = 0

        return False

    def get_rc(self):
        return self.RC_HTTP

    def get_level_str(self):
        """ format level str """
        if self.is_relative:
            level_str = str(100 * self.level) + "%"
        else:
            level_str = self.level
        return level_str

    def explain(self):
        items = (self.codes_mask, self.get_level_str(), self.seconds_count, self.cause_second.time)
        return "%s codes count higher than %s for %ss, since %s" % items

    def widget_explain(self):
        items = (self.codes_mask, self.get_level_str(), self.seconds_count, self.seconds_limit)
        return "HTTP %s>%s for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit


class NetCodesCriteria(AbstractCriteria):
    """ Net codes criteria """

    @staticmethod
    def get_type_string():
        return 'net'

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.codes_mask = param_str.split(',')[0].lower()
        self.codes_regex = re.compile(self.codes_mask.replace("x", '.'))
        self.autostop = autostop

        level_str = param_str.split(',')[1].strip()
        if level_str[-1:] == '%':
            self.level = float(level_str[:-1]) / 100
            self.is_relative = True
        else:
            self.level = int(level_str)
            self.is_relative = False
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[2])


    def notify(self, aggregate_second):
        codes = copy.deepcopy(aggregate_second.overall.net_codes)
        if '0' in codes.keys():
            codes.pop('0')
        matched_responses = self.count_matched_codes(self.codes_regex, codes)
        if self.is_relative:
            if aggregate_second.overall.RPS:
                matched_responses = float(matched_responses) / aggregate_second.overall.RPS
            else:
                matched_responses = 0
        self.log.debug("Net codes matching mask %s: %s/%s", self.codes_mask, matched_responses, self.level)

        if matched_responses >= self.level:
            if not self.seconds_count:
                self.cause_second = aggregate_second

            self.log.debug(self.explain())

            self.seconds_count += 1
            self.autostop.add_counting(self)
            if self.seconds_count >= self.seconds_limit:
                return True
        else:
            self.seconds_count = 0

        return False

    def get_rc(self):
        return self.RC_NET

    def get_level_str(self):
        """ format level str """
        if self.is_relative:
            level_str = str(100 * self.level) + "%"
        else:
            level_str = self.level
        return level_str

    def explain(self):
        items = (self.codes_mask, self.get_level_str(), self.seconds_count, self.cause_second.time)
        return "%s net codes count higher than %s for %ss, since %s" % items

    def widget_explain(self):
        items = (self.codes_mask, self.get_level_str(), self.seconds_count, self.seconds_limit)
        return "Net %s>%s for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit


class QuantileCriteria(AbstractCriteria):
    """ quantile criteria """

    @staticmethod
    def get_type_string():
        return 'quantile'

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.quantile = float(param_str.split(',')[0])
        self.rt_limit = tankcore.expand_to_milliseconds(param_str.split(',')[1])
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[2])
        self.autostop = autostop

    def notify(self, aggregate_second):
        if not (self.quantile in aggregate_second.overall.quantiles.keys()):
            self.log.warning("No quantile %s in %s", self.quantile, aggregate_second.overall.quantiles)
        if self.quantile in aggregate_second.overall.quantiles.keys() \
                and aggregate_second.overall.quantiles[self.quantile] > self.rt_limit:
            if not self.seconds_count:
                self.cause_second = aggregate_second

            self.log.debug(self.explain())

            self.seconds_count += 1
            self.autostop.add_counting(self)
            if self.seconds_count >= self.seconds_limit:
                return True
        else:
            self.seconds_count = 0

        return False

    def get_rc(self):
        return self.RC_TIME

    def explain(self):
        items = (self.quantile, self.rt_limit, self.seconds_count, self.cause_second.time)
        return "Percentile %s higher than %sms for %ss, since %s" % items

    def widget_explain(self):
        items = (self.quantile, self.rt_limit, self.seconds_count, self.seconds_limit)
        return "%s%% >%sms for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit


class SteadyCumulativeQuantilesCriteria(AbstractCriteria):
    """ quantile criteria """

    @staticmethod
    def get_type_string():
        return 'steady_cumulative'

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.hash = ""
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[0])
        self.autostop = autostop

    def notify(self, aggregate_second):
        hash = json.dumps(aggregate_second.cumulative.quantiles)
        logging.debug("Cumulative quantiles hash: %s", hash)
        if self.hash == hash:
            if not self.seconds_count:
                self.cause_second = aggregate_second

            self.log.debug(self.explain())

            self.seconds_count += 1
            self.autostop.add_counting(self)
            if self.seconds_count >= self.seconds_limit:
                return True
        else:
            self.seconds_count = 0

        self.hash = hash
        return False

    def get_rc(self):
        return self.RC_STEADY

    def explain(self):
        items = (self.seconds_count, self.cause_second.time)
        return "Cumulative percentiles are steady for %ss, since %s" % items

    def widget_explain(self):
        items = (self.seconds_count, self.seconds_limit)
        return "Steady for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit

########NEW FILE########
__FILENAME__ = guns
import time
from collections import namedtuple
from tankcore import AbstractPlugin
import logging
from random import randint
import threading as th
import sys

from sqlalchemy import create_engine
from sqlalchemy import exc

Sample = namedtuple(
    'Sample', 'marker,threads,overallRT,httpCode,netCode,sent,received,connect,send,latency,receive,accuracy')


class LogGun(AbstractPlugin):
    SECTION = 'log_gun'

    def __init__(self, core):
        self.log = logging.getLogger(__name__)
        AbstractPlugin.__init__(self, core)
        param = self.get_option("param", '15')
        self.log.info('Initialized log gun for BFG with param = %s' % param)

    def shoot(self, missile, marker):
        self.log.debug("Missile: %s\n%s", marker, missile)
        rt = randint(2, 30000)
        data_item = Sample(
            marker,             # marker
            th.active_count(),  # threads
            rt,                 # overallRT
            0,                  # httpCode
            0,                  # netCode
            0,                  # sent
            0,                  # received
            0,                  # connect
            0,                  # send
            rt,                 # latency
            0,                  # receive
            0,                  # accuracy
        )
        return (int(time.time()), data_item)

class SqlGun(AbstractPlugin):
    SECTION = 'sql_gun'

    def __init__(self, core):
        self.log = logging.getLogger(__name__)
        AbstractPlugin.__init__(self, core)
        self.engine = create_engine(self.get_option("db"))

    def shoot(self, missile, marker):
        self.log.debug("Missile: %s\n%s", marker, missile)
        start_time = time.time()
        errno = 0
        httpCode = 200
        try:
            cursor = self.engine.execute(missile.replace('%', '%%'))
            cursor.fetchall()
            cursor.close()
        except exc.TimeoutError as e:
            self.log.debug("Timeout: %s", e)
            errno = 110
        except exc.ResourceClosedError as e:
            self.log.debug(e)
        except exc.SQLAlchemyError as e:
            httpCode = 500
            self.log.debug(e.orig.args)
        except exc.SAWarning as e:
            httpCode = 400
            self.log.debug(e)
        except Exception as e:
            httpCode = 500
            self.log.debug(e)
        rt = int((time.time() - start_time) * 1000)
        data_item = Sample(
            marker,             # marker
            th.active_count(),  # threads
            rt,                 # overallRT
            httpCode,           # httpCode
            errno,              # netCode
            0,                  # sent
            0,                  # received
            0,                  # connect
            0,                  # send
            rt,                 # latency
            0,                  # receive
            0,                  # accuracy
        )
        return (int(time.time()), data_item)

class CustomGun(AbstractPlugin):
    SECTION = 'custom_gun'

    def __init__(self, core):
        self.log = logging.getLogger(__name__)
        AbstractPlugin.__init__(self, core)
        module_path = self.get_option("module_path")
        module_name = self.get_option("module_name")
        sys.path.append(module_path)
        self.module = __import__(module_name)

    def shoot(self, missile, marker):
        return self.module.shoot(self, missile, marker)
########NEW FILE########
__FILENAME__ = plugin
from tankcore import AbstractPlugin
from Tank.Plugins.Aggregator import AggregatorPlugin
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin
import logging
import time
from Tank.stepper import StepperWrapper
from guns import LogGun, SqlGun, CustomGun
from widgets import BFGInfoWidget
from worker import BFG
from reader import BFGReader


class BFGPlugin(AbstractPlugin):

    ''' Big Fucking Gun plugin '''
    SECTION = 'bfg'

    def __init__(self, core):
        self.log = logging.getLogger(__name__)
        AbstractPlugin.__init__(self, core)
        self.gun_type = None
        self.start_time = time.time()
        self.stepper_wrapper = StepperWrapper(self.core, BFGPlugin.SECTION)
        self.log.info("Initialized BFG")

        self.gun_classes = {
            'log': LogGun,
            'sql': SqlGun,
            'custom': CustomGun,
        }

    @staticmethod
    def get_key():
        return __file__

    def get_available_options(self):
        return ["gun_type", "instances", "cached_stpd"] + self.stepper_wrapper.get_available_options

    def configure(self):
        self.log.info("Configuring BFG...")
        self.stepper_wrapper.read_config()

    def prepare_test(self):
        self.stepper_wrapper.prepare_stepper()
        gun_type = self.get_option("gun_type")
        if gun_type in self.gun_classes:
            self.gun = self.gun_classes[gun_type](self.core)
        else:
            raise NotImplementedError(
                'No such gun type implemented: "%s"' % gun_type)
        cached_stpd_option = self.get_option("cached_stpd", '0')
        if cached_stpd_option == '1':
            cached_stpd = True
        else:
            cached_stpd = False
        self.bfg = BFG(
            gun=self.gun,
            instances=self.get_option("instances", '15'),
            threads=self.get_option("threads", '10'),
            stpd_filename=self.stepper_wrapper.stpd,
            cached_stpd=cached_stpd,
        )
        aggregator = None
        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
        except Exception, ex:
            self.log.warning("No aggregator found: %s", ex)

        if aggregator:
            result_cache_size = int(self.get_option("result_cache_size", '5'))
            aggregator.reader = BFGReader(
                aggregator, self.bfg, result_cache_size=result_cache_size)

        try:
            console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
        except Exception, ex:
            self.log.debug("Console not found: %s", ex)
            console = None

        if console:
            widget = BFGInfoWidget()
            console.add_info_widget(widget)
            if aggregator:
                aggregator.add_result_listener(widget)
        self.log.info("Prepared BFG")

    def start_test(self):
        self.log.info("Starting BFG")
        self.start_time = time.time()
        self.bfg.start()

    def is_test_finished(self):
        if self.bfg.running():
            return -1
        else:
            self.log.info("BFG finished")
            return 0

    def end_test(self, retcode):
        if self.bfg.running():
            self.log.info("Terminating BFG")
            self.bfg.stop()
        return retcode

########NEW FILE########
__FILENAME__ = reader
from Tank.Plugins.Aggregator import AbstractReader
from Tank.stepper import info as si


class BFGReader(AbstractReader):

    '''
    Listens results from BFG and provides them to Aggregator
    '''

    def __init__(self, aggregator, bfg, result_cache_size=5):
        AbstractReader.__init__(self, aggregator)
        self.bfg = bfg
        self.result_cache_size = result_cache_size
        self.steps = map(list, si.status.get_info().steps)

    def get_next_sample(self, force):
        new_data = []
        while not self.bfg.results.empty():
            new_data.append(self.bfg.results.get(1))
        for cur_time, sample in new_data:
            if not cur_time in self.data_buffer.keys():
                self.data_queue.append(cur_time)
                self.data_buffer[cur_time] = []
            self.data_buffer[cur_time].append(list(sample))
        if self.data_queue and len(self.data_queue) >= self.result_cache_size:
            res = self.pop_second()
            res.overall.planned_requests = self.__get_expected_rps()
            return res
        else:
            return None

    def __get_expected_rps(self):
        '''
        Mark second with expected rps
        '''
        while self.steps and self.steps[0][1] < 1:
            self.steps.pop(0)
        
        if not self.steps:
            return 0
        else:
            self.steps[0][1] -= 1
            return self.steps[0][0]
########NEW FILE########
__FILENAME__ = widgets
import datetime
import time
from Tank.Plugins.ConsoleOnline import AbstractInfoWidget


class BFGInfoWidget(AbstractInfoWidget):

    ''' Console widget '''

    def __init__(self):
        AbstractInfoWidget.__init__(self)
        self.active_threads = 0
        self.instances = 0
        self.planned = 0
        self.RPS = 0
        self.selfload = 0
        self.time_lag = 0
        self.planned_rps_duration = 0

    def get_index(self):
        return 0

    def aggregate_second(self, second_aggregate_data):
        self.instances = second_aggregate_data.overall.active_threads
        if self.planned == second_aggregate_data.overall.planned_requests:
            self.planned_rps_duration += 1
        else:
            self.planned = second_aggregate_data.overall.planned_requests
            self.planned_rps_duration = 1

        self.RPS = second_aggregate_data.overall.RPS
        self.selfload = second_aggregate_data.overall.selfload
        self.time_lag = int(
            time.time() - time.mktime(second_aggregate_data.time.timetuple()))

    def render(self, screen):
        res = ''

        res += "Active instances: "
        res += str(self.instances)

        res += "\nPlanned requests: %s for %s\nActual responses: " % (
            self.planned, datetime.timedelta(seconds=self.planned_rps_duration))
        if not self.planned == self.RPS:
            res += screen.markup.YELLOW + str(self.RPS) + screen.markup.RESET
        else:
            res += str(self.RPS)

        res += "\n        Accuracy: "
        if self.selfload < 80:
            res += screen.markup.RED + \
                ('%.2f' % self.selfload) + screen.markup.RESET
        elif self.selfload < 95:
            res += screen.markup.YELLOW + \
                ('%.2f' % self.selfload) + screen.markup.RESET
        else:
            res += ('%.2f' % self.selfload)

        res += "%\n        Time lag: "
        res += str(datetime.timedelta(seconds=self.time_lag))

        return res

########NEW FILE########
__FILENAME__ = worker
import logging
import time
from Tank.stepper import StpdReader
import multiprocessing as mp
import threading as th
from Queue import Empty, Full

def signal_handler(signum, frame):
    pass


class BFG(object):
    def __init__(self, gun, instances, threads, stpd_filename, cached_stpd=False):
        self.log = logging.getLogger(__name__)
        self.log.info(
            "BFG using stpd from %s", stpd_filename)
        self.gun = gun
        self.instances = int(instances)
        self.threads = int(threads)
        self.results = mp.Queue()
        self.quit = mp.Event()
        self.task_queue = mp.Queue(1024)
        self.cached_stpd = cached_stpd
        self.stpd_filename = stpd_filename
        self.pool = [mp.Process(target=self._worker) for i in xrange(0, self.instances)]
        self.feeder = th.Thread(target=self._feed)

    def start(self):
        self.start_time = time.time()
        map(lambda x: x.start(), self.pool)
        self.feeder.start()

    def running(self):
        return not self.quit.is_set()

    def stop(self):
        self.quit.set()

    def _feed(self):
        plan = StpdReader(self.stpd_filename)
        if self.cached_stpd:
            plan = list(plan)
        for task in plan:
            if self.quit.is_set():
                self.log.info("Stop feeding: gonna quit")
                return
            self.task_queue.put(task)
        self.log.info("Feeded all data.")
        try:
            self.log.info("Waiting for workers")
            map(lambda x: x.join(), self.pool)
            self.log.info("All workers exited.")
            self.quit.set()
        except (KeyboardInterrupt, SystemExit):
            self.quit.set()

    def _worker(self):
        self.log.info("Started shooter process with %s threads..." % self.threads)
        pool = [th.Thread(target=self._thread_worker) for i in xrange(0, self.threads)]
        map(lambda x: x.start(), pool)
        map(lambda x: x.join(), pool)
        self.log.info("Exiting shooter process...")

    def _thread_worker(self):
        self.log.debug("Starting shooter thread...")
        while not self.quit.is_set():
            try:
                task = self.task_queue.get(timeout=1)
                ts, missile, marker = task
                planned_time = self.start_time + (ts / 1000.0)
                delay = planned_time - time.time()
                if delay > 0:
                    time.sleep(delay)
                cur_time, sample = self.gun.shoot(missile, marker)
                self.results.put((cur_time, sample), timeout=1)
            except (KeyboardInterrupt, SystemExit):
                self.quit.set()
            except Empty:
                self.log.info("Empty queue. Exiting thread.")
                return
            except Full:
                self.log.warning("Couldn't put to result queue because it's full")
        self.log.debug("Exiting shooter thread...")

########NEW FILE########
__FILENAME__ = BFG
from bfg import BFGPlugin

########NEW FILE########
__FILENAME__ = Codes
''' helper module to find http and net codes descriptions '''
import httplib

# FIXME: use the original python class for net codes
# from os import strerror
# from httplib import responses


### HTTP codes
HTTP = httplib.responses

### Extended list of HTTP status codes(WEBdav etc.)
### HTTP://en.wikipedia.org/wiki/List_of_HTTP_status_codes
WEBDAV = {
    102: 'Processing',
    103: 'Checkpoint',
    122: 'Request-URI too long',
    207: 'Multi-Status',
    226: 'IM Used',
    308: 'Resume Incomplete',
    418: 'I\'m a teapot',
    422: 'Unprocessable Entity',
    423: 'Locked',
    424: 'Failed Dependency',
    425: 'Unordered Collection',
    426: 'Upgrade Required',
    444: 'No Response',
    449: 'Retry With',
    450: 'Blocked by Windows Parental Controls',
    499: 'Client Closed Request',
    506: 'Variant Also Negotiates',
    507: 'Insufficient Storage',
    509: 'Bandwidth Limit Exceeded',
    510: 'Not Extended',
    598: 'network read timeout error',
    599: 'network connect timeout error',
    999: 'Common Failure',
}
HTTP.update(WEBDAV)


### NET codes
NET = {
    0: "Success",
	1: "Operation not permitted",
	2: "No such file or directory",
	3: "No such process",
	4: "Interrupted system call",
	5: "Input/output error",
	6: "No such device or address",
	7: "Argument list too long",
	8: "Exec format error",
	9: "Bad file descriptor",
	10: "No child processes",
	11: "Resource temporarily unavailable",
	12: "Cannot allocate memory",
	13: "Permission denied",
	14: "Bad address",
	15: "Block device required",
	16: "Device or resource busy",
	17: "File exists",
	18: "Invalid cross-device link",
	19: "No such device",
	20: "Not a directory",
	21: "Is a directory",
	22: "Invalid argument",
	23: "Too many open files in system",
	24: "Too many open files",
	25: "Inappropriate ioctl for device",
	26: "Text file busy",
	27: "File too large",
	28: "No space left on device",
	29: "Illegal seek",
	30: "Read-only file system",
	31: "Too many links",
	32: "Broken pipe",
	33: "Numerical argument out of domain",
	34: "Numerical result out of range",
	35: "Resource deadlock avoided",
	36: "File name too long",
	37: "No locks available",
	38: "Function not implemented",
	39: "Directory not empty",
	40: "Too many levels of symbolic links",
	41: "Unknown error 41",
	42: "No message of desired type",
	43: "Identifier removed",
	44: "Channel number out of range",
	45: "Level 2 not synchronized",
	46: "Level 3 halted",
	47: "Level 3 reset",
	48: "Link number out of range",
	49: "Protocol driver not attached",
	50: "No CSI structure available",
	51: "Level 2 halted",
	52: "Invalid exchange",
	53: "Invalid request descriptor",
	54: "Exchange full",
	55: "No anode",
	56: "Invalid request code",
	57: "Invalid slot",
	58: "Unknown error 58",
	59: "Bad font file format",
	60: "Device not a stream",
	61: "No data available",
	62: "Timer expired",
	63: "Out of streams resources",
	64: "Machine is not on the network",
	65: "Package not installed",
	66: "Object is remote",
	67: "Link has been severed",
	68: "Advertise error",
	69: "Srmount error",
	70: "Communication error on send",
	71: "Protocol error",
	72: "Multihop attempted",
	73: "RFS specific error",
	74: "Bad message",
	75: "Value too large for defined data type",
	76: "Name not unique on network",
	77: "File descriptor in bad state",
	78: "Remote address changed",
	79: "Can not access a needed shared library",
	80: "Accessing a corrupted shared library",
	81: ".lib section in a.out corrupted",
	82: "Attempting to link in too many shared libraries",
	83: "Cannot exec a shared library directly",
	84: "Invalid or incomplete multibyte or wide character",
	85: "Interrupted system call should be restarted",
	86: "Streams pipe error",
	87: "Too many users",
	88: "Socket operation on non-socket",
	89: "Destination address required",
	90: "Message too long",
	91: "Protocol wrong type for socket",
	92: "Protocol not available",
	93: "Protocol not supported",
	94: "Socket type not supported",
	95: "Operation not supported",
	96: "Protocol family not supported",
	97: "Address family not supported by protocol",
	98: "Address already in use",
	99: "Cannot assign requested address",
	100: "Network is down",
	101: "Network is unreachable",
	102: "Network dropped connection on reset",
	103: "Software caused connection abort",
	104: "Connection reset by peer",
	105: "No buffer space available",
	106: "Transport endpoint is already connected",
	107: "Transport endpoint is not connected",
	108: "Cannot send after transport endpoint shutdown",
	109: "Too many references: cannot splice",
	110: "Connection timed out",
	111: "Connection refused",
	112: "Host is down",
	113: "No route to host",
	114: "Operation already in progress",
	115: "Operation now in progress",
	116: "Stale NFS file handle",
	117: "Structure needs cleaning",
	118: "Not a XENIX named type file",
	119: "No XENIX semaphores available",
	120: "Is a named type file",
	121: "Remote I/O error",
	122: "Disk quota exceeded",
	123: "No medium found",
	124: "Wrong medium type",
	125: "Operation canceled",
	126: "Required key not available",
	127: "Key has expired",
	128: "Key has been revoked",
	129: "Key was rejected by service",
	130: "Owner died",
	131: "State not recoverable",
    999: 'Common Failure',
}

########NEW FILE########
__FILENAME__ = ConsoleOnline
''' Plugin provides fullscreen console '''
import logging
import sys
import traceback

from Tank.Plugins.Aggregator import AggregatorPlugin, AggregateResultListener
from Tank.Plugins.ConsoleScreen import Screen
from tankcore import AbstractPlugin


class ConsoleOnlinePlugin(AbstractPlugin, AggregateResultListener):
    ''' Console plugin '''
    SECTION = 'console'

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.screen = None
        self.render_exception = None
        self.console_markup = None
        self.remote_translator = None
        self.info_panel_width = '33'
        self.short_only = 0

    @staticmethod
    def get_key():
        return __file__

    def get_available_options(self):
        return ["info_panel_width", "short_only", "disable_all_colors", "disable_colors"]

    def configure(self):
        self.info_panel_width = self.get_option("info_panel_width", self.info_panel_width)
        self.short_only = int(self.get_option("short_only", '0'))
        if not int(self.get_option("disable_all_colors", '0')):
            self.console_markup = RealConsoleMarkup()
        else:
            self.console_markup = NoConsoleMarkup()
        for color in self.get_option("disable_colors", '').split(' '):
            self.console_markup.__dict__[color] = ''
        self.screen = Screen(self.info_panel_width, self.console_markup)

        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
            aggregator.add_result_listener(self)
        except KeyError:
            self.log.debug("No aggregator for console")
            self.screen.block_rows = []
            self.screen.info_panel_percent = 100

            #nf = fcntl.fcntl(sys.stdout.fileno(), fcntl.F_UNLCK)
            #fcntl.fcntl(sys.stdout.fileno(), fcntl.F_SETFL , nf | os.O_NONBLOCK )

    def is_test_finished(self):
        try:
            console_view = self.screen.render_screen().encode('utf-8')
        except Exception, ex:
            self.log.warn("Exception inside render: %s", traceback.format_exc(ex))
            self.render_exception = ex
            console_view = ""

        if console_view:
            if not self.short_only:
                self.log.debug("Writing console view to STDOUT")
                sys.stdout.write(self.console_markup.clear)
                sys.stdout.write(console_view)
                sys.stdout.write(self.console_markup.TOTAL_RESET)

            if self.remote_translator:
                self.remote_translator.send_console(console_view)

        return -1


    def aggregate_second(self, second_aggregate_data):
        self.screen.add_second_data(second_aggregate_data)
        if self.short_only:
            tpl = "Time: %s\tExpected RPS: %s\tActual RPS: %s\tActive Threads: %s\tAvg RT: %s"
            ovr = second_aggregate_data.overall  # just to see the next line in IDE
            data = (second_aggregate_data.time, ovr.planned_requests, ovr.RPS,
                    ovr.active_threads, ovr.avg_response_time)
            self.log.info(tpl % data)


    def add_info_widget(self, widget):
        ''' add right panel widget '''
        if not self.screen:
            self.log.debug("No screen instance to add widget")
        else:
            self.screen.add_info_widget(widget)


# ======================================================


class RealConsoleMarkup(object):
    '''    
    Took colors from here: https://www.siafoo.net/snippet/88
    '''
    WHITE_ON_BLACK = '\033[37;40m'
    TOTAL_RESET = '\033[0m'
    clear = "\x1b[2J\x1b[H"
    new_line = u"\n"

    YELLOW = '\033[1;33m'
    RED = '\033[1;31m'
    RED_DARK = '\033[31;3m'
    RESET = '\033[1;m'
    CYAN = "\033[1;36m"
    GREEN = "\033[1;32m"
    WHITE = "\033[1;37m"
    MAGENTA = '\033[1;35m'
    BG_MAGENTA = '\033[1;45m'
    BG_GREEN = '\033[1;42m'
    BG_BROWN = '\033[1;43m'
    BG_CYAN = '\033[1;46m'

    def clean_markup(self, orig_str):
        ''' clean markup from string '''
        for val in [self.YELLOW, self.RED, self.RESET,
                    self.CYAN, self.BG_MAGENTA, self.WHITE,
                    self.BG_GREEN, self.GREEN, self.BG_BROWN,
                    self.RED_DARK, self.MAGENTA, self.BG_CYAN]:
            orig_str = orig_str.replace(val, '')
        return orig_str


# ======================================================
# FIXME: 3 better way to have it?

class NoConsoleMarkup(RealConsoleMarkup):
    ''' all colors are disabled '''
    WHITE_ON_BLACK = ''
    TOTAL_RESET = ''
    clear = ""
    new_line = u"\n"

    YELLOW = ''
    RED = ''
    RED_DARK = ''
    RESET = ''
    CYAN = ""
    GREEN = ""
    WHITE = ""
    MAGENTA = ''
    BG_MAGENTA = ''
    BG_GREEN = ''
    BG_BROWN = ''
    BG_CYAN = ''


# ======================================================


class AbstractInfoWidget:
    ''' parent class for all right panel widgets '''

    def __init__(self):
        self.log = logging.getLogger(__name__)

    def render(self, screen):
        raise NotImplementedError()

    def get_index(self):
        ''' get vertical priority index '''
        return 0



########NEW FILE########
__FILENAME__ = ConsoleScreen
''' Classes to build full console screen '''
import copy
import fcntl
import logging
import math
import os
import struct
import termios

from Tank.Plugins import Codes


def get_terminal_size():
    '''
    Gets width and height of terminal viewport
    '''
    default_size = (60, 140)
    env = os.environ

    def ioctl_gwinsz(file_d):
        '''
        Helper to get console size
        '''
        try:
            sizes = struct.unpack('hh', fcntl.ioctl(file_d, termios.TIOCGWINSZ, '1234'))
        except Exception:
            sizes = default_size
        return sizes

    sizes = ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)
    if not sizes:
        try:
            file_d = os.open(os.ctermid(), os.O_RDONLY)
            sizes = ioctl_gwinsz(file_d)
            os.close(file_d.fileno())
        except Exception:
            pass
    if not sizes:
        try:
            sizes = (env['LINES'], env['COLUMNS'])
        except Exception:
            sizes = default_size
    return int(sizes[1]), int(sizes[0])


def krutilka():
    pos = 0
    chars = "|/-\\"
    while True:
        yield chars[pos]
        pos += 1
        if pos >= len(chars):
            pos = 0


class Screen(object):
    '''     Console screen renderer class    '''
    RIGHT_PANEL_SEPARATOR = ' . '

    def __init__(self, info_panel_width, markup_provider):
        self.log = logging.getLogger(__name__)
        self.info_panel_percent = int(info_panel_width)
        self.info_widgets = {}
        self.markup = markup_provider
        self.term_height = 60
        self.term_width = 120
        self.right_panel_width = 10
        self.left_panel_width = self.term_width - self.right_panel_width - len(self.RIGHT_PANEL_SEPARATOR)

        block1 = VerticalBlock(CurrentHTTPBlock(self), CurrentNetBlock(self))
        block2 = VerticalBlock(block1, CasesBlock(self))
        block3 = VerticalBlock(block2, TotalQuantilesBlock(self))
        block4 = VerticalBlock(block3, AnswSizesBlock(self))
        block5 = VerticalBlock(block4, AvgTimesBlock(self))

        self.block_rows = [[CurrentTimesDistBlock(self), block5]]

    def __get_right_line(self, widget_output):
        '''        Gets next line for right panel        '''
        right_line = ''
        if widget_output:
            right_line = widget_output.pop(0)
            if len(right_line) > self.right_panel_width:
                right_line_plain = self.markup.clean_markup(right_line)
                if len(right_line_plain) > self.right_panel_width:
                    right_line = right_line[:self.right_panel_width] + self.markup.RESET
        return right_line


    def __render_left_panel(self):
        ''' Render left blocks '''
        self.log.debug("Rendering left blocks")
        lines = []
        for row in self.block_rows:
            space_left = self.left_panel_width
            # render blocks
            for block in row:
                block.render()
                space_left -= block.width

            # merge blocks output into row
            space = ' ' * int(math.floor(float(space_left) / (len(row) + 1)))
            had_lines = True
            while had_lines:
                had_lines = False
                line = space
                for block in row:
                    if block.lines:
                        block_line = block.lines.pop(0)
                        line += block_line + ' ' * (block.width - len(self.markup.clean_markup(block_line)))
                        had_lines = True
                    else:
                        line += ' ' * block.width
                    line += space
                lines.append(line)
                #lines.append(".  " * (1 + self.left_panel_width / 3))
                #lines.append("")
        return lines


    def render_screen(self):
        '''        Main method to render screen view        '''
        self.term_width, self.term_height = get_terminal_size()
        self.log.debug("Terminal size: %sx%s", self.term_width, self.term_height)
        self.right_panel_width = int(
            (self.term_width - len(self.RIGHT_PANEL_SEPARATOR)) * (float(self.info_panel_percent) / 100)) - 1
        if self.right_panel_width > 0:
            self.left_panel_width = self.term_width - self.right_panel_width - len(self.RIGHT_PANEL_SEPARATOR) - 2
        else:
            self.right_panel_width = 0
            self.left_panel_width = self.term_width - 1
        self.log.debug("Left/right panels width: %s/%s", self.left_panel_width, self.right_panel_width)

        widget_output = []
        if self.right_panel_width:
            widget_output = []
            for index, widget in sorted(self.info_widgets.iteritems(), key=lambda (k, v): (v.get_index(), k)):
                self.log.debug("Rendering info widget #%s: %s", index, widget)
                widget_out = widget.render(self).strip()
                if widget_out:
                    widget_output += widget_out.split("\n")
                    widget_output += [""]

        left_lines = self.__render_left_panel()

        self.log.debug("Composing final screen output")
        output = []
        for line_no in range(1, self.term_height):
            line = " "

            if line_no > 1 and left_lines:
                left_line = left_lines.pop(0)
                left_line_plain = self.markup.clean_markup(left_line)
                if len(left_line) > self.left_panel_width:
                    if len(left_line_plain) > self.left_panel_width:
                        left_line = left_line[:self.left_panel_width] + self.markup.RESET

                left_line += (' ' * (self.left_panel_width - len(left_line_plain)))
                line += left_line
            else:
                line += ' ' * self.left_panel_width
            if self.right_panel_width:
                line += self.RIGHT_PANEL_SEPARATOR
                right_line = self.__get_right_line(widget_output)
                line += right_line

            output.append(line)
        return self.markup.new_line.join(output) + self.markup.new_line


    def add_info_widget(self, widget):
        '''
        Add widget string to right panel of the screen
        '''
        index = widget.get_index()
        while index in self.info_widgets.keys():
            index += 1
        self.info_widgets[widget.get_index()] = widget


    def add_second_data(self, data):
        '''
        Notification method about new aggregator data
        '''
        for row in self.block_rows:
            for block in row:
                block.add_second(data)


                # ======================================================


class AbstractBlock:
    '''
    Parent class for all left panel blocks
    '''

    def __init__(self, screen):
        self.log = logging.getLogger(__name__)
        self.lines = []
        self.width = 0
        self.screen = screen

    def add_second(self, data):
        '''
        Notification about new aggregate data
        '''
        pass

    def render(self):
        '''
        Render method, fills .lines and .width properties with rendered data
        '''
        raise RuntimeError("Abstract method needs to be overridden")


# ======================================================

class VerticalBlock(AbstractBlock):
    '''
    Block to merge two other blocks vertically
    '''

    def __init__(self, top_block, bottom_block):
        AbstractBlock.__init__(self, None)
        self.top = top_block
        self.bottom = bottom_block

    def render(self):
        self.top.render()
        self.bottom.render()
        self.width = max(self.top.width, self.bottom.width)

        self.lines = []
        for line in self.top.lines:
            self.lines.append(line + ' ' * (self.width - self.top.width))

        if self.top.lines and self.bottom.lines:
            self.lines.append(' ' * self.width)

        for line in self.bottom.lines:
            self.lines.append(line + ' ' * (self.width - self.bottom.width))

    def add_second(self, data):
        self.top.add_second(data)
        self.bottom.add_second(data)


# ======================================================
class CurrentTimesDistBlock(AbstractBlock):
    '''
    Detailed distribution for current RPS
    '''

    def __init__(self, screen):
        AbstractBlock.__init__(self, screen)
        self.current_codes = {}
        self.current_rps = 0
        self.current_duration = 0
        self.current_count = 0
        self.current_max_rt = 0

    def add_second(self, data):
        self.log.debug("Arrived times dist: %s", data.overall.times_dist)
        rps = data.overall.planned_requests
        if not self.current_rps == rps:
            self.current_rps = rps
            self.current_count = 0
            self.current_max_rt = 0
            self.current_codes = {}
            self.current_duration = 0
        for item in data.overall.times_dist:
            self.current_count += item['count']
            self.current_max_rt = max(self.current_max_rt, item['to'])
            if item['from'] in self.current_codes.keys():
                self.current_codes[item['from']]['count'] += item['count']
            else:
                self.current_codes[item['from']] = copy.deepcopy(item)
        self.current_duration += 1
        self.log.debug("Current times dist: %s", self.current_codes)

    def render(self):
        self.lines = []
        quan = 0
        current_times = sorted(self.current_codes.iteritems())
        while current_times:
            line, quan = self.__format_line(current_times, quan)
            self.width = max(self.width, len(line))
            self.lines.append(line)
        self.lines.reverse()
        self.lines = [self.screen.markup.WHITE + 'Times for %s RPS:' % self.current_rps + self.screen.markup.RESET] \
                     + self.lines
        self.lines.append("")

        count_len = str(len(str(self.current_count)))
        tpl = ' %' + count_len + 'd %6.2f%%: Total'
        if self.current_count:
            self.lines.append(tpl % (self.current_count, 100))
        self.width = max(self.width, len(self.lines[0]))

    def __format_line(self, current_times, quan):
        ''' Format dist line '''
        left_line = ''
        if current_times:
            item = current_times.pop(0)[1]
            if self.current_count:
                perc = float(item['count']) / self.current_count
            else:
                perc = 1
            quan += perc
            # 30691    9.26%: 010  --  025       68.03%  <  025
            count_len = str(len(str(self.current_count)))
            timing_len = str(len(str(self.current_max_rt)))
            tpl = '  %' + count_len + 'd %6.2f%%: %' + timing_len + 'd -- %' + timing_len + 'd  %6.2f%% < %' + timing_len + 'd'
            data = (item['count'], perc * 100, item['from'], item['to'], quan * 100, item['to'])
            left_line = tpl % data
        return left_line, quan


# ======================================================

class CurrentHTTPBlock(AbstractBlock):
    ''' Http codes with highlight'''
    TITLE = 'HTTP for %s RPS:  '

    def __init__(self, screen):
        AbstractBlock.__init__(self, screen)
        self.times_dist = {}
        self.current_rps = 0
        self.total_count = 0
        self.highlight_codes = []


    def process_dist(self, rps, codes_dist):
        '''
        Analyze arrived codes distribution and highlight arrived
        '''
        self.log.debug("Arrived codes data: %s", codes_dist)
        self.highlight_codes = []
        if not self.current_rps == rps:
            self.current_rps = rps
            self.total_count = 0
            for key in self.times_dist.keys():
                self.times_dist[key] = 0

        for code, count in codes_dist.items():
            self.total_count += count
            self.highlight_codes.append(code)
            if code in self.times_dist.keys():
                self.times_dist[code] += count
            else:
                self.times_dist[code] = count

        self.log.debug("Current codes dist: %s", self.times_dist)

    def add_second(self, data):
        rps = data.overall.planned_requests
        codes_dist = data.overall.http_codes
        self.process_dist(rps, codes_dist)

    def render(self):
        self.lines = [self.screen.markup.WHITE + self.TITLE % self.current_rps + self.screen.markup.RESET]
        #self.width = len(self.lines[0])
        for code, count in sorted(self.times_dist.iteritems()):
            line = self.format_line(code, count)
            self.width = max(self.width, len(self.screen.markup.clean_markup(line)))
            self.lines.append(line)

    def format_line(self, code, count):
        ''' format line for display '''
        if self.total_count:
            perc = float(count) / self.total_count
        else:
            perc = 1
        # 11083   5.07%: 304 Not Modified         
        count_len = str(len(str(self.total_count)))
        if int(code) in Codes.HTTP:
            code_desc = Codes.HTTP[int(code)]
        else:
            code_desc = "N/A"
        tpl = '  %' + count_len + 'd %6.2f%%: %s %s'
        data = (count, perc * 100, code, code_desc)
        left_line = tpl % data

        if code in self.highlight_codes:
            code = str(code)
            if code[0] == '2':
                left_line = self.screen.markup.GREEN + left_line + self.screen.markup.RESET
            elif code[0] == '3':
                left_line = self.screen.markup.CYAN + left_line + self.screen.markup.RESET
            elif code[0] == '4':
                left_line = self.screen.markup.YELLOW + left_line + self.screen.markup.RESET
            elif code[0] == '5':
                left_line = self.screen.markup.RED + left_line + self.screen.markup.RESET
            else:
                left_line = self.screen.markup.MAGENTA + left_line + self.screen.markup.RESET

        return left_line


# ======================================================

class CurrentNetBlock(CurrentHTTPBlock):
    ''' NET codes with highlight'''
    TITLE = ' NET for %s RPS:  '

    def add_second(self, data):
        rps = data.overall.planned_requests
        codes_dist = copy.deepcopy(data.overall.net_codes)
        self.process_dist(rps, codes_dist)

    def format_line(self, code, count):
        if self.total_count:
            perc = float(count) / self.total_count
        else:
            perc = 1
        # 11083   5.07%: 304 Not Modified         
        count_len = str(len(str(self.total_count)))
        if int(code) in Codes.NET:
            code_desc = Codes.NET[int(code)]
        else:
            code_desc = "N/A"
        tpl = '  %' + count_len + 'd %6.2f%%: %s %s'
        data = (count, perc * 100, code, code_desc)
        left_line = tpl % data

        if code in self.highlight_codes:
            if code == '0':
                left_line = self.screen.markup.GREEN + left_line + self.screen.markup.RESET
            else:
                left_line = self.screen.markup.RED + left_line + self.screen.markup.RESET

        return left_line


# ======================================================

class TotalQuantilesBlock(AbstractBlock):
    ''' Total test quantiles '''

    def __init__(self, screen):
        AbstractBlock.__init__(self, screen)
        self.total_count = 0
        self.current_max_rt = 0
        self.quantiles = {}

    def add_second(self, data):
        self.quantiles = data.cumulative.quantiles

    def render(self):
        self.lines = []
        for quant in sorted(self.quantiles):
            line = self.__format_line(quant, self.quantiles[quant])
            self.width = max(self.width, len(line))
            self.lines.append(line)

        self.lines.reverse()
        self.lines = [self.screen.markup.WHITE + 'Cumulative Percentiles:' + self.screen.markup.RESET] + self.lines
        self.width = max(self.width, len(self.screen.markup.clean_markup(self.lines[0])))

    def __format_line(self, quan, timing):
        ''' Format line '''
        timing_len = str(len(str(self.current_max_rt)))
        tpl = '   %3d%% < %' + timing_len + 'd ms'
        data = (quan, timing)
        left_line = tpl % data
        return left_line


# ======================================================


class AnswSizesBlock(AbstractBlock):
    ''' Answer sizes, if available '''

    def __init__(self, screen):
        AbstractBlock.__init__(self, screen)
        self.sum_in = 0
        self.current_rps = -1
        self.sum_out = 0
        self.count = 0
        self.header = screen.markup.WHITE + 'Request/Response Sizes:' + screen.markup.RESET
        self.cur_count = 0
        self.cur_in = 0
        self.cur_out = 0

    def render(self):
        self.lines = [self.header]
        if self.count:
            self.lines.append("   Avg Request at %s RPS: %d bytes" % (self.current_rps, self.sum_out / self.count))
            self.lines.append("  Avg Response at %s RPS: %d bytes" % (self.current_rps, self.sum_in / self.count))
            self.lines.append("")
        if self.cur_count:
            self.lines.append("   Last Avg Request: %d bytes" % (self.cur_out / self.cur_count))
            self.lines.append("  Last Avg Response: %d bytes" % (self.cur_in / self.cur_count))
        else:
            self.lines.append("")
            self.lines.append("")
        for line in self.lines:
            self.width = max(self.width, len(self.screen.markup.clean_markup(line)))

    def add_second(self, data):
        if data.overall.planned_requests != self.current_rps:
            self.current_rps = data.overall.planned_requests
            self.sum_in = 0
            self.sum_out = 0
            self.count = 0

        self.count += data.overall.RPS
        self.sum_in += data.overall.input
        self.sum_out += data.overall.output

        self.cur_in = data.overall.input
        self.cur_out = data.overall.output
        self.cur_count = data.overall.RPS


# ======================================================


class AvgTimesBlock(AbstractBlock):
    ''' Average times breakdown '''

    def __init__(self, screen):
        AbstractBlock.__init__(self, screen)
        self.rps_connect = 0
        self.rps_send = 0
        self.rps_latency = 0
        self.rps_receive = 0
        self.rps_overall = 0
        self.rps_count = 0
        self.current_rps = 0

        self.all_connect = 0
        self.all_send = 0
        self.all_latency = 0
        self.all_receive = 0
        self.all_overall = 0
        self.all_count = 0

        self.last_connect = 0
        self.last_send = 0
        self.last_latency = 0
        self.last_receive = 0
        self.last_overall = 0
        self.last_count = 0

        self.header = 'Avg Times (all / %s RPS / last):'

    def add_second(self, data):
        if self.current_rps != data.overall.planned_requests:
            self.current_rps = data.overall.planned_requests
            self.rps_connect = 0
            self.rps_send = 0
            self.rps_latency = 0
            self.rps_receive = 0
            self.rps_overall = 0
            self.rps_count = 0

        self.rps_connect += data.overall.avg_connect_time * data.overall.RPS
        self.rps_send += data.overall.avg_send_time * data.overall.RPS
        self.rps_latency += data.overall.avg_latency * data.overall.RPS
        self.rps_receive += data.overall.avg_receive_time * data.overall.RPS
        self.rps_overall += data.overall.avg_response_time * data.overall.RPS
        self.rps_count += data.overall.RPS

        self.all_connect += data.overall.avg_connect_time * data.overall.RPS
        self.all_send += data.overall.avg_send_time * data.overall.RPS
        self.all_latency += data.overall.avg_latency * data.overall.RPS
        self.all_receive += data.overall.avg_receive_time * data.overall.RPS
        self.all_overall += data.overall.avg_response_time * data.overall.RPS
        self.all_count += data.overall.RPS

        self.last_connect = data.overall.avg_connect_time * data.overall.RPS
        self.last_send = data.overall.avg_send_time * data.overall.RPS
        self.last_latency = data.overall.avg_latency * data.overall.RPS
        self.last_receive = data.overall.avg_receive_time * data.overall.RPS
        self.last_overall = data.overall.avg_response_time * data.overall.RPS
        self.last_count = data.overall.RPS

    def render(self):
        self.lines = [self.screen.markup.WHITE + self.header % self.current_rps + self.screen.markup.RESET]
        if self.last_count:
            len_all = str(
                len(str(max([self.all_connect, self.all_latency, self.all_overall, self.all_receive, self.all_send]))))
            len_rps = str(
                len(str(max([self.rps_connect, self.rps_latency, self.rps_overall, self.rps_receive, self.rps_send]))))
            len_last = str(len(
                str(max([self.last_connect, self.last_latency, self.last_overall, self.last_receive, self.last_send]))))
            tpl = "%" + len_all + "d / %" + len_rps + "d / %" + len_last + "d"
            self.lines.append("  Overall: " + tpl % (
                float(self.all_overall) / self.all_count, float(self.rps_overall) / self.rps_count,
                float(self.last_overall) / self.last_count))
            self.lines.append("  Connect: " + tpl % (
                float(self.all_connect) / self.all_count, float(self.rps_connect) / self.rps_count,
                float(self.last_connect) / self.last_count))
            self.lines.append("     Send: " + tpl % (
                float(self.all_send) / self.all_count, float(self.rps_send) / self.rps_count,
                float(self.rps_send) / self.rps_count))
            self.lines.append("  Latency: " + tpl % (
                float(self.all_latency) / self.all_count, float(self.rps_latency) / self.rps_count,
                float(self.rps_latency) / self.rps_count))
            self.lines.append("  Receive: " + tpl % (
                float(self.all_receive) / self.all_count, float(self.rps_receive) / self.rps_count,
                float(self.rps_receive) / self.rps_count))
        else:
            self.lines.append("")
            self.lines.append("")
            self.lines.append("")
            self.lines.append("")
            self.lines.append("")
        for line in self.lines:
            self.width = max(self.width, len(self.screen.markup.clean_markup(line)))


# ======================================================


class CasesBlock(AbstractBlock):
    '''     Cases info    '''

    def __init__(self, screen):
        AbstractBlock.__init__(self, screen)
        self.cases = {}
        self.count = 0
        self.header = "Cumulative Cases Info:"
        self.highlight_cases = []
        self.max_case_len = 0

    def add_second(self, data):
        self.highlight_cases = []
        for name, case in data.cases.iteritems():
            self.highlight_cases.append(name)
            if not name in self.cases.keys():
                self.cases[name] = [0, 0]
                self.max_case_len = max(self.max_case_len, len(name))
            self.cases[name][0] += case.RPS
            self.cases[name][1] += case.avg_response_time * case.RPS
            self.count += case.RPS

    def render(self):
        self.lines = [self.screen.markup.WHITE + self.header + self.screen.markup.RESET]
        tpl = "  %s: %" + str(len(str(self.count))) + "d %5.2f%% / avg %.1f ms"
        for name, (count, resp_time) in sorted(self.cases.iteritems()):
            line = tpl % (" " * (self.max_case_len - len(name)) + name, count, 100 * float(count) / self.count,
                          float(resp_time) / count)
            if name in self.highlight_cases:
                self.lines.append(self.screen.markup.CYAN + line + self.screen.markup.RESET)
            else:
                self.lines.append(line)

        for line in self.lines:
            self.width = max(self.width, len(self.screen.markup.clean_markup(line)))

########NEW FILE########
__FILENAME__ = GraphiteUploader
'''Graphite Uploader plugin that sends aggregated data to Graphite server'''

from Tank.Plugins.Aggregator import AggregateResultListener, AggregatorPlugin
from tankcore import AbstractPlugin
import logging
import socket
import string
import time
import datetime
import os

class GraphiteUploaderPlugin(AbstractPlugin, AggregateResultListener):
    '''Graphite data uploader'''
    
    SECTION = 'graphite'

    @staticmethod
    def get_key():
        return __file__
    
    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.graphite_client = None

    def get_available_options(self):
        return ["address", "port", "prefix", "web_port"]

    def start_test(self):
        start_time = datetime.datetime.now()
        self.start_time = start_time.strftime("%H:%M%%20%Y%m%d")

    def end_test(self, retcode):
        end_time = datetime.datetime.now() + datetime.timedelta(minutes = 1)
        self.end_time = end_time.strftime("%H:%M%%20%Y%m%d")
        return retcode

    def configure(self):
        '''Read configuration'''
        self.address = self.get_option("address", "")
        if self.address == "": 
            self.log.warning("Graphite uploader is not configured and will not send any data")
        else:
            port = self.get_option("port", "2003")
            self.web_port = self.get_option("web_port", "8080")
            self.prefix = self.get_option("prefix", "one_sec.yandex_tank")
            default_template = "/graphite.tpl"
            if self.get_option("js", "1") == "1":
                default_template = "/graphite-js.tpl"
            self.template = self.get_option("template", os.path.dirname(__file__) + default_template)
            self.graphite_client = GraphiteClient(self.prefix, self.address, port)
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
            aggregator.add_result_listener(self)
            
    def aggregate_second(self, data):
        """
        @data: SecondAggregateData
        """
        #TODO: Use ts from data
        ts = int(time.mktime(data.time.timetuple()))
        if self.graphite_client:
            results = {}
            overall = GraphiteUploaderPlugin.__flatten(data.overall.__dict__, "overall")
            cumulative = GraphiteUploaderPlugin.__flatten(data.cumulative.__dict__, "cumulative")
            results.update(overall)
            results.update(cumulative)
            for marker in data.cases.keys():
                results.update(GraphiteUploaderPlugin.__flatten(data.cases[marker].__dict__, 'markers.%s' % marker))
            self.graphite_client.submit(results)

    def post_process(self, retcode):
        if self.graphite_client:
            template = open(self.template, 'r').read()
            graphite_html = self.core.mkstemp(".html", "graphite_")
            self.core.add_artifact_file(graphite_html)
            with open(graphite_html, 'w') as graphite_html_file:
                graphite_html_file.write(
                    string.Template(template).safe_substitute(
                        host=self.address,
                        width=1000,
                        height=400,
                        start_time=self.start_time,
                        end_time=self.end_time,
                        prefix=self.prefix,
                        web_port=self.web_port,
                    )
                )
        return retcode

    @staticmethod
    def __flatten(dic, prefix):
        '''recursively pass through a dict and flatten it\'s "internal" dicts'''
        results = {}
        if dic != None:
            try:
                for key in dic.keys():
                    if type(dic[key]) in [float, int]:
                        results["%s.%s" % (prefix, str(key).translate(string.maketrans(".", "_")))] = dic[key]
                    elif type(dic[key] in [dict]):
                        results.update(GraphiteUploaderPlugin.__flatten(dic[key], "%s.%s" % (prefix, key.translate(string.maketrans(".", "_")))))
            except AttributeError:
                pass
        return results


    
class GraphiteClient(object):
    '''Graphite client that writes metrics to Graphite server'''

    def __init__(self, prefix, address, port):
        self.address = address
        self.port = port
        self.prefix = prefix
        self.log = logging.getLogger(__name__)
        self.log.debug("Created a Graphite listener with address = '%s', port = '%s', prefix = '%s'" % (address, port, prefix))

    def submit(self, results):
        '''publish results to Graphite'''
        self.log.debug("Trying to send metrics to server...")
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect((self.address, int(self.port)))
            for metric in results.keys():
                sock.sendall("%s.%s\t%s\t%d\n" % \
                    (self.prefix, metric, results[metric], time.time()))
            sock.close()
            self.log.debug("Sent metrics to graphite server")
        except Exception, exc:
            self.log.exception("Failed to send metrics to graphite: %s", exc)

########NEW FILE########
__FILENAME__ = JMeter
''' jmeter load generator support '''
import logging
import os
import signal
import subprocess
import tempfile
import time
import datetime
import json

from Tank.Plugins.Aggregator import AbstractReader, AggregatorPlugin, \
    AggregateResultListener, SecondAggregateDataItem
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin, AbstractInfoWidget
from tankcore import AbstractPlugin
import tankcore
from Tank.Plugins import ConsoleScreen


class JMeterPlugin(AbstractPlugin):
    ''' JMeter tank plugin '''
    SECTION = 'jmeter'

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.jmeter_process = None
        self.args = None
        self.original_jmx = None
        self.jtl_file = None
        self.jmx = None
        self.user_args = None
        self.jmeter_path = None
        self.jmeter_log = None
        self.start_time = time.time()
        self.jmeter_buffer_size = None
        self.use_argentum = None

    @staticmethod
    def get_key():
        return __file__

    def get_available_options(self):
        return ["jmx", "args", "jmeter_path", "buffer_size", "buffered_seconds", "use_argentum"]

    def configure(self):
        self.original_jmx = self.get_option("jmx")
        self.core.add_artifact_file(self.original_jmx, True)
        self.jtl_file = self.core.mkstemp('.jtl', 'jmeter_')
        self.core.add_artifact_file(self.jtl_file)
        self.core.add_artifact_file(self.jmx)
        self.user_args = self.get_option("args", '')
        self.jmeter_path = self.get_option("jmeter_path", 'jmeter')
        self.jmeter_log = self.core.mkstemp('.log', 'jmeter_')
        self.jmeter_buffer_size = int(self.get_option('buffer_size', 
            self.get_option('buffered_seconds', '3')))
        self.core.add_artifact_file(self.jmeter_log, True)
        self.use_argentum = eval(self.get_option('use_argentum', 'False'))
        self.jmx = self.__add_jmeter_components(self.original_jmx, self.jtl_file, self._get_variables())

    def prepare_test(self):
        self.args = [self.jmeter_path, "-n", "-t", self.jmx, '-j', self.jmeter_log,
                     '-Jjmeter.save.saveservice.default_delimiter=\\t']
        self.args += tankcore.splitstring(self.user_args)

        aggregator = None
        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
        except Exception, ex:
            self.log.warning("No aggregator found: %s", ex)

        if aggregator:
            aggregator.reader = JMeterReader(aggregator, self)
            aggregator.reader.buffer_size = self.jmeter_buffer_size
            aggregator.reader.use_argentum = self.use_argentum

        try:
            console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
        except Exception, ex:
            self.log.debug("Console not found: %s", ex)
            console = None

        if console:
            widget = JMeterInfoWidget(self)
            console.add_info_widget(widget)
            if aggregator:
                aggregator.add_result_listener(widget)


    def start_test(self):
        self.log.info("Starting %s with arguments: %s", self.jmeter_path, self.args)
        self.jmeter_process = subprocess.Popen(self.args, executable=self.jmeter_path, preexec_fn=os.setsid,
                                               close_fds=True)  # stderr=subprocess.PIPE, stdout=subprocess.PIPE,
        self.start_time = time.time()


    def is_test_finished(self):
        retcode = self.jmeter_process.poll()
        if retcode is not None:
            self.log.info("JMeter process finished with exit code: %s", retcode)
            return retcode
        else:
            return -1


    def end_test(self, retcode):
        if self.jmeter_process:
            self.log.info("Terminating jmeter process group with PID %s", self.jmeter_process.pid)
            try:
                os.killpg(self.jmeter_process.pid, signal.SIGTERM)
            except OSError, exc:
                self.log.debug("Seems JMeter exited itself: %s", exc)
                # Utils.log_stdout_stderr(self.log, self.jmeter_process.stdout, self.jmeter_process.stderr, "jmeter")

        self.core.add_artifact_file(self.jmeter_log)
        return retcode


    def __add_jmeter_components(self, jmx, jtl, variables):
        ''' Genius idea by Alexey Lavrenyuk '''
        self.log.debug("Original JMX: %s", os.path.realpath(jmx))
        with open(jmx, 'r') as src_jmx:
            source_lines = src_jmx.readlines()

        try:
            closing = source_lines.pop(-1)
            closing = source_lines.pop(-1) + closing
            closing = source_lines.pop(-1) + closing
            self.log.debug("Closing statement: %s", closing)
        except Exception, exc:
            raise RuntimeError("Failed to find the end of JMX XML: %s" % exc)

        tpl_filepath = '/jmeter/jmeter_writer.xml'

        if self.use_argentum:
            self.log.warn("You are using argentum aggregator for JMeter. Be careful.")
            tpl_filepath = '/jmeter/jmeter_argentum.xml'

        with open(os.path.dirname(__file__) + tpl_filepath, 'r') as tpl_file:
            tpl = tpl_file.read()

        with open(os.path.dirname(__file__) + '/jmeter/jmeter_var_template.xml', 'r') as tpl_file:
            udv_tpl = tpl_file.read()

        udv_set = []
        for var_name, var_value in variables.iteritems():
            udv_set.append(udv_tpl % (var_name, var_name, var_value))

        try:
            file_handle, new_file = tempfile.mkstemp('.jmx', 'modified_', os.path.dirname(os.path.realpath(jmx)))
        except OSError, exc:
            self.log.debug("Can't create modified jmx near original: %s", exc)
            file_handle, new_file = tempfile.mkstemp('.jmx', 'modified_', self.core.artifacts_base_dir)
        self.log.debug("Modified JMX: %s", new_file)
        os.write(file_handle, ''.join(source_lines))

        if self.use_argentum:
            os.write(file_handle, tpl % (self.jmeter_buffer_size, jtl, "", ""))
        else:
            os.write(file_handle, tpl % (jtl, "\n".join(udv_set)))

        os.write(file_handle, closing)
        os.close(file_handle)
        return new_file

    def _get_variables(self):
        res = {}
        for option in self.core.config.get_options(self.SECTION):
            if option[0] not in self.get_available_options():
                res[option[0]] = option[1]
        logging.debug("Variables: %s", res)
        return res


class JMeterReader(AbstractReader):
    ''' JTL files reader '''
    KNOWN_EXC = {
        "java.net.NoRouteToHostException": 113,
        "java.net.ConnectException": 110,
        "java.net.BindException": 99,
        "java.net.PortUnreachableException": 101,
        "java.net.ProtocolException": 71,
        "java.net.SocketException": 32,
        "java.net.SocketTimeoutException": 110,
        "java.net.UnknownHostException": 14,
        "java.io.IOException": 5,
        "org.apache.http.conn.ConnectTimeoutException": 110,
    }

    def __init__(self, owner, jmeter):
        AbstractReader.__init__(self, owner)
        self.jmeter = jmeter
        self.results = None
        self.partial_buffer = ''
        self.buffer_size = 3
        self.use_argentum = False

    def check_open_files(self):
        if not self.results and os.path.exists(self.jmeter.jtl_file):
            self.log.debug("Opening jmeter out file: %s", self.jmeter.jtl_file)
            self.results = open(self.jmeter.jtl_file, 'r')

    def close_files(self):
        if self.results:
            self.results.close()

    def get_next_sample(self, force):
        if self.use_argentum:
            return self.get_next_sample_from_ag(force)
        else:
            return self.get_next_sample_from_sdw(force)

    def get_next_sample_from_ag(self, force):
        if self.results:
            read_line = self.results.readline()
            second = None
            if len(read_line) == 0:
                return None
            else:
                try:
                    if self.partial_buffer != '':
                        read_line = str(self.partial_buffer + read_line)
                        self.partial_buffer = ''

                    second = json.loads(read_line, 'ascii')
                except ValueError, e:
                    # not-ended second json-object
                    self.partial_buffer = read_line
                    self.log.warn('bad json-object', e)
                    return None
                else:
                    # good json-object. parse it!
                    second_ag = self.get_zero_sample(datetime.datetime.fromtimestamp(second['second']))
                    second_ag.overall.avg_connect_time = 0
                    second_ag.overall.avg_send_time = 0
                    second_ag.overall.avg_receive_time = second['avg_rt'] - second['avg_lt']
                    second_ag.overall.avg_response_time = second['avg_rt']
                    second_ag.overall.avg_latency = second['avg_lt']
                    second_ag.overall.RPS = second['th']
                    second_ag.overall.active_threads = second['active_threads']
                    second_ag.overall.times_dist = second['interval_dist']
                    second_ag.overall.input = second['traffic']['inbound']
                    second_ag.overall.output = second['traffic']['outbound']

                    rc_map = dict()
                    for item in second['rc'].items():
                        rc_map[self.exc_to_http(item[0])] = item[1]
                    second_ag.overall.http_codes = rc_map

                    for percentile in second['percentile'].keys():
                        second_ag.overall.quantiles[int(float(percentile))] = second['percentile'][percentile]
                        second_ag.cumulative.quantiles[int(float(percentile))] = second['cumulative_percentile'][
                            percentile]

                    self.cumulative.add_data(second_ag.overall)

                    for sampler in second['samplers'].keys():
                        sampler_ag_data_item = SecondAggregateDataItem()
                        sampler_ag_data_item.case = sampler
                        sampler_ag_data_item.active_threads = second['active_threads']
                        sampler_ag_data_item.RPS = int(second['samplers'][sampler])
                        sampler_ag_data_item.times_dist = second['sampler_interval_dist'][sampler]

                        sampler_ag_data_item.quantiles = second['sampler_percentile'][sampler]

                        sampler_ag_data_item.avg_response_time = second['sampler_avg_rt'][sampler]
                        second_ag.cases[sampler] = sampler_ag_data_item
                return second_ag
        return None

    def get_next_sample_from_sdw(self, force):
        if self.results:
            read_lines = self.results.readlines(2 * 1024 * 1024)
            self.log.debug("About to process %s result lines", len(read_lines))
            for line in read_lines:
                if not line:
                    return None
                    # timeStamp,elapsed,label,responseCode,success,bytes,grpThreads,allThreads,Latency
                if self.partial_buffer != '':
                    line = self.partial_buffer + line
                    self.partial_buffer = ''
                data = line.rstrip().split("\t")
                if line[-1] != '\n' or len(data) != 9:
                    self.partial_buffer = line
                    #self.log.warning("Wrong jtl line, skipped: %s", line)
                    continue
                cur_time = int(data[0]) / 1000
                netcode = '0' if data[4] == 'true' else self.exc_to_net(data[3])

                if not cur_time in self.data_buffer.keys():
                    if self.data_queue and self.data_queue[0] >= cur_time:
                        self.log.warning(
                            "Aggregator data dates must be sequential: %s vs %s" % (cur_time, self.data_queue[0]))
                        cur_time = self.data_queue[0]  # 0 or -1?
                    else:
                        self.data_queue.append(cur_time)
                        self.data_buffer[cur_time] = []
                #        marker, threads, overallRT, httpCode, netCode
                data_item = [data[2], int(data[7]), int(data[1]), self.exc_to_http(data[3]), netcode]
                # bytes:     sent    received
                data_item += [0, int(data[5])]
                #        connect    send    latency    receive
                data_item += [0, 0, int(data[8]), int(data[1]) - int(data[8])]
                #        accuracy
                data_item += [0]
                self.data_buffer[cur_time].append(data_item)

        if not force and self.data_queue and (self.data_queue[-1] - self.data_queue[0]) > self.buffer_size:
            return self.pop_second()
        elif force and self.data_queue:
            return self.pop_second()
        else:
            return None

    def exc_to_net(self, param1):
        ''' translate http code to net code '''
        if len(param1) <= 3:
            return '1'

        exc = param1.split(' ')[-1]
        if exc in self.KNOWN_EXC.keys():
            return self.KNOWN_EXC[exc]
        else:
            self.log.warning("Not known Java exception, consider adding it to dictionary: %s", param1)
            return '1'

    def exc_to_http(self, param1):
        ''' translate exception str to http code'''
        if len(param1) <= 3:
            return param1

        exc = param1.split(' ')[-1]
        if exc in self.KNOWN_EXC.keys():
            return '0'
        else:
            return '500'


# ===============================================================================

class JMeterInfoWidget(AbstractInfoWidget, AggregateResultListener):
    ''' Right panel widget with JMeter test info '''

    def __init__(self, jmeter):
        AbstractInfoWidget.__init__(self)
        self.krutilka = ConsoleScreen.krutilka()
        self.jmeter = jmeter
        self.active_threads = 0
        self.rps = 0

    def get_index(self):
        return 0

    def aggregate_second(self, second_aggregate_data):
        self.active_threads = second_aggregate_data.overall.active_threads
        self.rps = second_aggregate_data.overall.RPS

    def render(self, screen):
        jmeter = " JMeter Test %s" % self.krutilka.next()
        space = screen.right_panel_width - len(jmeter) - 1
        left_spaces = space / 2
        right_spaces = space / 2

        dur_seconds = int(time.time()) - int(self.jmeter.start_time)
        duration = str(datetime.timedelta(seconds=dur_seconds))

        template = screen.markup.BG_MAGENTA + '~' * left_spaces + jmeter + ' ' + '~' * right_spaces + \
                   screen.markup.RESET + "\n"
        template += "     Test Plan: %s\n"
        template += "      Duration: %s\n"
        template += "Active Threads: %s\n"
        template += "   Responses/s: %s"
        data = (os.path.basename(self.jmeter.original_jmx), duration, self.active_threads, self.rps)

        return template % data

########NEW FILE########
__FILENAME__ = Loadosophia
''' Module to have Loadosophia.org integration '''
from Tank.Plugins.Aggregator import AggregateResultListener, AggregatorPlugin
from Tank.Plugins.ApacheBenchmark import ApacheBenchmarkPlugin
from Tank.Plugins.Monitoring import MonitoringPlugin
from Tank.Plugins.Phantom import PhantomPlugin
from Tank.Plugins.WebOnline import WebOnlinePlugin
from tankcore import AbstractPlugin
from urllib2 import HTTPError
import StringIO
import cookielib
import gzip
import itertools
import json
import logging
import mimetools
import mimetypes
import os
import time
import urllib
import urllib2

class LoadosophiaPlugin(AbstractPlugin, AggregateResultListener):
    ''' Tank plugin with Loadosophia.org uploading '''

    SECTION = 'loadosophia'
    
    @staticmethod
    def get_key():
        return __file__

    def __init__(self, core):
        ''' Constructor '''
        AbstractPlugin.__init__(self, core)
        self.loadosophia = LoadosophiaClient()
        self.loadosophia.results_url = None
        self.project_key = None
        self.color = None
        self.title = None
        self.online_buffer = []
        self.online_initiated = False
        self.online_enabled = False
    
    def get_available_options(self):
        return ["token", "project", "test_title", "file_prefix", "color_flag", "online_enabled" ]
    
    def configure(self):
        self.loadosophia.address = self.get_option("address", "https://loadosophia.org/")
        self.loadosophia.token = self.get_option("token", "")
        self.loadosophia.file_prefix = self.get_option("file_prefix", "")
        self.project_key = self.get_option("project", 'DEFAULT')
        self.title = self.get_option("test_title", "")
        self.color = self.get_option("color_flag", "")
        if self.loadosophia.token:
            self.online_enabled = int(self.get_option("online_enabled", "1"))

        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
            aggregator.add_result_listener(self)
        except KeyError:
            self.log.debug("No aggregator for loadosophia")

        
    def start_test(self):
        if self.online_enabled:
            try:
                url=self.loadosophia.start_online(self.project_key, self.title)
                self.log.info("Started active test: %s", url)
            except Exception, exc:
                self.log.warning("Problems starting online: %s", exc)
                self.online_enabled = False
            
    
    def aggregate_second(self, second_aggregate_data):
        if self.online_enabled:
            self.log.debug("Online buffer: %s", self.online_buffer)
            self.online_buffer.append(second_aggregate_data)
            if len(self.online_buffer) >= 5 or not self.online_initiated:
                try:
                    self.loadosophia.send_online_data(self.online_buffer)
                    self.online_initiated = True
                except Exception, exc:
                    self.log.warning("Problems sending online data: %s", exc)
                self.online_buffer = []

    
    def post_process(self, retcode):
        if self.online_enabled:
            if self.online_buffer:
                try:
                    self.loadosophia.send_online_data(self.online_buffer)
                except Exception, exc:
                    self.log.warning("Problems sending online data rests: %s", exc)
                self.online_buffer = []
            # mark test closed
            try:
                self.loadosophia.end_online()
            except Exception, exc:
                self.log.warning("Problems ending online: %s", exc)

        main_file = None
        # phantom
        try:
            phantom = self.core.get_plugin_of_type(PhantomPlugin)
            if phantom.phantom:
                main_file = phantom.phantom.phout_file
        except KeyError:
            self.log.debug("Phantom not found")
            
        # ab
        try:
            apache_bench = self.core.get_plugin_of_type(ApacheBenchmarkPlugin)
            main_file = apache_bench.out_file
        except KeyError:
            self.log.debug("AB not found")
        
        # monitoring
        mon_file = None
        try:
            mon = self.core.get_plugin_of_type(MonitoringPlugin)
            mon_file = mon.data_file
        except KeyError:
            self.log.debug("Monitoring not found")
            
        queue_id = self.loadosophia.send_results(self.project_key, main_file, [mon_file])
        if self.title or self.color:
            test_id = self.loadosophia.get_test_by_upload(queue_id)
            if self.color:
                self.loadosophia.set_color_flag(test_id, self.color)
            if self.title:
                self.loadosophia.set_test_title(test_id, self.title)
        
        if queue_id:
            self.log.info("Loadosophia.org upload succeeded, report link: %s", self.loadosophia.results_url)
        
        try:
            web = self.core.get_plugin_of_type(WebOnlinePlugin)
            if not web.redirect:
                web.redirect = self.loadosophia.results_url
                time.sleep(1)
        except KeyError:
            self.log.debug("Web online not found")

        return retcode
    

class LoadosophiaClient:
    ''' Loadosophia service client class '''

    STATUS_DONE = 4
    def __init__(self):
        self.log = logging.getLogger(__name__)
        self.token = None
        self.address = None
        self.file_prefix = ''
        self.results_url = None
        self.cookie_jar = cookielib.CookieJar()
    
    
    def send_results(self, project, result_file, monitoring_files):
        ''' Send files to loadosophia '''
        if not self.token:
            self.log.warning("Loadosophia.org uploading disabled, please set loadosophia.token option to enable it, get token at https://loadosophia.org/service/upload/token/")
        else:
            if not self.address:
                self.log.warning("Loadosophia.org uploading disabled, please set loadosophia.address option to enable it")
            else:
                self.log.info("Uploading to Loadosophia.org: %s %s %s", project, result_file, monitoring_files)
                if not project:
                    self.log.info("Uploading to default project, please set loadosophia.project option to change this")
                if not result_file or not os.path.exists(result_file) or not os.path.getsize(result_file):
                    self.log.warning("Empty results file, skip Loadosophia.org uploading: %s", result_file)
                else:
                    return self.__send_checked_results(project, result_file, monitoring_files)
    
    
    def __send_checked_results(self, project, result_file, monitoring_files):
        ''' internal wrapper to send request '''
        # Create the form with simple fields
        form = MultiPartForm()
        form.add_field('projectKey', project)
        form.add_field('token', self.token)
        
        # Add main file
        form.add_file_as_string('jtl_file', self.file_prefix + os.path.basename(result_file) + ".gz", self.__get_gzipped_file(result_file))
    
        index = 0
        for mon_file in monitoring_files:
            if not mon_file or not os.path.exists(mon_file) or not os.path.getsize(mon_file):
                self.log.warning("Skipped mon file: %s", mon_file)
                continue
            form.add_file_as_string('perfmon_' + str(index), self.file_prefix + os.path.basename(mon_file) + ".gz", self.__get_gzipped_file(mon_file))
            index += 1
            
        # Build the request
        request = urllib2.Request(self.address + "api/file/upload/?format=json")
        request.add_header('User-Agent', 'Yandex.Tank Loadosophia Uploader Module')
        body = str(form)
        request.add_header('Content-Type', form.get_content_type())
        request.add_header('Content-Length', len(body))
        request.add_data(body)

        response = urllib2.urlopen(request)
        if response.getcode() != 200:
            self.log.debug("Full loadosophia.org response: %s", response.read())
            raise RuntimeError("Loadosophia.org upload failed, response code %s instead of 200, see log for full response text" % response.getcode())

        resp_str = response.read()
        try:
            res = json.loads(resp_str)
        except Exception, exc:
            self.log.debug("Failed to load json from str: %s", resp_str)
            raise exc
        self.results_url = self.address + 'api/file/status/' + res[1][0]['QueueID'] + '/?redirect=true'
        return res[1][0]['QueueID']
        
                
    def __get_gzipped_file(self, result_file):
        ''' gzip file '''
        out = StringIO.StringIO()
        fhandle = gzip.GzipFile(fileobj=out, mode='w')
        fhandle.write(open(result_file, 'r').read())
        fhandle.close()
        return out.getvalue()


    def get_test_by_upload(self, queue_id):
        self.log.info("Waiting for Loadosophia.org to process file...")
        
        while True:
            time.sleep(1)
            status = self.get_upload_status(queue_id)
            if status['UserError']:
                raise HTTPError("Loadosophia processing error: " + status['UserError'])
            
            if int(status['status']) == self.STATUS_DONE:
                self.results_url = self.address + 'gui/' + status['TestID'] + '/'
                return status['TestID']  
            

    def get_upload_status(self, queue_id):
        self.log.debug("Requesting file status: %s", queue_id)
        form = MultiPartForm()
        form.add_field('token', self.token)
        
        request = urllib2.Request(self.address + "api/file/status/" + queue_id + "/?format=json")
        request.add_header('User-Agent', 'Yandex.Tank Loadosophia Uploader Module')
        body = str(form)
        request.add_header('Content-Type', form.get_content_type())
        request.add_header('Content-Length', len(body))
        request.add_data(body)

        response = urllib2.urlopen(request)
        if response.getcode() != 200:
            self.log.debug("Full loadosophia.org response: %s", response.read())
            raise RuntimeError("Loadosophia.org request failed, response code %s instead of 200, see log for full response text" % response.getcode())

        res = json.loads(response.read())
        self.log.debug("Status info: %s", res)
        return res[1][0]
    
    
    def set_color_flag(self, test_id, color):
        form = MultiPartForm()
        form.add_field('token', self.token)
        
        request = urllib2.Request(self.address + "api/test/edit/color/" + test_id + "/?format=json&color=" + color)
        request.add_header('User-Agent', 'Yandex.Tank Loadosophia Uploader Module')
        body = str(form)
        request.add_header('Content-Type', form.get_content_type())
        request.add_header('Content-Length', len(body))
        request.add_data(body)

        response = urllib2.urlopen(request)
        if response.getcode() != 204:
            self.log.debug("Full loadosophia.org response: %s", response.read())
            raise RuntimeError("Loadosophia.org request failed, response code %s instead of 204, see log for full response text" % response.getcode())


    def set_test_title(self, test_id, title):
        self.log.debug("Set test title: %s", title)
        form = MultiPartForm()
        form.add_field('token', self.token)
        
        request = urllib2.Request(self.address + "api/test/edit/title/" + test_id + "/?format=json&" + urllib.urlencode({"title": title}))
        request.add_header('User-Agent', 'Yandex.Tank Loadosophia Uploader Module')
        body = str(form)
        request.add_header('Content-Type', form.get_content_type())
        request.add_header('Content-Length', len(body))
        request.add_data(body)

        response = urllib2.urlopen(request)
        if response.getcode() != 204:
            self.log.debug("Full loadosophia.org response: %s", response.read())
            raise RuntimeError("Loadosophia.org request failed, response code %s instead of 204, see log for full response text" % response.getcode())

    
    def start_online(self, project, title):
        self.log.info("Initiating Loadosophia.org active test...")
        data = urllib.urlencode({'projectKey': project, 'token':self.token, 'title': title})
        
        opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(self.cookie_jar))
        url = self.address + "api/active/receiver/start/"
        response = opener.open(url, data)
        if response.getcode() != 201:
            self.log.warn("Failed to start active test: %s", response.getcode())        
            self.log.debug("Failed to start active test: %s", response.read())        
            self.cookie_jar.clear_session_cookies()

        online_id=json.loads(response.read())
        return self.address + "gui/active/"+online_id['OnlineID']+'/'

    def end_online(self):
        self.log.debug("Ending Loadosophia online test")
        opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(self.cookie_jar))
        url = self.address + "api/active/receiver/stop/"
        response = opener.open(url)
        if response.getcode() != 205:
            self.log.warn("Failed to end active test: %s", response.getcode())        
            self.log.debug("Failed to end active test: %s", response.read())        
        self.cookie_jar.clear_session_cookies()

    
    def send_online_data(self, data_buffer):
        data = []
        for sec in data_buffer:
            item = sec.overall
            json_item = {
                       "ts": str(sec.time),
                       "threads": item.active_threads,
                       "rps": item.RPS,
                       "planned_rps": item.planned_requests,
                       "avg_rt": item.avg_response_time,
                       "quantiles": item.quantiles,
                       "rc": item.http_codes,
                       "net": item.net_codes
                    }
            data.append(json_item)
        
        self.log.debug("Sending online data: %s", json.dumps(data))
        data_str = urllib.urlencode({'data': json.dumps(data)})
        
        opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(self.cookie_jar))
        url = self.address + "api/active/receiver/data/"
        response = opener.open(url, data_str)
        if response.getcode() != 202:
            self.log.warn("Failed to push data: %s", response.getcode())        

        

# =================================================================    

class MultiPartForm(object):
    """Accumulate the data to be used when posting a form.
    http://blog.doughellmann.com/2009/07/pymotw-urllib2-library-for-opening-urls.html """

    def __init__(self):
        self.form_fields = []
        self.files = []
        self.boundary = mimetools.choose_boundary()
        return
    
    def get_content_type(self):
        ''' returns content type '''
        return 'multipart/form-data; boundary=%s' % self.boundary

    def add_field(self, name, value):
        """Add a simple field to the form data."""
        self.form_fields.append((name, value))
        return

    def add_file_as_string(self, fieldname, filename, body, mimetype=None):
        ''' add raw string file '''
        if mimetype is None:
            mimetype = mimetypes.guess_type(filename)[0] or 'application/octet-stream'
        self.files.append((fieldname, filename, mimetype, body))
        return

    def add_file(self, fieldname, filename, file_handle, mimetype=None):
        """Add a file to be uploaded."""
        body = file_handle.read()
        self.add_file_as_string(fieldname, filename, body, mimetype)
        return
    
    def __str__(self):
        """Return a string representing the form data, including attached files."""
        # Build a list of lists, each containing "lines" of the
        # request.  Each part is separated by a boundary string.
        # Once the list is built, return a string where each
        # line is separated by '\r\n'.  
        parts = []
        part_boundary = '--' + self.boundary
        
        # Add the form fields
        parts.extend(
            [ part_boundary,
              'Content-Disposition: form-data; name="%s"' % name,
              '',
              value,
            ]
            for name, value in self.form_fields
            )
        
        # Add the files to upload
        parts.extend(
            [ part_boundary,
              'Content-Disposition: file; name="%s"; filename="%s"' % \
                 (field_name, filename),
              'Content-Type: %s' % content_type,
              '',
              body,
            ]
            for field_name, filename, content_type, body in self.files
            )
        
        # Flatten the list and add closing boundary marker,
        # then return CR+LF separated data
        flattened = list(itertools.chain(*parts))
        flattened.append('--' + self.boundary + '--')
        flattened.append('')
        return '\r\n'.join(flattened)



########NEW FILE########
__FILENAME__ = Monitoring
"""Module to provide target monitoring"""

import os
import time
import traceback
import fnmatch
import datetime

from Tank.MonCollector.collector import MonitoringCollector, \
    MonitoringDataListener, MonitoringDataDecoder
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin, AbstractInfoWidget
from Tank.Plugins.Phantom import PhantomPlugin
from tankcore import AbstractPlugin
from Tank.Plugins.Autostop import AutostopPlugin, AbstractCriteria
import tankcore


class MonitoringPlugin(AbstractPlugin):
    """  resource mon plugin  """

    SECTION = 'monitoring'

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.jobno = None
        self.default_target = None
        self.config = None
        self.process = None
        self.monitoring = MonitoringCollector()
        self.die_on_fail = True
        self.data_file = None
        self.mon_saver = None
        self.address_resolver = None


    @staticmethod
    def get_key():
        return __file__

    def get_available_options(self):
        return ["config", "default_target", 'ssh_timeout']

    def configure(self):
        self.config = self.get_option("config", 'auto').strip()
        self.default_target = self.get_option("default_target", 'localhost')
        self.monitoring.ssh_timeout = tankcore.expand_to_seconds(self.get_option('ssh_timeout', "5s"))

        if self.config == 'none' or self.config == 'auto':
            self.die_on_fail = False
        else:
            if self.config and self.config[0] == '<':
                xmlfile = self.core.mkstemp(".xml", "monitoring_")
                self.core.add_artifact_file(xmlfile)
                xml = open(xmlfile, 'w')
                xml.write(self.config)
                xml.close()
                self.config = xmlfile

            if not os.path.exists(self.config):
                raise OSError("Monitoring config file not found: %s" % self.config)

        if self.config == 'none':
            self.monitoring = None

        if self.config == 'auto':
            self.config = os.path.dirname(__file__) + '/monitoring_default_config.xml'

        try:
            autostop = self.core.get_plugin_of_type(AutostopPlugin)
            autostop.add_criteria_class(MetricHigherCriteria)
            autostop.add_criteria_class(MetricLowerCriteria)
        except KeyError:
            self.log.debug("No autostop plugin found, not adding instances criteria")


    def prepare_test(self):
        try:
            phantom = self.core.get_plugin_of_type(PhantomPlugin)
            if phantom.phout_import_mode:
                self.log.info("Phout import mode, disabling monitoring")
                self.config = None
                self.monitoring = None

            info = phantom.get_info()
            if info:
                self.default_target = info.address
                self.log.debug("Changed monitoring target to %s", self.default_target)
        except KeyError, ex:
            self.log.debug("Phantom plugin not found: %s", ex)

        if self.address_resolver:
            try:
                self.default_target = self.address_resolver.resolve_virtual(self.default_target)
            except Exception, exc:
                self.log.error("Failed to get target info: %s", exc)

        if not self.config or self.config == 'none':
            self.log.info("Monitoring has been disabled")
        else:
            self.log.info("Starting monitoring with config: %s", self.config)
            self.core.add_artifact_file(self.config, True)
            self.monitoring.config = self.config
            if self.default_target:
                self.monitoring.default_target = self.default_target

            self.data_file = self.core.mkstemp('.data', 'monitoring_')
            self.mon_saver = SaveMonToFile(self.data_file)
            self.monitoring.add_listener(self.mon_saver)
            self.core.add_artifact_file(self.data_file)

            try:
                console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
            except Exception, ex:
                self.log.debug("Console not found: %s", ex)
                console = None
            if console:
                widget = MonitoringWidget(self)
                console.add_info_widget(widget)
                self.monitoring.add_listener(widget)

            try:
                self.monitoring.prepare()
                self.monitoring.start()
                count = 0
                while not self.monitoring.first_data_received and count < 15 * 5:
                    time.sleep(0.2)
                    self.monitoring.poll()
                    count += 1
            except Exception, exc:
                self.log.debug("Problems starting monitoring: %s", traceback.format_exc(exc))
                if self.die_on_fail:
                    raise exc
                else:
                    self.log.warning("Failed to start monitoring: %s", exc)
                    self.monitoring = None


    def is_test_finished(self):
        if self.monitoring and not self.monitoring.poll():
            if self.die_on_fail:
                raise RuntimeError("Monitoring died unexpectedly")
            else:
                self.log.warn("Monitoring died unexpectedly")
                self.monitoring = None
        return -1


    def end_test(self, retcode):
        self.log.info("Finishing monitoring")
        if self.monitoring:
            self.monitoring.stop()
            for log in self.monitoring.artifact_files:
                self.core.add_artifact_file(log)

            while self.monitoring.send_data:
                self.log.info("Sending monitoring data rests...")
                self.monitoring.send_collected_data()

        if self.mon_saver:
            self.mon_saver.close()
        return retcode


class SaveMonToFile(MonitoringDataListener):
    """
    Default listener - saves data to file
    """

    def __init__(self, out_file):
        MonitoringDataListener.__init__(self)
        if out_file:
            self.store = open(out_file, 'w')

    def monitoring_data(self, data_string):
        self.store.write(data_string)
        self.store.flush()

    def close(self):
        """ close open files """
        if self.store:
            self.store.close()


class MonitoringWidget(AbstractInfoWidget, MonitoringDataListener, MonitoringDataDecoder):
    """
    Screen widget
    """

    def __init__(self, owner):
        AbstractInfoWidget.__init__(self)
        MonitoringDataDecoder.__init__(self)
        self.owner = owner
        self.data = {}
        self.sign = {}
        self.time = {}
        self.max_metric_len = 0

    def get_index(self):
        return 50

    def __handle_data_item(self, host, data):
        """ store metric in data tree and calc offset signs """
        for metric, value in data.iteritems():
            if value == '' or value == self.NA:
                value = self.NA
                self.sign[host][metric] = -1
                self.data[host][metric] = value
            else:
                if self.data[host][metric] == self.NA:
                    self.sign[host][metric] = 1
                elif float(value) > float(self.data[host][metric]):
                    self.sign[host][metric] = 1
                elif float(value) < float(self.data[host][metric]):
                    self.sign[host][metric] = -1
                else:
                    self.sign[host][metric] = 0
                self.data[host][metric] = "%.2f" % float(value)


    def monitoring_data(self, data_string):
        self.log.debug("Mon widget data: %s", data_string)
        for line in data_string.split("\n"):
            if not line.strip():
                continue

            host, data, initial, timestamp = self.decode_line(line)
            self.time[host] = timestamp
            if initial:
                self.sign[host] = {}
                self.data[host] = {}
                for metric in data.keys():
                    self.sign[host][metric] = 0
                    self.data[host][metric] = self.NA
            else:
                self.__handle_data_item(host, data)


    def render(self, screen):
        if not self.owner.monitoring:
            return "Monitoring is " + screen.markup.RED + "offline" + screen.markup.RESET
        else:
            res = "Monitoring is " + screen.markup.GREEN + "online" + screen.markup.RESET + ":\n"
            for hostname, metrics in self.data.items():
                tm_stamp = datetime.datetime.fromtimestamp(float(self.time[hostname])).strftime('%H:%M:%S')
                res += ("   " + screen.markup.CYAN + "%s" + screen.markup.RESET + " at %s:\n") % (hostname, tm_stamp)
                for metric, value in sorted(metrics.iteritems()):
                    if self.sign[hostname][metric] > 0:
                        value = screen.markup.YELLOW + value + screen.markup.RESET
                    elif self.sign[hostname][metric] < 0:
                        value = screen.markup.CYAN + value + screen.markup.RESET
                    res += "      %s%s: %s\n" % (
                    ' ' * (self.max_metric_len - len(metric)), metric.replace('_', ' '), value)

            return res.strip()


class AbstractMetricCriteria(AbstractCriteria, MonitoringDataListener, MonitoringDataDecoder):
    """ Parent class for metric criteria """

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        MonitoringDataDecoder.__init__(self)

        try:
            self.mon = autostop.core.get_plugin_of_type(MonitoringPlugin)
            if self.mon.monitoring:
                self.mon.monitoring.add_listener(self)
        except KeyError:
            self.log.warning("No monitoring module, mon autostop disabled")
        self.triggered = False
        self.autostop = autostop

        self.host = param_str.split(',')[0].strip()
        self.metric = param_str.split(',')[1].strip()
        self.value_limit = float(param_str.split(',')[2])
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[3])
        self.last_second = None
        self.seconds_count = 0


    def monitoring_data(self, data_string):
        if self.triggered:
            return

        for line in data_string.split("\n"):
            if not line.strip():
                continue

            host, data, initial, timestamp = self.decode_line(line)
            if initial or not fnmatch.fnmatch(host, self.host):
                continue

            if self.metric not in data.keys() or not data[self.metric] or data[self.metric] == self.NA:
                data[self.metric] = 0

            self.log.debug("Compare %s %s/%s=%s to %s", self.get_type_string(), host, self.metric, data[self.metric],
                           self.value_limit)
            if self.comparison_fn(float(data[self.metric]), self.value_limit):
                if not self.seconds_count:
                    self.cause_second = self.last_second

                self.log.debug(self.explain())

                self.seconds_count += 1
                if self.seconds_count >= self.seconds_limit:
                    self.log.debug("Triggering autostop")
                    self.triggered = True
                    return
            else:
                self.seconds_count = 0


    def notify(self, aggregate_second):
        if self.seconds_count:
            self.autostop.add_counting(self)

        self.last_second = aggregate_second
        return self.triggered

    def comparison_fn(self, arg1, arg2):
        """ comparison function """
        raise NotImplementedError()


class MetricHigherCriteria(AbstractMetricCriteria):
    """ trigger if metric is higher than limit """

    def __init__(self, autostop, param_str):
        AbstractMetricCriteria.__init__(self, autostop, param_str)

    def get_rc(self):
        return 31

    @staticmethod
    def get_type_string():
        return 'metric_higher'

    def explain(self):
        items = (self.host, self.metric, self.value_limit, self.seconds_count)
        return "%s/%s metric value is higher than %s for %s seconds" % items

    def widget_explain(self):
        items = (self.host, self.metric, self.value_limit, self.seconds_count, self.seconds_limit)
        return "%s/%s > %s for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit

    def comparison_fn(self, arg1, arg2):
        return arg1 > arg2


class MetricLowerCriteria(AbstractMetricCriteria):
    """ trigger if metric is lower than limit """

    def __init__(self, autostop, param_str):
        AbstractMetricCriteria.__init__(self, autostop, param_str)

    def get_rc(self):
        return 32

    @staticmethod
    def get_type_string():
        return 'metric_lower'

    def explain(self):
        items = (self.host, self.metric, self.value_limit, self.seconds_count)
        return "%s/%s metric value is lower than %s for %s seconds" % items

    def widget_explain(self):
        items = (self.host, self.metric, self.value_limit, self.seconds_count, self.seconds_limit)
        return "%s/%s < %s for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit

    def comparison_fn(self, arg1, arg2):
        return arg1 < arg2


class AbstractResolver:
    """ Resolver class provides virtual to real host resolution """

    def __init__(self):
        pass

    def resolve_virtual(self, virt_address):
        """ get host address by virtual """
        raise NotImplementedError()

########NEW FILE########
__FILENAME__ = Phantom
""" Contains Phantom Plugin, Console widgets, result reader classes """
from Tank.Plugins import ConsoleScreen
from Tank.Plugins.Aggregator import AggregatorPlugin, AggregateResultListener, \
    AbstractReader
from Tank.Plugins.Autostop import AutostopPlugin, AbstractCriteria
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin, AbstractInfoWidget
from Tank.Plugins.PhantomUtils import PhantomConfig
from tankcore import AbstractPlugin
import os
import socket
import subprocess
import sys
import tankcore
import time
import datetime

# FIXME: 3 there is no graceful way to interrupt the process in phout
# import mode


class PhantomPlugin(AbstractPlugin, AggregateResultListener):

    """     Plugin for running phantom tool    """

    OPTION_CONFIG = "config"
    SECTION = PhantomConfig.SECTION

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.config = None
        self.process = None

        self.predefined_phout = None
        self.phout_import_mode = False
        self.did_phout_import_try = False

        self.phantom_path = None
        self.eta_file = None
        self.processed_ammo_count = 0
        self.phantom_start_time = time.time()
        self.buffered_seconds = "2"

        self.phantom = None
        self.cached_info = None
        self.phantom_stderr = None

    @staticmethod
    def get_key():
        return __file__

    def get_available_options(self):
        opts = ["eta_file", "phantom_path", "buffered_seconds", ]
        opts += [PhantomConfig.OPTION_PHOUT, self.OPTION_CONFIG]
        opts += PhantomConfig.get_available_options()
        return opts

    def configure(self):
        # plugin part
        self.config = self.get_option(self.OPTION_CONFIG, '')
        self.eta_file = self.get_option("eta_file", '')
        self.core.add_artifact_file(self.eta_file)
        self.phantom_path = self.get_option("phantom_path", 'phantom')
        self.buffered_seconds = int(
            self.get_option("buffered_seconds", self.buffered_seconds))

        try:
            autostop = self.core.get_plugin_of_type(AutostopPlugin)
            autostop.add_criteria_class(UsedInstancesCriteria)
        except KeyError:
            self.log.debug(
                "No autostop plugin found, not adding instances criteria")

        self.predefined_phout = self.get_option(PhantomConfig.OPTION_PHOUT, '')
        if not self.get_option(self.OPTION_CONFIG, '') and self.predefined_phout:
            self.phout_import_mode = True

        if not self.config and not self.phout_import_mode:
            self.phantom = PhantomConfig(self.core)
            self.phantom.read_config()

    def prepare_test(self):
        aggregator = None
        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
        except Exception, ex:
            self.log.warning("No aggregator found: %s", ex)

        if aggregator:
            aggregator.reader = PhantomReader(aggregator, self)
            aggregator.reader.buffered_seconds = self.buffered_seconds
            if self.phantom:
                self.phantom.set_timeout(aggregator.get_timeout())
            aggregator.add_result_listener(self)

        if not self.config and not self.phout_import_mode:
            if aggregator:
                aggregator.reader.phout_file = self.phantom.phout_file

            # generate config
            self.config = self.phantom.compose_config()
            args = [self.phantom_path, 'check', self.config]

            result = tankcore.execute(args, catch_out=True)
            retcode = result[0]
            if retcode:
                raise RuntimeError(
                    "Config check failed. Subprocess returned code %s" % retcode)
            if result[2]:
                raise RuntimeError(
                    "Subprocess returned message: %s" % result[2])

        else:
            if aggregator:
                aggregator.reader.phout_file = self.predefined_phout

        try:
            console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
        except Exception, ex:
            self.log.debug("Console not found: %s", ex)
            console = None

        if console:
            if not self.phout_import_mode:
                widget = PhantomProgressBarWidget(self)
                if self.eta_file:
                    widget.eta_file = self.eta_file
                console.add_info_widget(widget)
                aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
                aggregator.add_result_listener(widget)

            widget = PhantomInfoWidget(self)
            console.add_info_widget(widget)
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
            aggregator.add_result_listener(widget)

    def start_test(self):
        if not self.phout_import_mode:
            args = [self.phantom_path, 'run', self.config]
            self.log.debug(
                "Starting %s with arguments: %s", self.phantom_path, args)
            self.phantom_start_time = time.time()
            phantom_stderr_file = self.core.mkstemp(
                ".log", "phantom_stdout_stderr_")
            self.core.add_artifact_file(phantom_stderr_file)
            self.phantom_stderr = open(phantom_stderr_file, 'w')
            self.process = subprocess.Popen(
                args, stderr=self.phantom_stderr, stdout=self.phantom_stderr, close_fds=True)
        else:
            if not os.path.exists(self.predefined_phout):
                raise RuntimeError(
                    "Phout file not exists for import: %s" % self.predefined_phout)
            self.log.warn(
                "Will import phout file instead of running phantom: %s", self.predefined_phout)

    def is_test_finished(self):
        if not self.phout_import_mode:
            retcode = self.process.poll()
            if retcode is not None:
                self.log.info(
                    "Phantom done its work with exit code: %s", retcode)
                return abs(retcode)
            else:
                return -1
        else:
            if not self.processed_ammo_count or self.did_phout_import_try != self.processed_ammo_count:
                self.did_phout_import_try = self.processed_ammo_count
                return -1
            else:
                return 0

    def end_test(self, retcode):
        if self.process and self.process.poll() is None:
            self.log.warn(
                "Terminating phantom process with PID %s", self.process.pid)
            self.process.terminate()
            if self.phantom_stderr:
                self.phantom_stderr.close()
        else:
            self.log.debug("Seems phantom finished OK")
        return retcode

    def post_process(self, retcode):
        if not retcode:
            info = self.get_info()
            if info and info.ammo_count != self.processed_ammo_count:
                self.log.warning(
                    "Planned ammo count %s differs from processed %s", info.ammo_count, self.processed_ammo_count)
        return retcode

    def aggregate_second(self, second_aggregate_data):
        self.processed_ammo_count += second_aggregate_data.overall.RPS
        self.log.debug("Processed ammo count: %s/", self.processed_ammo_count)

    def get_info(self):
        """ returns info object """
        if not self.cached_info:
            if not self.phantom:
                return None
            self.cached_info = self.phantom.get_info()
        return self.cached_info


class PhantomProgressBarWidget(AbstractInfoWidget, AggregateResultListener):

    """
    Widget that displays progressbar
    """

    def get_index(self):
        return 0

    def __init__(self, sender):
        AbstractInfoWidget.__init__(self)
        self.krutilka = ConsoleScreen.krutilka()
        self.owner = sender
        self.ammo_progress = 0
        self.eta_file = None

        info = self.owner.get_info()
        if info:
            self.ammo_count = int(info.ammo_count)
            self.test_duration = int(info.duration)
        else:
            self.ammo_count = 1
            self.test_duration = 1

    def render(self, screen):
        res = ""

        dur_seconds = int(time.time()) - int(self.owner.phantom_start_time)

        eta_time = 'N/A'
        eta_secs = -1
        progress = 0
        color_bg = screen.markup.BG_CYAN
        color_fg = screen.markup.CYAN
        if self.test_duration and self.test_duration >= dur_seconds:
            color_bg = screen.markup.BG_GREEN
            color_fg = screen.markup.GREEN
            eta_secs = self.test_duration - dur_seconds
            eta_time = datetime.timedelta(seconds=eta_secs)
            progress = float(dur_seconds) / self.test_duration
        elif self.ammo_progress:
            left_part = self.ammo_count - self.ammo_progress
            if left_part > 0:
                eta_secs = int(float(dur_seconds) / float(
                    self.ammo_progress) * float(left_part))
            else:
                eta_secs = 0
            eta_time = datetime.timedelta(seconds=eta_secs)
            if self.ammo_progress < self.ammo_count:
                progress = float(self.ammo_progress) / float(self.ammo_count)
            else:
                progress = 0.5

        if self.eta_file:
            handle = open(self.eta_file, 'w')
            handle.write(str(eta_secs))
            handle.close()

        perc = float(int(1000 * progress)) / 10
        str_perc = str(perc) + "%"

        pb_width = screen.right_panel_width - 1 - len(str_perc)

        progress_chars = '=' * (int(pb_width * progress) - 1)
        progress_chars += self.krutilka.next()

        res += color_bg + progress_chars + screen.markup.RESET + color_fg + \
            '~' * \
            (pb_width - int(pb_width * progress)) + \
            screen.markup.RESET + ' '
        res += str_perc + "\n"

        eta = 'ETA: %s' % eta_time
        dur = 'Duration: %s' % str(datetime.timedelta(seconds=dur_seconds))
        spaces = ' ' * (screen.right_panel_width - len(eta) - len(dur) - 1)
        res += dur + ' ' + spaces + eta

        return res

    def aggregate_second(self, second_aggregate_data):
        self.ammo_progress += second_aggregate_data.overall.RPS


class PhantomInfoWidget(AbstractInfoWidget, AggregateResultListener):

    """
    Widget with information about current run state
    """

    def get_index(self):
        return 2

    def __init__(self, sender):
        AbstractInfoWidget.__init__(self)
        self.owner = sender
        self.instances = 0
        self.planned = 0
        self.RPS = 0
        self.selfload = 0
        self.time_lag = 0
        self.planned_rps_duration = 0

        info = self.owner.get_info()
        if info:
            self.instances_limit = int(info.instances)
            self.ammo_count = int(info.ammo_count)
        else:
            self.instances_limit = 1
            self.ammo_count = 1

    def render(self, screen):
        res = ''
        info = self.owner.get_info()
        if self.owner.phantom:
            template = "Hosts: %s => %s:%s\n Ammo: %s\nCount: %s\n Load: %s"
            data = (socket.gethostname(), info.address, info.port, os.path.basename(
                info.ammo_file), self.ammo_count, ' '.join(info.rps_schedule))
            res = template % data

            res += "\n\n"

        res += "Active instances: "
        if float(self.instances) / self.instances_limit > 0.8:
            res += screen.markup.RED + \
                str(self.instances) + screen.markup.RESET
        elif float(self.instances) / self.instances_limit > 0.5:
            res += screen.markup.YELLOW + \
                str(self.instances) + screen.markup.RESET
        else:
            res += str(self.instances)

        res += "\nPlanned requests: %s for %s\nActual responses: " % (
            self.planned, datetime.timedelta(seconds=self.planned_rps_duration))
        if not self.planned == self.RPS:
            res += screen.markup.YELLOW + str(self.RPS) + screen.markup.RESET
        else:
            res += str(self.RPS)

        res += "\n        Accuracy: "
        if self.selfload < 80:
            res += screen.markup.RED + \
                ('%.2f' % self.selfload) + screen.markup.RESET
        elif self.selfload < 95:
            res += screen.markup.YELLOW + \
                ('%.2f' % self.selfload) + screen.markup.RESET
        else:
            res += ('%.2f' % self.selfload)

        res += "%\n        Time lag: "
        if self.time_lag > self.owner.buffered_seconds * 5:
            self.log.debug("Time lag: %s", self.time_lag)
            res += screen.markup.RED + \
                str(datetime.timedelta(seconds=self.time_lag)) + \
                screen.markup.RESET
        elif self.time_lag > self.owner.buffered_seconds:
            res += screen.markup.YELLOW + \
                str(datetime.timedelta(seconds=self.time_lag)) + \
                screen.markup.RESET
        else:
            res += str(datetime.timedelta(seconds=self.time_lag))

        return res

    def aggregate_second(self, second_aggregate_data):
        self.instances = second_aggregate_data.overall.active_threads
        if self.planned == second_aggregate_data.overall.planned_requests:
            self.planned_rps_duration += 1
        else:
            self.planned = second_aggregate_data.overall.planned_requests
            self.planned_rps_duration = 1

        self.RPS = second_aggregate_data.overall.RPS
        self.selfload = second_aggregate_data.overall.selfload
        self.time_lag = int(
            time.time() - time.mktime(second_aggregate_data.time.timetuple()))


class PhantomReader(AbstractReader):

    """
    Adapter to read phout files
    """

    def __init__(self, owner, phantom):
        AbstractReader.__init__(self, owner)
        self.phantom = phantom
        self.phout_file = None
        self.phout = None
        self.stat = None
        self.stat_data = {}
        self.pending_datetime = None
        self.steps = []
        self.first_request_time = sys.maxint
        self.partial_buffer = ''
        self.pending_second_data_queue = []
        self.last_sample_time = 0
        self.read_lines_count = 0
        self.buffered_seconds = 3

    def check_open_files(self):
        info = self.phantom.get_info()
        if not self.phout and os.path.exists(self.phout_file):
            self.log.debug("Opening phout file: %s", self.phout_file)
            self.phout = open(self.phout_file, 'r')
            if info:
                self.steps = info.steps

        if not self.stat and info and os.path.exists(info.stat_log):
            self.log.debug(
                "Opening stat file: %s", self.phantom.phantom.stat_log)
            self.stat = open(self.phantom.phantom.stat_log, 'r')

    def close_files(self):
        if self.stat:
            self.stat.close()

        if self.phout:
            self.phout.close()

    def get_next_sample(self, force):
        if self.stat and len(self.data_queue) < self.buffered_seconds * 2:
            self.__read_stat_data()
        return self.__read_phout_data(force)

    def __read_stat_data(self):
        """ Read active instances info """
        stat = self.stat.readlines()
        for line in stat:
            if line.startswith('time\t'):
                date_str = line[len('time:\t') - 1:].strip()[:-5].strip()
                date_obj = datetime.datetime.strptime(
                    date_str, '%Y-%m-%d %H:%M:%S')
                self.pending_datetime = int(time.mktime(date_obj.timetuple()))
                self.stat_data[self.pending_datetime] = 0
            if line.startswith('tasks\t'):
                if not self.pending_datetime:
                    raise RuntimeError(
                        "Can't have tasks info without timestamp")

                self.stat_data[self.pending_datetime] += int(
                    line[len('tasks\t'):])
                self.log.debug(
                    "Active instances: %s=>%s", self.pending_datetime, self.stat_data[self.pending_datetime])

        self.log.debug("Instances info buffer size: %s", len(self.stat_data))

    def __read_phout_data(self, force):
        """         Read phantom results        """
        if self.phout and len(self.data_queue) < self.buffered_seconds * 2:
            self.log.debug("Reading phout, up to 10MB...")
            phout = self.phout.readlines(10 * 1024 * 1024)
        else:
            self.log.debug("Skipped phout reading")
            phout = []

        self.log.debug("About to process %s phout lines", len(phout))
        time_before = time.time()
        for line in phout:
            line = self.partial_buffer + line
            self.partial_buffer = ''
            if line[-1] != "\n":
                self.log.debug("Not complete line, buffering it: %s", line)
                self.partial_buffer = line
                continue

            # 1346949510.514        74420    66    78    65409    8867    74201    18    15662    0    200
            # self.log.debug("Phout line: %s", line)
            self.read_lines_count += 1
            data = line[:-1].split("\t")

            if len(data) != 12:
                self.log.warning("Wrong phout line, skipped: %s", line)
                continue
            rt_real = int(data[2])
            tstmp = float(data[0])
            cur_time = int(tstmp + float(rt_real) / 1000000)

            if cur_time in self.stat_data.keys():
                active = self.stat_data[cur_time]
            else:
                active = 0

            if not cur_time in self.data_queue:
                self.first_request_time = min(
                    self.first_request_time, int(tstmp))
                if self.data_queue and self.data_queue[-1] >= cur_time:
                    self.log.warning(
                        "Aggregator data dates must be sequential: %s vs %s" % (cur_time, self.data_queue[-1]))
                    cur_time = self.data_queue[-1]
                else:
                    self.data_queue.append(cur_time)
                    self.data_buffer[cur_time] = []

            #        marker, threads, overallRT, httpCode, netCode
            # bytes:     sent    received
            #        connect    send    latency    receive
            #        accuracy
            data_item = (data[1], active, rt_real / 1000, data[11], data[10],
                         int(data[8]), int(data[9]),
                         int(data[3]) / 1000, int(data[4]) / 1000, int(
                         data[5]) / 1000, int(data[6]) / 1000,
                        (float(data[7]) + 1) / (rt_real + 1))

            self.data_buffer[cur_time].append(data_item)

        spent = time.time() - time_before
        if spent:
            self.log.debug(
                "Parsing speed: %s lines/sec", int(len(phout) / spent))
        self.log.debug("Read lines total: %s", self.read_lines_count)
        self.log.debug("Seconds queue: %s", self.data_queue)
        self.log.debug("Seconds buffer (up to %s): %s",
                       self.buffered_seconds, self.data_buffer.keys())
        if len(self.data_queue) > self.buffered_seconds:
            self.log.debug("Should send!")
            return self.pop_second()

        if self.pending_second_data_queue:
            return self.__process_pending_second()

        if force and self.data_queue:
            return self.pop_second()
        else:
            self.log.debug("No queue data!")
            return None

    def __aggregate_next_second(self):
        """ calls aggregator if there is data """
        parsed_sec = AbstractReader.pop_second(self)
        if parsed_sec:
            self.pending_second_data_queue.append(parsed_sec)
            timestamp = int(time.mktime(parsed_sec.time.timetuple()))
            if timestamp in self.stat_data.keys():
                del self.stat_data[timestamp]
        else:
            self.log.debug("No new seconds present")

    def __process_pending_second(self):
        """ process data in queue """
        next_time = int(
            time.mktime(self.pending_second_data_queue[0].time.timetuple()))
        if self.last_sample_time and (next_time - self.last_sample_time) > 1:
            self.last_sample_time += 1
            self.log.debug(
                "Adding phantom zero sample: %s", self.last_sample_time)
            res = self.get_zero_sample(
                datetime.datetime.fromtimestamp(self.last_sample_time))
        else:
            res = self.pending_second_data_queue.pop(0)
        self.last_sample_time = int(time.mktime(res.time.timetuple()))
        res.overall.planned_requests = self.__get_expected_rps()
        self.log.debug("Pop result: %s", res)
        return res

    def pop_second(self):
        self.__aggregate_next_second()

        if not self.pending_second_data_queue:
            self.log.debug("pending_second_data_queue empty")
            return None
        else:
            self.log.debug(
                "pending_second_data_queue: %s", self.pending_second_data_queue)
            res = self.__process_pending_second()
            return res

    def __get_expected_rps(self):
        """
        Mark second with expected rps
        """
        while self.steps and self.steps[0][1] < 1:
            self.steps.pop(0)

        if not self.steps:
            return 0
        else:
            self.steps[0][1] -= 1
            return self.steps[0][0]


class UsedInstancesCriteria(AbstractCriteria):

    """
    Autostop criteria, based on active instances count
    """
    RC_INST = 24

    @staticmethod
    def get_type_string():
        return 'instances'

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.autostop = autostop
        self.threads_limit = 1

        level_str = param_str.split(',')[0].strip()
        if level_str[-1:] == '%':
            self.level = float(level_str[:-1]) / 100
            self.is_relative = True
        else:
            self.level = int(level_str)
            self.is_relative = False
        self.seconds_limit = tankcore.expand_to_seconds(
            param_str.split(',')[1])

        try:
            phantom = autostop.core.get_plugin_of_type(PhantomPlugin)
            info = phantom.get_info()
            if info:
                self.threads_limit = info.instances
            if not self.threads_limit:
                raise ValueError(
                    "Cannot create 'instances' criteria with zero instances limit")
        except KeyError:
            self.log.warning(
                "No phantom module, 'instances' autostop disabled")

    def notify(self, aggregate_second):
        threads = aggregate_second.overall.active_threads
        if self.is_relative:
            threads = float(threads) / self.threads_limit
        if threads > self.level:
            if not self.seconds_count:
                self.cause_second = aggregate_second

            self.log.debug(self.explain())

            self.seconds_count += 1
            self.autostop.add_counting(self)
            if self.seconds_count >= self.seconds_limit:
                return True
        else:
            self.seconds_count = 0

        return False

    def get_rc(self):
        return self.RC_INST

    def get_level_str(self):
        """
        String value for instances level
        """
        if self.is_relative:
            level_str = str(100 * self.level) + "%"
        else:
            level_str = self.level
        return level_str

    def explain(self):
        items = (self.get_level_str(),
                 self.seconds_count, self.cause_second.time)
        return "Testing threads (instances) utilization higher than %s for %ss, since %s" % items

    def widget_explain(self):
        items = (self.get_level_str(), self.seconds_count, self.seconds_limit)
        return "Instances >%s for %s/%ss" % items, float(self.seconds_count) / self.seconds_limit


# ========================================================================

########NEW FILE########
__FILENAME__ = PhantomUtils
""" Utility classes for phantom module """
from ipaddr import AddressValueError
import copy
import ipaddr
import logging
import multiprocessing
import os
import socket
import string
import traceback

from Tank.stepper import StepperWrapper




# TODO: use separate answ log per benchmark
class PhantomConfig:
    """ config file generator """
    OPTION_PHOUT = "phout_file"
    SECTION = 'phantom'

    def __init__(self, core):
        self.core = core
        self.log = logging.getLogger(__name__)
        self.streams = []

        # common
        self.timeout = -1
        self.answ_log = None
        self.answ_log_level = None
        self.phout_file = None
        self.stat_log = None
        self.phantom_log = None
        self.phantom_start_time = None
        self.phantom_modules_path = None
        self.threads = None
        self.additional_libs = None

    def get_option(self, opt_name, default=None):
        """ get option wrapper """
        return self.core.get_option(self.SECTION, opt_name, default)

    @staticmethod
    def get_available_options():
        opts = ["threads", "phantom_modules_path",
                "additional_libs", "writelog", ]
        opts += StreamConfig.get_available_options()
        return opts

    def read_config(self):
        """        Read phantom tool specific options        """
        self.threads = self.get_option(
            "threads", str(int(multiprocessing.cpu_count() / 2) + 1))
        self.phantom_modules_path = self.get_option(
            "phantom_modules_path", "/usr/lib/phantom")
        self.additional_libs = self.get_option("additional_libs", "")
        self.answ_log_level = self.get_option("writelog", "none")
        if self.answ_log_level == '0':
            self.answ_log_level = 'none'
        elif self.answ_log_level == '1':
            self.answ_log_level = 'all'

        self.answ_log = self.core.mkstemp(".log", "answ_")
        self.core.add_artifact_file(self.answ_log)
        self.phout_file = self.core.get_option(self.SECTION, self.OPTION_PHOUT, '')
        if not self.phout_file:
            self.phout_file = self.core.mkstemp(".log", "phout_")
            self.core.set_option(self.SECTION, self.OPTION_PHOUT, self.phout_file)
        self.core.add_artifact_file(self.phout_file)
        self.stat_log = self.core.mkstemp(".log", "phantom_stat_")
        self.core.add_artifact_file(self.stat_log)
        self.phantom_log = self.core.mkstemp(".log", "phantom_")
        self.core.add_artifact_file(self.phantom_log)

        main_stream = StreamConfig(
            self.core, len(self.streams), self.phout_file,
            self.answ_log, self.answ_log_level, self.timeout, self.SECTION)
        self.streams.append(main_stream)

        for section in self.core.config.find_sections(self.SECTION + '-'):
            self.streams.append(
                StreamConfig(self.core, len(self.streams), self.phout_file, self.answ_log, self.answ_log_level,
                             self.timeout, section))

        for stream in self.streams:
            stream.read_config()

    def compose_config(self):
        """        Generate phantom tool run config        """
        streams_config = ''
        stat_benchmarks = ''
        for stream in self.streams:
            streams_config += stream.compose_config()
            if stream.section != self.SECTION:
                stat_benchmarks += " " + "benchmark_io%s" % stream.sequence_no

        kwargs = {}
        kwargs['threads'] = self.threads
        kwargs['phantom_log'] = self.phantom_log
        kwargs['stat_log'] = self.stat_log
        kwargs['benchmarks_block'] = streams_config
        kwargs['stat_benchmarks'] = stat_benchmarks
        kwargs['additional_libs'] = self.additional_libs
        kwargs['phantom_modules_path'] = self.phantom_modules_path
        filename = self.core.mkstemp(".conf", "phantom_")
        self.core.add_artifact_file(filename)
        self.log.debug("Generating phantom config: %s", filename)
        tpl_file = open(os.path.dirname(__file__) + "/phantom/phantom.conf.tpl", 'r')
        template_str = tpl_file.read()
        tpl_file.close()
        tpl = string.Template(template_str)
        config = tpl.substitute(kwargs)

        handle = open(filename, 'w')
        handle.write(config)
        handle.close()
        return filename

    def set_timeout(self, timeout):
        """ pass timeout to all streams """
        for stream in self.streams:
            stream.timeout = timeout

    def get_info(self):
        """ get merged info about phantom conf """
        result = copy.copy(self.streams[0])
        result.stat_log = self.stat_log
        result.steps = []
        result.ammo_file = ''
        result.rps_schedule = None
        result.ammo_count = 0
        result.duration = 0

        result.instances = 0
        result.loadscheme = []
        result.loop_count = 0

        for stream in self.streams:
            sec_no = 0
            logging.debug("Steps: %s", stream.stepper_wrapper.steps)
            for item in stream.stepper_wrapper.steps:
                for x in range(0, item[1]):
                    if len(result.steps) > sec_no:
                        result.steps[sec_no][0] += item[0]
                    else:
                        result.steps.append([item[0], 1])
                    sec_no += 1

            if result.rps_schedule:
                result.rps_schedule = []
            else:
                result.rps_schedule = stream.stepper_wrapper.loadscheme
            if result.loadscheme:
                result.loadscheme = ''
            else:
                # FIXME: add formatted load scheme for server:
                # <step_size,step_type,first_rps,last_rps,original_step_params>
                # as a string
                result.loadscheme = ''

            if result.loop_count:
                result.loop_count = u'0'
            else:
                result.loop_count = stream.stepper_wrapper.loop_count

            result.ammo_file += stream.stepper_wrapper.ammo_file + ' '
            result.ammo_count += stream.stepper_wrapper.ammo_count
            result.duration = max(
                result.duration, stream.stepper_wrapper.duration)
            result.instances += stream.instances

        if not result.ammo_count:
            raise ValueError("Total ammo count cannot be zero")
        return result


class StreamConfig:
    """ each test stream's config """

    OPTION_INSTANCES_LIMIT = 'instances'

    def __init__(self, core, sequence, phout, answ, answ_level, timeout, section):
        self.core = core
        self.sequence_no = sequence
        self.log = logging.getLogger(__name__)
        self.section = section
        self.stepper_wrapper = StepperWrapper(self.core, self.section)
        self.phout_file = phout
        self.answ_log = answ
        self.answ_log_level = answ_level
        self.timeout = timeout

        # per benchmark
        self.instances = None
        self.ipv6 = None
        self.ssl = None
        self.address = None
        self.port = None
        self.tank_type = None
        self.stpd = None
        self.gatling = None
        self.phantom_http_line = None
        self.phantom_http_field_num = None
        self.phantom_http_field = None
        self.phantom_http_entity = None
        self.resolved_ip = None
        self.method_prefix = None
        self.source_log_prefix = None
        self.method_options = None

    def get_option(self, option_ammofile, default=None):
        """ get option wrapper """
        return self.core.get_option(self.section, option_ammofile, default)

    @staticmethod
    def get_available_options():
        opts = ["ssl", "tank_type", 'gatling_ip',
                "method_prefix", "source_log_prefix"]
        opts += ["phantom_http_line", "phantom_http_field_num",
                 "phantom_http_field", "phantom_http_entity"]
        opts += ['address', "port", StreamConfig.OPTION_INSTANCES_LIMIT]
        opts += StepperWrapper.get_available_options()
        return opts

    def read_config(self):
        """ reads config """
        # multi-options
        self.ssl = int(self.get_option("ssl", '0'))
        self.tank_type = self.get_option("tank_type", 'http')
        # TODO: refactor. Maybe we should decide how to interact with StepperWrapper here.
        self.instances = int(
            self.get_option(self.OPTION_INSTANCES_LIMIT, '1000'))
        self.gatling = ' '.join(self.get_option('gatling_ip', '').split("\n"))
        self.method_prefix = self.get_option("method_prefix", 'method_stream')
        self.method_options = self.get_option("method_options", '')
        self.source_log_prefix = self.get_option("source_log_prefix", '')

        self.phantom_http_line = self.get_option("phantom_http_line", "")
        self.phantom_http_field_num = self.get_option(
            "phantom_http_field_num", "")
        self.phantom_http_field = self.get_option("phantom_http_field", "")
        self.phantom_http_entity = self.get_option("phantom_http_entity", "")

        self.address = self.get_option('address', '127.0.0.1')
        self.port = self.get_option('port', '80')

        #address check section
        self.ip_resolved_check = False
        if not self.ip_resolved_check:
            self.__address_ipv4_check()
        if not self.ip_resolved_check:
            self.__address_ipv6_check()
        if not self.ip_resolved_check:
            self.__resolve_address()
        if not self.ip_resolved_check:
            raise RuntimeError("Address resolve/parse section failed.")
        self.stepper_wrapper.read_config()

    def compose_config(self):
        """ compose benchmark block """
        # step file
        self.stepper_wrapper.prepare_stepper()
        self.stpd = self.stepper_wrapper.stpd
        if self.stepper_wrapper.instances:
            self.instances = self.stepper_wrapper.instances

        if not self.stpd:
            raise RuntimeError("Cannot proceed with no STPD file")

        kwargs = {}
        kwargs['sequence_no'] = self.sequence_no
        kwargs[
            'ssl_transport'] = "transport_t ssl_transport = transport_ssl_t { timeout = 1s }\n transport = ssl_transport" if self.ssl else ""
        kwargs['method_stream'] = self.method_prefix + \
                                  "_ipv6_t" if self.ipv6 else self.method_prefix + "_ipv4_t"
        kwargs['phout'] = self.phout_file
        kwargs['answ_log'] = self.answ_log
        kwargs['answ_log_level'] = self.answ_log_level
        kwargs['comment_answ'] = "# " if self.answ_log_level == 'none' else ''
        kwargs['stpd'] = self.stpd
        kwargs['source_log_prefix'] = self.source_log_prefix
        kwargs['method_options'] = self.method_options
        if self.tank_type:
            kwargs[
                'proto'] = "proto=http_proto%s" % self.sequence_no if self.tank_type == 'http' else "proto=none_proto"
            kwargs['comment_proto'] = ""
        else:
            kwargs['proto'] = ""
            kwargs['comment_proto'] = "#"

        if self.gatling:
            kwargs['bind'] = 'bind={ ' + self.gatling + ' }'
        else:
            kwargs['bind'] = ''
        kwargs['ip'] = self.resolved_ip
        kwargs['port'] = self.port
        kwargs['timeout'] = self.timeout
        kwargs['instances'] = self.instances
        tune = ''
        if self.phantom_http_entity:
            tune += "entity = " + self.phantom_http_entity + "\n"
        if self.phantom_http_field:
            tune += "field = " + self.phantom_http_field + "\n"
        if self.phantom_http_field_num:
            tune += "field_num = " + self.phantom_http_field_num + "\n"
        if self.phantom_http_line:
            tune += "line = " + self.phantom_http_line + "\n"
        if tune:
            kwargs['reply_limits'] = 'reply_limits = {\n' + tune + "}"
        else:
            kwargs['reply_limits'] = ''

        if self.section == PhantomConfig.SECTION:
            fname = 'phantom_benchmark_main.tpl'
        else:
            fname = 'phantom_benchmark_additional.tpl'
        tplf = open(os.path.dirname(__file__) + '/phantom/' + fname, 'r')
        template_str = tplf.read()
        tplf.close()
        tpl = string.Template(template_str)
        config = tpl.substitute(kwargs)

        return config

    def __address_ipv4_check(self):
        """ Analyse target address, IPv4 """
        self.ip_resolved_check = False
        if not self.address:
            raise RuntimeError("Target address not specified")
        #IPv4 check
        try:
            address_final = ipaddr.IPv4Address(self.address)
        except AddressValueError:
            self.log.debug("%s is not IPv4 address", self.address)
        else:
            self.ipv6 = False
            self.ip_resolved_check = True
            self.resolved_ip = address_final
            self.log.debug("%s is IPv4 address", self.address)
        #IPv4:port check
        try:
            address_port = self.address.split(":")
            address_final = ipaddr.IPv4Address(address_port[0])
        except AddressValueError, exc:
            self.log.debug("%s is not IPv4 address:port %s", self.address, traceback.format_exc(exc))
        else:
            self.ipv6 = False
            self.ip_resolved_check = True
            self.resolved_ip = address_final
            self.address = address_port[0]
            if len(address_port) > 1:
                self.port = address_port[1]
                self.log.warning(
                    "Address and port should be specified separately via 'address' and 'port' options. "
                    "Old behavior when \":\" was used as an address/port separator "
                    "is deprecated and better be avoided")
            self.log.debug(
                "%s is IPv4 address and %s is port", address_final, self.port)

    def __address_ipv6_check(self):
        """ Analyse target address, IPv6 """
        self.ip_resolved_check = False
        if not self.address:
            raise RuntimeError("Target address not specified")
        try:
            address_final = ipaddr.IPv6Address(self.address)
        except AddressValueError:
            self.log.debug(
                "%s is not IPv6 address", self.address)
        else:
            self.ipv6 = True
            self.ip_resolved_check = True
            self.resolved_ip = address_final
            self.log.debug(
                "%s is IPv6 address", address_final)

    def __resolve_address(self):
        """ Resolve hostname to IPv4/IPv6 and analyse what has been resolved """
        self.ip_resolved_check = False
        if not self.address:
            raise RuntimeError("Target address not specified")
        #address:port split
        address_port = self.address.split(":")
        if len(address_port) > 1:
            self.port = address_port[1]
            address_port = address_port[0]
            self.log.warning(
                "Address and port should be specified separately via 'address' and 'port' options. "
                "Old behavior when \":\" was used as an address/port separator "
                "is deprecated and better be avoided")
        else:
            address_port = self.address
        test_sock = None
        try:
            lookup = socket.getaddrinfo(address_port, self.port, socket.AF_UNSPEC, socket.SOCK_STREAM)
        except Exception, msg:
            logging.debug("Problems resolving target name: %s", traceback.format_exc(msg))
            raise RuntimeError("Unable to resolve hostname for %s", address_port)
        #resolve and establish a connection to resolved ip
        for res in lookup:
            af, socktype, proto, canonname, sa = res
            try:
                test_sock = socket.socket(af, socktype, proto)
            except Exception as msg:
                self.log.debug("Failed to create socket, Error code: %s", msg[0])
                test_sock = None
                continue
            try:
                test_sock.settimeout(5)
                test_sock.connect(sa)
            except socket.error as msg:
                test_sock.close()
                test_sock = None
                continue
            else:
                address_final = sa[0]
                test_sock.close()
                try:
                    ipaddr.IPv4Address(address_final)
                except AddressValueError:
                    self.log.debug("Resolved address %s is not IPv4", address_final)
                else:
                    self.ipv6 = False
                    self.ip_resolved_check = True
                    self.resolved_ip = address_final
                    self.address = address_port
                    self.log.info(
                        "Successfully established connection to resolved IPv4 %s, port %s", address_final, self.port)
                    break
                try:
                    ipaddr.IPv6Address(address_final)
                except AddressValueError:
                    self.log.debug(
                        "Resolved address %s is not IPv6", address_final)
                else:
                    self.ipv6 = True
                    self.ip_resolved_check = True
                    self.resolved_ip = address_final
                    self.address = address_port
                    self.log.info(
                        "Successfully established connection to resolved IPv6 %s, port %s", address_final, self.port)
                    break

# ========================================================================

########NEW FILE########
__FILENAME__ = RCAssert
''' Tank exit code check plugin '''
from tankcore import AbstractPlugin

class RCAssertPlugin(AbstractPlugin):
    ''' Apache Benchmark plugin '''
    SECTION = 'rcassert'
    
    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.ok_codes = []
        self.fail_code = 10

    @staticmethod
    def get_key():
        return __file__
    
    def get_available_options(self):
        return ["pass", "fail_code"]
    
    def configure(self):
        codes = self.get_option("pass", '').split(' ')
        for code in codes:
            if code:
                self.ok_codes.append(int(code))
        self.fail_code = int(self.get_option("fail_code", self.fail_code))

    def post_process(self, retcode):
        if not self.ok_codes:
            return retcode
        
        for code in self.ok_codes:
            self.log.debug("Comparing %s with %s codes", code, retcode)
            if code == int(retcode):
                self.log.info("Exit code %s was changed to 0 by RCAssert plugin", code)
                return 0
        
        self.log.info("Changing exit code to %s because RCAssert pass list was unsatisfied", self.fail_code)
        return self.fail_code
        
            


########NEW FILE########
__FILENAME__ = Report
"""Report plugin that plots some graphs"""

import datetime
import time
import string
import json
import os
from collections import defaultdict

from Tank.Plugins.Aggregator import AggregateResultListener, AggregatorPlugin
from Tank.Plugins.Monitoring import MonitoringPlugin
from Tank.MonCollector.collector import MonitoringDataListener, MonitoringDataDecoder
from tankcore import AbstractPlugin


class ReportPlugin(AbstractPlugin, AggregateResultListener, MonitoringDataListener):
    """Graphite data uploader"""

    SECTION = 'report'

    @staticmethod
    def get_key():
        return __file__

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.decoder = MonitoringDataDecoder()
        self.mon_data = {}

        def create_storage():
            return {
                'avg': defaultdict(list),
                'quantiles': defaultdict(list),
                'threads': {
                    'active_threads': []
                },
                'rps': {
                    'RPS': []
                },
                'http_codes': defaultdict(list),
                'net_codes': defaultdict(list),
            }

        self.overall = create_storage()
        self.cases = defaultdict(create_storage)
        self.start_time = None
        self.end_time = None
        self.show_graph = None
        self.template = None

    def monitoring_data(self, data_string):
        self.log.debug("Mon report data: %s", data_string)
        for line in data_string.splitlines():
            if not line.strip():
                continue

            def append_data(host, ts, data):
                if host not in self.mon_data:
                    self.mon_data[host] = {}
                host_data = self.mon_data[host]
                for key, value in data.iteritems():
                    try:
                        value = float(value)
                        if '_' in key:
                            group, key = key.split('_', 1)
                        else:
                            group = key
                        if group not in host_data:
                            host_data[group] = {}
                        group_data = host_data[group]
                        if key not in group_data:
                            group_data[key] = []
                        group_data[key].append((int(ts), value))
                    except ValueError:
                        pass

            host1, data1, _, ts1 = self.decoder.decode_line(line)
            append_data(host1, ts1, data1)

    def get_available_options(self):
        return ["show_graph", "template"]

    def start_test(self):
        start_time = datetime.datetime.now()
        self.start_time = start_time.strftime("%H:%M%%20%Y%m%d")

    def end_test(self, retcode):
        end_time = datetime.datetime.now() + datetime.timedelta(minutes=1)
        self.end_time = end_time.strftime("%H:%M%%20%Y%m%d")

    def configure(self):
        """Read configuration"""
        self.show_graph = self.get_option("show_graph", "")
        default_template = "/report.tpl"
        self.template = self.get_option("template", os.path.dirname(__file__) + default_template)
        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
            aggregator.add_result_listener(self)
        except KeyError:
            self.log.warning("No aggregator module, no valid report will be available")

        try:
            mon = self.core.get_plugin_of_type(MonitoringPlugin)
            if mon.monitoring:
                mon.monitoring.add_listener(self)
        except KeyError:
            self.log.warning("No monitoring module, monitroing report disabled")

    def aggregate_second(self, data):
        """
        @data: SecondAggregateData
        """
        ts = int(time.mktime(data.time.timetuple()))

        def add_aggreagted_second(data_item, storage):
            data_dict = data_item.__dict__
            avg = storage['avg']
            for key in ["avg_connect_time", "avg_send_time", "avg_latency", "avg_receive_time"]:
                avg[key].append((ts, data_dict.get(key, None)))
            quantiles = storage['quantiles']
            for key, value in data_item.quantiles.iteritems():
                quantiles[key].append((ts, value))
            storage['threads']['active_threads'].append((ts, data_item.active_threads))
            storage['rps']['RPS'].append((ts, data_item.RPS))
            http_codes = storage['http_codes']
            for key, value in data_item.http_codes.iteritems():
                http_codes[key].append((ts, value))
            net_codes = storage['net_codes']
            for key, value in data_item.net_codes.iteritems():
                net_codes[key].append((ts, value))

        add_aggreagted_second(data.overall, self.overall)
        for case, case_data in data.cases.iteritems():
            add_aggreagted_second(case_data, self.cases[case])

    def post_process(self, retcode):
        self.log.info("Building HTML report...")
        results = {
            'overall': self.overall,
            'cases': self.cases,
            'monitoring': self.mon_data,
        }
        template = open(self.template, 'r').read()
        report_html = self.core.mkstemp(".html", "report_")
        self.core.add_artifact_file(report_html)
        with open(report_html, 'w') as report_html_file:
            report_html_file.write(
                string.Template(template).safe_substitute(
                    metrics=json.dumps(results),
                )
            )
        return retcode
########NEW FILE########
__FILENAME__ = ResourceCheck
''' Module to check system resources at load generator'''

import time
import logging

from tankcore import AbstractPlugin
import tankcore


class ResourceCheckPlugin(AbstractPlugin):
    '''   Plugin to check system resources    '''
    SECTION = "rcheck"

    @staticmethod
    def get_key():
        return __file__


    def __init__(self, core):
        '''         Constructor        '''
        AbstractPlugin.__init__(self, core)
        self.interval = "10s"
        self.disk_limit = 2048  # 2 GB
        self.mem_limit = 512  # 0.5 GB
        self.last_check = 0

    def get_available_options(self):
        return ["interval", "disk_limit", "mem_limit"]

    def configure(self):
        self.interval = tankcore.expand_to_seconds(self.get_option("interval", self.interval))
        self.disk_limit = int(self.get_option("disk_limit", self.disk_limit))
        self.mem_limit = int(self.get_option("mem_limit", self.mem_limit))

    def prepare_test(self):
        self.log.info("Checking tank resources...")
        self.__check_disk()
        self.__check_mem()

    def is_test_finished(self):
        self.log.debug("Checking tank resources...")
        if time.time() - self.last_check < self.interval:
            return -1
        self.__check_disk()
        self.__check_mem()
        self.last_check = time.time()
        return -1


    def __check_disk(self):
        ''' raise exception on disk space exceeded '''
        cmd = "sh -c \"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs "
        cmd += self.core.artifacts_base_dir
        cmd += " | tail -n 1 | awk '{print \$4}' \""
        res = tankcore.execute(cmd, True, 0.1, True)
        logging.debug("Result: %s", res)
        if not len(res[1]):
    	    self.log.debug("No disk usage info: %s", res[2])
    	    return
        disk_free = res[1]
        self.log.debug("Disk free space: %s/%s", disk_free.strip(), self.disk_limit)
        if int(disk_free.strip()) < self.disk_limit:
            raise RuntimeError("Not enough local resources: disk space less than %sMB in %s: %sMB" % (
                self.disk_limit, self.core.artifacts_base_dir, int(disk_free.strip())))


    def __check_mem(self):
        ''' raise exception on RAM exceeded '''
        cmd = "free -m | awk '$1==\"-/+\" {print $4}'"
        mem_free = int(tankcore.execute(cmd, True, 0.1, True)[1].strip())
        self.log.debug("Memory free: %s/%s", mem_free, self.mem_limit)
        if mem_free < self.mem_limit:
            raise RuntimeError("Not enough resources: free memory less than %sMB: %sMB" % (self.mem_limit, mem_free)) 


########NEW FILE########
__FILENAME__ = ShellExec
'''
Contains shellexec plugin
'''
from tankcore import AbstractPlugin
import tankcore
class ShellExecPlugin(AbstractPlugin):
    '''
    ShellExec plugin
    allows executing shell scripts before/after test
    '''
    SECTION = 'shellexec'
    
    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.end = None
        self.poll = None
        self.prepare = None
        self.start = None
        self.postprocess = None

    @staticmethod
    def get_key():
        return __file__
    
    def get_available_options(self):
        return ["prepare", "start", "end", "poll", "post_process"]
    
    def configure(self):
        self.prepare = self.get_option("prepare", '')
        self.start = self.get_option("start", '')
        self.end = self.get_option("end", '')
        self.poll = self.get_option("poll", '')
        self.postprocess = self.get_option("post_process", '')

    def prepare_test(self):
        if self.prepare:
            self.execute(self.prepare)          
            
    def start_test(self):
        if self.start:
            self.execute(self.start)

    def is_test_finished(self):
        if self.poll:
            self.log.info("Executing: %s", self.poll)
            retcode = tankcore.execute(self.poll, shell=True, poll_period=0.1)[0]
            if retcode:
                self.log.warn("Non-zero exit code, interrupting test: %s", retcode)
                return retcode
        return -1
            
    def end_test(self, retcode):
        if self.end:
            self.execute(self.end)
        return retcode

    def post_process(self, retcode):
        if self.postprocess:
            self.execute(self.postprocess)
        return retcode

    def execute(self, cmd):
        '''
        Execute and check exit code
        '''
        self.log.info("Executing: %s", cmd)
        retcode = tankcore.execute(cmd, shell=True, poll_period=0.1)[0]
        if retcode:
            raise RuntimeError("Subprocess returned %s" % retcode)    


########NEW FILE########
__FILENAME__ = TipsAndTricks
'''
Plugin showing tool learning hints in console
'''

from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin, AbstractInfoWidget
from tankcore import AbstractPlugin
import os
import random
import textwrap

class TipsAndTricksPlugin(AbstractPlugin, AbstractInfoWidget):
    '''
    Tips showing plugin
    '''
    SECTION = 'tips'
    
    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        AbstractInfoWidget.__init__(self)
        lines = open(os.path.dirname(__file__) + '/tips.txt').readlines()
        line = random.choice(lines)
        self.section = line[:line.index(':')]
        self.tip = line[line.index(':') + 1:].strip()
        self.disable = 0
        
    @staticmethod
    def get_key():
        return __file__
    
    def get_available_options(self):
        return ["disable"]
    
    def configure(self):
        self.disable = int(self.get_option('disable', '0'))
    
    def prepare_test(self):
        if not self.disable:
            try:
                console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
            except KeyError, ex:
                self.log.debug("Console not found: %s", ex)
                console = None
                
            if console:    
                console.add_info_widget(self)        
    
    def get_index(self):
        return 10000 # really last index            

    def render(self, screen):
        line = screen.markup.WHITE + "Tips & Tricks => " + self.section + screen.markup.RESET + ":\n  "
        line += "\n  ".join(textwrap.wrap(self.tip, screen.right_panel_width - 2))
        return line
        

########NEW FILE########
__FILENAME__ = TotalAutostop
''' Cummulative Autostops '''
from Tank.Plugins.Aggregator import AggregateResultListener
from Tank.Plugins.Autostop import AbstractCriteria, AutostopPlugin

from collections import deque
from tankcore import AbstractPlugin
import re
import tankcore
import math

class TotalAutostopPlugin(AbstractPlugin, AggregateResultListener):
    ''' Cummulative Criterias Plugin '''
    SECTION = 'autostop'
    @staticmethod
    def get_key():
        return __file__

    def configure(self):
        autostop = self.core.get_plugin_of_type(AutostopPlugin)
        autostop.add_criteria_class(TotalFracTimeCriteria)
        autostop.add_criteria_class(TotalHTTPCodesCriteria)
        autostop.add_criteria_class(TotalNetCodesCriteria)
        autostop.add_criteria_class(TotalNegativeHTTPCodesCriteria)
        autostop.add_criteria_class(TotalNegativeNetCodesCriteria)
        autostop.add_criteria_class(TotalHTTPTrendCriteria)
        autostop.add_criteria_class(QuantileOfSaturationCriteria)

    def prepare_test(self):
        pass

    def start_test(self):
        pass

    def end_test(self, retcode):
        pass

    def aggregate_second(self, second_aggregate_data):
        pass

class TotalFracTimeCriteria(AbstractCriteria):
    ''' Cummulative Time Criteria '''
    @staticmethod
    def get_type_string():
        return 'total_time'
    
    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        param = param_str.split(',')
        self.seconds_count = 0
        self.rt_limit = tankcore.expand_to_milliseconds(param[0])
        self.frac = param[1][:-1]
        self.seconds_limit = tankcore.expand_to_seconds(param[2])
        self.autostop = autostop
        self.data = deque()
        self.second_window = deque()
        self.real_frac = float()

    def notify(self, aggregate_second):
        failcnt = 0
        cnt = 0
        for i in reversed(aggregate_second.overall.times_dist):
            if i['from'] >= self.rt_limit :
                failcnt += i['count']
            cnt += i['count']
        if cnt != 0 :
            value = float(failcnt) / cnt
        else :
            value = 0
        self.data.append(value)
        self.second_window.append(aggregate_second)
        if len(self.data) > self.seconds_limit:
            self.data.popleft()
            self.second_window.popleft()

        self.real_frac = float(sum(self.data)) / len(self.data) * 100
        if self.real_frac >= float(self.frac) and len(self.data) >= self.seconds_limit:
            self.cause_second = self.second_window[0]
            self.log.debug(self.explain())
#            self.autostop.add_counting(self)
            return True
        return False

    def get_rc(self):
        return 25

    def explain(self):
        items = (round(self.real_frac, 2), self.rt_limit, self.seconds_limit, self.cause_second.time)
        return "%s%% responses times higher than %sms for %ss since: %s" % items

    def widget_explain(self):
        items = (round(self.real_frac, 2), self.rt_limit, self.seconds_limit)
        return ("%s%% Times >%sms for %ss" % items, self.real_frac)

class TotalHTTPCodesCriteria(AbstractCriteria):
    ''' Cummulative HTTP Criteria '''
    @staticmethod
    def get_type_string():
        return 'total_http'
    
    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.codes_mask = param_str.split(',')[0].lower()
        self.codes_regex = re.compile(self.codes_mask.replace("x", '.'))
        self.autostop = autostop
        self.data = deque()
        self.second_window = deque()
        
        level_str = param_str.split(',')[1].strip()
        if level_str[-1:] == '%':
            self.level = float(level_str[:-1])
            self.is_relative = True
        else:
            self.level = int(level_str)
            self.is_relative = False
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[2])
    
    def notify(self, aggregate_second):
        matched_responses = self.count_matched_codes(self.codes_regex, aggregate_second.overall.http_codes)
        if self.is_relative:
            if aggregate_second.overall.RPS:
                matched_responses = float(matched_responses) / aggregate_second.overall.RPS * 100
            else:
                matched_responses = 1
        self.log.debug("HTTP codes matching mask %s: %s/%s", self.codes_mask, matched_responses, self.level)
        self.data.append(matched_responses)
        self.second_window.append(aggregate_second)
        if len(self.data) > self.seconds_limit :
            self.data.popleft()
            self.second_window.popleft()
        queue_len = 1
        if self.is_relative :
            queue_len = len(self.data)
        if (sum(self.data) / queue_len) >= self.level and len(self.data) >= self.seconds_limit:
            self.cause_second = self.second_window[0]
            self.log.debug(self.explain())
#            self.autostop.add_counting(self)
            return True
        return False

    def get_rc(self):
        return 26

    def get_level_str(self):
        ''' format level str '''
        if self.is_relative:
            level_str = str(self.level) + "%"
        else:
            level_str = self.level
        return level_str

    def explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
            return "%s codes count higher than %s for %ss, ended at: %s" % items
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
        return "%s codes count higher than %s for %ss, since %s" % items 
    
    def widget_explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
            return ("HTTP %s>%s for %ss" % items, sum(self.data))
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
        return ("HTTP %s>%s for %ss" % items, 1.0)

class TotalNetCodesCriteria(AbstractCriteria):
    ''' Cummulative Net Criteria '''
    @staticmethod
    def get_type_string():
        return 'total_net'

    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.codes_mask = param_str.split(',')[0].lower()
        self.codes_regex = re.compile(self.codes_mask.replace("x", '.'))
        self.autostop = autostop
        self.data = deque()
        self.second_window = deque()

        level_str = param_str.split(',')[1].strip()
        if level_str[-1:] == '%':
            self.level = float(level_str[:-1])
            self.is_relative = True
        else:
            self.level = int(level_str)
            self.is_relative = False
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[2])
    

    def notify(self, aggregate_second):
        codes = aggregate_second.overall.net_codes.copy()
        if '0' in codes.keys():
            codes.pop('0')
        matched_responses = self.count_matched_codes(self.codes_regex, codes)
        if self.is_relative:
            if aggregate_second.overall.RPS:
                matched_responses = float(matched_responses) / aggregate_second.overall.RPS * 100
                self.log.debug("Net codes matching mask %s: %s%%/%s", self.codes_mask, round(matched_responses, 2), self.get_level_str())
            else:
                matched_responses = 1
        else : self.log.debug("Net codes matching mask %s: %s/%s", self.codes_mask, matched_responses, self.get_level_str())

        self.data.append(matched_responses)
        self.second_window.append(aggregate_second)
        if len(self.data) > self.seconds_limit :
            self.data.popleft()
            self.second_window.popleft()

        queue_len = 1
        if self.is_relative :
            queue_len = len(self.data)
        if (sum(self.data) / queue_len) >= self.level and len(self.data) >= self.seconds_limit:
            self.cause_second = self.second_window[0]
            self.log.debug(self.explain())
#            self.autostop.add_counting(self)
            return True
        return False

    def get_rc(self):
        return 27

    def get_level_str(self):
        ''' format level str '''
        if self.is_relative:
            level_str = str(self.level) + "%"
        else:
            level_str = str(self.level)
        return level_str

    def explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
            return "%s net codes count higher than %s for %ss, since %s" % items 
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
        return "%s net codes count higher than %s for %ss, since %s" % items 
    
    def widget_explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
            return ("Net %s>%s for %ss" % items, self.level)
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
        return ("Net %s>%s for %ss" % items, self.level)

class TotalNegativeHTTPCodesCriteria(AbstractCriteria):
    ''' Reversed HTTP Criteria '''
    @staticmethod
    def get_type_string():
        return 'negative_http'
    
    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.codes_mask = param_str.split(',')[0].lower()
        self.codes_regex = re.compile(self.codes_mask.replace("x", '.'))
        self.autostop = autostop
        self.data = deque()
        self.second_window = deque()
        
        level_str = param_str.split(',')[1].strip()
        if level_str[-1:] == '%':
            self.level = float(level_str[:-1])
            self.is_relative = True
        else:
            self.level = int(level_str)
            self.is_relative = False
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[2])
    
    def notify(self, aggregate_second):
        matched_responses = self.count_matched_codes(self.codes_regex, aggregate_second.overall.http_codes)
        if self.is_relative:
            if aggregate_second.overall.RPS:
                matched_responses = float(matched_responses) / aggregate_second.overall.RPS * 100
                matched_responses = 100 - matched_responses
            else:
                matched_responses = 1
            self.log.debug("HTTP codes matching mask not %s: %s/%s", self.codes_mask, round(matched_responses, 1), self.level)
        else :
            matched_responses = aggregate_second.overall.RPS - matched_responses
            self.log.debug("HTTP codes matching mask not %s: %s/%s", self.codes_mask, matched_responses, self.level)
        self.data.append(matched_responses)
        self.second_window.append(aggregate_second)
        if len(self.data) > self.seconds_limit :
            self.data.popleft()
            self.second_window.popleft()

        queue_len = 1
        if self.is_relative :
            queue_len = len(self.data)
        if (sum(self.data) / queue_len) >= self.level and len(self.data) >= self.seconds_limit:
            self.cause_second = self.second_window[0]
            self.log.debug(self.explain())
#            self.autostop.add_counting(self)
            return True
        return False

    def get_rc(self):
        return 28

    def get_level_str(self):
        ''' format level str'''
        if self.is_relative:
            level_str = str(self.level) + "%"
        else:
            level_str = self.level
        return level_str

    def explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
            return "Not %s codes count higher than %s for %ss, since %s" % items
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
        return "Not %s codes count higher than %s for %ss, since %s" % items 
    
    def widget_explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
            return ("HTTP not %s>%s for %ss" % items, sum(self.data))
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
        return ("HTTP not %s>%s for %ss" % items, 1.0)

class TotalNegativeNetCodesCriteria(AbstractCriteria):
    ''' Reversed NET Criteria '''
    @staticmethod
    def get_type_string():
        return 'negative_net'
    
    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.codes_mask = param_str.split(',')[0].lower()
        self.codes_regex = re.compile(self.codes_mask.replace("x", '.'))
        self.autostop = autostop
        self.data = deque()
        self.second_window = deque()
        
        level_str = param_str.split(',')[1].strip()
        if level_str[-1:] == '%':
            self.level = float(level_str[:-1])
            self.is_relative = True
        else:
            self.level = int(level_str)
            self.is_relative = False
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[2])
    
    def notify(self, aggregate_second):
        codes = aggregate_second.overall.net_codes.copy()
        # if '0' in codes.keys():
        #     codes.pop('0')
        matched_responses = self.count_matched_codes(self.codes_regex, codes)
        if self.is_relative:
            if aggregate_second.overall.RPS:
                matched_responses = float(matched_responses) / aggregate_second.overall.RPS * 100
                matched_responses = 100 - matched_responses
            else:
                matched_responses = 1
            self.log.debug("Net codes matching mask not %s: %s/%s", self.codes_mask, round(matched_responses, 1), self.level)
        else :
            matched_responses = aggregate_second.overall.RPS - matched_responses
            self.log.debug("Net codes matching mask not %s: %s/%s", self.codes_mask, matched_responses, self.level)
        self.data.append(matched_responses)
        self.second_window.append(aggregate_second)
        if len(self.data) > self.seconds_limit :
            self.data.popleft()
            self.second_window.popleft()

        queue_len = 1
        if self.is_relative :
            queue_len = len(self.data)
        if (sum(self.data) / queue_len) >= self.level and len(self.data) >= self.seconds_limit:
            self.cause_second = self.second_window[0]
            self.log.debug(self.explain())
            return True
        return False

    def get_rc(self):
        return 29

    def get_level_str(self):
        ''' format level str'''
        if self.is_relative:
            level_str = str(self.level) + "%"
        else:
            level_str = self.level
        return level_str

    def explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
            return "Not %s codes count higher than %s for %ss, since %s" % items
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit, self.cause_second.time)
        return "Not %s codes count higher than %s for %ss, since %s" % items 
    
    def widget_explain(self):
        if self.is_relative:
            items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
            return ("Net not %s>%s for %ss" % items, sum(self.data))
        items = (self.codes_mask, self.get_level_str(), self.seconds_limit)
        return ("Net not %s>%s for %ss" % items, 1.0)

class TotalHTTPTrendCriteria(AbstractCriteria):
    ''' HTTP Trend Criteria '''

    @staticmethod
    def get_type_string():
        return 'http_trend'
    
    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.seconds_count = 0
        self.codes_mask = param_str.split(',')[0].lower()
        self.codes_regex = re.compile(self.codes_mask.replace("x", '.'))
        self.autostop = autostop
        self.tangents = deque()
        self.second_window = deque()
        self.total_tan = float()

        self.tangents.append(0)
        self.last = 0
        self.seconds_limit = tankcore.expand_to_seconds(param_str.split(',')[1])
        self.measurement_error = float()
    
    def notify(self, aggregate_second):
        matched_responses = self.count_matched_codes(self.codes_regex, aggregate_second.overall.http_codes)

        self.tangents.append(matched_responses - self.last)
        self.second_window.append(aggregate_second)

        self.last = matched_responses

        if len(self.tangents) > self.seconds_limit :
            self.tangents.popleft()
            self.second_window.popleft()

        self.measurement_error = self.calc_measurement_error(self.tangents)

        self.total_tan = float(sum(self.tangents) / len (self.tangents))
        self.log.debug("Last trend for http codes %s: %.2f +/- %.2f", self.codes_mask, self.total_tan, self.measurement_error)

        if self.total_tan + self.measurement_error < 0 :
            self.cause_second = self.second_window[0]
            self.log.debug(self.explain())
            return True

        return False

    def calc_measurement_error(self, tangents):
        ''' formula for measurement error sqrt ( (sum(1, n, (k_i - <k>)**2) / (n*(n-1))) '''

        if len(tangents) < 2 :
            return 0.0

        avg_tan = float(sum(tangents) / len(tangents))
        numerator = float()
        for i in tangents:
            numerator += (i - avg_tan) * (i - avg_tan)

        return math.sqrt (numerator / len(tangents) / (len(tangents) - 1))

    def get_rc(self):
        return 30

    def explain(self):
        items = (self.codes_mask, self.total_tan, self.measurement_error, self.seconds_limit, self.cause_second.time)
        return "Last trend for %s http codes is %.2f +/- %.2f for %ss, since %s" % items 
    
    def widget_explain(self):
        items = (self.codes_mask, self.total_tan, self.measurement_error, self.seconds_limit)
        return ("HTTP(%s) trend is %.2f +/- %.2f < 0 for %ss" % items, 1.0)



class QuantileOfSaturationCriteria(AbstractCriteria):
    ''' Quantile of Saturation Criteria 
        example: qsat(50ms, 3m, 10%) '''

    @staticmethod
    def get_type_string():
        return 'qsat'
    
    def __init__(self, autostop, param_str):
        AbstractCriteria.__init__(self)
        self.autostop = autostop
        self.data = deque()
        self.second_window = deque()

        params = param_str.split(',')
        # qunatile in ms
        self.timing = tankcore.expand_to_milliseconds(params[0])
        # width of time in seconds
        self.width = tankcore.expand_to_seconds(params[1])
        # max height of deviations in percents
        self.height = float(params[2].split('%')[0])
        # last deviation in percents
        self.deviation = float()


    def __get_timing_quantile(self, aggr_data):
        ''' get quantile level for criteria timing '''
        quan = 0.0
        for timing in sorted(aggr_data.cumulative.times_dist.keys()):
            timing_item = aggr_data.cumulative.times_dist[timing]
            quan += float(timing_item['count']) / aggr_data.cumulative.total_count
            self.log.debug("tt: %s %s", self.timing, timing_item['to'])
            if self.timing <= timing_item['to']:
                return quan
        return quan
            

    def notify(self, aggregate_second):
        quan = 100 * self.__get_timing_quantile(aggregate_second)
        self.log.debug("Quantile for %s: %s", self.timing, quan)
        
        self.data.append(quan)
        self.second_window.append(aggregate_second)

        if len(self.data) > self.width :
            self.autostop.add_counting(self)
            self.data.popleft()
            self.second_window.popleft()

            self.deviation = max(self.data) - min(self.data)
            self.log.debug(self.explain())

            if self.deviation < self.height:
                return True
        return False

    def get_rc(self):
        return 33

    def explain(self):
        items = (self.timing, self.width, self.deviation, self.height)
        return "%sms variance for %ss: %.3f%% (<%s%%)" % items
    
    
    def widget_explain(self):
        level = self.height / self.deviation
        items = (self.timing, self.width, self.deviation, self.height)
        return ("%sms variance for %ss: %.3f%% (<%s%%)" % items, level)




########NEW FILE########
__FILENAME__ = UniversalPhoutShooter
''' Contains Universal Plugin for phout-compatible shooter '''
from Tank.Plugins.Aggregator import AggregatorPlugin, AggregateResultListener
from Tank.Plugins.Phantom import PhantomReader
from tankcore import AbstractPlugin
import subprocess
import time
import shlex
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin, AbstractInfoWidget
from Tank.Plugins import ConsoleScreen
import datetime

class UniversalPhoutShooterPlugin(AbstractPlugin, AggregateResultListener):
    '''     Plugin for running any script tool    '''

    OPTION_CONFIG = "config"
    SECTION = 'uniphout'
    
    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.buffered_seconds=2
        self.process=None
        self.process_stderr = None
        self.process_start_time = None
            
    @staticmethod
    def get_key():
        return __file__
    
    def get_available_options(self):
        opts = ["cmdline", "buffered_seconds", ]
        return opts
    
    def configure(self):       
        # plugin part
        self.cmdline = self.get_option("cmdline")
        self.buffered_seconds = int(self.get_option("buffered_seconds", self.buffered_seconds))
        
        self.phout = self.get_option("phout", "")
        if not self.phout:
            self.phout=self.core.mkstemp(".phout", "results_")
            # TODO: pass generated phout to the script
       
        self.core.add_artifact_file(self.phout)

    def prepare_test(self):
        aggregator = None
        try:
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
        except Exception, ex:
            self.log.warning("No aggregator found: %s", ex)

        if aggregator:
            aggregator.reader = PhantomReader(aggregator, self)
            aggregator.reader.buffered_seconds = self.buffered_seconds
            aggregator.add_result_listener(self)
            aggregator.reader.phout_file = self.phout
        
        try:
            console = self.core.get_plugin_of_type(ConsoleOnlinePlugin)
        except Exception, ex:
            self.log.debug("Console not found: %s", ex)
            console = None
        
        if console:
            widget = UniphoutInfoWidget(self)
            console.add_info_widget(widget)
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
            aggregator.add_result_listener(widget)
        
        
    def start_test(self):
        args = shlex.split(self.cmdline)
        self.log.info("Starting: %s", args)
        self.process_start_time = time.time()
        process_stderr_file = self.core.mkstemp(".log", "phantom_stdout_stderr_")
        self.core.add_artifact_file(process_stderr_file)
        self.process_stderr = open(process_stderr_file, 'w')
        self.process = subprocess.Popen(args, stderr=self.process_stderr, stdout=self.process_stderr, close_fds=True)
    

    def is_test_finished(self):
        retcode = self.process.poll()
        if retcode != None:
            self.log.info("Subprocess done its work with exit code: %s", retcode)
            return abs(retcode)
        else:
            return -1
    
    
    def end_test(self, retcode):
        if self.process and self.process.poll() == None:
            self.log.warn("Terminating worker process with PID %s", self.process.pid)
            self.process.terminate()
            if self.process_stderr:
                self.process_stderr.close()
        else:
            self.log.debug("Seems subprocess finished OK")
        return retcode
    
    def get_info(self):
        return None
    
    def aggregate_second(self, second_aggregate_data):
        pass

        
class UniphoutInfoWidget(AbstractInfoWidget):        
    ''' Right panel widget '''
    def __init__(self, uniphout):
        AbstractInfoWidget.__init__(self)
        self.krutilka = ConsoleScreen.krutilka()
        self.owner = uniphout
        self.rps = 0

    def get_index(self):
        return 0

    def aggregate_second(self, second_aggregate_data):
        self.active_threads = second_aggregate_data.overall.active_threads
        self.rps = second_aggregate_data.overall.RPS

    def render(self, screen):        
        text = " Uniphout Test %s" % self.krutilka.next()
        space = screen.right_panel_width - len(text) - 1 
        left_spaces = space / 2
        right_spaces = space / 2
        
        dur_seconds = int(time.time()) - int(self.owner.process_start_time)
        duration = str(datetime.timedelta(seconds=dur_seconds))        
        
        template = screen.markup.BG_BROWN + '~' * left_spaces + text + ' ' + '~' * right_spaces + screen.markup.RESET + "\n" 
        template += "Command Line: %s\n"
        template += "    Duration: %s\n"
        template += " Responses/s: %s"
        data = (self.owner.cmdline, duration, self.rps)
        
        return template % data



########NEW FILE########
__FILENAME__ = WebOnline
''' local webserver with online graphs '''
from BaseHTTPServer import HTTPServer, BaseHTTPRequestHandler
from threading import Thread
import json
import logging
import os.path
import socket
import time

from Tank.Plugins.Aggregator import AggregatorPlugin, AggregateResultListener
from tankcore import AbstractPlugin
import tankcore


class WebOnlinePlugin(AbstractPlugin, Thread, AggregateResultListener):
    ''' web online plugin '''
    SECTION = "web"

    @staticmethod
    def get_key():
        return __file__

    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        Thread.__init__(self)
        self.daemon = True  # Thread auto-shutdown
        self.port = 8080
        self.last_sec = None
        self.server = None
        self.interval = 60
        self.quantiles_data = []
        self.codes_data = []
        self.avg_data = []
        self.redirect = ''
        self.manual_stop = 0

    def get_available_options(self):
        return ["port", "interval", "redirect", "manual_stop"]

    def configure(self):
        self.port = int(self.get_option("port", self.port))
        self.interval = int(tankcore.expand_to_seconds(self.get_option("interval", '1m')))
        self.redirect = self.get_option("redirect", self.redirect)
        self.manual_stop = int(self.get_option('manual_stop', self.manual_stop))


    def prepare_test(self):
        try:
            self.server = OnlineServer(('', self.port), WebOnlineHandler)
            self.server.owner = self
            aggregator = self.core.get_plugin_of_type(AggregatorPlugin)
            aggregator.add_result_listener(self)
        except Exception, ex:
            self.log.warning("Failed to start web results server: %s", ex)


    def start_test(self):
        self.start()


    def end_test(self, retcode):
        # self.log.info("Shutting down local server")
        # self.server.shutdown() don't enable it since it leads to random deadlocks
        if self.manual_stop:
            raw_input('Press Enter, to close webserver.')

        if self.redirect:
            time.sleep(2)

        del self.server
        self.server = None
        return retcode


    def run(self):
        if (self.server):
            address = socket.gethostname()
            self.log.info("Starting local HTTP server for online view at port: http://%s:%s/", address, self.port)
            self.server.serve_forever()


    def __calculate_quantiles(self, data):
        ''' prepare response quantiles data '''
        if not self.quantiles_data:
            header = ["timeStamp", "requestCount"]
            quantiles = [int(x) for x in sorted(data.overall.quantiles.keys(), reverse=True)]
            header += quantiles
            self.quantiles_data = [header, []]
        item_data = {"timeStamp": time.mktime(data.time.timetuple()),
                     "requestCount": data.overall.planned_requests if data.overall.planned_requests else data.overall.RPS}
        for level, timing in data.overall.quantiles.iteritems():
            item_data[str(int(level))] = timing

        self.quantiles_data[1] += [item_data]
        while len(self.quantiles_data[1]) > self.interval:
            self.quantiles_data[1].pop(0)


    def __calculate_avg(self, data):
        ''' prepare response avg times data '''
        if not self.avg_data:
            header = ["timeStamp", "connect", "send", "latency", "receive"]
            self.avg_data = [header, []]
        item_data = {
            "timeStamp": time.mktime(data.time.timetuple()),
            'connect': data.overall.avg_connect_time,
            'send': data.overall.avg_send_time, 'latency': data.overall.avg_latency,
            'receive': data.overall.avg_receive_time
        }

        self.avg_data[1] += [item_data]
        while len(self.avg_data[1]) > self.interval:
            self.avg_data[1].pop(0)


    def __calculate_codes(self, data):
        ''' prepare response codes data '''
        if not self.codes_data:
            header = ["timeStamp", "net", "2xx", "3xx", "4xx", "5xx", "Non-HTTP"]
            self.codes_data = [header, []]

        item_data = {"timeStamp": time.mktime(data.time.timetuple()), "net": 0, "2xx": 0, "3xx": 0, "4xx": 0, "5xx": 0,
                     "Non-HTTP": 0}

        net = 0
        for code, count in data.overall.net_codes.iteritems():
            if code != "0":
                net += count
        item_data['net'] = net

        for code, count in data.overall.http_codes.iteritems():
            if code[0] == '2':
                item_data['2xx'] += count
            elif code[0] == '3':
                item_data['3xx'] += count
            elif code[0] == '4':
                item_data['4xx'] += count
            elif code[0] == '5':
                item_data['5xx'] += count
            else:
                item_data['Non-HTTP'] += count

        self.codes_data[1] += [item_data]
        while len(self.codes_data[1]) > self.interval:
            self.codes_data[1].pop(0)


    def aggregate_second(self, data):
        self.last_sec = data

        self.__calculate_quantiles(data)
        self.__calculate_avg(data)
        self.__calculate_codes(data)


# http://fragments.turtlemeat.com/pythonwebserver.php
class OnlineServer(HTTPServer):
    ''' web server starter '''

    def __init__(self, server_address, handler_class, bind_and_activate=True):
        HTTPServer.allow_reuse_address = True
        HTTPServer.__init__(self, server_address, handler_class, bind_and_activate)
        self.owner = None


class WebOnlineHandler(BaseHTTPRequestHandler):
    ''' request handler '''

    def __init__(self, request, client_address, server):
        self.log = logging.getLogger(__name__)
        BaseHTTPRequestHandler.__init__(self, request, client_address, server)

    def log_error(self, fmt, *args):
        self.log.error(fmt % args)

    def log_message(self, fmt, *args):
        self.log.debug(fmt % args)

    def do_GET(self):
        ''' handle GET request '''
        try:
            if self.path == '/':
                self.send_response(200)
                self.send_header('Content-Type', 'text/html')
                self.end_headers()

                fhandle = open(os.path.dirname(__file__) + '/online.html')
                self.wfile.write(fhandle.read())
                fhandle.close()

            elif self.path.endswith(".ico"):
                self.send_response(200)
                self.send_header('Content-Type', 'text/html')
                self.end_headers()
                self.wfile.write("")
            elif self.path.endswith(".json"):
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                if self.path == '/Q.json':
                    self.wfile.write(json.dumps(self.server.owner.quantiles_data))
                if self.path == '/HTTP.json':
                    self.wfile.write(json.dumps(self.server.owner.codes_data))
                if self.path == '/Avg.json':
                    self.wfile.write(json.dumps(self.server.owner.avg_data))
                elif self.path == '/redirect.json':
                    self.wfile.write('["%s"]' % self.server.owner.redirect)
                elif self.path == '/numbers.json':
                    sec = self.server.owner.last_sec
                    net = 0
                    if sec:
                        for code, count in sec.overall.net_codes.iteritems():
                            if code != "0":
                                net += count
                        data = (sec.overall.active_threads, sec.overall.planned_requests, sec.overall.RPS,
                                sec.overall.avg_response_time, net)
                    else:
                        data = (0, 0, 0, 0, 0)
                    self.wfile.write('{"instances": %s, "planned": %s, "actual": %s, "avg": %s, "net": %s}' % data)
            else:
                self.send_response(200)
                self.send_header('Content-Type', 'text/html')
                self.end_headers()

                fhandle = open(os.path.dirname(__file__) + self.path)
                self.wfile.write(fhandle.read())
                fhandle.close()
        except IOError:
            self.log.warning("404: %s" % self.path)
            self.send_error(404, 'File Not Found: %s' % self.path)



########NEW FILE########
__FILENAME__ = config
from module_exceptions import StepperConfigurationError, AmmoFileError
import load_plan as lp
import instance_plan as ip
import missile
from mark import get_marker
from util import get_opener
import info
import logging


class ComponentFactory():

    def __init__(
        self,
        rps_schedule=None,
        http_ver='1.1',
        ammo_file=None,
        instances_schedule=None,
        instances=1000,
        loop_limit=-1,
        ammo_limit=-1,
        uris=None,
        headers=None,
        autocases=None,
        ammo_type='phantom',
        chosen_cases=[],
    ):
        self.log = logging.getLogger(__name__)
        self.ammo_file = ammo_file
        self.ammo_type = ammo_type
        self.rps_schedule = rps_schedule
        self.http_ver = http_ver
        self.instances_schedule = instances_schedule
        loop_limit = int(loop_limit)
        if loop_limit == -1:  # -1 means infinite
            loop_limit = None
        ammo_limit = int(ammo_limit)
        if ammo_limit == -1:  # -1 means infinite
            ammo_limit = None
        if loop_limit is None and ammo_limit is None and not rps_schedule:
            # we should have only one loop if we have instance_schedule
            loop_limit = 1
        info.status.loop_limit = loop_limit
        info.status.ammo_limit = ammo_limit
        info.status.publish("instances", instances)
        self.uris = uris
        if self.uris and loop_limit:
            info.status.ammo_limit = len(self.uris) * loop_limit
        self.headers = headers
        self.marker = get_marker(autocases)
        self.chosen_cases = chosen_cases

    def get_load_plan(self):
        """
        return load plan (timestamps generator)
        """
        if self.rps_schedule and self.instances_schedule:
            raise StepperConfigurationError(
                'Both rps and instances schedules specified. You must specify only one of them')
        elif self.rps_schedule:
            info.status.publish('loadscheme', self.rps_schedule)
            return lp.create(self.rps_schedule)
        elif self.instances_schedule:
            info.status.publish('loadscheme', self.instances_schedule)
            return ip.create(self.instances_schedule)
        else:
            self.instances_schedule = []
            info.status.publish('loadscheme', self.instances_schedule)
            return ip.create(self.instances_schedule)

    def get_ammo_generator(self):
        """
        return ammo generator
        """
        af_readers = {
            'phantom': missile.AmmoFileReader,
            'slowlog': missile.SlowLogReader,
            'line': missile.LineReader,
            'uri': missile.UriReader,
            'uripost': missile.UriPostReader,
            'access': missile.AccessLogReader,
        }
        if self.uris and self.ammo_file:
            raise StepperConfigurationError(
                'Both uris and ammo file specified. You must specify only one of them')
        elif self.uris:
            ammo_gen = missile.UriStyleGenerator(
                self.uris,
                self.headers,
                http_ver=self.http_ver
            )
        elif self.ammo_file:
            
            if self.ammo_type in af_readers:
                if self.ammo_type is 'phantom':
                    with get_opener(self.ammo_file)(self.ammo_file, 'rb') as ammo:
                        try:
                            if not ammo.next()[0].isdigit():
                                self.ammo_type = 'uri'
                                self.log.info(
                                    "Setting ammo_type 'uri' because ammo is not started with digit and you did not specify ammo format")
                            else:
                                self.log.info(
                                    "Default ammo type ('phantom') used, use 'phantom.ammo_type' option to override it")
                        except StopIteration, e:
                            self.log("Couldn't read first line of ammo file: %s" % e)
                            raise AmmoFileError("Couldn't read first line of ammo file")
                        
            else:
                raise NotImplementedError(
                    'No such ammo type implemented: "%s"' % self.ammo_type)
            ammo_gen = af_readers[self.ammo_type](
                self.ammo_file,
                headers=self.headers,
                http_ver=self.http_ver
            )
        else:
            raise StepperConfigurationError(
                'Ammo not found. Specify uris or ammo file')
        self.log.info("Using %s ammo reader" % type(ammo_gen).__name__)
        return ammo_gen

    def get_marker(self):
        return self.marker

    def get_filter(self):
        if len(self.chosen_cases):
            def is_chosen_case(ammo_tuple):
                return ammo_tuple[1] in self.chosen_cases
            return is_chosen_case
        else:
            return lambda ammo_tuple: True

########NEW FILE########
__FILENAME__ = format
'''
Ammo formatters
'''
from module_exceptions import StpdFileError
import logging

class Stpd(object):
    '''
    STPD ammo formatter
    '''
    def __init__(self, ammo_factory):
        self.af = ammo_factory

    def __iter__(self):
        return ("%s %s %s\n%s\n" % (len(missile), timestamp, marker, missile) for timestamp, marker, missile in self.af)

class StpdReader(object):

    '''Read missiles from stpd file'''

    def __init__(self, filename):
        self.filename = filename
        self.log = logging.getLogger(__name__)
        self.log.info("Loading stepped missiles from '%s'" % filename)

    def __iter__(self):
        def read_chunk_header(ammo_file):
            chunk_header = ''
            while chunk_header is '':
                line = ammo_file.readline()
                if line is '':
                    return line # EOF
                chunk_header = line.strip('\r\n')
            return chunk_header
        with open(self.filename, 'rb') as ammo_file:
            chunk_header = read_chunk_header(ammo_file)
            while chunk_header <> '':
                try:
                    fields = chunk_header.split()
                    chunk_size = int(fields[0])
                    timestamp = int(fields[1])
                    marker = fields[2] if len(fields) > 2 else ''
                    missile = ammo_file.read(chunk_size)
                    if len(missile) < chunk_size:
                        raise StpdFileError(
                            "Unexpected end of file: read %s bytes instead of %s" % (len(missile), chunk_size))
                    yield (timestamp, missile, marker)
                except (IndexError, ValueError) as e:
                    raise StpdFileError(
                        "Error while reading ammo file. Position: %s, header: '%s', original exception: %s" % (ammo_file.tell(), chunk_header, e))
                chunk_header = read_chunk_header(ammo_file)
        self.log.info("Reached the end of stpd file")
########NEW FILE########
__FILENAME__ = info
from collections import namedtuple
import logging
from sys import stdout
import time


StepperInfo = namedtuple(
    'StepperInfo',
    'loop_count,steps,loadscheme,duration,ammo_count,instances'
)


class StepperStatus(object):

    '''
    Raises StopIteration when limits are reached.
    '''

    def __init__(self):
        self.log = logging.getLogger(__name__)
        self.info = {
            'loop_count': 0,
            'steps': None,
            'loadscheme': None,
            'duration': None,
            'ammo_count': 0,
            'instances': None,
        }
        self._ammo_count = 0
        self._old_ammo_count = 0
        self._loop_count = 0
        self._af_position = None
        self.af_size = None
        self.loop_limit = None
        self.ammo_limit = None
        self.lp_len = None
        self.lp_progress = 0
        self.af_progress = 0
        self._timer = time.time()

    def publish(self, key, value):
        if key not in self.info:
            raise RuntimeError(
                "Tryed to publish to a non-existent key: %s" % key)
        self.log.debug('Published %s to %s', value, key)
        self.info[key] = value

    @property
    def af_position(self):
        return self._af_position

    @af_position.setter
    def af_position(self, value):
        self._af_position = value
        self.update_af_progress()

    @property
    def ammo_count(self):
        return self._ammo_count

    @ammo_count.setter
    def ammo_count(self, value):
        self._ammo_count = value
        self.update_lp_progress()
        if self.ammo_limit and value > self.ammo_limit:
            print
            self.log.info("Ammo limit reached: %s", self.ammo_limit)
            raise StopIteration

    def inc_ammo_count(self):
        self.ammo_count += 1

    @property
    def loop_count(self):
        return self._loop_count

    @loop_count.setter
    def loop_count(self, value):
        self._loop_count = value
        if self.loop_limit and value >= self.loop_limit:
            print  # do not overwrite status (go to new line)
            self.log.info("Loop limit reached: %s", self.loop_limit)
            raise StopIteration

    def inc_loop_count(self):
        self.loop_count += 1

    def get_info(self):
        self.info['ammo_count'] = self._ammo_count
        self.info['loop_count'] = self._loop_count
        for key in self.info:
            if self.info[key] is None:
                raise RuntimeError(
                    "Information for %s is not published yet." % key)
        return StepperInfo(**self.info)

    def update_view(self):
        ammo_generated = self._ammo_count - self._old_ammo_count
        self._old_ammo_count = self._ammo_count
        cur_time = time.time()
        time_delta = cur_time - self._timer
        self._timer = cur_time
        stdout.write("AF: %3s%%, LP: %3s%%, loops: %10s, speed: %5s Krps\r" % \
            (self.af_progress, self.lp_progress, self.loop_count, int(ammo_generated / time_delta / 1000.0)))
        stdout.flush()

    def update_af_progress(self):
        if self.af_size and self.loop_limit and self.af_position is not None:
            bytes_read = self.af_size * self.loop_count + self.af_position
            total_bytes = self.af_size * self.loop_limit
            progress = int(float(bytes_read) / float(total_bytes) * 100.0)
        else:
            progress = 100
        if self.af_progress != progress:
            self.af_progress = progress
            self.update_view()

    def update_lp_progress(self):
        if self.ammo_limit or self.lp_len:
            if self.ammo_limit:
                if self.lp_len:
                    max_ammo = min(self.ammo_limit, self.lp_len)
                else:
                    max_ammo = self.ammo_limit
            else:
                max_ammo = self.lp_len
            progress = int(float(self.ammo_count) / float(max_ammo) * 100.0)
        else:
            progress = 100
        if self.lp_progress != progress:
            self.lp_progress = progress
            self.update_view()


status = StepperStatus()

########NEW FILE########
__FILENAME__ = instance_plan
from itertools import cycle, repeat, chain
from util import parse_duration
from module_exceptions import StepperConfigurationError
import re
import info
import logging


class LoadPlanBuilder(object):

    def __init__(self):
        self.generators = []
        self.steps = []
        self.instances = 0
        self.duration = 0
        self.log = logging.getLogger(__name__)

    def start(self, count):
        self.log.debug("Start %s instances at %sms" % (count, self.duration))
        if count < 0:
            raise StepperConfigurationError(
                "Can not stop instances in instances_schedule.")
        self.generators.append(repeat(int(self.duration), count))
        self.instances += count
        return self

    def wait(self, duration):
        self.log.debug("Wait for %sms from %sms" % (duration, self.duration))
        self.duration += duration
        self.steps.append((self.instances, int(duration) / 1000))
        return self

    def ramp(self, count, duration):
        self.log.debug("Ramp %s instances in %sms from %sms" %
                       (count, duration, self.duration))
        if count < 0:
            raise StepperConfigurationError(
                "Can not stop instances in instances_schedule.")
        interval = float(duration) / count
        start_time = self.duration
        self.generators.append(int(start_time + i * interval)
                               for i in xrange(0, count))
        self.steps += [(self.instances + i, int(interval / 1000.0)) for i in xrange(0, count)]
        self.instances += count
        self.duration += duration
        return self

    def const(self, instances, duration):
        self.start(instances - self.instances)
        self.wait(duration)
        return self

    def line(self, initial_instances, final_instances, duration):
        self.start(initial_instances - self.instances)
        self.ramp(final_instances - initial_instances, duration)
        return self

    def stairway(self, initial_instances, final_instances, step_size, step_duration):
        step_count = (final_instances - initial_instances) / step_size
        self.log.debug("Making a stairway: %s steps" % step_count)
        self.start(initial_instances - self.instances)
        for i in xrange(1, step_count + 1):
            self.wait(step_duration).start(step_size)
        if final_instances != self.instances:
            self.wait(step_duration).start(final_instances - self.instances)
        self.wait(step_duration)
        return self

    def add_step(self, step_config):
        def parse_ramp(params):
            template = re.compile('(\d+),\s*([0-9.]+[dhms]?)+\)')
            s_res = template.search(params)
            if s_res:
                instances, interval = s_res.groups()
                self.ramp(int(instances), parse_duration(interval))
            else:
                self.log.info(
                    "Ramp step format: 'ramp(<instances_to_start>, <step_duration>)'")
                raise StepperConfigurationError(
                    "Error in step configuration: 'ramp(%s'" % params)

        def parse_const(params):
            template = re.compile('(\d+),\s*([0-9.]+[dhms]?)+\)')
            s_res = template.search(params)
            if s_res:
                instances, interval = s_res.groups()
                self.const(int(instances), parse_duration(interval))
            else:
                self.log.info(
                    "Const step format: 'const(<instances_count>, <step_duration>)'")
                raise StepperConfigurationError(
                    "Error in step configuration: 'const(%s'" % params)

        def parse_start(params):
            template = re.compile('(\d+)\)')
            s_res = template.search(params)
            if s_res:
                instances = s_res.groups()
                self.start(int(instances))
            else:
                self.log.info(
                    "Start step format: 'start(<instances_count>)'")
                raise StepperConfigurationError(
                    "Error in step configuration: 'start(%s'" % params)

        def parse_line(params):
            template = re.compile('(\d+),\s*(\d+),\s*([0-9.]+[dhms]?)+\)')
            s_res = template.search(params)
            if s_res:
                initial_instances, final_instances, interval = s_res.groups()
                self.line(
                    int(initial_instances),
                    int(final_instances),
                    parse_duration(interval)
                )
            else:
                self.log.info(
                    "Line step format: 'line(<initial_instances>, <final_instances>, <step_duration>)'")
                raise StepperConfigurationError(
                    "Error in step configuration: 'line(%s'" % params)

        def parse_wait(params):
            template = re.compile('([0-9.]+[dhms]?)+\)')
            s_res = template.search(params)
            if s_res:
                duration = s_res.groups()[0]
                self.wait(parse_duration(duration))
            else:
                self.log.info("Wait step format: 'wait(<step_duration>)'")
                raise StepperConfigurationError(
                    "Error in step configuration: 'wait(%s'" % params)

        def parse_stairway(params):
            template = re.compile(
                '(\d+),\s*(\d+),\s*(\d+),\s*([0-9.]+[dhms]?)+\)')
            s_res = template.search(params)
            if s_res:
                initial_instances, final_instances, step_size, step_duration = s_res.groups(
                )
                self.stairway(int(initial_instances), int(final_instances), int(
                    step_size), parse_duration(step_duration))
            else:
                self.log.info(
                    "Stairway step format: 'step(<initial_instances>, <final_instances>, <step_size>, <step_duration>)'")
                raise StepperConfigurationError(
                    "Error in step configuration: 'step(%s'" % params)

        _plans = {
            'line': parse_line,
            'const': parse_const,
            'step': parse_stairway,
            'ramp': parse_ramp,
            'wait': parse_wait,
            'start': parse_start,
        }
        step_type, params = step_config.split('(')
        step_type = step_type.strip()
        if step_type in _plans:
            _plans[step_type](params)
        else:
            raise NotImplementedError(
                'No such load type implemented for instances_schedule: "%s"' % step_type)

    def add_all_steps(self, steps):
        for step in steps:
            self.add_step(step)
        return self

    def create(self):
        self.generators.append(cycle([0]))
        return chain(*self.generators)


def create(instances_schedule):
    '''
    Creates load plan timestamps generator

    >>> from util import take

    >>> take(7, LoadPlanBuilder().ramp(5, 5000).create())
    [0, 1000, 2000, 3000, 4000, 0, 0]

    >>> take(7, create(['ramp(5, 5s)']))
    [0, 1000, 2000, 3000, 4000, 0, 0]

    >>> take(12, create(['ramp(5, 5s)', 'wait(5s)', 'ramp(5,5s)']))
    [0, 1000, 2000, 3000, 4000, 10000, 11000, 12000, 13000, 14000, 0, 0]

    >>> take(7, create(['wait(5s)', 'ramp(5, 0)']))
    [5000, 5000, 5000, 5000, 5000, 0, 0]

    >>> take(7, create([]))
    [0, 0, 0, 0, 0, 0, 0]

    >>> take(12, create(['const(3, 5s)', 'line(7, 10, 5s)']))
    [0, 0, 0, 5000, 5000, 5000, 5000, 5000, 6666, 8333, 0, 0]

    >>> take(12, create(['step(2, 10, 2, 3s)']))
    [0, 0, 3000, 3000, 6000, 6000, 9000, 9000, 12000, 12000, 0, 0]

    >>> take(12, LoadPlanBuilder().const(3, 1000).line(5, 10, 5000).steps)
    [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]

    >>> take(12, LoadPlanBuilder().stairway(100, 950, 100, 30000).steps)
    [(100, 30), (200, 30), (300, 30), (400, 30), (500, 30), (600, 30), (700, 30), (800, 30), (900, 30), (950, 30)]

    >>> LoadPlanBuilder().stairway(100, 950, 100, 30000).instances
    950

    >>> LoadPlanBuilder().const(3, 1000).line(5, 10, 5000).instances
    10

    >>> LoadPlanBuilder().line(1, 100, 60000).instances
    100
    '''
    lpb = LoadPlanBuilder().add_all_steps(instances_schedule)
    lp = lpb.create()
    info.status.publish('duration', 0)
    # info.status.publish('steps', lpb.steps)
    info.status.publish('steps', [])
    info.status.publish('instances', lpb.instances)
    return lp

########NEW FILE########
__FILENAME__ = load_plan
'''
Load Plan generators
'''
import re
from util import parse_duration, solve_quadratic
from itertools import chain, groupby
import info
import logging


class Const(object):

    '''
    Load plan with constant load
    '''

    def __init__(self, rps, duration):
        self.rps = float(rps)
        self.duration = duration

    def __iter__(self):
        if self.rps == 0:
            return iter([])
        interval = 1000.0 / self.rps
        return (int(i * interval) for i in xrange(0, int(self.rps * self.duration / 1000)))

    def rps_at(self, t):
        '''Return rps for second t'''
        if t <= self.duration:
            return self.rps
        else:
            return 0

    def get_duration(self):
        '''Return step duration'''
        return self.duration

    def __len__(self):
        '''Return total ammo count'''
        return self.duration / 1000 * self.rps

    def get_rps_list(self):
        return [(int(self.rps), self.duration / 1000)]

    def __repr__(self):
        return 'const(%s, %s)' % (self.rps, self.duration / 1000)


class Line(object):

    '''Load plan with linear load'''

    def __init__(self, minrps, maxrps, duration):
        self.minrps = float(minrps)
        self.maxrps = float(maxrps)
        self.duration = duration / 1000.0
        self.b = self.minrps
        self.k = (self.maxrps - self.minrps) / self.duration

    def ts(self, n):
            root1, root2 = solve_quadratic(self.k / 2.0, self.b, -n)
            return int(root2 * 1000)

    def __iter__(self):
        return (self.ts(n) for n in xrange(0, self.__len__()))

    def rps_at(self, t):
        '''Return rps for second t'''
        if t <= self.duration:
            return self.minrps + float(self.maxrps - self.minrps) * t / self.duration
        else:
            return 0

    def get_duration(self):
        '''Return step duration'''
        return int(self.duration * 1000)

    def __len__(self):
        '''Return total ammo count'''
        return int(self.k / 2.0 * (self.duration ** 2) + self.b * self.duration)

    def get_float_rps_list(self):
        '''
        get list of constant load parts (we have no constant load at all, but tank will think so),
        with parts durations (float)
        '''
        int_rps = xrange(int(self.minrps), int(self.maxrps) + 1)
        step_duration = float(self.duration) / len(int_rps)
        return [(rps, int(step_duration)) for rps in int_rps]

    def get_rps_list(self):
        '''
        get list of each second's rps
        '''
        seconds = xrange(0, int(self.duration))
        rps_groups = groupby([int(self.rps_at(t))
                              for t in seconds], lambda x: x)
        rps_list = [(rps, len(list(rpl))) for rps, rpl in rps_groups]
        return rps_list


class Composite(object):

    '''Load plan with multiple steps'''

    def __init__(self, steps):
        self.steps = steps

    def __iter__(self):
        base = 0
        for step in self.steps:
            for ts in step:
                yield ts + base
            base += step.get_duration()

    def get_duration(self):
        '''Return total duration'''
        return sum(step.get_duration() for step in self.steps)

    def __len__(self):
        '''Return total ammo count'''
        return sum(step.__len__() for step in self.steps)

    def get_rps_list(self):
        return list(chain.from_iterable(step.get_rps_list() for step in self.steps))


class Stairway(Composite):

    def __init__(self, minrps, maxrps, increment, duration):
        if maxrps < minrps:
            increment = -increment
        n_steps = int((maxrps - minrps) / increment)
        steps = [
            Const(minrps + i * increment, duration)
            for i in xrange(0, n_steps + 1)
        ]
        if (n_steps + 1) * increment < maxrps:
            steps.append(Const(maxrps, duration))
        logging.info(steps)
        super(Stairway, self).__init__(steps)


class StepFactory(object):

    @staticmethod
    def line(params):
        template = re.compile('([0-9.]+),\s*([0-9.]+),\s*([0-9.]+[dhms]?)+\)')
        minrps, maxrps, duration = template.search(params).groups()
        return Line(float(minrps), float(maxrps), parse_duration(duration))

    @staticmethod
    def const(params):
        template = re.compile('([0-9.]+),\s*([0-9.]+[dhms]?)+\)')
        rps, duration = template.search(params).groups()
        return Const(float(rps), parse_duration(duration))

    @staticmethod
    def stairway(params):
        template = re.compile('([0-9.]+),\s*([0-9.]+),\s*([0-9.]+),\s*([0-9.]+[dhms]?)+\)')
        minrps, maxrps, increment, duration = template.search(params).groups()
        return Stairway(float(minrps), float(maxrps), float(increment), parse_duration(duration))

    @staticmethod
    def produce(step_config):
        _plans = {
            'line': StepFactory.line,
            'const': StepFactory.const,
            'step': StepFactory.stairway,
        }
        load_type, params = step_config.split('(')
        load_type = load_type.strip()
        if load_type in _plans:
            return _plans[load_type](params)
        else:
            raise NotImplementedError(
                'No such load type implemented: "%s"' % load_type)


def create(rps_schedule):
    '''
    Create Load Plan as defined in schedule. Publish info about its duration.

    >>> from util import take

    >>> take(100, create(['line(1, 5, 2s)']))
    [0, 618, 1000, 1302, 1561, 1791]

    >>> take(100, create(['line(1.1, 5.8, 2s)']))
    [0, 566, 917, 1196, 1435, 1647]

    >>> take(100, create(['line(5, 1, 2s)']))
    [0, 208, 438, 697, 1000, 1381]

    >>> take(100, create(['const(1, 10s)']))
    [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000]

    >>> take(100, create(['const(200, 0.1s)']))
    [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]

    >>> take(100, create(['const(1, 2s)', 'const(2, 2s)']))
    [0, 1000, 2000, 2500, 3000, 3500]

    >>> take(100, create(['const(1.5, 10s)']))
    [0, 666, 1333, 2000, 2666, 3333, 4000, 4666, 5333, 6000, 6666, 7333, 8000, 8666, 9333]

    >>> take(10, create(['step(1, 5, 1, 5s)']))
    [0, 1000, 2000, 3000, 4000, 5000, 5500, 6000, 6500, 7000]

    >>> take(10, create(['step(1.2, 5.7, 1.1, 5s)']))
    [0, 833, 1666, 2500, 3333, 4166, 5000, 5434, 5869, 6304]

    >>> take(10, create(['const(1, 1)']))
    [0]

    '''
    if len(rps_schedule) > 1:
        lp = Composite([StepFactory.produce(step_config)
                       for step_config in rps_schedule])
    else:
        lp = StepFactory.produce(rps_schedule[0])
    info.status.publish('duration', lp.get_duration() / 1000)
    info.status.publish('steps', lp.get_rps_list())
    info.status.lp_len = len(lp)
    return lp

########NEW FILE########
__FILENAME__ = main
'''
Module contains top-level generators.
'''
from itertools import izip
import format as fmt
from config import ComponentFactory
import info
import os
import json
import hashlib
import logging
import re


class AmmoFactory(object):

    '''
    A generator that produces ammo.
    '''

    def __init__(self, factory):
        '''
        Factory parameter is a configured ComponentFactory that
        is able to produce load plan and ammo generator.
        '''
        self.factory = factory
        self.load_plan = factory.get_load_plan()
        self.ammo_generator = factory.get_ammo_generator()
        self.filter = factory.get_filter()
        self.marker = factory.get_marker()

    def __iter__(self):
        '''
        Returns a generator of (timestamp, marker, missile) tuples
        where missile is in a string representation. Load Plan (timestamps
        generator) and ammo generator are taken from the previously
        configured ComponentFactory, passed as a parameter to the
        __init__ method of this class.
        '''
        ammo_stream = (ammo for ammo in (
            (missile, marker or self.marker(missile)) 
            for missile, marker
            in self.ammo_generator 
        ) if self.filter(ammo))
        
        return (
            (timestamp, marker or self.marker(missile), missile)
            for timestamp, (missile, marker)
            in izip(self.load_plan, ammo_stream)
        )

class Stepper(object):

    def __init__(self, **kwargs):
        info.status = info.StepperStatus()
        self.af = AmmoFactory(ComponentFactory(**kwargs))
        self.ammo = fmt.Stpd(self.af)

    def write(self, f):
        for missile in self.ammo:
            f.write(missile)
            try:
                info.status.inc_ammo_count()
            except StopIteration:
                break


class StepperWrapper(object):
    # TODO: review and rewrite this class

    '''
    Wrapper for cached stepper functionality
    '''
    OPTION_STPD = 'stpd_file'
    OPTION_STEPS = 'steps'
    OPTION_TEST_DURATION = 'test_duration'
    OPTION_AMMO_COUNT = 'ammo_count'
    OPTION_LOOP = 'loop'
    OPTION_LOOP_COUNT = 'loop_count'
    OPTION_AMMOFILE = "ammofile"
    OPTION_SCHEDULE = 'rps_schedule'
    OPTION_LOADSCHEME = 'loadscheme'
    OPTION_INSTANCES_LIMIT = 'instances'

    def __init__(self, core, section):
        self.log = logging.getLogger(__name__)
        self.core = core
        self.section = section

        self.cache_dir = '.'

        # per-shoot params
        self.instances = 1000
        self.rps_schedule = []
        self.http_ver = '1.0'
        self.ammo_file = None
        self.instances_schedule = ''
        self.loop_limit = -1
        self.ammo_limit = -1
        self.uris = []
        self.headers = []
        self.autocases = 0
        self.use_caching = True
        self.force_stepping = None
        self.ammo_type = 'phantom'
        self.chosen_cases = []

        # out params
        self.stpd = None
        self.steps = []
        self.ammo_count = 1
        self.duration = 0
        self.loop_count = 0
        self.loadscheme = ""
        self.file_cache = 8192

    def get_option(self, option_ammofile, param2=None):
        ''' get_option wrapper'''
        result = self.core.get_option(self.section, option_ammofile, param2)
        self.log.debug(
            "Option %s.%s = %s", self.section, option_ammofile, result)
        return result

    @staticmethod
    def get_available_options():
        opts = [StepperWrapper.OPTION_AMMOFILE, StepperWrapper.OPTION_LOOP,
                StepperWrapper.OPTION_SCHEDULE, StepperWrapper.OPTION_STPD]
        opts += ["instances_schedule", "uris",
                 "headers", "header_http", "autocases", "ammo_type", "ammo_limit"]
        opts += ["use_caching", "cache_dir", "force_stepping", "file_cache", "chosen_cases"]
        return opts

    def read_config(self):
        ''' stepper part of reading options '''
        self.log.info("Configuring StepperWrapper...")
        self.ammo_file = self.get_option(self.OPTION_AMMOFILE, '')
        self.ammo_type = self.get_option('ammo_type', self.ammo_type)
        if self.ammo_file:
            self.ammo_file = os.path.expanduser(self.ammo_file)
        self.loop_limit = int(self.get_option(self.OPTION_LOOP, "-1"))
        self.ammo_limit = int(self.get_option("ammo_limit", "-1"))

        def make_steps(schedule):
            steps = []
            for step in " ".join(schedule.split("\n")).split(')'):
                if step.strip():
                    steps.append(step.strip() + ')')
            return steps
        self.rps_schedule = make_steps(
            self.get_option(self.OPTION_SCHEDULE, ''))
        self.instances_schedule = make_steps(
            self.get_option("instances_schedule", ''))
        self.instances = int(
            self.get_option(self.OPTION_INSTANCES_LIMIT, '1000'))
        self.uris = self.get_option("uris", '').strip().split("\n")
        while '' in self.uris:
            self.uris.remove('')
        rx = re.compile('\[(.*?)\]')
        self.headers = rx.findall(self.get_option("headers", ''))
        self.http_ver = self.get_option("header_http", self.http_ver)
        self.autocases = self.get_option("autocases", '0')
        self.use_caching = int(self.get_option("use_caching", '1'))

        self.file_cache = int(self.get_option('file_cache', '8192'))
        cache_dir = self.core.get_option(
            self.section, "cache_dir", self.core.artifacts_base_dir)
        self.cache_dir = os.path.expanduser(cache_dir)
        self.force_stepping = int(self.get_option("force_stepping", '0'))
        self.stpd = self.get_option(self.OPTION_STPD, "")
        self.chosen_cases = self.get_option("chosen_cases", "").split()
        if self.chosen_cases:
            self.log.info("chosen_cases LIMITS: %s", self.chosen_cases)

    def prepare_stepper(self):
        ''' Generate test data if necessary '''
        def publish_info(stepper_info):
            info.status.publish('loadscheme', stepper_info.loadscheme)
            info.status.publish('loop_count', stepper_info.loop_count)
            info.status.publish('steps', stepper_info.steps)
            info.status.publish('duration', stepper_info.duration)
            info.status.ammo_count = stepper_info.ammo_count
            info.status.publish('instances', stepper_info.instances)
            return stepper_info
        if not self.stpd:
            self.stpd = self.__get_stpd_filename()
            self.core.set_option(self.section, self.OPTION_STPD, self.stpd)
            if self.use_caching and not self.force_stepping and os.path.exists(self.stpd) and os.path.exists(self.__si_filename()):
                self.log.info("Using cached stpd-file: %s", self.stpd)
                stepper_info = publish_info(self.__read_cached_options())  
            else:
                if self.force_stepping and os.path.exists(self.__si_filename()):
                    os.remove(self.__si_filename())
                self.__make_stpd_file()
                stepper_info = info.status.get_info()
                self.__write_cached_options(stepper_info)
        else:
            self.log.info("Using specified stpd-file: %s", self.stpd)
            stepper_info = publish_info(self.__read_cached_options())            
        self.ammo_count = stepper_info.ammo_count
        self.duration = stepper_info.duration
        self.loop_count = stepper_info.loop_count
        self.loadscheme = stepper_info.loadscheme
        self.steps = stepper_info.steps
        if stepper_info.instances:
            self.instances = stepper_info.instances

    def __si_filename(self):
        '''Return name for stepper_info json file'''
        return "%s_si.json" % self.stpd

    def __get_stpd_filename(self):
        ''' Choose the name for stepped data file '''
        if self.use_caching:
            sep = "|"
            hasher = hashlib.md5()
            hashed_str = "cache version 4" + sep + \
                ';'.join(self.instances_schedule) + sep + str(self.loop_limit)
            hashed_str += sep + str(self.ammo_limit) + sep + ';'.join(
                self.rps_schedule) + sep + str(self.autocases)
            hashed_str += sep + \
                ";".join(self.uris) + sep + ";".join(
                    self.headers) + sep + self.http_ver + sep + ";".join(self.chosen_cases)

            if self.ammo_file:
                if not os.path.exists(self.ammo_file):
                    raise RuntimeError(
                        "Ammo file not found: %s" % self.ammo_file)

                hashed_str += sep + os.path.realpath(self.ammo_file)
                stat = os.stat(self.ammo_file)
                cnt = 0
                for stat_option in stat:
                    if cnt == 7:  # skip access time
                        continue
                    cnt += 1
                    hashed_str += ";" + str(stat_option)
                hashed_str += ";" + str(os.path.getmtime(self.ammo_file))
            else:
                if not self.uris:
                    raise RuntimeError(
                        "Neither ammofile nor uris specified")
                hashed_str += sep + \
                    ';'.join(self.uris) + sep + ';'.join(self.headers)
            self.log.debug("stpd-hash source: %s", hashed_str)
            hasher.update(hashed_str)
            if not os.path.exists(self.cache_dir):
                os.makedirs(self.cache_dir)
            stpd = self.cache_dir + '/' + \
                os.path.basename(self.ammo_file) + \
                "_" + hasher.hexdigest() + ".stpd"
        else:
            stpd = os.path.realpath("ammo.stpd")
        self.log.debug("Generated cache file name: %s", stpd)
        return stpd

    def __read_cached_options(self):
        '''
        Read stepper info from json
        '''
        self.log.debug("Reading cached stepper info: %s", self.__si_filename())
        with open(self.__si_filename(), 'r') as si_file:
            si = info.StepperInfo(**json.load(si_file))
        return si

    def __write_cached_options(self, si):
        '''
        Write stepper info to json
        '''
        # TODO: format json
        self.log.debug("Saving stepper info: %s", self.__si_filename())
        with open(self.__si_filename(), 'w') as si_file:
            json.dump(si._asdict(), si_file, indent=4)

    def __make_stpd_file(self):
        ''' stpd generation using Stepper class '''
        self.log.info("Making stpd-file: %s", self.stpd)
        stepper = Stepper(
            rps_schedule=self.rps_schedule,
            http_ver=self.http_ver,
            ammo_file=self.ammo_file,
            instances_schedule=self.instances_schedule,
            instances=self.instances,
            loop_limit=self.loop_limit,
            ammo_limit=self.ammo_limit,
            uris=self.uris,
            headers=[header.strip('[]') for header in self.headers],
            autocases=self.autocases,
            ammo_type=self.ammo_type,
            chosen_cases=self.chosen_cases,
        )
        with open(self.stpd, 'w', self.file_cache) as os:
            stepper.write(os)

########NEW FILE########
__FILENAME__ = mark
from uuid import uuid4


__test_missile = """\
POST /example/search/hello/help/us?param1=50&param2=0&param3=hello HTTP/1.1\r
Connection: close\r
Host: example.org\r
Content-length: 32\r
\r
param1=50&param2=0&param3=hello
"""


def __mark_by_uri(missile):
    return '_'.join(missile.split('\n', 1)[0].split(' ', 2)[1].split('?')[0].split('/'))


class __UriMarker(object):
    '''
    Returns a uri marker function with requested limit

    >>> marker = __UriMarker(2)
    >>> marker(__test_missile)
    '_example_search'
    '''
    def __init__(self, limit):
        self.limit = limit

    def __call__(self, missile):
        return '_'.join(missile.split('\n', 1)[0].split(' ', 2)[1].split('?')[0].split('/')[0:self.limit+1])

__markers = {
    'uniq': lambda m: uuid4().hex,
    'uri': __mark_by_uri,
}


def get_marker(marker_type):
    '''
    Returns a marker function of the requested marker_type

    >>> marker = get_marker('uniq')(__test_missile)
    >>> type(marker)
    <type 'str'>
    >>> len(marker)
    32

    >>> get_marker('uri')(__test_missile)
    '_example_search_hello_help_us'

    >>> marker = get_marker('non-existent')(__test_missile)
    Traceback (most recent call last):
      ...
    NotImplementedError: No such marker: "non-existent"

    >>> get_marker('3')(__test_missile)
    '_example_search_hello'
    '''
    if marker_type and marker_type is not '0':
        try:
            limit = int(marker_type)
            return __UriMarker(limit)
        except ValueError:
            if marker_type in __markers:
                return __markers[marker_type]
            else:
                raise NotImplementedError(
                    'No such marker: "%s"' % marker_type)
    else:
        return lambda m: ''

########NEW FILE########
__FILENAME__ = missile
'''
Missile object and generators

You should update Stepper.status.ammo_count and Stepper.status.loop_count in your custom generators!
'''
from util import get_opener
from itertools import cycle
from module_exceptions import AmmoFileError
import os.path
import info
import logging


class HttpAmmo(object):
    '''
    Represents HTTP missile

    >>> print HttpAmmo('/', []).to_s()  # doctest: +NORMALIZE_WHITESPACE
    GET / HTTP/1.1

    >>> print HttpAmmo('/', ['Connection: Close', 'Content-Type: Application/JSON']).to_s()  # doctest: +NORMALIZE_WHITESPACE
    GET / HTTP/1.1
    Connection: Close
    Content-Type: Application/JSON

    >>> print HttpAmmo('/', ['Connection: Close'], method='POST', body='hello!').to_s()  # doctest: +NORMALIZE_WHITESPACE
    POST / HTTP/1.1
    Connection: Close
    Content-Length: 6
    <BLANKLINE>
    hello!
    '''

    def __init__(self, uri, headers, method='GET', http_ver='1.1', body=''):
        self.method = method
        self.uri = uri
        self.proto = 'HTTP/%s' % http_ver
        self.headers = set(headers)
        self.body = body
        if len(body):
            self.headers.add("Content-Length: %s" % len(body))

    def to_s(self):
        if self.headers:
            headers = '\r\n'.join(self.headers) + '\r\n'
        else:
            headers = ''
        return "%s %s %s\r\n%s\r\n%s" % (self.method, self.uri, self.proto, headers, self.body)


class SimpleGenerator(object):

    '''
    Generates ammo based on a given sample.
    '''

    def __init__(self, missile_sample):
        '''
        Missile sample is any object that has to_s method which
        returns its string representation.
        '''
        self.missiles = cycle([(missile_sample.to_s(), None)])

    def __iter__(self):
        for m in self.missiles:
            info.status.inc_loop_count()
            yield m


class UriStyleGenerator(object):

    '''
    Generates GET ammo based on given URI list.
    '''

    def __init__(self, uris, headers, http_ver='1.1'):
        '''
        uris - a list of URIs as strings.
        '''
        self.uri_count = len(uris)
        self.missiles = cycle(
            [(HttpAmmo(uri, headers, http_ver=http_ver).to_s(), None) for uri in uris])

    def __iter__(self):
        for m in self.missiles:
            yield m
            info.status.loop_count = info.status.ammo_count / self.uri_count


class AmmoFileReader(object):

    '''Read missiles from ammo file'''

    def __init__(self, filename, **kwargs):
        self.filename = filename
        self.log = logging.getLogger(__name__)
        self.log.info("Loading ammo from '%s'" % filename)

    def __iter__(self):
        def read_chunk_header(ammo_file):
            chunk_header = ''
            while chunk_header is '':
                line = ammo_file.readline()
                if line is '':
                    return line
                chunk_header = line.strip('\r\n')
            return chunk_header
        with get_opener(self.filename)(self.filename, 'rb') as ammo_file:
            info.status.af_size = os.path.getsize(self.filename)
            chunk_header = read_chunk_header(ammo_file) #  if we got StopIteration here, the file is empty
            while chunk_header:
                if chunk_header is not '':
                    try:
                        fields = chunk_header.split()
                        chunk_size = int(fields[0])
                        if chunk_size == 0:
                            if info.status.loop_count == 0:
                                self.log.info('Zero-sized chunk in ammo file at %s. Starting over.' % ammo_file.tell())
                            ammo_file.seek(0)
                            info.status.inc_loop_count()
                            chunk_header = read_chunk_header(ammo_file)
                            continue
                        marker = fields[1] if len(fields) > 1 else None
                        missile = ammo_file.read(chunk_size)
                        if len(missile) < chunk_size:
                            raise AmmoFileError(
                                "Unexpected end of file: read %s bytes instead of %s" % (len(missile), chunk_size))
                        yield (missile, marker)
                    except (IndexError, ValueError) as e:
                        raise AmmoFileError(
                            "Error while reading ammo file. Position: %s, header: '%s', original exception: %s" % (ammo_file.tell(), chunk_header, e))
                chunk_header = read_chunk_header(ammo_file)
                if chunk_header == '':
                    ammo_file.seek(0)
                    info.status.inc_loop_count()
                    chunk_header = read_chunk_header(ammo_file)
                info.status.af_position = ammo_file.tell()


class SlowLogReader(object):

    '''Read missiles from SQL slow log. Not usable with Phantom'''

    def __init__(self, filename, **kwargs):
        self.filename = filename

    def __iter__(self):
        with open(self.filename, 'rb') as ammo_file:
            info.status.af_size = os.path.getsize(self.filename)
            request = ""
            while True:
                for line in ammo_file:
                    info.status.af_position = ammo_file.tell()
                    if line.startswith('#'):
                        if request != "":
                            yield (request, None)
                            request = ""
                    else:
                        request += line
                ammo_file.seek(0)
                info.status.af_position = 0
                info.status.inc_loop_count()

class LineReader(object):

    '''One line -- one missile'''

    def __init__(self, filename, **kwargs):
        self.filename = filename

    def __iter__(self):
        with get_opener(self.filename)(self.filename, 'rb') as ammo_file:
            while True:
                for line in ammo_file:
                    info.status.af_position = ammo_file.tell()
                    yield (line.rstrip('\r\n'), None)
                ammo_file.seek(0)
                info.status.af_position = 0
                info.status.inc_loop_count()


class AccessLogReader(object):

    '''Missiles from access log'''

    def __init__(self, filename, headers=[], http_ver='1.1', **kwargs):
        self.filename = filename
        self.warned = False
        self.headers = set(headers)
        self.log = logging.getLogger(__name__)

    def warn(self, message):
        if not self.warned:
            self.warned = True
            self.log.warning("There are some skipped lines. See full log for details.")
        self.log.debug(message)

    def __iter__(self):
        with get_opener(self.filename)(self.filename, 'rb') as ammo_file:
            while True:
                for line in ammo_file:
                    info.status.af_position = ammo_file.tell()
                    try:
                        request = line.split('"')[1]
                        method, uri, proto = request.split()
                        http_ver = proto.split('/')[1]
                        if method == "GET":
                            yield (
                                HttpAmmo(
                                    uri,
                                    headers=self.headers,
                                    http_ver=http_ver,
                                ).to_s(), None)
                        else:
                            self.warn("Skipped line: %s (unsupported method)" % line)
                    except (ValueError, IndexError), e:
                        self.warn("Skipped line: %s (%s)" % (line, e))
                    
                ammo_file.seek(0)
                info.status.af_position = 0
                info.status.inc_loop_count()


class UriReader(object):
    def __init__(self, filename, headers=[], http_ver='1.1', **kwargs):
        self.filename = filename
        self.headers = set(headers)
        self.http_ver = http_ver
        self.log = logging.getLogger(__name__)
        self.log.info("Loading ammo from '%s' using URI format." % filename)

    def __iter__(self):
        with get_opener(self.filename)(self.filename, 'rb') as ammo_file:
            while True:
                for line in ammo_file:
                    info.status.af_position = ammo_file.tell()
                    if line.startswith('['):
                        self.headers.add(line.strip('\r\n[]\t '))
                    elif len(line.rstrip('\r\n')):
                        fields = line.split()
                        uri = fields[0]
                        if len(fields) > 1:
                            marker = fields[1]
                        else:
                            marker = None
                        yield (
                            HttpAmmo(
                                uri,
                                headers=self.headers,
                                http_ver=self.http_ver,
                            ).to_s(), marker)
                if info.status.ammo_count == 0:
                    self.log.error("No ammo in uri-style file")
                    raise AmmoFileError("No ammo! Cover me!")
                ammo_file.seek(0)
                info.status.af_position = 0
                info.status.inc_loop_count()

class UriPostReader(object):

    '''Read POST missiles from ammo file'''

    def __init__(self, filename, headers=[], http_ver='1.1', **kwargs):
        self.filename = filename
        self.headers = set(headers)
        self.http_ver = http_ver
        self.log = logging.getLogger(__name__)
        self.log.info("Loading ammo from '%s' using URI+POST format" % filename)

    def __iter__(self):
        def read_chunk_header(ammo_file):
            chunk_header = ''
            while chunk_header is '':
                line = ammo_file.readline()
                if line.startswith('['):
                        self.headers.add(line.strip('\r\n[]\t '))
                elif line is '':
                    return line
                else:
                    chunk_header = line.strip('\r\n')
            return chunk_header
        with get_opener(self.filename)(self.filename, 'rb') as ammo_file:
            info.status.af_size = os.path.getsize(self.filename)
            chunk_header = read_chunk_header(ammo_file) #  if we got StopIteration here, the file is empty
            while chunk_header:
                if chunk_header is not '':
                    try:
                        fields = chunk_header.split()
                        chunk_size = int(fields[0])
                        if chunk_size == 0:
                            self.log.debug('Zero-sized chunk in ammo file at %s. Starting over.' % ammo_file.tell())
                            ammo_file.seek(0)
                            info.status.inc_loop_count()
                            chunk_header = read_chunk_header(ammo_file)
                            continue
                        uri = fields[1]
                        marker = fields[2] if len(fields) > 2 else None
                        missile = ammo_file.read(chunk_size)
                        if len(missile) < chunk_size:
                            raise AmmoFileError(
                                "Unexpected end of file: read %s bytes instead of %s" % (len(missile), chunk_size))
                        yield (
                            HttpAmmo(
                                uri=uri,
                                headers=self.headers,
                                method='POST',
                                body=missile,
                                http_ver=self.http_ver,
                            ).to_s(),
                            marker
                        )
                    except (IndexError, ValueError) as e:
                        raise AmmoFileError(
                            "Error while reading ammo file. Position: %s, header: '%s', original exception: %s" % (ammo_file.tell(), chunk_header, e))
                chunk_header = read_chunk_header(ammo_file)
                if chunk_header == '':
                    self.log.debug('Reached the end of ammo file. Starting over.')
                    ammo_file.seek(0)
                    info.status.inc_loop_count()
                    chunk_header = read_chunk_header(ammo_file)
                info.status.af_position = ammo_file.tell()
########NEW FILE########
__FILENAME__ = module_exceptions
class StepperConfigurationError(Exception):
    '''
    Raised when error in stepper configuration found.
    '''


class AmmoFileError(Exception):
    '''
    Raised when failed to read ammo file properly.
    '''

class StpdFileError(Exception):
	'''
	Raised when failed to read stpd file properly.
	'''

########NEW FILE########
__FILENAME__ = util
'''
Utilities: parsers, converters, etc.
'''
import re
import logging
from itertools import islice
from module_exceptions import StepperConfigurationError
import math
import gzip


def take(number, iter):
    return list(islice(iter, 0, number))


def parse_duration(duration):
    '''
    Parse duration string, such as '3h2m3s' into milliseconds

    >>> parse_duration('3h2m3s')
    10923000

    >>> parse_duration('0.3s')
    300

    >>> parse_duration('5')
    5000
    '''
    _re_token = re.compile("([0-9.]+)([dhms]?)")

    def parse_token(time, multiplier):
        multipliers = {
            'h': 3600,
            'm': 60,
            's': 1,
        }
        if multiplier:
            if multiplier in multipliers:
                return int(float(time) * multipliers[multiplier] * 1000)
            else:
                raise StepperConfigurationError(
                    'Failed to parse duration: %s' % duration)
        else:
            return int(float(time) * 1000)

    return sum(parse_token(*token) for token in _re_token.findall(duration))

def solve_quadratic(a, b, c):
    '''
    >>> solve_quadratic(1.0, 2.0, 1.0)
    (-1.0, -1.0)
    '''
    discRoot = math.sqrt((b * b) - 4 * a * c)
    root1 = (-b - discRoot) / (2 * a)
    root2 = (-b + discRoot) / (2 * a)
    return (root1, root2)

def s_to_ms(f_sec):
    return int(f_sec * 1000.0)


def get_opener(f_path):
    """ Returns opener function according to file extensions:
        bouth open and gzip.open calls return fileobj.

    Args:
        f_path: str, ammo file path.

    Returns:
        function, to call for file open.
    """
    if f_path.endswith('.gz'):
        logging.info("Using gzip opener")
        return gzip.open
    else:
        return open

########NEW FILE########
__FILENAME__ = tank
#! /usr/bin/env python2
from Tank.ConsoleWorker import ConsoleTank, CompletionHelperOptionParser
from optparse import OptionParser
import logging
import os
import sys
import traceback

if __name__ == "__main__":
    sys.path.append(os.path.dirname(__file__))
    parser = OptionParser()
    parser.add_option('-c', '--config', action='append', help="Path to INI file containing run options, multiple options accepted")
    parser.add_option(      '--lock-dir', action='store', dest='lock_dir', type="string", help="Directory for lock file")
    parser.add_option('-i', '--ignore-lock', action='store_true', dest='ignore_lock', help="Ignore lock files from concurrent instances, has precedence before --lock-fail")
    parser.add_option('-f', '--fail-lock', action='store_true', dest='lock_fail', help="Don't wait for lock to release, fail test instead")
    parser.add_option('-l', '--log', action='store', default="tank.log", help="Tank log file location")
    parser.add_option('-m', '--manual-start', action='store_true', dest='manual_start', help="Wait for Enter key to start the test")
    parser.add_option('-n', '--no-rc', action='store_true', dest='no_rc', help="Don't load config files from /etc/yandex-tank and ~/.yandex-tank")
    parser.add_option('-o', '--option', action='append', help="Set config option, multiple options accepted, example: -o 'shellexec.start=pwd'")
    parser.add_option('-q', '--quiet', action='store_true', help="Less console output, only errors and warnings")
    parser.add_option('-s', '--scheduled-start', action='store', dest='scheduled_start', help="Start test at specified time, format 'YYYY-MM-DD hh:mm:ss', date part is optional")
    parser.add_option('-v', '--verbose', action='store_true', help="More console output, +debug messages")

    completion_helper = CompletionHelperOptionParser()
    completion_helper.handle_request(parser)
        
    options, ammofile = parser.parse_args()

    worker = ConsoleTank(options, ammofile)
    worker.init_logging()
    try:     
        worker.configure()
        rc = worker.perform_test()
        exit(rc)
    except Exception, ex:
        logging.error("Exception: %s", ex)
        logging.debug("Exception: %s", traceback.format_exc(ex))
        exit(1)
        
    
                

########NEW FILE########
__FILENAME__ = tankcore
""" The central part of the tool: Core """
from ConfigParser import NoSectionError
import ConfigParser
import datetime
import errno
import itertools
import logging
import os
import re
import select
import shlex
import shutil
import subprocess
import sys
import tempfile
import time
import traceback
import fnmatch
import psutil


def log_stdout_stderr(log, stdout, stderr, comment=""):
    """
    This function polls stdout and stderr streams and writes their contents to log
    """
    readable = select.select([stdout], [], [], 0)[0]
    if stderr:
        exceptional = select.select([stderr], [], [], 0)[0]
    else:
        exceptional = []

    log.debug("Selected: %s, %s", readable, exceptional)

    for handle in readable:
        line = handle.read()
        readable.remove(handle)
        if line:
            log.debug("%s stdout: %s", comment, line.strip())

    for handle in exceptional:
        line = handle.read()
        exceptional.remove(handle)
        if line:
            log.warn("%s stderr: %s", comment, line.strip())


def expand_to_milliseconds(str_time):
    """
    converts 1d2s into milliseconds
    """
    return expand_time(str_time, 'ms', 1000)


def expand_to_seconds(str_time):
    """
    converts 1d2s into seconds
    """
    return expand_time(str_time, 's', 1)


def expand_time(str_time, default_unit='s', multiplier=1):
    """
    helper for above functions
    """
    parser = re.compile('(\d+)([a-zA-Z]*)')
    parts = parser.findall(str_time)
    result = 0.0
    for value, unit in parts:
        value = int(value)
        unit = unit.lower()
        if unit == '':
            unit = default_unit

        if unit == 'ms':
            result += value * 0.001
            continue
        elif unit == 's':
            result += value
            continue
        elif unit == 'm':
            result += value * 60
            continue
        elif unit == 'h':
            result += value * 60 * 60
            continue
        elif unit == 'd':
            result += value * 60 * 60 * 24
            continue
        elif unit == 'w':
            result += value * 60 * 60 * 24 * 7
            continue
        else:
            raise ValueError(
                "String contains unsupported unit %s: %s" % (unit, str_time))
    return int(result * multiplier)


def pid_exists(pid):
    """Check whether pid exists in the current process table."""
    if pid < 0:
        return False
    try:
        os.kill(pid, 0)
    except OSError, exc:
        logging.debug("No process[%s]: %s", exc.errno, exc)
        return exc.errno == errno.EPERM
    else:
        p = psutil.Process(pid)
        return p.status != psutil.STATUS_ZOMBIE


def execute(cmd, shell=False, poll_period=1.0, catch_out=False):
    """
    Wrapper for Popen
    """
    log = logging.getLogger(__name__)
    log.debug("Starting: %s", cmd)

    stdout = ""
    stderr = ""

    if not shell and isinstance(cmd, basestring):
        cmd = shlex.split(cmd)

    if catch_out:
        process = subprocess.Popen(
            cmd, shell=shell, stderr=subprocess.PIPE, stdout=subprocess.PIPE, close_fds=True)
    else:
        process = subprocess.Popen(cmd, shell=shell, close_fds=True)

    while process.poll() is None:
        log.debug("Waiting for process to finish: %s", process)
        time.sleep(poll_period)

    if catch_out:
        for line in process.stderr.readlines():
            stderr += line
            log.warn(line.strip())
        for line in process.stdout.readlines():
            stdout += line
            log.debug(line.strip())

    retcode = process.poll()
    log.debug("Process exit code: %s", retcode)
    return retcode, stdout, stderr


def splitstring(string):
    """
    >>> string = 'apple orange "banana tree" green'
    >>> splitstring(string)
    ['apple', 'orange', 'green', '"banana tree"']
    """
    patt = re.compile(r'"[\w ]+"')
    if patt.search(string):
        quoted_item = patt.search(string).group()
        newstring = patt.sub('', string)
        return newstring.split() + [quoted_item]
    else:
        return string.split()


def pairs(lst):
    """
    Iterate over pairs in the list
    """
    return itertools.izip(lst[::2], lst[1::2])


class TankCore:
    """
    JMeter + dstat inspired :)
    """
    SECTION = 'tank'
    PLUGIN_PREFIX = 'plugin_'
    PID_OPTION = 'pid'
    LOCK_DIR = '/var/lock'

    def __init__(self):
        self.log = logging.getLogger(__name__)
        self.config = ConfigManager()
        self.plugins = {}
        self.artifacts_dir = None
        self.artifact_files = {}
        self.plugins_order = []
        self.artifacts_base_dir = '.'
        self.manual_start = False
        self.scheduled_start = None
        self.interrupted = False
        self.lock_file = None
        self.flush_config_to = None
        self.lock_dir = None

    def get_available_options(self):
        return ["artifacts_base_dir", "artifacts_dir", "flush_config_to"]

    def load_configs(self, configs):
        """        Tells core to load configs set into options storage        """
        self.log.info("Loading configs...")
        self.config.load_files(configs)
        dotted_options = []
        for option, value in self.config.get_options(self.SECTION):
            if '.' in option:
                dotted_options += [option + '=' + value]
        self.apply_shorthand_options(dotted_options, self.SECTION)
        self.config.flush()
        self.add_artifact_file(self.config.file)
        self.set_option(self.SECTION, self.PID_OPTION, os.getpid())
        self.flush_config_to = self.get_option(
            self.SECTION, "flush_config_to", "")
        if self.flush_config_to:
            self.config.flush(self.flush_config_to)

    def load_plugins(self):
        """        Tells core to take plugin options and instantiate plugin classes        """
        self.log.info("Loading plugins...")

        self.artifacts_base_dir = os.path.expanduser(
            self.get_option(self.SECTION, "artifacts_base_dir", self.artifacts_base_dir))
        self.artifacts_dir = self.get_option(self.SECTION, "artifacts_dir", "")
        if self.artifacts_dir:
            self.artifacts_dir = os.path.expanduser(self.artifacts_dir)

        options = self.config.get_options(self.SECTION, self.PLUGIN_PREFIX)
        for (plugin_name, plugin_path) in options:
            if not plugin_path:
                self.log.debug(
                    "Seems the plugin '%s' was disabled", plugin_name)
                continue
            instance = self.__load_plugin(plugin_name, plugin_path)
            key = os.path.realpath(instance.get_key())
            self.plugins[key] = instance
            self.plugins_order.append(key)

        self.log.debug("Plugin instances: %s", self.plugins)
        self.log.debug("Plugins order: %s", self.plugins_order)

    def __load_plugin(self, name, path):
        """        Load single plugin using 'exec' statement        """
        self.log.debug("Loading plugin %s from %s", name, path)
        for basedir in [''] + sys.path:
            if basedir:
                new_dir = basedir + '/' + path
            else:
                new_dir = path
            if os.path.exists(new_dir):
                new_dir = os.path.dirname(new_dir)
                if new_dir not in sys.path:
                    self.log.debug('Append to path: %s', new_dir)
                    sys.path.append(new_dir)
        res = None
        classname = os.path.basename(path)[:-3]
        self.log.debug("sys.path: %s", sys.path)
        exec ("import " + classname)
        script = "res=" + classname + "." + classname + "Plugin(self)"
        self.log.debug("Exec: " + script)
        exec script
        self.log.debug("Instantiated: %s", res)
        return res

    def plugins_configure(self):
        """        Call configure() on all plugins        """
        if not os.path.exists(self.artifacts_base_dir):
            os.makedirs(self.artifacts_base_dir)
            os.chmod(self.artifacts_base_dir, 0755)

        self.log.info("Configuring plugins...")
        for plugin_key in self.plugins_order:
            plugin = self.__get_plugin_by_key(plugin_key)
            self.log.debug("Configuring %s", plugin)
            plugin.configure()
            self.config.flush()
        if self.flush_config_to:
            self.config.flush(self.flush_config_to)

    def plugins_prepare_test(self):
        """ Call prepare_test() on all plugins        """
        self.log.info("Preparing test...")
        for plugin_key in self.plugins_order:
            plugin = self.__get_plugin_by_key(plugin_key)
            self.log.debug("Preparing %s", plugin)
            plugin.prepare_test()
        if self.flush_config_to:
            self.config.flush(self.flush_config_to)

    def plugins_start_test(self):
        """        Call start_test() on all plugins        """
        self.log.info("Starting test...")
        for plugin_key in self.plugins_order:
            plugin = self.__get_plugin_by_key(plugin_key)
            self.log.debug("Starting %s", plugin)
            plugin.start_test()
        if self.flush_config_to:
            self.config.flush(self.flush_config_to)

    def wait_for_finish(self):
        """        Call is_test_finished() on all plugins 'till one of them initiates exit        """

        self.log.info("Waiting for test to finish...")
        if not self.plugins:
            raise RuntimeError("It's strange: we have no plugins loaded...")

        while not self.interrupted:
            begin_time = time.time()
            for plugin_key in self.plugins_order:
                plugin = self.__get_plugin_by_key(plugin_key)
                self.log.debug("Polling %s", plugin)
                retcode = plugin.is_test_finished()
                if retcode >= 0:
                    return retcode
            end_time = time.time()
            diff = end_time - begin_time
            self.log.debug("Polling took %s", diff)
            # screen refresh every 0.5 s
            if diff < 0.5:
                time.sleep(0.5 - diff)
        return 1

    def plugins_end_test(self, retcode):
        """        Call end_test() on all plugins        """
        self.log.info("Finishing test...")

        for plugin_key in self.plugins_order:
            plugin = self.__get_plugin_by_key(plugin_key)
            self.log.debug("Finalize %s", plugin)
            try:
                self.log.debug("RC before: %s", retcode)
                plugin.end_test(retcode)
                self.log.debug("RC after: %s", retcode)
            except Exception, ex:
                self.log.error("Failed finishing plugin %s: %s", plugin, ex)
                self.log.debug(
                    "Failed finishing plugin: %s", traceback.format_exc(ex))
                if not retcode:
                    retcode = 1

        if self.flush_config_to:
            self.config.flush(self.flush_config_to)
        return retcode

    def plugins_post_process(self, retcode):
        """
        Call post_process() on all plugins
        """
        self.log.info("Post-processing test...")

        for plugin_key in self.plugins_order:
            plugin = self.__get_plugin_by_key(plugin_key)
            self.log.debug("Post-process %s", plugin)
            try:
                self.log.debug("RC before: %s", retcode)
                retcode = plugin.post_process(retcode)
                self.log.debug("RC after: %s", retcode)
            except Exception, ex:
                self.log.error(
                    "Failed post-processing plugin %s: %s", plugin, ex)
                self.log.debug(
                    "Failed post-processing plugin: %s", traceback.format_exc(ex))
                del self.plugins[plugin_key]
                if not retcode:
                    retcode = 1

        if self.flush_config_to:
            self.config.flush(self.flush_config_to)

        self.__collect_artifacts()

        return retcode

    def __collect_artifacts(self):
        self.log.debug("Collecting artifacts")
        if not self.artifacts_dir:
            date_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S.")
            self.artifacts_dir = tempfile.mkdtemp(
                "", date_str, self.artifacts_base_dir)
        if not os.path.isdir(self.artifacts_dir):
            os.makedirs(self.artifacts_dir)

        os.chmod(self.artifacts_dir, 0755)

        self.log.info("Artifacts dir: %s", self.artifacts_dir)
        for filename, keep in self.artifact_files.items():
            try:
                self.__collect_file(filename, keep)
            except Exception, ex:
                self.log.warn("Failed to collect file %s: %s", filename, ex)

    def get_option(self, section, option, default=None):
        """
        Get an option from option storage
        """
        if not self.config.config.has_section(section):
            self.log.debug("No section '%s', adding", section)
            self.config.config.add_section(section)

        try:
            value = self.config.config.get(section, option).strip()
        except ConfigParser.NoOptionError as ex:
            if default is not None:
                default = str(default)
                self.config.config.set(section, option, default)
                self.config.flush()
                value = default.strip()
            else:
                self.log.warn("Mandatory option %s was not found in section %s", option, section)
                raise ex

        if len(value) > 1 and value[0] == '`' and value[-1] == '`':
            self.log.debug("Expanding shell option %s", value)
            retcode, stdout, stderr = execute(value[1:-1], True, 0.1, True)
            if retcode or stderr:
                raise ValueError(
                    "Error expanding option %s, RC: %s" % (value, retcode))
            value = stdout.strip()

        return value

    def set_option(self, section, option, value):
        """
        Set an option in storage
        """
        if not self.config.config.has_section(section):
            self.config.config.add_section(section)
        self.config.config.set(section, option, value)
        self.config.flush()

    def get_plugin_of_type(self, needle):
        """
        Retrieve a plugin of desired class, KeyError raised otherwise
        """
        self.log.debug("Searching for plugin: %s", needle)
        key = os.path.realpath(needle.get_key())

        return self.__get_plugin_by_key(key)

    def __get_plugin_by_key(self, key):
        """
        Get plugin from loaded by its key
        """
        if key in self.plugins.keys():
            return self.plugins[key]

        ext = os.path.splitext(key)[1].lower()

        if ext == '.py' and key + 'c' in self.plugins.keys():  # .py => .pyc
            return self.plugins[key + 'c']

        if ext == '.pyc' and key[:-1] in self.plugins.keys():  # .pyc => .py:
            return self.plugins[key[:-1]]

        raise KeyError("Requested plugin type not found: %s" % key)

    def __collect_file(self, filename, keep_original=False):
        """
        Move or copy single file to artifacts dir
        """
        if not self.artifacts_dir:
            self.log.warning("No artifacts dir configured")
            return

        dest = self.artifacts_dir + '/' + os.path.basename(filename)
        self.log.debug("Collecting file: %s to %s", filename, dest)
        if not filename or not os.path.exists(filename):
            self.log.warning("File not found to collect: %s", filename)
            return

        if os.path.exists(dest):
            # FIXME: 3 find a way to store artifacts anyway
            self.log.warning("File already exists: %s", dest)
            return

        if keep_original:
            shutil.copy(filename, self.artifacts_dir)
        else:
            shutil.move(filename, self.artifacts_dir)

        os.chmod(dest, 0644)

    def add_artifact_file(self, filename, keep_original=False):
        """
        Add file to be stored as result artifact on post-process phase
        """
        if filename:
            self.artifact_files[filename] = keep_original

    def apply_shorthand_options(self, options, default_section='DEFAULT'):
        for option_str in options:
            try:
                section = option_str[:option_str.index('.')]
                option = option_str[option_str.index('.') + 1:option_str.index('=')]
            except ValueError:
                section = default_section
                option = option_str[:option_str.index('=')]
            value = option_str[option_str.index('=') + 1:]
            self.log.debug(
                "Override option: %s => [%s] %s=%s", option_str, section, option, value)
            self.set_option(section, option, value)

    def get_lock_dir(self):
        if not self.lock_dir:
            self.lock_dir = self.get_option(self.SECTION, "lock_dir", self.LOCK_DIR)

        return os.path.expanduser(self.lock_dir)

    def get_lock(self, force=False):
        if not force and self.__there_is_locks():
            raise RuntimeError("There is lock files")

        fh, self.lock_file = tempfile.mkstemp(
            '.lock', 'lunapark_', self.get_lock_dir())
        os.close(fh)
        os.chmod(self.lock_file, 0644)
        self.config.file = self.lock_file

    def release_lock(self):
        self.config.file = None
        if self.lock_file and os.path.exists(self.lock_file):
            os.remove(self.lock_file)

    def __there_is_locks(self):
        retcode = False
        lock_dir = self.get_lock_dir()
        for filename in os.listdir(lock_dir):
            if fnmatch.fnmatch(filename, 'lunapark_*.lock'):
                full_name = os.path.join(lock_dir, filename)
                self.log.warn("Lock file present: %s", full_name)

                try:
                    info = ConfigParser.ConfigParser()
                    info.read(full_name)
                    pid = info.get(TankCore.SECTION, self.PID_OPTION)
                    if not pid_exists(int(pid)):
                        self.log.debug(
                            "Lock PID %s not exists, ignoring and trying to remove", pid)
                        try:
                            os.remove(full_name)
                        except Exception, exc:
                            self.log.debug(
                                "Failed to delete lock %s: %s", full_name, exc)
                    else:
                        retcode = True
                except Exception, exc:
                    self.log.warn(
                        "Failed to load info from lock %s: %s", full_name, exc)
                    retcode = True
        return retcode

    def mkstemp(self, suffix, prefix):
        """
        Generate temp file name in artifacts base dir
        and close temp file handle
        """
        fd, fname = tempfile.mkstemp(suffix, prefix, self.artifacts_base_dir)
        os.close(fd)
        return fname


class ConfigManager:
    """    Option storage class    """

    def __init__(self):
        self.file = None
        self.log = logging.getLogger(__name__)
        self.config = ConfigParser.ConfigParser()

    def load_files(self, configs):
        """         Read configs set into storage        """
        self.log.debug("Reading configs: %s", configs)
        try:
            self.config.read(configs)
        except Exception as ex:
            self.log.error("Can't load configs: %s", ex)
            raise ex

    def flush(self, filename=None):
        """        Flush current stat to file        """
        if not filename:
            filename = self.file

        if filename:
            self.log.debug("Flushing config to: %s", filename)
            handle = open(filename, 'wb')
            self.config.write(handle)
            handle.close()

    def get_options(self, section, prefix=''):
        """        Get options list with requested prefix        """
        res = []
        self.log.debug(
            "Looking in section '%s' for options starting with '%s'", section, prefix)
        try:
            for option in self.config.options(section):
                self.log.debug("Option: %s", option)
                if not prefix or option.find(prefix) == 0:
                    self.log.debug("Option: %s matched", option)
                    res += [
                        (option[len(prefix):], self.config.get(section, option))]
        except NoSectionError, ex:
            self.log.debug("No section: %s", ex)

        self.log.debug("Found options: %s", res)
        return res

    def find_sections(self, prefix):
        """ return sections with specified prefix """
        res = []
        for section in self.config.sections():
            if section.startswith(prefix):
                res.append(section)
        return res


class AbstractPlugin:
    """    Parent class for all plugins/modules    """

    SECTION = 'DEFAULT'

    @staticmethod
    def get_key():
        """        Get dictionary key for plugin, should point to __file__ magic constant        """
        raise TypeError("Abstract method needs to be overridden")

    def __init__(self, core):
        self.log = logging.getLogger(__name__)
        self.core = core

    def configure(self):
        """        A stage to read config values and instantiate objects        """
        pass

    def prepare_test(self):
        """        Test preparation tasks        """
        pass

    def start_test(self):
        """        Launch test process        """
        pass

    def is_test_finished(self):
        """        Polling call, if result differs from -1 then test end will be triggeted        """
        return -1

    def end_test(self, retcode):
        """        Stop processes launched at 'start_test', change return code if necessary        """
        return retcode

    def post_process(self, retcode):
        """        Post-process test data        """
        return retcode

    def get_option(self, option_name, default_value=None):
        """        Wrapper to get option from plugins' section        """
        return self.core.get_option(self.SECTION, option_name, default_value)

    def set_option(self, option_name, value):
        """        Wrapper to set option to plugins' section        """
        return self.core.set_option(self.SECTION, option_name, value)

    def get_available_options(self):
        """ returns array containing known options for plugin """
        return []

########NEW FILE########
__FILENAME__ = ABTest
from Tests.TankTests import TankTestCase
import unittest
from Tank.Plugins.ApacheBenchmark import ABReader, ApacheBenchmarkPlugin
from Tank.Plugins.Aggregator import AggregatorPlugin

class ABTestCase(TankTestCase):
    def setUp(self):
        self.core = self.get_core()
        self.foo = ApacheBenchmarkPlugin(self.core)

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_run(self):
        aggregator = AggregatorPlugin(self.core)
        self.foo.out_file = 'data/ab_results.txt'
        reader = ABReader(aggregator, self.foo)
        reader.check_open_files()
        self.assertNotEquals(None, reader.get_next_sample(False))
        self.assertNotEquals(None, reader.get_next_sample(True))
        cnt = 2
        while reader.get_next_sample(True):
            cnt += 1
        self.assertEquals(25, cnt)
    
if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = AggregatorTest
import time
import unittest

from Tank.Plugins.Aggregator import AggregatorPlugin, AbstractReader, \
    SecondAggregateDataTotalItem
from Tests.TankTests import TankTestCase


class AggregatorPluginTestCase(TankTestCase):
    def setUp(self):
        core = self.get_core()
        core.load_configs(['config/aggregator.conf'])
        self.foo = AggregatorPlugin(core)

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_run(self):
        self.foo.configure()
        self.assertEquals(11000, self.foo.get_timeout())
        self.foo.prepare_test()
        self.foo.reader = FakeReader(self.foo)
        self.foo.start_test()
        retry = 0
        while self.foo.is_test_finished() < 0 and retry < 5:
            self.foo.log.debug("Not finished")
            time.sleep(0.01)
            retry += 1
        self.foo.end_test(0)

    def test_run_final_read(self):
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.reader = FakeReader(self.foo)
        self.foo.start_test()
        self.foo.end_test(0)

    def test_run_interrupt(self):
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        time.sleep(2)
        self.foo.end_test(0)

    def test_total_quantiles(self):
        self.foo = SecondAggregateDataTotalItem()
        self.foo.total_count = 1000
        self.foo.times_dist = {2: {'count': 14, 'to': 3, 'from': 2}, 3: {'count': 815, 'to': 4, 'from': 3},
                               4: {'count': 55, 'to': 5, 'from': 4}, 5: {'count': 29, 'to': 6, 'from': 5},
                               6: {'count': 26, 'to': 7, 'from': 6}, 7: {'count': 27, 'to': 8, 'from': 7},
                               8: {'count': 14, 'to': 9, 'from': 8}, 9: {'count': 8, 'to': 10, 'from': 9},
                               10: {'count': 12, 'to': 20, 'from': 10}}
        res = self.foo.calculate_total_quantiles()
        exp = {98.0: 9, 99.0: 20, 100.0: 20, 75.0: 4, 85.0: 5, 80.0: 4, 50.0: 4, 25.0: 4, 90.0: 6, 95.0: 8}
        self.assertDictEqual(exp, res)


class FakeReader(AbstractReader):
    pass


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = AutostopTest
from Tank.Plugins.Aggregator import SecondAggregateData
from Tank.Plugins.Autostop import AutostopPlugin
from Tests.TankTests import TankTestCase
import tempfile
import unittest


class AutostopTestCase(TankTestCase):
    def setUp(self):
        core = self.get_core()
        core.load_configs(['config/autostop.conf'])
        core.load_plugins()
        core.plugins_configure()
        self.foo = AutostopPlugin(core)

    def tearDown(self):
        del self.foo
        self.foo = None 

    def test_run(self):
        data = SecondAggregateData()
        data.overall.avg_response_time = 11
        self.foo.core.set_option(self.foo.SECTION, "autostop", "time(1,10)")
        
        self.foo.configure()
        self.foo.prepare_test()
        
        self.foo.start_test()
        for n in range(1, 15):
            self.foo.aggregate_second(data)
        if self.foo.is_test_finished() < 0:
            raise RuntimeError()
        self.foo.end_test(0)
        
    def test_run_http(self):
        data = SecondAggregateData()
        data.overall.http_codes = {'200':11}
        self.foo.core.set_option(self.foo.SECTION, "autostop", "http (200, 10, 5 )\nhttp (3xx, 1.5%, 10m)")
        
        self.foo.configure()
        self.foo.prepare_test()
        
        self.foo.start_test()
        for n in range(1, 15):
            self.foo.aggregate_second(data)
        if self.foo.is_test_finished() < 0:
            raise RuntimeError()
        self.foo.end_test(0)
        
    def test_run_net(self):
        data = SecondAggregateData()
        data.overall.net_codes = {71:11}
        self.foo.core.set_option(self.foo.SECTION, "autostop", "net (71, 1, 5)\nnet (xx, 1.5%, 10m )")
        
        self.foo.configure()
        self.foo.prepare_test()
        
        self.foo.start_test()
        for n in range(1, 15):
            self.foo.aggregate_second(data)
        if self.foo.is_test_finished() < 0:
            raise RuntimeError()
        self.foo.end_test(0)
        
    def test_run_quan(self):
        data = SecondAggregateData()
        data.overall.quantiles = {99.0:11}
        self.foo.core.set_option(self.foo.SECTION, "autostop", "quantile(99,2,3)")
        
        self.foo.configure()
        self.foo.prepare_test()
        
        self.foo.start_test()
        for n in range(1, 15):
            self.foo.aggregate_second(data)
        if self.foo.is_test_finished() < 0:
            raise RuntimeError()
        self.foo.end_test(0)

    def test_run_false_trigger_bug(self):
        data = SecondAggregateData()
        data.overall.http_codes = {}
        self.foo.core.set_option(self.foo.SECTION, "autostop", "http (5xx, 100%, 1)")
        
        self.foo.configure()
        self.foo.prepare_test()
        
        self.foo.start_test()
        for n in range(1, 15):
            self.foo.aggregate_second(data)
        if self.foo.is_test_finished() >= 0:
            raise RuntimeError()
        self.foo.end_test(0)
        
if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = CommonUtilsTest
import unittest

from Tests.TankTests import TankTestCase
import tankcore


class CommonUtilsTest(TankTestCase):
    def setUp(self):
        self.stest = [
            ('5s', 5),
            ('1w1s', 604801),
            ('1w2d3h4m5s', 788645),
            ('1w2d3h4m6', 788646),
        ]
        self.mstest = [('1w2d3h4m5s6ms', 788645006), ('1w2d3h4m5s6', 788645006)]

    def test_expand_to_seconds(self):
        for i in self.stest:
            self.assertEqual(tankcore.expand_to_seconds(i[0]), i[1])

    def test_expand_to_seconds_fail(self):
        try:
            tankcore.expand_to_seconds("100n")
            raise RuntimeError("Exception expected")
        except ValueError, ex:
            # it's ok, we have excpected exception
            print ex


    def test_expand_to_milliseconds(self):
        for i in self.mstest:
            self.assertEqual(tankcore.expand_to_milliseconds(i[0]), i[1])


    def test_expand_to_milliseconds_fail(self):
        try:
            tankcore.expand_to_milliseconds("100n")
            raise RuntimeError("Exception expected")
        except ValueError, ex:
            # it's ok, we have excpected exception
            pass


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = ConfigManagerTest
from Tank.ConsoleWorker import ConsoleTank
from Tests.TankTests import TankTestCase, FakeOptions
from tankcore import ConfigManager
import tempfile
import unittest

class  ConfigManagerTestCase(TankTestCase):
    def setUp(self):
        tank = ConsoleTank(FakeOptions(), None)
        tank.init_logging()
        self.foo = ConfigManager()    

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_load_files(self):
        confs = ['config/load_1.conf', 'config/load_2.conf']
        self.foo.load_files(confs)
        self.foo.flush()
        
if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = ConsoleOnlinePluginTest
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin, AbstractInfoWidget, \
    RealConsoleMarkup
from Tests.TankTests import TankTestCase
import tempfile
import unittest
from Tank.Plugins.ConsoleScreen import krutilka

class FakeConsoleMarkup(RealConsoleMarkup):
    clear = "\n[clear]\n"
    new_line = "\n"
    
    YELLOW = '<y>'
    RED = '<r>'
    RED_DARK = '<rd>'
    RESET = '<rst>'
    CYAN = "<c>"
    WHITE = "<w>"
    GREEN = "<g>"
    MAGENTA = '<m>'
    BG_MAGENTA = '<M>'
    BG_GREEN = '<G>'


class ConsoleOnlinePluginTestCase(TankTestCase):
    def setUp(self):
        core = self.get_core()
        core.load_configs(['config/console.conf'])
        core.load_plugins()
        self.foo = ConsoleOnlinePlugin(core)
        self.foo.console_markup = FakeConsoleMarkup()


    def tearDown(self):
        del self.foo
        self.foo = None 


    def test_run(self):
        self.data = self.get_aggregate_data('data/preproc_single2.txt')
        self.foo.set_option('disable_colors', 'WHITE')
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.add_info_widget(TestWidget())
        self.foo.add_info_widget(TestWidget2())
        
        self.foo.start_test()
        k=krutilka()
        for i in range(1, 10):
            print k.next()
            self.foo.aggregate_second(self.data)
        self.foo.end_test(0)
        self.assertFalse(self.foo.render_exception)
        
class TestWidget(AbstractInfoWidget):
    def render(self, screen):
        return "Widget Data";

class TestWidget2(AbstractInfoWidget):
    def get_index(self):
        return 100;
    
    def render(self, screen):
        return "Widget Data 2";


if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = ConsoleWorkerTest
from Tank.ConsoleWorker import ConsoleTank
from Tank.Plugins.ConsoleOnline import ConsoleOnlinePlugin
from Tests.ConsoleOnlinePluginTest import FakeConsoleMarkup
from Tests.TankTests import FakeOptions
import TankTests
import logging
import unittest
import datetime


class  ConsoleWorkerTestCase(TankTests.TankTestCase):
    def setUp(self):
        opts = FakeOptions()
        opts.no_rc = False
        opts.scheduled_start=datetime.datetime.now().strftime('%H:%M:%S')
        self.foo = ConsoleTank(opts, None)
        self.foo.set_baseconfigs_dir('full')

    def tearDown(self):
        del self.foo
        self.foo = None            

    def test_perform(self):
        self.foo.configure()

        try:
            console = self.foo.core.get_plugin_of_type(ConsoleOnlinePlugin)
            console.console_markup = FakeConsoleMarkup()
        except:
            pass
        
        if self.foo.perform_test() != 0:
            raise RuntimeError()
        
        
    def test_option_override(self):
        options = FakeOptions()
        options.config = ["config/phantom.conf"]
        options.option = ["owner.address=overridden"]
        self.foo = ConsoleTank(options, None)
        self.foo.configure()
        res = self.foo.core.get_option("owner", "address")
        logging.debug(res)
        self.assertEquals("overridden", res)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = Dummy
from tankcore import AbstractPlugin
import time

class DummyPlugin(AbstractPlugin):
    def __init__(self, core):
        AbstractPlugin.__init__(self, core)
        self.count = 0
        
    @staticmethod
    def get_key():
        return __file__
    
    def configure(self):
        self.log.warn("Configure")
    
    def prepare_test(self):
        self.log.warn("Prepare")
    
    def start_test(self):
        self.log.warn("Start")

    def is_test_finished(self):
        self.count += 1
        if self.count > 3:
            self.log.warn("Triggering exit")
            return 0
        else:
            self.log.warn("Delaying")
            time.sleep(0.2)
            return -1
        
    def end_test(self, retcode):
        self.log.warn("End")
        
    def post_process(self, retcode):
        self.log.warn("Post-process")

########NEW FILE########
__FILENAME__ = JMeterPluginTest
import logging
import time
import unittest

from Tank.Plugins.JMeter import JMeterPlugin, JMeterReader
from Tests.TankTests import TankTestCase
from Tank.Plugins.Aggregator import AggregatorPlugin


class JMeterPluginTestCase(TankTestCase):
    def setUp(self):
        self.core = self.get_core()
        self.core.load_configs(['config/jmeter.conf'])
        self.foo = JMeterPlugin(self.core)

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_run(self):
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        while self.foo.is_test_finished() < 0:
            self.foo.log.debug("Not finished")
            time.sleep(1)
        self.foo.end_test(0)
        results = open(self.foo.jtl_file, 'r').read()
        logging.debug("Results: %s", results)
        self.assertNotEquals('', results.strip(), open(self.foo.jmeter_log, 'r').read())

    def test_run_interrupt(self):
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        time.sleep(2)
        self.foo.end_test(0)

    def test_reader(self):
        aggregator = AggregatorPlugin(self.core)
        self.foo.jtl_file = 'data/jmeter_mtuV0x.jtl'
        reader = JMeterReader(aggregator, self.foo)
        reader.check_open_files()
        self.assertNotEquals(None, reader.get_next_sample(False))
        self.assertNotEquals(None, reader.get_next_sample(True))
        cnt = 2
        while reader.get_next_sample(True):
            cnt += 1
        self.assertEquals(55, cnt)

    def test_reader_errors(self):
        aggregator = AggregatorPlugin(self.core)
        self.foo.jtl_file = 'data/jmeter_HifF2z.jtl'
        reader = JMeterReader(aggregator, self.foo)
        reader.check_open_files()
        self.assertNotEquals(None, reader.get_next_sample(False))
        self.assertNotEquals(None, reader.get_next_sample(True))
        cnt = 2
        while reader.get_next_sample(True):
            cnt += 1
        self.assertEquals(5, cnt)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = metrics_test
import socket
import os
import tempfile
import time
import unittest
import base64

from Tank.MonCollector.agent.agent import CpuStat, Custom, Disk, NetTcp, NetTxRx


if __name__ == '__main__':
    unittest.main()


class MemTestCase(unittest.TestCase):
    def setUp(self):
        self.foo = CpuStat()

    def test_get(self):
        print self.foo.check()
        self.assertEquals(len(self.foo.columns()), len(self.foo.check()))
        time.sleep(1)
        self.assertNotEquals(['0', '0'], self.foo.check())


class CustomTestCase(unittest.TestCase):
    def setUp(self):
        pass

        #def tearDown(self):

    #    self.foo.dispose()
    #    self.foo = None

    def test_custom_(self):
        custom_config = {'tail': [],
                         'call': ['ZGlmZkV4:aWZjb25maWcgLXMgZXRoMCB8IGF3ayAnJDE9PSJldGgwIiB7cHJpbnQgJDR9Jw==:1']}
        self.foo = Custom(**custom_config)

        #self.assertEqual(x, y, "Msg");
        x = self.foo.check()
        print x
        self.assertEquals(["0"], x)
        time.sleep(1)
        y = self.foo.check()
        print y
        assert x != y
        time.sleep(0.5)
        print self.foo.check()

    def test_custom_nodiff(self):
        tail_fd, tailfile = tempfile.mkstemp()
        tail = ["%s:%s:%s" % (base64.b64encode('lbl'), base64.b64encode(tailfile), 0)]
        call = ["%s:%s:%s" % (base64.b64encode('lbl2'), base64.b64encode("date +%s"), 0)]
        self.foo = Custom(call, tail)

        x = self.foo.check()
        print "second test", x
        self.assertNotEquals(["0.0"], x)
        self.assertEquals('0', x[0])
        time.sleep(1)

        tailval = str(time.time())
        os.write(tail_fd, "%s\n" % tailval)
        y = self.foo.check()
        self.assertNotEquals(x[1], y[1])
        self.assertEquals(tailval, y[0])

        time.sleep(2)
        tailval = str(time.time())
        os.write(tail_fd, "%s\n" % tailval)
        z = self.foo.check()
        self.assertEquals(tailval, z[0])
        self.assertNotEquals(y[1], z[1])

    def test_custom_fail(self):
        tail = ["%s:%s:%s" % (base64.b64encode('lbl'), base64.b64encode("notexistent"), 0)]
        call = ["%s:%s:%s" % (base64.b64encode('lbl2'), base64.b64encode("notexistent"), 0)]
        self.foo = Custom(call, tail)

        x = self.foo.check()
        self.assertEquals("0", x[0])
        self.assertEquals("0", x[1])

    def test_custom_fail2(self):
        custom_config = {'tail': [], 'call': ['TnVtUGhyYXNlcw==:Y2F0IC92YXIvdG1wL3N0YXQx:0']}
        self.foo = Custom(**custom_config)

        self.foo.check()


class DiskTestCase(unittest.TestCase):
    def setUp(self):
        self.foo = Disk()

    def test_get(self):
        print self.foo.check()
        self.assertEquals(2, len(self.foo.check()))
        self.assertNotEquals(['', ''], self.foo.check())
        fd = tempfile.mkstemp()[0]
        os.write(fd, ' ' * 5000000)
        time.sleep(5)
        res = self.foo.check()
        print res
        self.assertNotEquals(['', ''], res)

    def test_cols(self):
        res = self.foo.columns()
        self.assertEquals(['Disk_read', 'Disk_write'], res)


class NetTcpTestCase(unittest.TestCase):
    def setUp(self):
        self.foo = NetTcp()

    def test_net_tcp_(self):
        print self.foo.check()
        self.assertEquals(3, len(self.foo.check()))
        self.assertNotEquals(['0', '0', '0'], self.foo.check())


class NetTxRxTestCase(unittest.TestCase):
    def setUp(self):
        self.foo = NetTxRx()

    def test_net_tx_rx_(self):
        self.assertEquals(['0', '0'], self.foo.check())
        time.sleep(2)
        self.assertNotEquals(['0', '0'], self.foo.check())
        socket.gethostbyname("google.com")
        socket.create_connection(("google.com", 80), 5000)
        time.sleep(2)
        print self.foo.check()

    def test_net_tx_rx_cols(self):
        res = self.foo.columns()
        self.assertEquals(['Net_tx', 'Net_rx', ], res)

########NEW FILE########
__FILENAME__ = MonitoringTest
from Tank.MonCollector.collector import MonitoringCollector, \
    MonitoringDataListener, SSHWrapper
from Tank.Plugins.ConsoleOnline import Screen
from Tank.Plugins.Monitoring import MonitoringPlugin, MonitoringWidget
from Tests.ConsoleOnlinePluginTest import FakeConsoleMarkup
from Tests.TankTests import TankTestCase
import logging
import time
import tempfile

class  MonitoringCollectorTestCase(TankTestCase):
    data = None
    
    def test_collector(self):
        mon = MonitoringCollector()
        mon.config = "config/mon1.conf"
        mon.ssh_wrapper_class = SSHEmulator
        listener = TestMonListener()
        mon.add_listener(listener)
        mon.prepare()
        mon.start()
        mon.poll()

    def test_plugin_disabled(self):
        core = self.get_core()
        core.artifacts_base_dir = tempfile.mkdtemp()
        mon = MonitoringPlugin(core)
        core.set_option(mon.SECTION, 'config', 'none')
        mon.configure()
        mon.prepare_test()
        mon.start_test()
        self.assertEquals(-1, mon.is_test_finished())
        self.assertEquals(None, mon.monitoring)
        time.sleep(1)
        self.assertEquals(-1, mon.is_test_finished())
        mon.end_test(0)
        mon.post_process(0)

    def test_plugin_default(self):
        core = self.get_core()
        core.artifacts_base_dir = tempfile.mkdtemp()
        core.load_configs(['config/monitoring.conf'])
        core.load_plugins()
        core.plugins_configure()
        core.plugins_prepare_test()
        mon = MonitoringPlugin(core)
        mon.configure()
        mon.monitoring.ssh_wrapper_class = SSHEmulator
        mon.prepare_test()
        mon.start_test()
        self.assertEquals(-1, mon.is_test_finished())
        self.assertNotEquals(None, mon.monitoring)
        time.sleep(1)
        self.assertEquals(-1, mon.is_test_finished())
        mon.end_test(0)
        mon.post_process(0)

    def test_plugin_config(self):
        core = self.get_core()
        core.artifacts_base_dir = tempfile.mkdtemp()
        core.load_configs(['config/monitoring.conf'])
        core.load_plugins()
        core.plugins_configure()
        core.plugins_prepare_test()
        mon = MonitoringPlugin(core)
        mon.monitoring.ssh_wrapper_class = SSHEmulator
        core.set_option(mon.SECTION, 'config', "config/mon1.conf")
        mon.configure()
        mon.prepare_test()
        mon.start_test()
        self.assertEquals(-1, mon.is_test_finished())
        self.assertNotEquals(None, mon.monitoring)
        time.sleep(1)
        self.assertEquals(-1, mon.is_test_finished())
        mon.end_test(0)
        mon.post_process(0)

    def test_plugin_inline_config(self):
        core = self.get_core()
        core.artifacts_base_dir = tempfile.mkdtemp()
        core.load_configs(['config/monitoring.conf'])
        core.load_plugins()
        core.plugins_configure()
        core.plugins_prepare_test()
        mon = MonitoringPlugin(core)
        mon.monitoring.ssh_wrapper_class = SSHEmulator
        core.set_option(mon.SECTION, 'config', "<Monitoring>\n<Host address='[target]'/>\n</Monitoring>")
        mon.configure()
        mon.prepare_test()
        mon.start_test()
        self.assertEquals(-1, mon.is_test_finished())
        self.assertNotEquals(None, mon.monitoring)
        time.sleep(1)
        self.assertEquals(-1, mon.is_test_finished())
        mon.end_test(0)
        mon.post_process(0)
    
    def test_widget(self):
        core = self.get_core()
        core.artifacts_base_dir = tempfile.mkdtemp()
        owner = MonitoringPlugin(core)
        owner.monitoring = 1
        widget = MonitoringWidget(owner)
        screen = Screen(50, FakeConsoleMarkup())
        res = widget.render(screen)
        self.assertEquals("Monitoring is <g>online<rst>:", res)

        widget.monitoring_data("start;127.0.0.1;1347631472;Memory_total;Memory_used;Memory_free;Memory_shared;Memory_buff;Memory_cached;Net_recv;Net_send;")
        res = widget.render(screen)
        
        widget.monitoring_data("127.0.0.1;1347631473;1507.65625;576.9609375;8055;1518;0;143360;34.9775784753;16.1434977578;0.0")
        res = widget.render(screen)
        self.assertNotEquals("Monitoring is <g>online<rst>:", res)
        
        widget.monitoring_data("127.0.0.1;1347631473;1506.65625;574.9609375;8055;1518;0;143360;34.9775784753;16.1434977578;0.0")
        res = widget.render(screen)

    def test_plugin_default_failedInstall(self):
        core = self.get_core()
        core.artifacts_base_dir = tempfile.mkdtemp()
        core.load_configs(['config/monitoring.conf'])
        core.load_plugins()
        core.plugins_configure()
        core.plugins_prepare_test()
        mon = MonitoringPlugin(core)
        mon.configure()
        mon.monitoring.ssh_wrapper_class = SSHEmulatorFailer
        mon.prepare_test()
        mon.start_test()
        self.assertEquals(-1, mon.is_test_finished())
        self.assertEquals(None, mon.monitoring)
        time.sleep(1)
        self.assertEquals(-1, mon.is_test_finished())
        mon.end_test(0)
        mon.post_process(0)

class TestMonListener(MonitoringDataListener):
    def __init__(self):
        self.data = []
    
    def monitoring_data(self, data_string):
        logging.debug("MON DATA: %s", data_string)
        self.data.append(data_string)
        
class SSHEmulator(SSHWrapper):

    def __init__(self, timeout):
        SSHWrapper.__init__(self, timeout)
    
    def get_scp_pipe(self, cmd):
        self.scp_pipe = PipeEmul('data/ssh_out.txt', 'data/ssh_err.txt')
        return self.scp_pipe
    
    def get_ssh_pipe(self, cmd):
        self.ssh_pipe = PipeEmul('data/ssh_out.txt', 'data/ssh_err.txt')
        return self.ssh_pipe
    
class PipeEmul:
    def __init__(self, out, err):
        self.stderr = open(err, 'rU')
        self.stdout = open(out, 'rU')
        self.stdin = open(tempfile.mkstemp()[1], 'w')
        self.returncode = 0
        self.pid = 0
        
    def wait(self):
        pass
   
    def readline(self):
        return self.stdout.readline()
    
class SSHEmulatorFailer(SSHEmulator):
    def get_scp_pipe(self, cmd):
        raise RuntimeError()
    
    def get_ssh_pipe(self, cmd):
        raise RuntimeError()



########NEW FILE########
__FILENAME__ = PhantomConfigTest
from Tank.Plugins.Phantom import PhantomConfig, PhantomPlugin
from Tests.TankTests import TankTestCase
import unittest
import logging
from Tank.Plugins.PhantomUtils import StepperWrapper

class  PhantomConfigTestCase(TankTestCase):
    def setUp(self):
        pass

    def test_simple(self):
        core = self.get_core()
        core.load_configs(['config/phantom.conf'])
        core.load_plugins()
        core.plugins_configure()
        core.plugins_prepare_test()
        
        foo = PhantomConfig(core)
        foo.read_config()
        config = foo.compose_config()
        conf_str = open(config).read()
        logging.info(conf_str)
        self.assertEquals(conf_str.count("io_benchmark_t"), 1)
        
    def test_double(self):
        core = self.get_core()
        core.load_configs(['config/phantom_double.conf'])
        core.load_plugins()
        core.plugins_configure()
        core.plugins_prepare_test()
        
        foo = PhantomConfig(core)
        foo.read_config()
        config = foo.compose_config()
        info=foo.get_info()
        logging.info(info.steps)
        self.assertEquals(len(info.steps), 450)
        
        conf_str = open(config).read()
        logging.info(conf_str)
        self.assertEquals(conf_str.count("io_benchmark_t"), 3)
        self.assertEquals(conf_str.count("benchmark_io "), 2)
        self.assertEquals(conf_str.count("benchmark_io1 "), 2)
        self.assertEquals(conf_str.count("benchmark_io2 "), 2)
        

    def test_multiload_parsing(self):
        core = self.get_core()
        foo = StepperWrapper(core, PhantomPlugin.SECTION)
        foo.core.set_option('phantom', 'rps_schedule', 'const(1,1) line(1,100,60)\nstep(1,10,1,10)')
        foo.read_config()
        self.assertEquals(['const(1,1)', 'line(1,100,60)', 'step(1,10,1,10)'], foo.rps_schedule)
    
    

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = PhantomPluginTest
import os
import time
import unittest

from Tank.Plugins.Aggregator import AggregatorPlugin, SecondAggregateData
from Tank.Plugins.Phantom import PhantomPlugin, PhantomReader
from Tests.TankTests import TankTestCase
from Tank.Plugins.PhantomUtils import StepperWrapper


class PhantomPluginTestCase(TankTestCase):
    def setUp(self):
        core = self.get_core()
        core.load_configs(['config/phantom.conf'])
        core.load_plugins()
        core.plugins_configure()
        core.plugins_prepare_test()
        self.foo = PhantomPlugin(core)

    def tearDown(self):
        del self.foo
        self.foo = None
        if os.path.exists("ready_conf_phout.txt"):
            os.remove("ready_conf_phout.txt")

    def test_run(self):
        self.foo.core.set_option(PhantomPlugin.SECTION, "config", '')
        self.foo.configure()
        self.foo.prepare_test()
        reader = PhantomReader(AggregatorPlugin(self.foo.core), self.foo)
        reader.phout_file = self.foo.phantom.phout_file
        self.foo.start_test()

        while self.foo.is_test_finished() < 0:
            self.foo.log.debug("Not finished")
            reader.check_open_files()
            reader.get_next_sample(False)
            time.sleep(1)
        if self.foo.is_test_finished() != 0:
            raise RuntimeError("RC: %s" % self.foo.is_test_finished())
        self.foo.end_test(0)
        reader.get_next_sample(True)


    def test_run_ready_conf(self):
        self.foo.core.set_option(PhantomPlugin.SECTION, "config", 'data/phantom_ready.conf')
        self.foo.core.add_artifact_file("ready_conf_phout.txt")
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        while self.foo.is_test_finished() < 0:
            self.foo.log.debug("Not finished")
            time.sleep(1)
        if self.foo.is_test_finished() != 0:
            raise RuntimeError("RC: %s" % self.foo.is_test_finished())
        self.assertTrue(os.path.getsize("ready_conf_phout.txt") > 0)
        self.foo.end_test(0)


    def test_run_uri_style(self):
        self.foo.set_option("ammofile", "")
        self.foo.set_option("uris", "/")
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        while self.foo.is_test_finished() < 0:
            self.foo.log.debug("Not finished")
            time.sleep(1)
        if self.foo.is_test_finished() != 0:
            raise RuntimeError("RC: %s" % self.foo.is_test_finished())
        self.foo.end_test(0)

    def test_run_interrupt(self):
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        time.sleep(2)
        self.foo.end_test(0)

    def test_run_stepper_cache(self):
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.prepare_test()

    def test_domain_name(self):
        self.foo.core.set_option('phantom', 'address', 'yandex.ru:443')
        self.foo.configure()
        self.assertEqual("443", self.foo.get_info().port)
        self.assertEqual("yandex.ru", self.foo.get_info().address)

    def test_domain_name_and_port(self):
        self.foo.core.set_option('phantom', 'address', 'yandex.ru:80')
        self.foo.configure()

    def test_ipv4(self):
        self.foo.core.set_option('phantom', 'address', '127.0.0.1')
        self.foo.configure()

    def test_ipv6(self):
        self.foo.core.set_option('phantom', 'address', '2a02:6b8:0:c1f::161:cd')
        self.foo.configure()

    def test_ipv4_and_port(self):
        self.foo.core.set_option('phantom', 'address', '127.0.0.1:80')
        self.foo.configure()

    def test_domain_name_fail(self):
        self.foo.core.set_option('phantom', 'address', 'ya.ru')
        try:
            self.foo.configure()
            raise RuntimeError()
        except:
            pass


    def test_reader(self):
        self.foo.phantom_start_time = time.time()
        reader = PhantomReader(AggregatorPlugin(self.foo.core), self.foo)
        reader.phout_file = 'data/phout_timeout_mix.txt'
        reader.check_open_files()

        data = reader.get_next_sample(False)
        while data:
            times_sum = 0
            for timing in data.overall.times_dist:
                times_sum += timing['count']
            # FIXME: kinda strange problem here
            #self.assertEquals(sum(data.overall.net_codes.values()), times_sum)
            data = reader.get_next_sample(False)


    def test_stepper_no_steps(self):
        self.foo.core.set_option('phantom', 'rps_schedule', '')
        self.foo.core.set_option('phantom', 'instances_schedule', '')
        wrapper = StepperWrapper(self.foo.core, PhantomPlugin.SECTION)
        wrapper.ammo_file = 'data/dummy.ammo'
        wrapper.prepare_stepper()
        wrapper.prepare_stepper()

    def test_stepper_instances_sched(self):
        self.foo.core.set_option('phantom', 'instances', '1000')
        self.foo.core.set_option('phantom', 'rps_schedule', '')
        self.foo.core.set_option('phantom', 'instances_schedule', 'line(1,100,1m)')
        self.foo.core.set_option('phantom', 'use_caching', '0')
        self.foo.core.set_option('phantom', 'ammo_file', 'data/dummy.ammo')
        wrapper = StepperWrapper(self.foo.core, PhantomPlugin.SECTION)
        wrapper.read_config()
        wrapper.prepare_stepper()
        self.assertEqual(100, wrapper.instances)

    def test_stepper_instances_override(self):
        self.foo.core.set_option('phantom', 'instances', '20000')
        self.foo.core.set_option('phantom', 'rps_schedule', 'line(1,100,1m)')
        self.foo.core.set_option('phantom', 'use_caching', '0')
        self.foo.core.set_option('phantom', 'ammo_file', 'data/dummy.ammo')
        wrapper = StepperWrapper(self.foo.core, PhantomPlugin.SECTION)
        wrapper.read_config()
        wrapper.prepare_stepper()
        self.assertEqual(20000, wrapper.instances)


    def test_cached_stepper_instances_sched(self):
        
        # Making cache file
        self.foo.core.set_option('phantom', 'instances', '1000')
        self.foo.core.set_option('phantom', 'rps_schedule', '')
        self.foo.core.set_option('phantom', 'instances_schedule', 'line(1,100,1m)')
        self.foo.core.set_option('phantom', 'ammo_file', 'data/dummy.ammo')
        wrapper = StepperWrapper(self.foo.core, PhantomPlugin.SECTION)
        wrapper.read_config()
        wrapper.prepare_stepper()
        self.tearDown()
        
        self.setUp()
        self.foo.core.set_option('phantom', 'instances', '1000')
        self.foo.core.set_option('phantom', 'rps_schedule', '')
        self.foo.core.set_option('phantom', 'instances_schedule', 'line(1,100,1m)')
        self.foo.core.set_option('phantom', 'ammo_file', 'data/dummy.ammo')
        wrapper = StepperWrapper(self.foo.core, PhantomPlugin.SECTION)
        wrapper.read_config()
        wrapper.prepare_stepper()
        self.assertEqual(100, wrapper.instances)

    def test_phout_import(self):
        self.foo.core.set_option('phantom', 'phout_file', 'data/phout_timeout_mix.txt')
        self.foo.core.set_option('phantom', 'instances', '1')
        self.foo.core.set_option('phantom', 'ammo_count', '1')
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        self.assertEqual(self.foo.is_test_finished(), -1)
        sec = SecondAggregateData()
        sec.overall.RPS = 1
        self.foo.aggregate_second(sec)
        self.assertEqual(self.foo.is_test_finished(), -1)
        self.assertEqual(self.foo.is_test_finished(), 0)
        self.foo.end_test(0)
        self.foo.post_process(0)

    def test_cached_stpd_info(self):
        self.foo.core.set_option('phantom', 'stpd_file', 'data/dummy.ammo.stpd')
        wrapper = StepperWrapper(self.foo.core, PhantomPlugin.SECTION)
        wrapper.read_config()
        wrapper.prepare_stepper()
        self.assertEqual(10, wrapper.instances)
        self.assertEqual(60, wrapper.duration)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = RCAssertTest
from Tests.TankTests import TankTestCase
import unittest
from Tank.Plugins.RCAssert import RCAssertPlugin

class RCAssertTestCase(TankTestCase):
    def setUp(self):
        self.core = self.get_core()
        self.foo = RCAssertPlugin(self.core)

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_no_setting(self):
        #self.core.set_option(self.foo.SECTION, 'pass', '')
        self.foo.configure()        
        self.assertEquals(25, self.foo.post_process(25))
        self.assertEquals(0, self.foo.post_process(0))

    def test_set_one(self):
        self.core.set_option(self.foo.SECTION, 'pass', '23')
        self.foo.configure()        
        self.assertEquals(0, self.foo.post_process(23))
        self.assertNotEquals(0, self.foo.post_process(0))

    def test_set_multi(self):
        self.core.set_option(self.foo.SECTION, 'pass', '23 0')
        self.foo.configure()        
        self.assertEquals(0, self.foo.post_process(23))
        self.assertEquals(0, self.foo.post_process(0))
        self.assertNotEquals(0, self.foo.post_process(22))


if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = RCheckTest
import unittest

from Tank.Plugins.ResourceCheck import ResourceCheckPlugin

from Tests.TankTests import TankTestCase


class RCheckTestCase(TankTestCase):
    def setUp(self):
        self.core = self.get_core()
        self.foo = ResourceCheckPlugin(self.core)

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_no_setting(self):
        #self.core.set_option(self.foo.SECTION, 'pass', '')
        self.foo.configure()
        self.foo.prepare_test()


if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = ScreenBlocksTest
from Tank.Plugins.ConsoleScreen import Screen, CurrentHTTPBlock, CurrentNetBlock
from Tests.ConsoleOnlinePluginTest import FakeConsoleMarkup
from Tests.TankTests import TankTestCase

class BlocksTestCase(TankTestCase):
    def test_HTTP(self):
        screen = Screen(50, FakeConsoleMarkup())
        block = CurrentHTTPBlock(screen)
        block.render()
        print block.lines
        self.assertEquals('<w>HTTP for 0 RPS:  <rst>', block.lines[0].strip())
        self.assertEquals(1, len(block.lines))
    
        data = self.get_aggregate_data('data/preproc_single.txt')
        data.overall.planned_requests = 100
        data.overall.http_codes = {'400': 10}
        block.add_second(data)
        block.render()
        print block.lines
        self.assertEquals(2, len(block.lines))

        data.overall.http_codes = {'200': 4}
        block.add_second(data)
        block.render()
        print block.lines
        self.assertEquals(3, len(block.lines))

    def test_Net(self):
        screen = Screen(50, FakeConsoleMarkup())
        block = CurrentNetBlock(screen)
        block.render()
        print block.lines
        self.assertEquals('<w> NET for 0 RPS:  <rst>', block.lines[0].strip())
        self.assertEquals(1, len(block.lines))
    
        data = self.get_aggregate_data('data/preproc_single.txt')
        data.overall.planned_requests = 100
        data.overall.net_codes = {'0': 10}
        block.add_second(data)
        block.render()
        print block.lines
        self.assertEquals(2, len(block.lines))

        data.overall.net_codes = {'71': 4}
        block.add_second(data)
        block.render()
        print block.lines
        self.assertEquals(3, len(block.lines))

########NEW FILE########
__FILENAME__ = ShellExecPluginTest
from Tank.Plugins.ShellExec import ShellExecPlugin
from Tests.TankTests import TankTestCase
import logging
import select
import subprocess
import tempfile
import unittest

class  ShellExecPluginTestCase(TankTestCase):
    def setUp(self):
        core = self.get_core()
        core.load_configs(['config/shellexec.conf'])
        self.foo = ShellExecPlugin(core)

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_run(self):
        self.foo.configure()
        self.foo.prepare_test()
        self.foo.start_test()
        self.foo.end_test(0)
        
    def test_select(self):
        pipes = subprocess.Popen(["pwd"], stdout=subprocess.PIPE)
        r, w, x = select.select([pipes.stdout], [], [], 1)
        logging.info("selected: %s %s %s", r, w, x)
        
if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = StepperTest
import tempfile
import os

from Tank.stepper import Stepper
from Tank.stepper.format import StpdReader
from Tests.TankTests import TankTestCase


class StepperTestCase(TankTestCase):
    data = None

    def test_regular(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=["const(1,10)"],
                http_ver='1.1',
                ammo_file='data/dummy.ammo',
                instances_schedule=[],
                loop_limit=-1,
                ammo_limit=-1,
                uris=[],
                headers=[],
                autocases=0,
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(277, os.path.getsize(temp_stpd))

    def test_regular_gziped(self):
        """ Call stepper on dummy HTTP ammo file with 1 req.
            Source ammo file compressed  with gzip 1.4 lib.
        """
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=["const(1,10)"],
                http_ver='1.1',
                ammo_file='data/dummy-ammo-compressed.gz',
                instances_schedule=[],
                loop_limit=-1,
                ammo_limit=-1,
                uris=[],
                headers=[],
                autocases=0,
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(277, os.path.getsize(temp_stpd))

    def test_uri(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=["const(1,10)"],
                http_ver='1.1',
                instances_schedule=[],
                loop_limit=-1,
                ammo_limit=-1,
                uris=["/", "/test"],
                headers=["[Host: ya.ru]", "[Connection: close]"],
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(657, os.path.getsize(temp_stpd))

    def test_uri_style(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=["const(1,10)"],
                http_ver='1.1',
                ammo_file="data/uri.ammo",
                instances_schedule=[],
                loop_limit=-1,
                ammo_limit=-1,
                headers=["[Host: ya.ru]", "[Connection: close]"],
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(1567, os.path.getsize(temp_stpd))

    def test_free_inst_sched(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=[],
                http_ver='1.1',
                instances_schedule=["line(1,5,15)"],
                loop_limit=15,
                ammo_limit=-1,
                uris=["/", "/test"],
                headers=["[Host: ya.ru]", "[Connection: close]"],
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(1900, os.path.getsize(temp_stpd))


    def test_manual_inst(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                instances=20000,
                rps_schedule=["line(1,5,15)"],
                http_ver='1.1',
                uris=["/", "/test"],
                headers=["[Host: ya.ru]", "[Connection: close]"],
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(2985, os.path.getsize(temp_stpd))


    def test_free_inst(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=[],
                http_ver='1.1',
                instances_schedule=["line(1,5,15)"],
                loop_limit=2,
                ammo_limit=-1,
                uris=["/", "/test"],
                headers=["[Host: ya.ru]", "[Connection: close]"],
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(258, os.path.getsize(temp_stpd))

    def test_default(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=[],
                http_ver='1.1',
                instances_schedule=[],
                loop_limit=-1,
                ammo_limit=-1,
                uris=["/", "/test"],
                headers=["[Host: ya.ru]", "[Connection: close]"],
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(126, os.path.getsize(temp_stpd))

    def test_access_log(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=[],
                instances_schedule=[],
                loop_limit=-1,
                ammo_limit=10,
                ammo_type='access',
                headers=[],
                ammo_file="data/access1.log",
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(1459, os.path.getsize(temp_stpd))

    def test_empty_uri(self):
        from Tank.stepper.module_exceptions import AmmoFileError
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            with self.assertRaises(AmmoFileError):
                Stepper(
                    rps_schedule=[],
                    instances_schedule=[],
                    loop_limit=-1,
                    ammo_limit=10,
                    headers=[],
                    ammo_file="data/empty.uri",
                ).write(stpd_file)

    def test_chosen_cases(self):
        temp_stpd = tempfile.mkstemp()[1]
        with open(temp_stpd, 'w') as stpd_file:
            Stepper(
                rps_schedule=["const(1, 10)"],
                loop_limit=-1,
                ammo_limit=10,
                uris=["/one", "/two"],
                autocases=1,
                headers=["[Connection: close]"],
                chosen_cases="_one",
            ).write(stpd_file)
        res = open(temp_stpd, 'r').read()
        self.assertNotEquals("", res)
        self.assertEquals(res.count('_two'), 0)
        self.assertEquals(res.count('_one'), 10)

    def test_stpd_reader(self):
        sr = StpdReader("data/dummy.ammo.stpd")
        self.assertEquals(len(list(sr)), 10)

    def test_bad_stpd_reader(self):
        from Tank.stepper.module_exceptions import StpdFileError
        sr = StpdReader("data/bad.ammo.stpd")
        with self.assertRaises(StpdFileError):
            list(sr)

########NEW FILE########
__FILENAME__ = TankCoreTest
from Tests.TankTests import TankTestCase
import unittest
import tankcore

class  TankCoreTestCase(TankTestCase):
    def setUp(self):
        self.foo = self.get_core()

    def tearDown(self):
        del self.foo
        self.foo = None

    def test_tankCoreFail(self):
        paths = ['config_err/load_err.conf']
        self.foo.load_configs(paths)
        try:
            self.foo.load_plugins()
            self.fail()
        except ImportError:
            pass

    def test_tankCore(self):
        paths = ['config/load.conf']
        self.foo.load_configs(paths)
        self.assertEquals('passed', self.foo.get_option('dotted', 'test'))
        
        self.foo.load_plugins()
        self.foo.plugins_configure()
        self.foo.plugins_prepare_test()
        self.foo.plugins_start_test()
        self.foo.wait_for_finish()
        self.foo.add_artifact_file(__file__, 1)
        self.foo.plugins_end_test(0)
        self.foo.plugins_post_process(0)
        
    def test_strstplit(self):
        str1='-Jtarget.address=www.yandex.ru -Jtarget.port=26 -J "load_profile=const(1,60s) line(0,1000,10m)"'
        arr1=tankcore.splitstring(str1)
        self.assertEquals(len(arr1), 5)

if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = TankTests
import logging
import sys
import tempfile
import unittest

from Tank.Plugins.Aggregator import SecondAggregateData, \
    SecondAggregateDataTotalItem
from tankcore import TankCore


class TankTestCase(unittest.TestCase):
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s\t%(message)s")
    logger = logging.getLogger('')
    logger.setLevel(logging.DEBUG)
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.DEBUG)
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    logger.debug("Starting Unit Test")

    def get_aggregate_data(self, filename):
        return SecondAggregateData(SecondAggregateDataTotalItem())

    def callback(self, data):
        self.data = SecondAggregateData(data)

    def get_core(self):
        self.core = TankCore()
        self.core.artifacts_base_dir = tempfile.mkdtemp()
        self.core.artifacts_dir = self.core.artifacts_base_dir
        return self.core


class FakeOptions(object):
    log = ''
    verbose = True
    config = []
    option = ['testsection.testoption=testvalue']
    ignore_lock = True
    lock_fail = False
    no_rc = True
    manual_start = False
    scheduled_start = None
    lock_dir = None

########NEW FILE########
__FILENAME__ = TotalCriteriasTest
from Tank.Plugins.Aggregator import SecondAggregateData, \
    SecondAggregateDataTotalItem
from Tank.Plugins.TotalAutostop import TotalFracTimeCriteria, TotalHTTPCodesCriteria, TotalNegativeHTTPCodesCriteria, TotalNetCodesCriteria, TotalNegativeNetCodesCriteria, TotalHTTPTrendCriteria, QuantileOfSaturationCriteria
from Tests.TankTests import TankTestCase
import unittest
from Tank.Plugins.Autostop import AutostopPlugin

class TotalCriteriasTest(TankTestCase):
    def setUp(self):
        autostop = AutostopPlugin(self.get_core())
        self.frac_criteria = TotalFracTimeCriteria(autostop, "10ms, 50%, 3s")
        self.http_relcriteria = TotalHTTPCodesCriteria(autostop, "50x, 10%, 3s")
        self.http_abscriteria = TotalHTTPCodesCriteria(autostop, "50x, 30, 4s")
        self.negative_http_relcriteria = TotalNegativeHTTPCodesCriteria(autostop, "2xx, 10%, 3s")
        self.negative_http_abscriteria = TotalNegativeHTTPCodesCriteria(autostop, "20x, 30, 4s")
        self.net_relcriteria = TotalNetCodesCriteria(autostop, "110, 37%, 3s")
        self.net_abscriteria = TotalNetCodesCriteria(autostop, "71, 30, 2s")
        self.negative_net_relcriteria = TotalNegativeNetCodesCriteria(autostop, "0, 45%, 5s")
        self.negative_net_abscriteria = TotalNegativeNetCodesCriteria(autostop, "0, 100, 5s")
        self.http_trend = TotalHTTPTrendCriteria(autostop, "2xx, 10s")

        self.qsat_absrel = QuantileOfSaturationCriteria(autostop, "200ms, 70s, 20%")

    def tearDown(self):
        # frac time
        del self.frac_criteria
        self.frac_criteria = None

        # http
        del self.http_relcriteria
        self.http_relcriteria = None
        del self.http_abscriteria
        self.http_abscriteria = None

        # negative http
        del self.negative_http_relcriteria
        self.negative_http_relcriteria = None
        del self.negative_http_abscriteria
        self.negative_http_abscriteria = None

        # net
        del self.net_relcriteria
        self.net_relcriteria = None
        del self.net_abscriteria
        self.net_abscriteria = None

        # negative net
        del self.negative_net_relcriteria
        self.negative_net_relcriteria = None
        del self.negative_net_abscriteria
        self.negative_net_abscriteria = None

        # tangent of total_count
        del self.http_trend
        self.http_trend = None

        del self.qsat_absrel
        self.qsat_absrel = None

    def test_frac_null(self):
        data = list()
        for i in range(0, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"

            if i % 5 != 0:
                data.overall.times_dist = [
                    {'count': 10, 'to': 10, 'from': 0},
                    {'count': i + 1, 'to': 20, 'from': 10}]
            if self.frac_criteria.notify(data) :
                break
        if i != 13 :
                raise RuntimeError();

    def test_frac_run(self):
        data = list()
        for i in range(0, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.times_dist = [
                {'count': 10, 'to': 10, 'from': 0},
                {'count': i + 1, 'to': 20, 'from': 10}]
            if self.frac_criteria.notify(data):
                break
        if i != 11 :
            raise RuntimeError()

    def test_http_run_relative(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 100 + i * 2
            data.overall.http_codes = {'200': 100, '501': i, '503': i}
            if self.http_relcriteria.notify(data):
                break
        if i != 7 : raise RuntimeError()

    def test_http_run_absolute(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 100 + i * 2
            data.overall.http_codes = {'200': 100, '501': i, '503': i}
            if self.http_abscriteria.notify(data) :
                break
        if i != 6 : raise RuntimeError()

    def test_negative_http_run_relative(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 200 + 2 * i
            data.overall.http_codes = {'200': 100, '201': 100, '501': i, '503': i}
            if self.negative_http_relcriteria.notify(data):
                break
        if i != 13 : raise RuntimeError()

    def test_negative_http_run_absolute(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 200 + 2 * i
            data.overall.http_codes = {'200': 100, '201': 100, '302': i * 2}
            if self.negative_http_abscriteria.notify(data) :
                break
        if i != 6 : raise RuntimeError()

    def test_net_run_relative(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 100 + i ** 2
            data.overall.net_codes = {'0': 100, '110': i ** 2}
            if self.net_relcriteria.notify(data) :
                break
        if i != 9 : raise RuntimeError()

    def test_net_run_absolute(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 100 + i ** 2 + i
            data.overall.net_codes = {'0': 100, '71': i ** 2, '110' : i}
            if self.net_abscriteria.notify(data) :
                break
        if i != 5 : raise RuntimeError()

    def test_negative_net_run_relative(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 100 + i ** 2
            data.overall.net_codes = {'0': 100, '110': i ** 2}
            if self.negative_net_relcriteria.notify(data) :
                break
        if i != 12 : raise RuntimeError()

    def test_negative_net_run_absolute(self):
        data = list()
        for i in range(1, 20):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 100 + i ** 2
            data.overall.net_codes = {'0': 100, '110': i ** 2}
            if self.negative_net_abscriteria.notify(data) :
                break
        if i != 7 : raise RuntimeError()

    def test_http_trend_run(self):
        data = list()
        for i in range(1, 30):
            data = SecondAggregateData()
            data.time = "2012-09-25 18:18:18"
            data.overall.RPS = 200
            
            if i < 10 :
                data.overall.RPS += i
                data.overall.http_codes = {'200': 100 + i, '201': 100}

            elif i >= 10 and i < 20:
                if i % 2:
                    diff = -3
                else :
                    diff = 3

                data.overall.RPS += 10 + diff
                data.overall.http_codes = {'200': 110 + diff, '201': 100}

            elif i >= 20 and i < 30:
                diff = i - 20
                data.overall.RPS += 10 - diff + i
                data.overall.http_codes = {'200': 110 - diff, '201': 100, '502': i}

            if self.http_trend.notify(data) :
                break
        
        if i != 28 : 
            raise RuntimeError()

    def test_qsat_data(self):
        dist = {0: {'count': 36, 'to': 1, 'from': 0}, 40: {'count': 5, 'to': 50, 'from': 40}, 3: {'count': 1, 'to': 4, 'from': 3}, 20: {'count': 5, 'to': 30, 'from': 20}, 30: {'count': 27, 'to': 40, 'from': 30}}
        data = SecondAggregateData(SecondAggregateDataTotalItem())
        data.cumulative.times_dist = dist
        data.cumulative.total_count = 74
        self.qsat_absrel.timing = 30
        self.qsat_absrel.notify(data)
        self.assertEquals(56.75675675675676, self.qsat_absrel.data[0])


if __name__ == '__main__':
    unittest.main()


########NEW FILE########
